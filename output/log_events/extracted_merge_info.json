{
  "6a5645fb_1": {
    "exec_flow": "<entry_point>RouterRpcServer:listXAttrs</entry_point> <flow>ENTRY→CALL:listXAttrs→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:listXAttrs→CALL:rpcServer.checkOperation→CALL:getLocationsForPath→CALL:invokeSequential→RETURN→EXIT</flow>",
    "log": "<log>LOG.error(\"Cannot get mount point\", e) - RouterRpcServer:isPathReadOnly</log> <log>[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)]</log> <log>[INFO] Initial directory matched with component</log> <log>[DEBUG] Resolved child node under snapshot</log>"
  },
  "6a5645fb_2": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "6a5645fb_3": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "6a5645fb_4": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "6a5645fb_5": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "65f5e525_1": {
    "exec_flow": "<step>Check if user is superuser</step> <step>Acquire read lock</step> <step>Check user permission against cache pool owner</step> <step>Check group permission</step> <step>Check other permissions</step> <step>Permission denied - AccessControlException thrown</step> <step>Release read lock</step>",
    "log": "<log> <template>Permission denied while accessing pool {PoolName}: user {User} does not have {Access} permissions.</template> <parameters> <parameter name=\"PoolName\" value=\"{pool.getPoolName()}\" /> <parameter name=\"User\" value=\"{getUser()}\" /> <parameter name=\"Access\" value=\"{access.toString()}\" /> </parameters> </log>"
  },
  "8a0263a7_1": {
    "exec_flow": "ENTRY→CALL:Resources.clone→IF_TRUE:maxResource.getMemorySize()==0→CALL:maxResource.setMemorySize→IF_TRUE:maxResource.getVirtualCores()==0→CALL:maxResource.setVirtualCores→CALL:scheduler.getRootQueueMetrics().fillInValuesFromAvailableResources→CALL:multiplyAndRoundUp→LOG:LOG.WARN:String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)→RETURN→EXIT",
    "log": "[WARN] String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)"
  },
  "8a0263a7_2": {
    "exec_flow": "ENTRY→CALL:Resources.clone→CALL:getMaxShare→IF_FALSE:maxResource.getMemorySize()==0→IF_FALSE:maxResource.getVirtualCores()==0→CALL:scheduler.getRootQueueMetrics().fillInValuesFromAvailableResources→CALL:multiplyAndRoundUp→TRY→LOG:LOG.WARN:Resource is missing:+ye.getMessage()→CATCH→RETURN→EXIT",
    "log": "[WARN] Resource is missing:+ye.getMessage()"
  },
  "be98ef7d_1": {
    "exec_flow": "ENTRY→CALL:makeQualified→CALL:getAuthParameters→CALL:getNamenodeURL→LOG:TRACE url={}→RETURN→EXIT ENTRY→IF_TRUE:!op.getRequireAuth→CALL:getDelegationToken→IF_TRUE:token!=null→CALL:authParams.add→CALL:encodeToUrlString→CALL:toArray→RETURN→EXIT",
    "log": "[TRACE] url={} [DEBUG] Delegation token encoded [INFO] Returning authentication parameters"
  },
  "be98ef7d_2": {
    "exec_flow": "ENTRY→CALL:makeQualified→CALL:getAuthParameters→CALL:getNamenodeURL→LOG:TRACE url={}→RETURN→EXIT ENTRY→IF_FALSE:!op.getRequireAuth→IF_TRUE:token!=null→CALL:authParams.add→CALL:toArray→RETURN→EXIT",
    "log": "[TRACE] url={} [DEBUG] Token not required, returning default user parameters"
  },
  "be98ef7d_3": {
    "exec_flow": "ENTRY→CALL:makeQualified→CALL:getAuthParameters→CALL:getNamenodeURL→LOG:TRACE url={}→RETURN→EXIT ENTRY→IF_FALSE:!op.getRequireAuth→IF_FALSE:token!=null→IF_TRUE:realUgi!=null→CALL:authParams.add→IF_TRUE:isInsecureCluster→CALL:authParams.add→CALL:toArray→RETURN→EXIT",
    "log": "[TRACE] url={} [DEBUG] Auth parameters added for proxy user [INFO] Insecure cluster detected"
  },
  "9977a94f_1": {
    "exec_flow": "ENTRY -> IF_TRUE: counter == null -> LOG: LOG.DEBUG: Ignoring counter increment for unknown counter {}, key -> RETURN -> EXIT ENTRY -> IF_FALSE: counter == null -> IF_TRUE: value < 0 -> LOG: LOG.DEBUG: Ignoring negative increment value {} for counter {}, value, key -> RETURN -> EXIT ENTRY -> IF_FALSE: counter == null -> IF_FALSE: value < 0 -> LOG: LOG.TRACE: Incrementing counter {} by {} with final value {}, key, value, l -> RETURN -> EXIT",
    "log": "[DEBUG] Ignoring counter increment for unknown counter {} [DEBUG] Ignoring negative increment value {} for counter {} [TRACE] Incrementing counter {} by {} with final value {}"
  },
  "f8732c85_1": {
    "exec_flow": "ENTRY→TRY→LOG: [DEBUG] Probing NN at service address: {}→CALL: org.apache.hadoop.hdfs.NameNodeProxies:createProxy→IF_FALSE: nn != null→CALL: org.apache.hadoop.hdfs.tools.NNHAServiceTarget:getProxy→CALL: org.apache.hadoop.ha.HAServiceProtocol:getServiceStatus→RETURN→EXIT",
    "log": "[DEBUG] Probing NN at service address: {} [DEBUG] Proxying operation: {} [INFO] buildTokenServiceForLogicalUri [DEBUG] getNNAddressCheckLogical"
  },
  "f8732c85_2": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→IF_TRUE:!authorizer.isAdmin(user)→LOG: [WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'→CALL:RMAuditLogger.logFailure→THROW: new AccessControlException(\"User \" + user.getShortUserName() + \" doesn’t have permission\" + \" to call '${method}'\")→EXIT",
    "log": "[WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'"
  },
  "f8732c85_3": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→IF_FALSE:!authorizer.isAdmin(user)→IF_TRUE:LOG.isTraceEnabled()→LOG: [TRACE] ${method} invoked by user ${user.getShortUserName()}→RETURN→EXIT",
    "log": "[TRACE] ${method} invoked by user ${user.getShortUserName()}"
  },
  "f8732c85_4": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→CATCH→LOG: [WARN] Couldn't get current user→CALL:RMAuditLogger.logFailure→THROW: IOException→EXIT",
    "log": "[WARN] Couldn't get current user"
  },
  "91c348be_1": {
    "exec_flow": "ENTRY → IF_TRUE: numSegmentsCleanedUp == numSegmentsContained → LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}] → TRY → CALL: getFileStatus → LOG: [DEBUG] Path: [{}] is a file. COS key: [{}] → LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}] → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE: meta != null → IF_TRUE: meta.isFile() → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}] [DEBUG] Path: [{}] is a file. COS key: [{}] [DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "91c348be_2": {
    "exec_flow": "ENTRY → IF_TRUE: numSegmentsCleanedUp == numSegmentsContained → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE: meta != null → IF_FALSE: meta.isFile() → LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "[DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "91c348be_3": {
    "exec_flow": "ENTRY → IF_TRUE: numSegmentsCleanedUp == numSegmentsContained → LOG: [DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive → CALL: statIncrement → CALL: makeQualified → IF_FALSE: f.isRoot() → TRY → CALL: abfsStore.delete → EXCEPTION: delete → CATCH: AzureBlobFileSystemException ex → CALL: checkException → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive"
  },
  "91c348be_4": {
    "exec_flow": "ENTRY → IF_TRUE: numSegmentsCleanedUp == numSegmentsContained → [VIRTUAL_CALL] → CosNativeFileSystemStore:delete → LOG: [DEBUG] Delete object key: [{}] from bucket: {}. → TRY → CALL: callCOSClientWithRetry → EXCEPTION: callCOSClientWithRetry → CATCH: Exception e → LOG: [ERROR] Delete key: [{}] occurs an exception: [{}]. → CALL: handleException → RETURN → EXIT",
    "log": "[DEBUG] Delete object key: [{}] from bucket: {}. [ERROR] Delete key: [{}] occurs an exception: [{}]."
  },
  "08be0a7b_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.FSImage:<init>→FOR_INIT→FOR_COND:it.hasNext()→CALL:org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:canRollBack→LOG:org.slf4j.Logger:info→CALL:org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doRollBack→FOR_EXIT→IF_TRUE:fsns.isHaEnabled()→CALL:editLog.initJournalsForWrite→CALL:org.apache.hadoop.hdfs.server.namenode.FSEditLog:canRollBackSharedLog→LOG:org.slf4j.Logger:info→CALL:editLog.doRollback→CALL:org.apache.hadoop.hdfs.server.namenode.FSImage:close→EXIT",
    "log": "[INFO] Can perform rollback for sd [INFO] Rolling back storage directory ... [INFO] Can perform rollback for shared edit log. [INFO] Rollback of [directory] is complete."
  },
  "08be0a7b_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:doRollback()→TRY→CALL:waitFor→WHILE: true→WHILE_COND: true→CALL: restartQuorumStopWatch→CALL: checkAssertionErrors→IF_FALSE: minResponses > 0 && countResponses() >= minResponses→IF_FALSE: minSuccesses > 0 && countSuccesses() >= minSuccesses→IF_FALSE: maxExceptions >= 0 && countExceptions() > maxExceptions→IF_TRUE: now > nextLogTime→CALL: QuorumJournalManager.LOG.info→CALL: QuorumJournalManager.LOG.warn→IF_TRUE: rem <= 0→CALL: getQuorumTimeoutIncreaseMillis→IF_FALSE: timeoutIncrease > 0→THROW: new TimeoutException()→EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil</class> <method>doRollBack</method> <level>INFO</level> <message>Rollback of {sd.getRoot()} is complete.</message> </log_entry> <log_entry> [INFO] Waited X ms (timeout=Y ms) for a response for Z... </log_entry> <log_entry> [WARN] Waited A ms (timeout=B ms) for a response for C... </log_entry>"
  },
  "9b1cced7_1": {
    "exec_flow": "ENTRY→IF_TRUE: null == timeout→CALL: readDecommissioningTimeout→IF_FALSE: null == yarnConf→CALL: YarnConfiguration:get→CALL: YarnConfiguration:get→LOG: LOG.INFO: refreshNodes excludesFile + excludesFile→IF_TRUE: graceful→CALL: HostsFileReader:lazyRefresh→CALL: refreshInternal→LOG: LOG.INFO: Refreshing hosts (include/exclude) list→IF_FALSE: inFileInputStream!=null→IF_TRUE: exFileInputStream!=null→NEW: HashMap→CALL: readFileToMapWithFileInputStream→CALL: unmodifiableMap→CALL: set→CALL: refreshInternal→CALL: printConfiguredHosts→LOG: LOG.INFO: hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + }→CALL: handleExcludeNodeList→LOG: LOG.INFO: Recommission node with state DECOMMISSIONING→EXIT",
    "log": "[INFO] refreshNodes excludesFile + excludesFile [INFO] Refreshing hosts (include/exclude) list [INFO] hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + } [INFO] Recommission node with state DECOMMISSIONING"
  },
  "9b1cced7_2": {
    "exec_flow": "ENTRY→IF_TRUE: null == timeout→CALL: readDecommissioningTimeout→IF_FALSE: null == yarnConf→CALL: YarnConfiguration:get→CALL: YarnConfiguration:get→LOG: LOG.INFO: refreshNodes excludesFile + excludesFile→IF_TRUE: graceful→CALL: HostsFileReader:lazyRefresh→CALL: refreshInternal→LOG: LOG.INFO: Refreshing hosts (include/exclude) list→IF_FALSE: inFileInputStream!=null→IF_TRUE: exFileInputStream!=null→NEW: HashMap→CALL: readFileToMapWithFileInputStream→CALL: unmodifiableMap→CALL: set→CALL: refreshInternal→CALL: printConfiguredHosts→LOG: LOG.INFO: hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + }→CALL: handleExcludeNodeList→LOG: LOG.INFO: Gracefully decommission node with state not DECOMMISSIONED/DECOMMISSIONING→EXIT",
    "log": "[INFO] refreshNodes excludesFile + excludesFile [INFO] Refreshing hosts (include/exclude) list [INFO] hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + } [INFO] Gracefully decommission node with state not DECOMMISSIONED/DECOMMISSIONING"
  },
  "9b1cced7_3": {
    "exec_flow": "ENTRY→IF_TRUE: null == timeout→CALL: readDecommissioningTimeout→IF_FALSE: null == yarnConf→CALL: YarnConfiguration:get→CALL: YarnConfiguration:get→LOG: LOG.INFO: refreshNodes excludesFile + excludesFile→IF_TRUE: graceful→CALL: HostsFileReader:lazyRefresh→CALL: refreshInternal→LOG: LOG.INFO: Refreshing hosts (include/exclude) list→IF_FALSE: inFileInputStream!=null→IF_TRUE: exFileInputStream!=null→NEW: HashMap→CALL: readFileToMapWithFileInputStream→CALL: unmodifiableMap→CALL: set→CALL: refreshInternal→CALL: printConfiguredHosts→LOG: LOG.INFO: hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + }→CALL: handleExcludeNodeList→LOG: LOG.INFO: Forcefully decommission node with state not DECOMMISSIONED→EXIT",
    "log": "[INFO] refreshNodes excludesFile + excludesFile [INFO] Refreshing hosts (include/exclude) list [INFO] hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + } [INFO] Forcefully decommission node with state not DECOMMISSIONED"
  },
  "9b1cced7_4": {
    "exec_flow": "ENTRY→FOREACH:confFileNames→CALL:org.apache.hadoop.yarn.conf.ConfigurationProvider:getConfigurationInputStream→CALL:org.apache.hadoop.conf.Configuration:addResource→FOREACH_EXIT→RETURN→EXIT",
    "log": "[INFO] name not found [INFO] found resource name at url [INFO] {filePath} not found"
  },
  "49de38f4_1": {
    "exec_flow": "<entry>PARENT_NODE_ENTRY</entry> <virtual_call>VIRTUAL_CALL_TO_CHILD</virtual_call> <child_path>Child_Path_Logic</child_path>",
    "log": "<log>org.apache.hadoop.mapreduce.Counters:INFO:Starting initialization</log> <log>org.apache.hadoop.mapreduce.counters.AbstractCounters:DEBUG:Initialization details</log> <log>[DEBUG] Connecting to HistoryServer at: serviceAddr</log> <log>[DEBUG] Connected to HistoryServer at: serviceAddr</log> <log>[WARN] Could not connect to History server.</log> <log>[DEBUG] Failed to contact AM/History for job retrying..</log> <log>[WARN] ClientServiceDelegate invoke call interrupted</log>"
  },
  "626ab9ea_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <entry> <node>org.apache.hadoop.hdfs.ClientContext:<init></node> <transition>ENTRY</transition> <node>CALL:getBoolean</node> </entry> <entry> <node>org.apache.hadoop.hdfs.PeerCache:<init></node> <transition>VIRTUAL_CALL</transition> <node>org.slf4j.Logger:warn</node> </entry> <entry> <node>IF_FALSE:!topologyResolutionEnabled</node> <transition>RETURN</transition> <node>EXIT</node> </entry>",
    "log": "<log>[org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:<init>][WARN] Some specific message or context</log> <log>[org.slf4j.Logger:warn] Warning message details</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[INFO] message</log>"
  },
  "510e0cba_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:createNewAttempt → CALL:org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:recover → IF:recoveredFinalState == null → LOG: [INFO] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=NONE → ELSE IF:isDebugEnabled → LOG: [DEBUG] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=[recoveredFinalState] → CALL:info → ELSE IF → CALL:isDebugEnabled → CALL:debug → CALL:getAppAttemptTokens → CALL:setMasterContainer → CALL:recoverAppAttemptCredentials → IF_TRUE:appAttemptTokens == null || state == RMAppAttemptState.FAILED || state == RMAppAttemptState.FINISHED || state == RMAppAttemptState.KILLED → VIRTUAL_CALL → ENTRY → IF_TRUE:UserGroupInformation.isSecurityEnabled → VIRTUAL_CALL → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL:<init> → RETURN → EXIT → LOG: Handling deprecation for all properties in config → CALL:getProps → CALL:addAll → FOREACH:keys → LOG: Handling deprecation for (String)item → EXIT → CALL:getProps → CALL:getResourceSecondsMap → CALL:getPreemptedResourceSecondsMap → LOG: [WARN] Unexpected SecurityException in Configuration → EXIT → CALL:writeLock.lock → TRY → LOG: [DEBUG] Processing {resourcePath} of type {event.getType()} → TRY → CALL:doTransition → THROW:InvalidStateTransitionException → CATCH → LOG: [ERROR] Can't handle this event at current state → CALL:writeLock.unlock → EXIT",
    "log": "[INFO] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=NONE [DEBUG] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=[recoveredFinalState] [DEBUG] Creating new Groups object [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration [DEBUG] Processing {resourcePath} of type {event.getType()} [ERROR] Can't handle this event at current state"
  },
  "9861ef55_1": {
    "exec_flow": "ENTRY→IF_TRUE: create→CALL: handle→IF_TRUE: action != null → SWITCH: action → CASE: [WARNING] → IF_TRUE: message != null → CALL:conversionOptions.handleWarning → ENTRY → IF_FALSE:dryRun → CALL:log.WARN:msg → EXIT → BREAK → EXIT",
    "log": "[LOG] conversionOptions.handleWarning [WARN] msg"
  },
  "9861ef55_2": {
    "exec_flow": "ENTRY→IF_FALSE: create→CALL: handle→IF_TRUE: action != null → SWITCH: action → CASE: [WARNING] → IF_TRUE: message != null → CALL:conversionOptions.handleWarning → ENTRY → IF_FALSE:dryRun → CALL:log.WARN:msg → EXIT → BREAK → EXIT",
    "log": "[LOG] conversionOptions.handleWarning [WARN] msg"
  },
  "c36070c2_1": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}→EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "c36070c2_2": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: org.apache.hadoop.service.AbstractService:enterState →CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→CALL: noteFailure→THROW: ServiceStateException.convert(e)→EXIT",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] noteFailure [INFO] Service {} failed in state {} (exception details)"
  },
  "c66570ea_1": {
    "exec_flow": "ENTRY→IF_TRUE:pendingInstances.size() == 0→LOG:LOG.INFO(\"[COMPONENT {}]: No pending component instance left, release surplus container {}\")→CALL:releaseContainer→RETURN→EXIT",
    "log": "[INFO] [COMPONENT {}]: No pending component instance left, release surplus container {}"
  },
  "c66570ea_2": {
    "exec_flow": "ENTRY→IF_FALSE:pendingInstances.size() == 0→LOG:LOG.INFO(\"[COMPONENT {}]: {} allocated, num pending component instances reduced to {}\")→CALL:instance.setContainer→CALL:scheduler.addLiveCompInstance→LOG:LOG.INFO(\"[COMPONENT {}]: Assigned {} to component instance {} and launch on host {}\")→CALL:scheduler.getContainerLaunchService.launchCompInstance→CALL:instance.updateResolvedLaunchParams→EXIT",
    "log": "[INFO] [COMPONENT {}]: {} allocated, num pending component instances reduced to {} [INFO] [COMPONENT {}]: Assigned {} to component instance {} and launch on host {}"
  },
  "c66570ea_3": {
    "exec_flow": "ENTRY→IF_FALSE:pendingInstances.size() == 0→LOG:LOG.INFO(\"[COMPONENT {}]: {} allocated, num pending component instances reduced to {}\")→CALL:instance.setContainer→CALL:scheduler.addLiveCompInstance→LOG:LOG.INFO(\"[COMPONENT {}]: Assigned {} to component instance {} and launch on host {}\")→CALL:scheduler.getContainerLaunchService.launchCompInstance→EXCEPTION→LOG:LOG.ERROR(\"{}: Failed to launch container.\", instance.getCompInstanceId())→CALL:handle→EXIT",
    "log": "[INFO] [COMPONENT {}]: {} allocated, num pending component instances reduced to {} [INFO] [COMPONENT {}]: Assigned {} to component instance {} and launch on host {} [ERROR] {}: Failed to launch container., instance.getCompInstanceId()"
  },
  "67552540_1": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "<log> <template>Filesystem {} created while awaiting semaphore</template> <level>debug</level> </log>"
  },
  "67552540_2": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_FALSE:fs != null → CALL:createFileSystem → CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC:this → IF_TRUE:map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL:ShutdownHookManager.get().addShutdownHook → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "<log> <template>Filesystem {} created while awaiting semaphore</template> <level>debug</level> </log> <log> <template>Duplicate FS created for {}; discarding {}</template> <level>debug</level> </log>"
  },
  "67552540_3": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log> <template>Handling deprecation for all properties in config...</template> <level>debug</level> </log> <log> <template>Handling deprecation for (String)item</template> <level>debug</level> </log> <log> <template>Duplicate FS created for {}; discarding {}</template> <level>debug</level> </log>"
  },
  "67552540_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT",
    "log": "<log> <template>Bypassing cache to create filesystem {uri}</template> <level>debug</level> </log>"
  },
  "6cfbe8cb_1": {
    "exec_flow": "ENTRY → CALL:rewind → CALL:reset → CALL:readTokenStorageStream → CALL:rewind → IF_TRUE:LOG.isDebugEnabled → FOREACH:credentials.getAllTokens → CALL:debug → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Token read from token storage: {token}"
  },
  "6cfbe8cb_2": {
    "exec_flow": "ENTRY→SYNC: Token.class→IF_TRUE: tokenKindMap == null→CALL: newHashMap→WHILE: tokenIdentifiers.hasNext()→WHILE_COND: tokenIdentifiers.hasNext()→CALL: ServiceLoader.load→CALL: tokenIdentifiers.next→LOG: LOG.DEBUG: Added {}:{} into tokenKindMap, id.getKind(), id.getClass()→WHILE_EXIT→CALL: get→IF_TRUE: cls == null→LOG: LOG.DEBUG: Cannot find class for token kind {}, kind→RETURN→EXIT",
    "log": "[DEBUG] Added {}:{} into tokenKindMap [DEBUG] Cannot find class for token kind {}"
  },
  "59fbfa09_1": {
    "exec_flow": "ENTRY→TRY→CALL:zkClient.getData.forPath→IF_TRUE:(data == null) || (data.length == 0)→RETURN→EXIT",
    "log": "<!-- No log sequence in this path -->"
  },
  "59fbfa09_2": {
    "exec_flow": "ENTRY→TRY→CALL:zkClient.getData.forPath→IF_FALSE:(data == null) || (data.length == 0)→CALL:createIdentifier().readFields→EXCEPTION:readFields→CATCH:KeeperException.NoNodeException e→IF_TRUE:!quiet→LOG:ERROR:No node in path [ + nodePath + ]→RETURN→EXIT",
    "log": "ERROR:No node in path [ + nodePath + ]"
  },
  "59fbfa09_3": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK → CATCH → CALL:org.slf4j.Logger:error → EXIT",
    "log": "[ERROR] Error retrieving tokenInfo [sequenceNumber] from ZK"
  },
  "2d6f7866_1": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <virtual_call>Child paths</virtual_call> <entry>IF_FALSE: service == null</entry> <log>LOG.DEBUG: Looking for a token with service {}, service</log> <foreach>tokens</foreach> <log>LOG.DEBUG: Token kind is {} and the token's service name is {}, token.getKind(), token.getService()</log> <if_true>RMDelegationTokenIdentifier.KIND_NAME.equals(token.getKind()) && checkService(service, token)</if_true> <return></return> <exit></exit>",
    "log": "<log>[DEBUG] Looking for a token with service {}</log> <log>[DEBUG] Token kind is {} and the token's service name is {}</log>"
  },
  "2b4fd303_1": {
    "exec_flow": "ENTRY → CALL:beginTransaction → TRY → CALL:editLogStream.writeRaw → CALL:endTransaction → EXIT → CALL:logSync() → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "484e753a_1": {
    "exec_flow": "ENTRY → IF_TRUE:secretProvider==null → CALL:getAttribute → CALL:getServletContext → CALL:constructSecretProvider → IF_FALSE: curatorClientObj != null && curatorClientObj instanceof CuratorFramework → CALL: createCuratorClient → CALL: servletContext.setAttribute → CALL: parseBoolean → CALL: getProperty → CALL: getProperty → CALL: getProperty → IF_FALSE: path == null → TRY → CALL: client.create().creatingParentsIfNeeded().forPath → CALL: generateZKData → CALL: generateRandomSecret → CALL: generateRandomSecret → LOG: org.slf4j.Logger:info(\"Creating secret znode\") → EXIT",
    "log": "[INFO] Creating secret znode"
  },
  "49eaf87e_1": {
    "exec_flow": "ENTRY -> IF_FALSE: !isDeprecated(name) -> CALL: handleDeprecation -> LOG: Handling deprecation for all properties in config... -> CALL: getProps -> CALL: addAll -> FOREACH: keys -> LOG: Handling deprecation for (String)item -> CALL: handleDeprecation -> FOREACH_EXIT -> EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "279b6086_1": {
    "exec_flow": "<step> <description>Start method append execution</description> </step> <step> <description>Increment write operation statistics</description> </step> <step> <description>Increment storage operation counter for APPEND</description> </step> <step> <description>Initialize FsPathOutputStreamRunner with necessary parameters</description> </step> <step> <description>Run operation via FsPathOutputStreamRunner</description> </step> <step> <description>ENTRY → CALL: append → CALL: org.apache.hadoop.fs.FileSystem:append → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:append → TRY</description> </step> <step> <description>LOG: DEBUG: AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</description> </step> <step> <description>LOG: DEBUG: openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</description> </step> <step> <description>CALL: startTracking → CALL: client.getPathStatus → CALL: perfInfo.registerResult</description> </step> <step> <description>IF_FALSE: parseIsDirectory(resourceType) → CALL: perfInfo.registerSuccess</description> </step> <step> <description>IF_FALSE: isAppendBlobKey → NEW: AbfsOutputStream</description> </step> <step> <description>CALL: populateAbfsOutputStreamContext → CALL: perfInfo.close → RETURN → RETURN → EXIT</description> </step>",
    "log": "<log> <level>INFO</level> <message>Statistics incremented for write operations</message> </log> <log> <level>INFO</level> <message>Storage operation counter incremented for APPEND</message> </log> <log> <level>DEBUG</level> <message>AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</message> </log> <log> <level>DEBUG</level> <message>openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</message> </log>"
  },
  "279b6086_2": {
    "exec_flow": "ENTRY → IF_TRUE: permission==null → CALL: getDefault → CALL: applyUMask → CALL: getUMask → CALL: getConf → CALL: getConf → CALL: getUMask → CALL: getConf → CALL: getConf → RETURN → EXIT → CALL: getUMask(org.apache.hadoop.conf.Configuration) → CALL: incrementWriteOps → CALL: toShort → CALL: toOctalString → CALL: toRelativeFilePath → CALL: createFile → NEW: AdlFsOutputStream → NEW: FSDataOutputStream → RETURN → EXIT → CALL: append → CALL: org.apache.hadoop.fs.FileSystem:append → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:append → TRY → CALL: startTracking → CALL: client.getPathStatus → CALL: perfInfo.registerResult → IF_FALSE: parseIsDirectory(resourceType) → CALL: perfInfo.registerSuccess → IF_FALSE: isAppendBlobKey → NEW: AbfsOutputStream → CALL: populateAbfsOutputStreamContext → CALL: perfInfo.close → RETURN → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</message> </log> <log> <level>DEBUG</level> <message>openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</message> </log>"
  },
  "279b6086_3": {
    "exec_flow": "ENTRY → CALL: getUriPath → CALL: resolve → IF_FALSE: key.length() == 0 → IF_TRUE: meta == null && !key.endsWith(\"/\") → CALL: getObjectMetadata → EXCEPTION: getObjectMetadata → CATCH: OSSException osse → CALL: LOG.debug → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Exception thrown when get object meta: + key + , exception: + osse</message> </log>"
  },
  "279b6086_4": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE: containsColon(f) → CALL: performAuthCheck → CALL: createInternal → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Creating file: {}</message> </log>"
  },
  "96ac222a_1": {
    "exec_flow": "ENTRY → IF_TRUE: !request.getContainerIdsList().isEmpty() → FOREACH: request.getContainerIdsList() → CALL: org.apache.hadoop.security.UserGroupInformation:getCurrentUser → CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) → CALL: build → CALL: newBuilder → RETURN → EXIT",
    "log": "[INFO] Upgrade container {containerId}"
  },
  "96ac222a_2": {
    "exec_flow": "Entry → [VIRTUAL_CALL] → Entry → CALL: ensureInitialized → IF_TRUE: !isInitialized() → SYNC: UserGroupInformation.class → IF_TRUE: !isInitialized() → CALL: initialize → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → ENTRY → CALL:getLoginUser → IF_TRUE: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null,newLoginUser) → CALL: createLoginUser → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log>"
  },
  "96ac222a_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {tokenFile.getCanonicalPath()}</log> <log>[DEBUG] Loaded {cred.numberOfTokens()} tokens from {tokenFile.getCanonicalPath()}</log> <log>[INFO] Token file {tokenFile.getCanonicalPath()} does not exist</log> <log>[ERROR] Cannot add token {tokenBase64}: {ioe.getMessage()}</log> <log>[DEBUG] Loaded {numTokenBase64} base64 tokens</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {loginUser}</log>"
  },
  "96ac222a_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {tokenFile.getCanonicalPath()}</log> <log>[DEBUG] Loaded {cred.numberOfTokens()} tokens from {tokenFile.getCanonicalPath()}</log> <log>[INFO] Token file {tokenFile.getCanonicalPath()} does not exist</log> <log>[ERROR] Cannot add token {tokenBase64}: {ioe.getMessage()}</log> <log>[DEBUG] Loaded {numTokenBase64} base64 tokens</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {loginUser}</log>"
  },
  "96ac222a_5": {
    "exec_flow": "ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: LOG.DEBUG: Creating new Groups object → NEW: Groups → CALL:<init> → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating new Groups object</log>"
  },
  "f6fa1e03_1": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Handling deprecation for all properties in config...→IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>WARN: Unexpected SecurityException in Configuration</log>"
  },
  "f6fa1e03_2": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Creating file: {}, f.toString()→IF_FALSE: containsColon(f)→CALL: performAuthCheck→CALL: createInternal→RETURN→EXIT→ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→LOG:debug→CATCH:FileNotFoundException→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log> <log>[DEBUG] Overwriting file {}</log>"
  },
  "f6fa1e03_3": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Creating file: {}, f.toString()→IF_FALSE: containsColon(f)→CALL: performAuthCheck→CALL: createInternal→RETURN→EXIT→ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→LOG:debug→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log> <log>[DEBUG] Overwriting file {}</log>"
  },
  "f6fa1e03_4": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Creating file: {}, f.toString()→IF_FALSE: containsColon(f)→CALL: performAuthCheck→CALL: createInternal→RETURN→EXIT→ENTRY→LOG: [DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}→CALL:statIncrement→CALL:trailingPeriodCheck→CALL:makeQualified→TRY→CALL:createFile→CALL:statIncrement→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log> <log>[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</log>"
  },
  "f6fa1e03_5": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "678d2b28_1": {
    "exec_flow": "ENTRY → LOG:DEBUG:Moving {} to {}, src, dst → CALL:initiateRename → CALL:RenameOperation<init> → CALL:execute → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Moving {} to {}, src, dst</message> </log> <log> <level>DEBUG</level> <message>Rename source path: [{}] to dest path: [{}]</message> </log> <log> <level>DEBUG</level> <message>Path: [{}] is a file. COS key: [{}]</message> </log> <log> <level>DEBUG</level> <message>Ready to delete path: [{}]. recursive: [{}].</message> </log> <log> <level>ERROR</level> <message>Delete key: [{}] occurs an exception: [{}]</message> </log>"
  },
  "678d2b28_2": {
    "exec_flow": "ENTRY → IF_FALSE:this.azureAuthorization → CALL:deleteWithoutAuth → ENTRY → LOG:DEBUG:Deleting file: {}, f → TRY → CALL:retrieveMetadata → IF_FALSE:null == metaFile → IF_TRUE:!metaFile.isDirectory() → IF_TRUE:parentPath.getParent() != null → TRY → CALL:retrieveMetadata → IF_FALSE:parentMetadata == null → IF_FALSE:!parentMetadata.isDirectory() → IF_FALSE:parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit → IF_FALSE:!skipParentFolderLastModifiedTimeUpdate → TRY → IF_TRUE:store.delete(key) → CALL:fileDeleted → LOG:DEBUG:Delete Successful for: {}, f → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Deleting file: {}</message> </log> <log> <level>DEBUG</level> <message>Delete Successful for: {}</message> </log>"
  },
  "678d2b28_3": {
    "exec_flow": "ENTRY → IF_FALSE:this.azureAuthorization → CALL:deleteWithoutAuth → ENTRY → LOG:DEBUG:Deleting file: {}, f → TRY → CALL:retrieveMetadata → IF_FALSE:null == metaFile → IF_FALSE:!metaFile.isDirectory() → LOG:DEBUG:Directory Delete encountered: {}, f → TRY → CALL:list → LOG:DEBUG:Time taken to list {} blobs for delete operation: {} ms, contents.length, (end - start) → IF_TRUE:contents.length > 0 → IF_TRUE:!recursive → THROW:IOException → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Deleting file: {}</message> </log> <log> <level>DEBUG</level> <message>Directory Delete encountered: {}</message> </log> <log> <level>DEBUG</level> <message>Time taken to list {} blobs for delete operation: {} ms</message> </log>"
  },
  "678d2b28_4": {
    "exec_flow": "ENTRY → CALL:setLogEnabled → CALL:getObjectMetadata → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT → [VIRTUAL_CALL] → ENTRY → CALL:qualify → LOG:DEBUG: Getting path status for {f}; needEmptyDirectory={} → CALL:makeQualified → LOG:DEBUG: Stripping trailing '/' from {q} → CALL:s3GetFileStatus → RETURN → EXIT → TRY → CALL:openConnection → CALL:setRequestMethod → IF_TRUE:method.equals(HTTP_POST) || method.equals(HTTP_PUT) → CALL:setDoOutput → RETURN → EXIT → [VIRTUAL_CALL] → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:access$6 → [VIRTUAL_CALL] → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus → LOG:DEBUG AzureBlobFileSystem.getFileStatus path: {} → CALL:statIncrement → CALL:makeQualified → TRY → CALL:AzureBlobFileSystemStore.getFileStatus → ENTRY → CALL:getIsNamespaceEnabled → CALL:debug → IF → CALL:getPathStatus → CALL:registerCallee → CALL:getResult → CALL:registerResult → CALL:parseLastModifiedTime → RETURN → EXIT → RETURN → EXIT → VIRTUAL_CALL:run() → GetOpParam.Op.GETFILESTATUS → FsPathResponseRunner<HdfsFileStatus> → decodeResponse → return status",
    "log": "<log_entry> <class_method>org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Exception thrown when get object meta: + key + , exception: + osse</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Getting path status for {f}; needEmptyDirectory={}</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:makeQualified</class_method> <level>DEBUG</level> <template>Stripping trailing '/' from {q}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Getting the file status for {f}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Retrieving metadata for {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Path {f} is a folder.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Found the path: {f} as a file.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a file. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a dir. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>List COS key: {key} to check the existence of the path.</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus</class_method> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry>"
  },
  "8930dee5_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:handleDeprecation</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:getProps</step> <step>FOREACH:names</step> <step>CALL:substituteVars</step> <step>CALL:getRaw</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>LOG.warn(\"Unexpected SecurityException in Configuration\", se)</log>"
  },
  "989e61ea_1": {
    "exec_flow": "<sequence> ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → CALL: getDatanodeDescriptor → CALL: get → IF_TRUE: storageTypes == null → CALL: getRequiredStorageTypes → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_FALSE: requiredStorageTypes.size() == 0 → CALL: chooseTargetInOrder → RETURN → EXIT </sequence>",
    "log": "<log>[TRACE] storageTypes={}, storageTypes</log>"
  },
  "989e61ea_2": {
    "exec_flow": "<sequence> ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → CALL: getDatanodeDescriptor → CALL: get → IF_FALSE: storageTypes == null → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_TRUE: requiredStorageTypes.size() == 0 → THROW: new NotEnoughReplicasException → LOG: LOG.TRACE: message, e → LOG: LOG.WARN: message + e.getMessage() → EXIT </sequence>",
    "log": "<log>[TRACE] storageTypes={}, storageTypes</log> <log>[TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e</log> <log>[WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()</log>"
  },
  "989e61ea_3": {
    "exec_flow": "<sequence> ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_FALSE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → IF_TRUE: storageTypes == null → CALL: getRequiredStorageTypes → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_TRUE: requiredStorageTypes.size() == 0 → THROW: new NotEnoughReplicasException → LOG: LOG.TRACE: message, e → LOG: LOG.WARN: message + e.getMessage() → EXIT </sequence>",
    "log": "<log>[TRACE] storageTypes={}, storageTypes</log> <log>[TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e</log> <log>[WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()</log>"
  },
  "989e61ea_4": {
    "exec_flow": "<sequence> ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_FALSE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → IF_FALSE: storageTypes == null → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_TRUE: requiredStorageTypes.size() == 0 → THROW: new NotEnoughReplicasException → LOG: LOG.TRACE: message, e → LOG: LOG.WARN: message + e.getMessage() → EXIT </sequence>",
    "log": "<log>[TRACE] storageTypes={}, storageTypes</log> <log>[TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e</log> <log>[WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()</log>"
  },
  "93e18561_1": {
    "exec_flow": "ENTRY→IF_TRUE:backOffByResponseTimeEnabled→IF_TRUE:LOG.isDebugEnabled()→CALL:LOG.debug→LOG:LOG.DEBUG:Current Caller:{}Priority:{}→FOR_INIT→FOR_COND:i<numLevels→CALL:LOG.debug→FOR_EXIT→FOR_INIT→FOR_COND:i<priorityLevel+1→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Current Caller: {} Priority: {} [DEBUG] Queue: {} responseTime: {} backoffThreshold: {}"
  },
  "a2fefaeb_1": {
    "exec_flow": "ENTRY→CALL:maybeInitBuilder→IF_FALSE:attemptTokens == null→CALL:convertCredentialsToByteBuffer→ENTRY→TRY→IF_TRUE:credentials != null→CALL:writeTokenStorageToStream→CALL:wrap→CALL:getData→CALL:getLength→CALL:getData→CALL:getLength→RETURN→CALL:org.slf4j.Logger:error→EXIT",
    "log": "[ERROR] Failed to convert Credentials to ByteBuffer."
  },
  "a2fefaeb_2": {
    "exec_flow": "ENTRY→CALL:maybeInitBuilder→IF_FALSE:attemptTokens == null→CALL:convertCredentialsToByteBuffer→ENTRY→TRY→IF_TRUE:credentials != null→CALL:writeTokenStorageToStream→EXCEPTION:IOException→CALL:org.slf4j.Logger:error→EXIT",
    "log": "[ERROR] Failed to convert Credentials to ByteBuffer."
  },
  "2bb86d87_1": {
    "exec_flow": "<step>ENTRY</step> <step>SYNC:UserGroupInformation.class</step> <step>CALL:initialize</step> <step>ENTRY</step> <step>CALL:substituteVars</step> <step>CALL:getRaw</step> <step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:Groups:getUserToGroupsMappingService</step> <step>IF_TRUE:GROUPS == null</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG:Creating new Groups object</step> <step>NEW:Groups</step> <step>CALL:<init></step> <step>RETURN</step> <step>EXIT</step> <step>TRY</step> <step>CALL:doSubjectLogin</step> <step>IF_TRUE:proxyUser==null</step> <step>CALL:getProperty</step> <step>CALL:createProxyUser</step> <step>CALL:tokenFileLocations.addAll</step> <step>CALL:getTrimmedStringCollection</step> <step>CALL:get</step> <step>CALL:getTokenFileLocation</step> <step>CALL:exists</step> <step>CALL:isFile</step> <step>CALL:readTokenStorageFile</step> <step>ENTRY</step> <step>TRY</step> <step>NEW:DataInputStream</step> <step>NEW:BufferedInputStream</step> <step>CALL:newInputStream</step> <step>CALL:toPath</step> <step>CALL:toPath</step> <step>CALL:readTokenStorageStream</step> <step>RETURN</step> <step>CALL:IOUtils.cleanupWithLogger</step> <step>EXIT</step> <step>CALL:addCredentials</step> <step>CALL:debug</step> <step>CALL:model:length</step> <step>CALL:load</step> <step>CALL:readTokenStorageFile</step> <step>CALL:addToken</step> <step>CALL:debug</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log> <log>[WARN] Unexpected SecurityException in Configuration, se</log> <log>[INFO] message</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Cleaning up resources</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[ERROR] Cannot add token {}: {}</log> <log>[DEBUG] Loaded {} base64 tokens</log> <log>[WARN] Null token ignored for {}</log>"
  },
  "2bb86d87_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:Preconditions.checkArgument</step> <step>CALL:withClientConfiguration</step> <step>CALL:withCredentials</step> <step>IF_TRUE:isNotEmpty(stsEndpoint) && !destIsStandardEndpoint</step> <step>CALL:Preconditions.checkArgument</step> <step>LOG:[DEBUG] STS Endpoint={}; region='{}'</step> <step>CALL:withEndpointConfiguration</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] STS Endpoint={}; region='{}'</log>"
  },
  "2bb86d87_3": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:initConnectionSettings</step> <step>CALL:initProxySupport</step> <step>IF_TRUE:!proxyHost.isEmpty()</step> <step>CALL:awsConf.setProxyHost</step> <step>IF_FALSE:proxyPort>=0</step> <step>CALL:awsConf.setProxyPort</step> <step>CALL:org.apache.hadoop.conf.Configuration:getBoolean</step> <step>CALL:org.slf4j.Logger:warn</step> <step>LOG:[WARN] Proxy host set without port. Using HTTPS default 443</step> <step>IF_TRUE:(proxyUsername==null)!=(proxyPassword==null)</step> <step>LOG:[ERROR] Proxy error: proxyUsername or proxyPassword set without the other.</step> <step>CALL:org.slf4j.Logger:error</step> <step>THROW:new IllegalArgumentException(msg)</step> <step>EXIT</step> <step>CALL:initUserAgent</step> <step>IF_FALSE:StringUtils.isNotEmpty(awsServiceIdentifier)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN] Proxy host set without port. Using HTTPS default 443</log> <log>[ERROR] Proxy error: proxyUsername or proxyPassword set without the other.</log> <log>[DEBUG] Using User-Agent: {}</log> <log>[DEBUG] Signer override for {} = {}</log>"
  },
  "2bb86d87_4": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:initConnectionSettings</step> <step>CALL:initProxySupport</step> <step>IF_TRUE:!proxyHost.isEmpty()</step> <step>CALL:awsConf.setProxyHost</step> <step>IF_FALSE:proxyPort>=0</step> <step>CALL:awsConf.setProxyPort</step> <step>CALL:org.apache.hadoop.conf.Configuration:getBoolean</step> <step>CALL:org.slf4j.Logger:warn</step> <step>LOG:[WARN] Proxy host set without port. Using HTTP default 80</step> <step>IF_TRUE:(proxyUsername==null)!=(proxyPassword==null)</step> <step>LOG:[ERROR] Proxy error: proxyUsername or proxyPassword set without the other.</step> <step>CALL:org.slf4j.Logger:error</step> <step>THROW:new IllegalArgumentException(msg)</step> <step>EXIT</step> <step>CALL:initUserAgent</step> <step>IF_FALSE:StringUtils.isNotEmpty(awsServiceIdentifier)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN] Proxy host set without port. Using HTTP default 80</log> <log>[ERROR] Proxy error: proxyUsername or proxyPassword set without the other.</log> <log>[DEBUG] Using User-Agent: {}</log> <log>[DEBUG] Signer override for {} = {}</log>"
  },
  "dc785c0c_1": {
    "exec_flow": "ENTRY → CALL:schemeFromPath → CALL:authorityFromPath → LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString → FOREACH:flattenedPatterns → CALL:fixRelativePart → LOG:LOG.DEBUG:Pattern: {} → CALL:getPathComponents → CALL:listStatus → CALL:getFileStatus → LOG:LOG.DEBUG:Component {}, patterned={} → IF_TRUE:(!sawWildcard) && results.isEmpty() && (flattenedPatterns.size() ≤ 1) → LOG:LOG.WARN:File/directory {} not found: it may have been deleted. If this is an object store, this can be a sign of eventual consistency problems. → LOG:LOG.DEBUG:No matches found and there was no wildcard in the path {} → RETURN → EXIT",
    "log": "[DEBUG] Filesystem glob {} [DEBUG] Pattern: {} [DEBUG] Component {}, patterned={} [WARN] File/directory {} not found: it may have been deleted. If this is an object store, this can be a sign of eventual consistency problems. [DEBUG] No matches found and there was no wildcard in the path {}"
  },
  "1f523747_1": {
    "exec_flow": "ENTRY→CALL:IOUtils.closeStream→IF_TRUE:stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug('Exception in closing {', c)→FOREACH_EXIT→NEW:File→NEW:PersistentLongFile→NEW:File→NEW:PersistentLongFile→NEW:File→NEW:BestEffortLongFile→NEW:File→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "cc73f7db_1": {
    "exec_flow": "ENTRY→IF_TRUE:(subject==null || subject.getPrincipals(User.class).isEmpty())→CALL:getLoginUser→ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→ENTRY→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials"
  },
  "60ee33cf_1": {
    "exec_flow": "<![CDATA[ENTRY→CALL:stop→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→CALL: noteFailure→THROW: ServiceStateException.convert(e)→EXIT]]>",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "60ee33cf_2": {
    "exec_flow": "<![CDATA[ENTRY→CALL:stop→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners→EXIT]]>",
    "log": "[DEBUG] Ignoring re-entrant call to stop()"
  },
  "60ee33cf_3": {
    "exec_flow": "<![CDATA[ENTRY→CALL:stop→CALL: noteFailure→SYNC: this→IF_TRUE: failureCause == null→SET: failureCause = exception→SET: failureState = getServiceState()→LOG: LOG.INFO: Service {} failed in state {}]]>",
    "log": "[INFO] Service {} failed in state {}"
  },
  "119676dc_1": {
    "exec_flow": "ENTRY → IF_FALSE: bnState != BNState.IN_SYNC → LOG: [INFO] Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace. → WHILE: bnState == BNState.IN_SYNC → WHILE_EXIT → LOG: [INFO] BackupNode namespace frozen. → ADD: Find edits file → ADD: edictStreams → FOREACH_EXIT → LOG: [INFO] Checkpointer about to load edits from [size] stream(s). → CALL: FSImage.loadEdits with editsStreams → LOG: [DEBUG] About to load edits → TRY → FOREACH: editStreams in FSImage → IF_TRUE: logAction.shouldLog() → LOG: [INFO] Reading + editIn + expecting start txid # + (lastAppliedTxId + 1) + logSuppressed → TRY → CALL: loadFSEdits → IF_TRUE: preLogAction.shouldLog() → LOG: [INFO] Start loading edits file {edits.getName()} maxTxnsToRead = {maxTxnsToRead} {LogThrottlingHelper.getLogSuppressionMessage(preLogAction)} → IF_TRUE: postLogAction.shouldLog() → LOG: [INFO] Loaded {} edits file(s) (the last named {}) of total size {}, total edits {}, total load time {} ms → CALL: fsNamesys.writeUnlock → CALL: endStep → RETURN → IF_FALSE: editIn.getLastTxId() != HdfsServerConstants.INVALID_TXID && recovery != null → IF_TRUE: remainingReadTxns <= 0 → BREAK → CALL: FSEditLog.closeAllStreams → FOREACH: streams → CALL: org.apache.hadoop.io.IOUtils:closeStream → FOREACH_EXIT → EXIT",
    "log": "<log_statement> <template>INFO Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.</template> </log_statement> <log_statement> <template>INFO BackupNode namespace frozen.</template> </log_statement> <log_statement> <template>INFO Checkpointer about to load edits from [size] stream(s).</template> </log_statement> <log_statement> <template>DEBUG About to load edits</template> </log_statement> <log_statement> <template>INFO Reading + editIn + expecting start txid # + (lastAppliedTxId + 1) + logSuppressed</template> </log_statement> <log_statement> <template>INFO Start loading edits file {edits.getName()} maxTxnsToRead = {maxTxnsToRead} {LogThrottlingHelper.getLogSuppressionMessage(preLogAction)}</template> </log_statement> <log_statement> <template>INFO Loaded {} edits file(s) (the last named {}) of total size {}, total edits {}, total load time {} ms</template> </log_statement>"
  },
  "119676dc_2": {
    "exec_flow": "ENTRY → IF_TRUE: bmSafeMode.isInSafeMode() → CALL: doConsistencyCheck → IF_FALSE: !assertsOn → RETURN → EXIT → IF_TRUE: blockTotal!=activeBlocks && !(blockSafe>=0 && blockSafe<=blockTotal) → CALL: org.slf4j.Logger:warn(java.lang.String,java.lang.Object[]) → LOG:LOG.WARN: SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={} → RETURN → EXIT → CALL: bmSafeMode.setBlockTotal → CALL: bmSafeMode.checkSafeMode",
    "log": "<log_statement> <template>WARN SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}</template> </log_statement>"
  },
  "119676dc_3": {
    "exec_flow": "ENTRY → IF_TRUE: bytesInFuture > 0 → IF_FALSE: force → LOG:LOG.ERROR Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of {} byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable. → RETURN → EXIT",
    "log": "<log_statement> <template>ERROR Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of {} byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.</template> </log_statement>"
  },
  "119676dc_4": {
    "exec_flow": "ENTRY → LOG: LOG.INFO: initializing replication queues → CALL: processMisReplicatedBlocks → EXIT",
    "log": "<log_statement> <template>INFO initializing replication queues</template> </log_statement>"
  },
  "119676dc_5": {
    "exec_flow": "ENTRY → IF_FALSE: namesystem.inTransitionToActive() → SWITCH: status → CASE: [PENDING_THRESHOLD] → IF_TRUE: areThresholdsMet() → IF_TRUE: blockTotal > 0 && extension > 0 → CALL: reachedTime.set → CALL: start → CALL: initializeReplQueuesIfNecessary → CALL: reportStatus → BREAK → EXIT",
    "log": "<log_statement> <template>DEBUG STATE* Safe mode extension entered.</template> </log_statement>"
  },
  "119676dc_6": {
    "exec_flow": "ENTRY → IF_FALSE: namesystem.inTransitionToActive() → SWITCH: status → CASE: [PENDING_THRESHOLD] → IF_FALSE: areThresholdsMet() → CALL: initializeReplQueuesIfNecessary → CALL: reportStatus → BREAK → EXIT",
    "log": "<log_statement> <template>DEBUG STATE* Safe mode ON.</template> </log_statement>"
  },
  "119676dc_7": {
    "exec_flow": "ENTRY → IF_FALSE: namesystem.inTransitionToActive() → SWITCH: status → CASE: [EXTENSION] → CALL: reportStatus → BREAK → EXIT",
    "log": "<log_statement> <template>DEBUG STATE* Safe mode ON.</template> </log_statement>"
  },
  "119676dc_8": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → LOG: org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:initializeReplQueuesIfNecessary → LOG.INFO: initializing replication queues → CALL: processMisReplicatedBlocks → EXIT",
    "log": "<log_statement> <template>INFO initializing replication queues</template> </log_statement>"
  },
  "119676dc_9": {
    "exec_flow": "ENTRY → IF_TRUE: checkpointManager != null → IF_TRUE: reportError == true AND namenode != null AND getRegistration() != null → TRY → CALL: errorReport → EXCEPTION: errorReport → CATCH: IOException e → LOG: [ERROR] Failed to report to name-node., e → IF_TRUE: namenode != null → CALL: RPC.stopProxy → IF_TRUE: checkpointManager != null → CALL: checkpointManager.interrupt → IF_TRUE: namesystem != null → CALL: getFSImage().getEditLog().abortCurrentLogSegment → CALL: stop → EXIT",
    "log": "<log_statement> <template>ERROR Failed to report to name-node., e</template> </log_statement>"
  },
  "a3a3fcf5_1": {
    "exec_flow": "ENTRY → IF_FALSE: namenodes == null OR namenodes.isEmpty() → CALL: addClientInfoToCallerContext → IF_FALSE: rpcMonitor != null → FOREACH: namenodes → TRY → CALL: getConnection → LOG: LOG.DEBUG: User {} NN {} is using connection {}, ugi.getUserName(), rpcAddress, connection → CALL: invoke → IF_FALSE: failover → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT → TRY → CALL: invoke → CONDITIONAL → CALL: shouldRetry → CALL: invoke → RETURN → EXIT → TRY → IF_TRUE: types != null → CALL: getDeclaredMethod → LOG: org.slf4j.Logger:error(\"Cannot get method {} with types {} from {}\", methodName, Arrays.toString(types), protocol.getSimpleName()) → THROW: IOException → EXIT",
    "log": "[DEBUG] User {} NN {} is using connection {} [ERROR] Cannot get method {} with types {} from {}"
  },
  "a3a3fcf5_2": {
    "exec_flow": "ENTRY → IF_FALSE: namenodes == null OR namenodes.isEmpty() → CALL: addClientInfoToCallerContext → IF_FALSE: rpcMonitor != null → FOREACH: namenodes → TRY → CALL: getConnection → LOG: LOG.DEBUG: User {} NN {} is using connection {}, ugi.getUserName(), rpcAddress, connection → CALL: invoke → IF_FALSE: failover → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT → TRY → CALL: invoke → CONDITIONAL → CALL: shouldRetry → CALL: invoke → RETURN → EXIT → TRY → IF_FALSE: types != null → CALL: getDeclaredMethod → LOG: org.slf4j.Logger:error(\"Cannot access method {} with types {} from {}\", methodName, Arrays.toString(types), protocol.getSimpleName()) → THROW: IOException → EXIT",
    "log": "[DEBUG] User {} NN {} is using connection {} [ERROR] Cannot access method {} with types {} from {}"
  },
  "a3a3fcf5_3": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ → CALL: invokeSingle → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "a3a3fcf5_4": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ → CALL: checkSafeMode → CALL: invokeSingle → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "a3a3fcf5_5": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ → CALL: invokeSingle → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "a3a3fcf5_6": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ → CALL: checkSafeMode → CALL: invokeSingle → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "a3a3fcf5_7": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → RETURN → EXIT",
    "log": "[LOG] getLoginUser"
  },
  "a8f79d59_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CATCH: Throwable e → LOG.warn(\"Exception while notifying listeners of {}\", this, e) → EXCEPTION: serviceInit → CATCH: Exception e → CALL: noteFailure → SYNC: this → LOG.debug(\"noteFailure\", exception) → IF_TRUE: failureCause == null → ASSIGN: failureCause = exception → ASSIGN: failureState = getServiceState() → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → EXIT_SYNC → LOG.debug(\"noteFailure\", exception) → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → CALL: ServiceOperations.stopQuietly → IF_TRUE: service != null → CALL: stop → CATCH_EXCEPTION → CALL: log.warn → EXIT",
    "log": "<log>[DEBUG] Service: {getName()} entered state {getServiceState()}</log> <log>[DEBUG] Config has been overridden during init</log> <log>LOG.debug(\"noteFailure\", exception);</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>org.apache.hadoop.service.ServiceOperations:stopQuietly - WARN When stopping the service {service_name}</log>"
  },
  "da2b020c_1": {
    "exec_flow": "ENTRY → IF_FALSE:srcStatus==null → TRY → CALL:getFileLinkStatus → IF_TRUE:dstStatus!=null → CALL:org.apache.hadoop.fs.FileSystem:rename → TRY → CALL:trackDurationAndSpan → EXCEPTION:RenameFailedException → CATCH:RenameFailedException e → LOG:INFO:{} , e.getMessage() → LOG:DEBUG:rename failure, e → CALL:getExitCode → RETURN → EXIT ENTRY → LOG: Moving {} to {}, src, dst → IF_FALSE: containsColon(dst) → IF_FALSE: srcParentFolder == null → IF_FALSE: srcKey.length() == 0 → TRY → CALL: performAuthCheck → IF_FALSE: dstMetadata != null && dstMetadata.isDirectory() → IF_TRUE: dstMetadata != null → LOG: Destination {} is already existing file, failing the rename. → RETURN → EXIT",
    "log": "[INFO] {} [DEBUG] rename failure [DEBUG] Moving {} to {}, src, dst [DEBUG] Destination {} is already existing file, failing the rename."
  },
  "da2b020c_2": {
    "exec_flow": "ENTRY → LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}] → TRY → CALL: getFileStatus → IF_FALSE: key.compareToIgnoreCase(\"/\") == 0 → LOG: [DEBUG] Delete the file: {}, f → CALL: createParent → CALL: store.delete → RETURN → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}] [DEBUG] Delete the file: {}"
  },
  "da2b020c_3": {
    "exec_flow": "ENTRY → IF_FALSE:this.azureAuthorization → CALL:deleteWithoutAuth → LOG: LOG.DEBUG: Deleting file: {}, f → TRY → CALL: retrieveMetadata → IF_FALSE: null == metaFile → IF_TRUE: !metaFile.isDirectory() → IF_TRUE: parentPath.getParent() != null → TRY → CALL: retrieveMetadata → IF_FALSE: parentMetadata == null → IF_FALSE: !parentMetadata.isDirectory() → IF_FALSE: parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit → IF_FALSE: !skipParentFolderLastModifiedTimeUpdate → TRY → IF_TRUE: store.delete(key) → CALL: fileDeleted → LOG: DEBUG: Delete Successful for : {}, f → RETURN → EXIT",
    "log": "[DEBUG] Deleting file: {} [DEBUG] Delete Successful for : {}"
  },
  "da2b020c_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→CALL:addKVAnnotation→CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass→ENTRY→IF_TRUE:!FILE_SYSTEMS_LOADED→CALL:loadFileSystems→LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme→IF_TRUE:conf != null→LOG:LOGGER.DEBUG:looking for configuration option {}, property→CALL:getClass→IF_FALSE:clazz == null→LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme→IF_FALSE:clazz == null→LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz→RETURN→EXIT→CALL:org.apache.hadoop.util.ReflectionUtils:newInstance→TRY→CALL:initialize→EXCEPTION:initialize→CATCH:IOException | RuntimeException e→LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString()→LOG:LOGGER.DEBUG:Failed to initialize filesystem, e→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→THROW:e→EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Failed to initialize filesystem, e [DEBUG] Exception in closing {}"
  },
  "da2b020c_5": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)→CALL:getInternal→TRY→CALL:creatorPermits.acquireUninterruptibly→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→SYNC:this→CALL:get→IF_TRUE:fs != null→RETURN→EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "da2b020c_6": {
    "exec_flow": "ENTRY→CALL:FileSystem:get→IF_TRUE:scheme != null && authority == null→NOT (scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null)→IF_TRUE:conf.getBoolean(disableCacheName, false)→LOG:DEBUG: Bypassing cache to create filesystem {uri}→CALL:FileSystem:createFileSystem→RETURN→EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {uri}"
  },
  "da2b020c_7": {
    "exec_flow": "ENTRY→CALL:FileSystem:get→NOT (scheme == null && authority == null)→NOT (scheme != null && authority == null)→IF_TRUE:conf.getBoolean(disableCacheName, false)→LOG:DEBUG: Bypassing cache to create filesystem {uri}→CALL:FileSystem:createFileSystem→RETURN→EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {uri}"
  },
  "88bc6065_1": {
    "exec_flow": "ENTRY → CALL:getFileStatus → VIRTUAL_CALL → ENTRY → CALL:setLogEnabled → CALL:getObjectMetadata → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT → ENTRY → CALL:qualify → LOG:DEBUG [Getting path status for {f}; needEmptyDirectory={}] → CALL:makeQualified → LOG:DEBUG [Stripping trailing '/' from {q}] → CALL:s3GetFileStatus → RETURN → EXIT → CALL:openConnection → CALL:setRequestMethod → IF_TRUE:method.equals(HTTP_POST) || method.equals(HTTP_PUT) → CALL:setDoOutput → RETURN → EXIT → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:access$6 → VIRTUAL_CALL → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus → LOG:DEBUG [AzureBlobFileSystem.getFileStatus path: {}] → CALL:statIncrement → CALL:makeQualified → CALL:AzureBlobFileSystemStore.getFileStatus → ENTRY → CALL:getIsNamespaceEnabled → CALL:debug → IF → CALL:getPathStatus → CALL:registerCallee → CALL:getResult → CALL:registerResult → CALL:parseLastModifiedTime → RETURN → EXIT → RETURN → EXIT → VIRTUAL_CALL:run() → GetOpParam.Op.GETFILESTATUS → FsPathResponseRunner<HdfsFileStatus> → decodeResponse → return status → RETURN → EXIT",
    "log": "<log_entry> <class_method>org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Exception thrown when get object meta: + key + , exception: + osse</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Getting path status for {f}; needEmptyDirectory={}</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:makeQualified</class_method> <level>DEBUG</level> <template>Stripping trailing '/' from {q}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Getting the file status for {f}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Retrieving metadata for {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Path {f} is a folder.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Found the path: {f} as a file.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a file. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a dir. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>List COS key: {key} to check the existence of the path.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a directory. COS key: {key}</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus</class_method> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry>"
  },
  "d416bfcb_1": {
    "exec_flow": "ENTRY→TRY→CALL:getUser→IF_FALSE: callerUGI == null→CALL:doAs→IF_TRUE: LOG.isDebugEnabled()→LOG: PrivilegedAction [as: {}][action: {}]→NEW: PrivilegedExceptionAction→CALL:newInstance→CALL:getApplicationReport→IF_TRUE: app == null→LOG: PrivilegedActionException as: {}→THROW: new NotFoundException→EXIT",
    "log": "[DEBUG] User information retrieved [DEBUG] PrivilegedAction [as: {}][action: {}] [INFO] Privileged action performed [DEBUG] PrivilegedActionException as: {}"
  },
  "99754463_1": {
    "exec_flow": "ENTRY → CALL:disallowSnapshot → TRY → CALL:openConnection → CALL:setRequestMethod → CALL:getConnection → EXCEPTION:setRequestMethod → CATCH:Exception ex → THROW:new IOException(ex) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [INFO] Cleaning up resources [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "59173c41_1": {
    "exec_flow": "ENTRY→LOG.INFO: Starting JVM pause monitor→WHILE: shouldRun→WHILE_COND: shouldRun→CALL: start→TRY→CALL: Thread.sleep→CALL: getGcTimes→CALL: getGcTimes→CALL: warn→CALL: info→EXCEPTION: sleep→CATCH: InterruptedException ie→RETURN→EXIT →PARENT.ENTRY→VIRTUAL_CALL",
    "log": "[INFO] Starting JVM pause monitor [WARN] formatMessage(extraSleepTime, gcTimesAfterSleep, gcTimesBeforeSleep) [INFO] formatMessage(extraSleepTime, gcTimesAfterSleep, gcTimesBeforeSleep)"
  },
  "59173c41_2": {
    "exec_flow": "ENTRY→LOG.INFO: Starting JVM pause monitor→WHILE: shouldRun→WHILE_EXIT→EXIT →PARENT.ENTRY→VIRTUAL_CALL",
    "log": "[INFO] Starting JVM pause monitor"
  },
  "bb35303d_1": {
    "exec_flow": "ENTRY -> SYNC: block -> SYNC: movedBlocks -> IF_TRUE: isGoodBlockCandidate(source, target, targetStorageType, block) -> IF_TRUE: block instanceof DBlockStriped -> CALL: getInternalBlock -> IF_TRUE: reportedBlock == null -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> LOG: LOG.INFO: No striped internal block on source {}, block {}. Skipping., source, block -> RETURN -> EXIT",
    "log": "[INFO] No striped internal block on source {}, block {}. Skipping."
  },
  "bb35303d_2": {
    "exec_flow": "ENTRY -> FOR_INIT -> FOR_COND: i.hasNext() -> IF_TRUE: markMovedIfGoodBlock(i.next(), t) -> CALL: remove -> RETURN -> EXIT -> SYNC: block -> SYNC: movedBlocks -> IF_TRUE: isGoodBlockCandidate(source, target, targetStorageType, block) -> IF_TRUE: block instanceof DBlockStriped -> CALL: getInternalBlock -> IF_FALSE: reportedBlock == null -> IF_TRUE: chooseProxySource() -> CALL: movedBlocks.put -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:debug(java.lang.String) -> LOG: LOG.DEBUG: Decided to move + this -> RETURN -> EXIT",
    "log": "[DEBUG] Decided to move + this"
  },
  "bb35303d_3": {
    "exec_flow": "ENTRY -> FOR_INIT -> FOR_COND: i.hasNext() -> IF_TRUE: markMovedIfGoodBlock(i.next(), t) -> CALL: remove -> RETURN -> EXIT -> SYNC: block -> SYNC: movedBlocks -> IF_TRUE: isGoodBlockCandidate(source, target, targetStorageType, block) -> IF_FALSE: block instanceof DBlockStriped -> IF_TRUE: chooseProxySource() -> CALL: movedBlocks.put -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:debug(java.lang.String) -> LOG: LOG.DEBUG: Decided to move + this -> RETURN -> EXIT",
    "log": "[DEBUG] Decided to move + this"
  },
  "03731075_1": {
    "exec_flow": "ENTRY→TRY→CALL:executeDockerInspect→LOG:LOG.INFO:Docker inspect output for containerId : output→CALL:replaceAll→IF_TRUE:index == -1→LOG:LOG.ERROR:Incorrect format for ip and host→RETURN→EXIT",
    "log": "[INFO] Docker inspect output for containerId : output [ERROR] Incorrect format for ip and host"
  },
  "03731075_2": {
    "exec_flow": "ENTRY→TRY→CALL:executeDockerInspect→LOG:LOG.INFO:Docker inspect output for containerId : output→EXCEPTION:info→CATCH:ContainerExecutionException e→LOG:LOG.ERROR:Error when writing command to temp file, e→RETURN→EXIT",
    "log": "[INFO] Docker inspect output for containerId : output [ERROR] Error when writing command to temp file, e"
  },
  "03731075_3": {
    "exec_flow": "ENTRY→TRY→CALL:executeDockerInspect→LOG:LOG.INFO:Docker inspect output for containerId : output→EXCEPTION:info→CATCH:PrivilegedOperationException e→LOG:LOG.ERROR:Error when executing command., e→RETURN→EXIT",
    "log": "[INFO] Docker inspect output for containerId : output [ERROR] Error when executing command., e"
  },
  "03731075_4": {
    "exec_flow": "ENTRY→TRY→CALL:executeDockerInspect→LOG:LOG.INFO:Docker inspect output for containerId : output→CALL:replaceAll→IF_FALSE:index == -1→IF_TRUE:ips.equals(\"\")→TRY→CALL:get→CALL:getEnvironment→CALL:getLaunchContext→IF_FALSE:network == null || network.isEmpty()→IF_FALSE:useHostNetwork→RETURN→EXIT",
    "log": "[INFO] Docker inspect output for containerId : output"
  },
  "03731075_5": {
    "exec_flow": "ENTRY→TRY→CALL:executeDockerInspect→LOG:LOG.INFO:Docker inspect output for containerId : output→CALL:replaceAll→IF_FALSE:index == -1→IF_FALSE:ips.equals(\"\")→RETURN→EXIT",
    "log": "[INFO] Docker inspect output for containerId : output"
  },
  "03731075_6": {
    "exec_flow": "ENTRY→TRY→CALL:getHostAddress→CALL:getHostName→CATCH:UnknownHostException→CALL:org.slf4j.Logger:error→RETURN→EXIT",
    "log": "[ERROR] Unable to get Local hostname and ip for {}"
  },
  "104e2376_1": {
    "exec_flow": "ENTRY→CALL:CoderUtil.toBuffers→CALL:CoderUtil.toBuffers→CALL:decode→CALL:PerformanceAdvisory.LOG.debug→CALL:doDecode→FOR_INIT→FOR_COND:i < decodingState.outputs.length→FOR_EXIT→EXIT",
    "log": "[DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs"
  },
  "48d708fb_1": {
    "exec_flow": "ENTRY→IF_FALSE: remainder > 0→IF_TRUE: NameNode.LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→CALL: NameNode.LOG.debug→CALL: org.slf4j.Logger:debug→RETURN→EXIT ENTRY→CALL:checkOperation→CALL:checkErasureCodingSupported→CALL:getPermissionChecker→CALL:FSPermissionChecker.setOperationType→CALL:readLock→TRY→CALL:checkOperation→CALL:FSDirErasureCodingOp.getErasureCodingPolicy→CALL:readUnlock→CALL:logAuditEvent→RETURN→EXIT ENTRY→CALL:coarseLock.readLock().unlock→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→EXIT",
    "log": "[INFO] Finished executing getErasureCodingPolicy for path src [DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)] [INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "0e5a74ad_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → FOREACH:names → CALL:createEnsembleProvider → FOR_COND: i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → SYNC:CuratorService.class → CALL:registrySecurity.applySecurityEnvironment → IF_TRUE:isSecureRegistry() → SWITCH:access → CASE:[sasl] → IF_TRUE:existingJaasConf==null||existingJaasConf.isEmpty() → IF_FALSE:principal==null||keytab==null → LOG:LOG.INFO:Enabling ZK sasl client: jaasClientEntry=+jaasClientEntry+, principal=+principal+, keytab=+keytab → CALL:javax.security.auth.login.Configuration.setConfiguration → CALL:setSystemPropertyIfUnset → CALL:setSystemPropertyIfUnset → BREAK → EXIT → CALL:buildSecurityDiagnostics → CALL:LOG.isDebugEnabled → CALL:LOG.debug → EXIT",
    "log": "[INFO] Creating CuratorService with connection {} [INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Enabling ZK sasl client: jaasClientEntry= + jaasClientEntry + , principal= + principal +, keytab= + keytab [DEBUG] securityConnectionDiagnostics"
  },
  "7f1ee582_1": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→CALL: getRandom→CALL: nextInt→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_TRUE: !excludedNodes.contains(ret)→LOG: LOG.DEBUG: Chosen node {} from first random, ret→RETURN→EXIT",
    "log": "<log> [DEBUG] nthValidToReturn is {} </log> <log> [DEBUG] Chosen node {} from first random </log> <log> [DEBUG] Choosing data node </log> <log> [INFO] Chosen node: ... </log>"
  },
  "7f1ee582_2": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→CALL: getRandom→CALL: nextInt→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_FALSE: !excludedNodes.contains(ret)→FOR_INIT→FOR_COND: i < totalInScopeNodes→CALL: getLeaf→LOG: LOG.DEBUG: Node {} is excluded, continuing.→FOR_EXIT→IF_TRUE: ret == null && lastValidNode != null→LOG: LOG.ERROR: BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}→RETURN→EXIT",
    "log": "<log> [DEBUG] nthValidToReturn is {} </log> <log> [DEBUG] Node {} is excluded, continuing. </log> <log> [ERROR] BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={} </log> <log> [DEBUG] Choosing data node </log> <log> [INFO] Chosen node: ... </log>"
  },
  "7f1ee582_3": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→CALL: getRandom→CALL: nextInt→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_FALSE: !excludedNodes.contains(ret)→FOR_INIT→FOR_COND: i < totalInScopeNodes→CALL: getLeaf→LOG: LOG.DEBUG: Node {} is excluded, continuing.→FOR_EXIT→IF_FALSE: ret == null && lastValidNode != null→RETURN→EXIT",
    "log": "<log> [DEBUG] nthValidToReturn is {} </log> <log> [DEBUG] Node {} is excluded, continuing. </log> <log> [DEBUG] Choosing data node </log> <log> [INFO] Chosen node: ... </log>"
  },
  "8011edf5_1": {
    "exec_flow": "ENTRY → CALL:cacheCleanup.scheduleWithFixedDelay → CALL:createServer → SERVER.initialize → ENTRY → LOG:INFO: Localizer started on port + server.getPort() → CALL:setRules → CALL:setRuleMechanism → CALL:handleDeprecation_Handle → FOREACH:keys → LOG:DEBUG: Handling deprecation for (String)item → CALL:substituteVars → IF_TRUE:!(groups instanceof TestingGroups) → CALL:getUserToGroupsMappingService → CALL:isDebugEnabled → IF_TRUE:LOG.isDebugEnabled() → LOG:DEBUG: Creating new Groups object → NEW:Groups → ENTRY → CALL:<init> → RETURN → EXIT → IF_TRUE:metrics.getGroupsQuantiles == null → CALL:getInts → IF_FALSE:intervals != null && intervals.length > 0 → EXIT → LOG:DEBUG: Creating YarnRPC for {IPC_RPC_IMPL} → CALL: getConnectionManager → CALL:start → ENTRY → LOG:LOG.INFO: Localizer started on port + server.getPort() → WHILE_LOOP → TRY → CATCH: IOException → WHILE_END → TRY → CATCH: IOException → CALL: connectionManager.stopIdleScan → START_EXIT",
    "log": "[INFO] Localizer started on port + server.getPort() [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [DEBUG] Creating YarnRPC for {IPC_RPC_IMPL} [INFO] Localizer started on port + server.getPort()"
  },
  "ebc473cd_1": {
    "exec_flow": "ENTRY → TRY → CALL: DurationInfo:<init> → CALL: getJobAttemptPath → CALL: getFileSystem → [DEBUG] Handling deprecation for all properties in config... CALL: getProps → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: addAll → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\") → CALL: createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → IF_TRUE: fs != null → LOG: LOGGER.debug(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → CALL: listAndFilter → CALL: DurationInfo:close → IF_TRUE: logAtInfo → CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object) → [INFO] {} → ELSE → IF_TRUE: log.isDebugEnabled() → CALL: org.apache.hadoop.fs.FileSystem:handleFileStat → [DEBUG] Directory {path} deleted while attempting for recursive listing → CALL: org.apache.hadoop.Logger:debug(java.lang.String,java.lang.Object) → [DEBUG] {} → EXIT → CATCH: IOException → CALL: maybeIgnore → [DEBUG] action, ex → EXIT CALL: AbstractS3ACommitter$ActiveCommit:fromStatusIterator [VIRTUAL_CALL] CALL: RemoteIterators:toList",
    "log": "[DurationInfo LOG] Listing pending uploads [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {} [DEBUG] Duplicate FS created for {}; discarding {} [DEBUG] Directory {path} deleted while attempting for recursive listing [INFO] {} [DEBUG] {} [DEBUG] action, ex"
  },
  "869b8e6c_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "869b8e6c_2": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "1d969df3_1": {
    "exec_flow": "ENTRY → CALL:checkOperation → CALL:getPermissionChecker → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:checkOperation → CALL:checkNameNodeSafeMode → THROW:RetriableException → LOG:RetriableException thrown if HA is enabled and active in safe mode. → EXIT",
    "log": "<entry> <log>RetriableException thrown if HA is enabled and active in safe mode.</log> </entry>"
  },
  "1d969df3_2": {
    "exec_flow": "ENTRY → CALL:checkOperation → CALL:getPermissionChecker → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:checkOperation → CALL:checkNameNodeSafeMode → THROW:SafeModeException → LOG:SafeModeException thrown if NameNode is in SafeMode. → EXIT",
    "log": "<entry> <log>SafeModeException thrown if NameNode is in SafeMode.</log> </entry>"
  },
  "1d969df3_3": {
    "exec_flow": "<entry>FSDirSnapshotOp</entry> <call>CreateSnapshot</call> <call>FSEditLog</call>",
    "log": "<log> <entry class=\"FSDirSnapshotOp\" method=\"createSnapshot\"> <level>INFO</level> <message>Log create snapshot at the root: {snapshotRoot} with name: {snapshotName}</message> </entry> <entry class=\"FSEditLog\" method=\"logCreateSnapshot\"> <level>DEBUG</level> <message>Log that a snapshot is created</message> </entry> </log>"
  },
  "1d969df3_4": {
    "exec_flow": "ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}"
  },
  "1d969df3_5": {
    "exec_flow": "ENTRY → CALL: this.fsLock.writeUnlock → CALL: org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:writeUnlock → IF_TRUE:needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs → CALL:numWriteLockLongHold.increment → IF_TRUE:longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs → NEW:LockHeldInfo → CALL:getStackTrace → CALL:currentThread → CALL:currentThread → CALL:record → IF_TRUE:logAction.shouldLog() → NEW:LockHeldInfo → CALL:coarseLock.writeLock().unlock → IF_TRUE:needReport → CALL:addMetric → IF_TRUE:logAction.shouldLog() → CALL:FSNamesystem.LOG.info → EXIT",
    "log": "CALL:FSNamesystem.LOG.info"
  },
  "4a2d6b03_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register</step> <step>CALL: namedCallbacks.put</step> <step>CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder</step> <step>CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource</step> <step>CALL:checkNotNull</step> <step>CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init></step> <step>CALL:put</step> <step>CALL:start</step> <step>IF_TRUE: startMBeans</step> <step>CALL: startMBeans</step> <step>CALL: org.slf4j.Logger:debug</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Registering the metrics source</log> <log>[DEBUG] Registered source + name</log>"
  },
  "1df72e37_1": {
    "exec_flow": "ENTRY→IF_TRUE:exists→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()→CALL:access$7→CALL:close→IF_TRUE:connection.user!=null&&connection.connectionContextRead→CALL:decrUserConnections→RETURN→EXIT",
    "log": "[DEBUG] Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()"
  },
  "1df72e37_2": {
    "exec_flow": "ENTRY→IF_TRUE:exists→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()→CALL:access$7→CALL:close→IF_FALSE:connection.user!=null&&connection.connectionContextRead→RETURN→EXIT",
    "log": "[DEBUG] Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()"
  },
  "ec7896c5_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: LOG.DEBUG: Service: {} entered state {} → TRY → CALL: serviceStop → CATCH: Exception e → CALL: noteFailure → LOG: LOG.DEBUG: noteFailure, exception → LOG: LOG.INFO: Service {} failed in state {}, getName(), failureState, exception → FINALLY → terminationNotification.set(true) → SYNC: terminationNotification → terminationNotification.notifyAll() → CALL: notifyListeners → ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → EXIT → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] noteFailure, exception [INFO] Service {} failed in state {}"
  },
  "ec7896c5_2": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → CALL: notifyListeners → ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "7fa4a8ab_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Handling deprecation for all properties in config...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for (String) item</message> </log>"
  },
  "7fa4a8ab_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.yarn.conf.YarnConfiguration:timelineServiceV2Enabled → IF_FALSE: !YarnConfiguration.timelineServiceV2Enabled(conf) → CALL: org.apache.hadoop.security.UserGroupInformation:getCurrentUser → IF_TRUE: realUgi != null → CALL: getShortUserName → NEW: TimelineConnector → CALL: addIfService → CALL: constructResURI → CALL: get → CALL: org.slf4j.Logger:info → CALL: serviceInit → EXIT",
    "log": "<log> <level>INFO</level> <message>Initialized TimelineReader URI=... , clusterId=...</message> </log>"
  },
  "7fa4a8ab_3": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.yarn.conf.YarnConfiguration:timelineServiceV2Enabled → IF_FALSE: !YarnConfiguration.timelineServiceV2Enabled(conf) → CALL: org.apache.hadoop.security.UserGroupInformation:getCurrentUser → IF_FALSE: realUgi != null → NEW: TimelineConnector → CALL: addIfService → CALL: constructResURI → CALL: get → CALL: org.slf4j.Logger:info → CALL: serviceInit → EXIT",
    "log": "<log> <level>INFO</level> <message>Initialized TimelineReader URI=... , clusterId=...</message> </log>"
  },
  "7fa4a8ab_4": {
    "exec_flow": "ENTRY → IF_TRUE: object instanceof Service → CALL: addService → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.debug → CALL: org.slf4j.Logger:debug → SYNC: serviceList → CALL: serviceList.add → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Adding service + service.getName()</message> </log>"
  },
  "5a3bdff0_1": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> RETURN -> EXIT",
    "log": "[INFO] Service {} failed in state {}"
  },
  "5a3bdff0_2": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> RETURN -> EXIT",
    "log": "[DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {}"
  },
  "9d0a18fc_1": {
    "exec_flow": "ENTRY → CALL:setupTask → ENTRY → IF_FALSE:scheme == null && authority == null → IF_TRUE:scheme != null && authority == null → CALL:getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE:scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE:conf.getBoolean(disableCacheName, false) → LOG:LOGGER.DEBUG:Bypassing cache to create filesystem {}, uri → CALL:createFileSystem → ENTRY → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_TRUE:clazz == null → LOG:LOGGER.DEBUG:Looking in service filesystems for implementation class → CALL:get → IF_TRUE:clazz == null → THROW:new UnsupportedFileSystemException(\"No FileSystem for scheme \" + \"\\\"\" + scheme + \"\\\"\") → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → LOG:org.slf4j.Logger:debug:Exception in closing {} → FOREACH_EXIT → THROW:e → EXIT → RETURN → EXIT → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {uri} [DEBUG] Looking for FS supporting {scheme} [DEBUG] looking for configuration option {property} [DEBUG] Looking in service filesystems for implementation class [WARN] Failed to initialize filesystem {uri}: {e.toString()} [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {} [DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "9d0a18fc_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:AzureBlobFileSystem.listStatus path: {f.toString()} → CALL:statIncrement → CALL:makeQualified → TRY → CALL:abfsStore.listStatus → RETURN → EXIT → FOREACH:ls → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$1:<init>(org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem,org.apache.hadoop.fs.FileStatus) → FOREACH_EXIT → FOREACH:deleteTasks → FOREACH_EXIT → CALL:executorService.shutdownNow → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}"
  },
  "9d0a18fc_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:AzureBlobFileSystem.listStatus path: {}, f.toString() → CALL:statIncrement → CALL:makeQualified → TRY → CALL:abfsStore.listStatus → EXCEPTION:AzureBlobFileSystemException → CALL:checkException → RETURN → EXIT → FOREACH:ls → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$1:<init>(org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem,org.apache.hadoop.fs.FileStatus) → FOREACH_EXIT → FOREACH:deleteTasks → FOREACH_EXIT → CALL:executorService.shutdownNow → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}"
  },
  "9d0a18fc_4": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → CALL:innerDelete[EXCEPTION:FileNotFoundException] → CATCH → CALL:debug → RETURN → EXIT",
    "log": "DEBUG: \"Couldn't delete {} - does not exist\""
  },
  "9d0a18fc_5": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CosNativeFileSystemStore:delete → LOG:[DEBUG] Delete object key: [{}] from bucket: {}. → TRY → CALL:callCOSClientWithRetry → EXCEPTION:callCOSClientWithRetry → CATCH: Exception e → LOG:[ERROR] Delete key: [{}] occurs an exception: [{}]. → CALL:handleException → RETURN → EXIT",
    "log": "[DEBUG] Delete object key: [{}] from bucket: {}. [ERROR] Delete key: [{}] occurs an exception: [{}]."
  },
  "9d0a18fc_6": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CosNativeFileSystemStore:delete → LOG:[DEBUG] Delete object key: [{}] from bucket: {}. → TRY → CALL:callCOSClientWithRetry → RETURN → EXIT",
    "log": "[DEBUG] Delete object key: [{}] from bucket: {}."
  },
  "9d0a18fc_7": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Creating directory: {}, f.toString() → IF_FALSE:containsColon(f) → IF_FALSE:absolutePath.equals(ancestor) → CALL:performAuthCheck → IF_FALSE:noUmask → CALL:createPermissionStatus → CALL:applyUMask → CALL:createPermissionStatus → CALL:applyUMask → FOR_INIT → FOR_COND:parent != null → IF_TRUE:currentMetadata != null && !currentMetadata.isDirectory() → THROW:new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\") → EXIT",
    "log": "[DEBUG] Creating directory: {}"
  },
  "9d0a18fc_8": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Creating directory: {}, f.toString() → IF_FALSE:containsColon(f) → IF_FALSE:absolutePath.equals(ancestor) → CALL:performAuthCheck → IF_FALSE:noUmask → CALL:createPermissionStatus → CALL:applyUMask → CALL:createPermissionStatus → CALL:applyUMask → FOR_INIT → FOR_COND:parent != null → IF_TRUE:currentMetadata != null && !currentMetadata.isDirectory() → THROW:new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\") → IF_TRUE:props != null → CALL:loadResources → IF_FALSE:overlay != null → EXIT",
    "log": "[DEBUG] Creating directory: {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "9d0a18fc_9": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.conf.Configuration:get → IF_FALSE:mode==null → CALL:org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode → IF_FALSE:upper.equals(\"STREAM_TRANSFER_MODE\") → IF_FALSE:upper.equals(\"COMPRESSED_TRANSFER_MODE\") → IF_TRUE:!upper.equals(\"BLOCK_TRANSFER_MODE\") → CALL:org.slf4j.Logger:warn → RETURN → EXIT",
    "log": "[WARN] Cannot parse the value for FS_FTP_TRANSFER_MODE: mode. Using default."
  },
  "9d0a18fc_10": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "cc1a0d6b_1": {
    "exec_flow": "<entry> <call_site>org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getLinkTarget</call_site> <conditions>(rpcServer.checkOperation executed) AND (getLocationsForPath executed) AND (invokeSequential executed)</conditions> <virtual_call> <callee>org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getLocationsForPath</callee> <fall_through>false</fall_through> <callee>org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeSequential</callee> <fall_through>false</fall_through> </virtual_call> </entry>",
    "log": "<log>org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.getLocationsForPath - AccessControlException thrown: {reasons}</log> <log>[DEBUG] Proxying operation: {}</log> <log>[DEBUG] User {} NN {} is using connection {}</log> <log>[ERROR] Unexpected exception {} proxying {} to {}</log> <log>[ERROR] No namenode available to invoke...</log> <log>[ERROR] Get connection for {} {} error: {}</log> <log>[ERROR] Cannot get available namenode for {} {} error: {}</log> <log>[ERROR] {} at {} is in Standby: {}</log> <log>[ERROR] {} at {} cannot be reached: {}</log> <log>[ERROR] {} at {} error: \"{}\"</log> <log>[ERROR] Unexpected exception while proxying API</log> <log>[ERROR] Cannot get method {} with types {} from {}</log> <log>[ERROR] Cannot access method {} with types {} from {}</log>"
  },
  "50ea6ac6_1": {
    "exec_flow": "ENTRY→IF_TRUE: container.wasLaunched→CALL: endRunningContainer→CALL:failedContainer→CALL:[VIRTUAL_CALL]→ENTRY→IF_TRUE: LOG.isWarnEnabled()→CALL: org.slf4j.Logger:isWarnEnabled()→CALL: LOG.WARN: createFailureLog(user, operation, target, description, null, null)→CALL: org.slf4j.Logger:warn(java.lang.String)→EXIT→CALL: transition→ENTRY→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Processing [TaskAttemptID] of type [eventType]→CALL: writeLock.lock→TRY→TRY→CALL: stateMachine.doTransition→IF_TRUE: oldState != getInternalState()→IF_TRUE: getInternalState() == TaskAttemptStateInternal.FAILED→LOG: LOG.INFO: [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]→CALL: writeLock.unlock→CALL: releaseContainer→CALL: org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getResource→IF_TRUE: container.containerMetrics != null→CALL: container.containerMetrics.recordFinishTimeAndExitCode→CALL: container.containerMetrics.finished→CALL: sendFinishedEvents→IF_TRUE: container.getCurrentState() != org.apache.hadoop.yarn.api.records.ContainerState.NEW→CALL: container.dispatcher.getEventHandler().handle→CALL: container.context.getNodeStatusUpdater().sendOutofBandHeartBeat→EXIT",
    "log": "[INFO] Container metrics ended running container [ERROR] Container metrics failed container [WARN] Audit log: Container failed with state: ... [DEBUG] Processing [TaskAttemptID] of type [eventType] [INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId] [INFO] Container metrics released [DEBUG] Container metrics finished <log>Got unknown resource type: {entry.getKey()}; skipping</log>"
  },
  "46d9823b_1": {
    "exec_flow": "ENTRY → CALL: namedCallbacks.put → CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder → CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource → CALL:checkNotNull → CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init> → CALL:put → CALL:start → IF_TRUE:startMBeans → CALL:startMBeans → LOG:LOG.DEBUG:Registered source + name → EXIT",
    "log": "[DEBUG] Registering the metrics source [DEBUG] Registered source + name"
  },
  "94a89c12_1": {
    "exec_flow": "ENTRY→TRY→CALL:setLogEnabled→CALL:getObjectMetadata(request)→EXCEPTION:getObjectMetadata→CATCH:OSSException osse→CALL:LOG.debug→RETURN→EXIT",
    "log": "[DEBUG] Exception thrown when get object meta: + key +, exception: + osse"
  },
  "94a89c12_2": {
    "exec_flow": "ENTRY→TRY→CALL:setLogEnabled→CALL:incrementReadOps→RETURN→EXIT",
    "log": ""
  },
  "94a89c12_3": {
    "exec_flow": "<seq>ENTRY -> IF_TRUE: path.length <= 1 -> NEW: ResolveResult<T> -> RETURN -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_4": {
    "exec_flow": "<seq>ENTRY -> IF_FALSE: path.length <= 1 -> IF_TRUE: root.isLink() -> FOR_INIT -> FOR_COND: i < path.length -> FOR_EXIT -> NEW: Path -> CALL: toString -> NEW: ResolveResult<T> -> CALL: getTargetFileSystem -> CALL: getRootLink -> RETURN -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_5": {
    "exec_flow": "<seq>ENTRY -> IF_FALSE: path.length <= 1 -> IF_FALSE: root.isLink() -> CALL: Preconditions.checkState -> CALL: tryResolveInRegexMountpoint -> IF_TRUE: resolveResult != null -> RETURN -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_6": {
    "exec_flow": "<seq>ENTRY -> IF_FALSE: path.length <= 1 -> IF_FALSE: root.isLink() -> CALL: Preconditions.checkState -> CALL: tryResolveInRegexMountpoint -> IF_FALSE: resolveResult != null -> FOR_INIT -> FOR_COND: i < path.length - (resolveLastComponent ? 0 : 1) -> IF_TRUE: nextInode == null -> IF_FALSE: hasFallbackLink() -> FOR_INIT -> FOR_COND: j <= i -> FOR_EXIT -> THROW: (new FileNotFoundException(\"File/Directory does not exist: \" + failedAt.toString())) -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_7": {
    "exec_flow": "<seq>ENTRY -> IF_FALSE: path.length <= 1 -> IF_FALSE: root.isLink() -> CALL: Preconditions.checkState -> CALL: tryResolveInRegexMountpoint -> IF_FALSE: resolveResult != null -> FOR_INIT -> FOR_COND: i < path.length - (resolveLastComponent ? 0 : 1) -> FOR_EXIT -> IF_TRUE: resolveLastComponent -> NEW: ResolveResult<T> -> CALL: getInternalDirFs -> RETURN -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_8": {
    "exec_flow": "<seq>ENTRY -> IF_FALSE: path.length <= 1 -> IF_FALSE: root.isLink() -> CALL: Preconditions.checkState -> CALL: tryResolveInRegexMountpoint -> IF_FALSE: resolveResult != null -> FOR_INIT -> FOR_COND: i < path.length - (resolveLastComponent ? 0 : 1) -> IF_FALSE: nextInode == null -> FOR_EXIT -> IF_FALSE: resolveLastComponent -> NEW: ResolveResult<T> -> RETURN -> EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_9": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_10": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_11": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_12": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_13": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.s3a.S3AFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_14": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_15": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_16": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_17": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_18": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_19": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.HarFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_20": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_21": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_22": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_23": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_24": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.adl.AdlFileSystem:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "94a89c12_25": {
    "exec_flow": "<seq>ENTRY→CALL:org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus→EXIT</seq>",
    "log": "<log></log>"
  },
  "b4e06f28_1": {
    "exec_flow": "ENTRY → IF_FALSE:src.isRoot() → CALL:getFileStatus → IF_TRUE:!src.equals(dst) → FOR → IF_TRUE:commonParent → LOG:DEBUG: It is not allowed to rename a parent directory: [{}] to its subdirectory: [{}] → THROW:IOException → EXIT → LOG:INFO: {} , e.getMessage() → LOG:DEBUG: rename failure → CALL:getExitCode → RETURN → EXIT → CALL:org.apache.hadoop.fs.s3a.S3AFileSystem:rename → TRY → FOR:NflyNode nflyNode → CALL:rename → CATCH:FileNotFoundException → numNotFounds++ → CATCH:Throwable → processThrowable → succ = false → MAY_THROW:MultipleIOException → RETURN:succ → CALL:org.apache.hadoop.hdfs.web.WebHdfsFileSystem:rename → CALL:org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathBooleanRunner:run → PATH_VALIDATION → RENAME_EXECUTION → LOG:DEBUG: Preparing to write atomic rename state to {} → LOG:DEBUG: Destination dst is already existing file, failing the rename. → LOG:DEBUG: Deleting file: {} → LOG:DEBUG: Directory Delete encountered: {} → LOG:DEBUG: Time taken to list {} blobs for delete operation: {} ms",
    "log": "[DEBUG] Rename source path: [{}] to dest path: [{}] [DEBUG] It is not allowed to rename a parent directory: [{}] to its subdirectory: [{}] [INFO] {} [DEBUG] rename failure [DEBUG] Rename path {} to {}, src, dst [DEBUG] Moving srcKey to dstKey [DEBUG] Preparing to write atomic rename state to {} [DEBUG] Destination dst is already existing file, failing the rename. [DEBUG] Deleting file: {} [DEBUG] Directory Delete encountered: {} [DEBUG] Time taken to list {} blobs for delete operation: {} ms [INFO] Attempting to rename from Path {src} to Path {dst} [ERROR] Source path {src} does not exist [ERROR] Destination path {dst} already exists [ERROR] Cannot rename {absoluteSrc} under itself: {absoluteDst} [ERROR] Cannot rename source: {absoluteSrc} to {absoluteDst} - same directory only [INFO] Renamed from {from} to {to} successfully"
  },
  "b4e06f28_2": {
    "exec_flow": "statistics.incrementWriteOps(1) → storageStatistics.incrementOpCounter(OpType.DELETE) → FsPathBooleanRunner.run()",
    "log": "<log> <class>WebHdfsFileSystem</class> <method>delete</method> <level>INFO</level> <template>Deleting path: {path}</template> </log> <log> <class>FsPathBooleanRunner</class> <method>run</method> <level>INFO</level> <template>Execution started for path: {path}</template> </log>"
  },
  "b4e06f28_3": {
    "exec_flow": "ENTRY → LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}] → TRY → CALL: getFileStatus → IF_FALSE: key.compareToIgnoreCase(\"/\") == 0 → LOG: [DEBUG] Delete the file: {} → CALL: createParent → CALL: store.delete → RETURN → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}] [DEBUG] Delete the file: {} [DEBUG] Deleting file: {} [DEBUG] Delete Successful for : {}"
  },
  "b4e06f28_4": {
    "exec_flow": "ENTRY → LOG: [INFO] Attempting to get file status → LOG: [DEBUG] Getting the file status for {f} → LOG: [DEBUG] Path {f} is a folder. → LOG: [DEBUG] Found the path: {f} as a file. → LOG: [DEBUG] Tracking duration and span for path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: getFileStatus → RETURN → EXIT",
    "log": "<log>[INFO] Attempting to get file status</log> <log>[DEBUG] Getting the file status for {f}</log> <log>[DEBUG] Path {f} is a folder.</log> <log>[DEBUG] Found the path: {f} as a file.</log> <log>[DEBUG] Tracking duration and span for path</log>"
  },
  "b4e06f28_5": {
    "exec_flow": "ENTRY → TRY → CALL: openConnection → CALL: setRequestMethod → CALL: org.apache.hadoop.fs.http.client.HttpFSFileSystem:makeQualified → EXCEPTION: setRequestMethod → CATCH: Exception ex → THROW: new IOException(ex) → EXIT",
    "log": "<log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "b4e06f28_6": {
    "exec_flow": "ENTRY → LOG: [DEBUG] AzureBlobFileSystem.getFileStatus path: {f} → CALL: statIncrement → CALL: makeQualified → TRY → CALL: getFileStatus → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {f}</log>"
  },
  "81aa4e56_1": {
    "exec_flow": "<context>org.apache.hadoop.service.CompositeService:start</context> <step>noteFailure</step> <step>info</step>",
    "log": "<log>[DEBUG] noteFailure, exception</log> <log>[INFO] Service {getName()} failed in state {getServiceState()}, exception</log>"
  },
  "a8ad2d59_1": {
    "exec_flow": "ENTRY→TRY→CALL:getRBFMetrics→EXCEPTION:IOException→CALL:debug→RETURN→EXIT→CALL:getNameserviceAggregatedLong→RETURN→EXIT→TRY→CALL:getActiveNamenodeRegistrations→CALL:stream→CALL:map→CALL:collect→CALL:summingLong→RETURN→EXIT",
    "log": "[DEBUG] Failed to get the used capacity [ERROR] Unable to extract metrics: {e.getMessage()}"
  },
  "f58cd58e_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}].</step> <step>TRY</step> <step>CALL: getFileStatus</step> <step>ENTRY</step> <step>IF_FALSE: key.length()==0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE: meta!=null</step> <step>IF_TRUE: meta.isFile()</step> <step>LOG: [DEBUG] Path: [{}] is a file. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:delete</step> <step>RETURN</step> <step>IF_FALSE: key.compareToIgnoreCase(\"/\") == 0</step> <step>IF_TRUE: status.isDirectory()</step> <step>IF_FALSE: !key.endsWith(PATH_DELIMITER)</step> <step>CALL: createParent</step> <step>IF_TRUE: parent != null</step> <step>LOG: [DEBUG] Create parent key: {parentKey}</step> <step>IF_TRUE: !parentKey.equals(PATH_DELIMITER)</step> <step>IF_TRUE: key.length() > 0</step> <step>TRY</step> <step>CALL: storeEmptyFile</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Create parent key: {parentKey}</log_entry>"
  },
  "f58cd58e_2": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}].</step> <step>TRY</step> <step>CALL: getFileStatus</step> <step>ENTRY</step> <step>IF_FALSE: key.length()==0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE: meta.isFile()</step> <step>LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:delete</step> <step>RETURN</step> <step>IF_FALSE: key.compareToIgnoreCase(\"/\") == 0</step> <step>IF_FALSE: status.isDirectory()</step> <step>LOG: [DEBUG] Delete the file: {}</step> <step>CALL: createParent</step> <step>IF_TRUE: parent != null</step> <step>LOG: [DEBUG] Create parent key: {parentKey}</step> <step>IF_TRUE: !parentKey.equals(PATH_DELIMITER)</step> <step>IF_TRUE: key.length() > 0</step> <step>TRY</step> <step>CALL: storeEmptyFile</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] Delete the file: {}</log_entry> <log_entry>[DEBUG] Create parent key: {parentKey}</log_entry>"
  },
  "3dcf3bf8_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "3dcf3bf8_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:isDiskStatsEnabled</step> <step>IF_TRUE:fileIOSamplingPercentage>100</step> <step>LOG:warn</step> <step>EXIT</step>",
    "log": "<log>[WARN] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY + \" value cannot be more than 100. Setting value to 100\"</log>"
  },
  "3dcf3bf8_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:fileIOSamplingPercentage <= 0</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Disabling file IO profiling</log>"
  },
  "3dcf3bf8_4": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE:fileIOSamplingPercentage > 0</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Enabling file IO profiling</log>"
  },
  "5f9c2f7c_1": {
    "exec_flow": "ENTRY → IF_TRUE → CALL:org.slf4j.Logger:debug(java.lang.String) → CALL:setConfig → IF_TRUE (exception == null) → return → SYNC (condition: failureCause == null) → failureCause = exception → failureState = getServiceState → CALL:org.slf4j.Logger:info(java.lang.String, java.lang.Object, java.lang.Object, java.lang.Throwable) → Service: {getName()} entered state {getServiceState()} → CALL: org.slf4j.Logger:warn(java.lang.String, java.lang.Object, java.lang.Throwable)",
    "log": "<log> <level>DEBUG</level> <message>Config has been overridden during init</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log> <log> <level>DEBUG</level> <message>Service: {getName()} entered state {getServiceState()}</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {}</message> <exception>{this}</exception> </log>"
  },
  "dc4d6af2_1": {
    "exec_flow": "ENTRY → NEW:HdfsConfiguration → CALL:get → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Using NN principal: + nameNodePrincipal → CALL:set → CALL:setConf → EXIT → [VIRTUAL_CALL] → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → FOREACH:names → CALL:substituteVars → RETURN → EXIT",
    "log": "<log>[DEBUG] Using NN principal: + nameNodePrincipal</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "dc4d6af2_2": {
    "exec_flow": "ENTRY → CALL:getInt → IF_TRUE:this.maxConcurrentTrackedNodes < 0 → CALL:Configuration:getInt → LOG:LOG.ERROR {} is set to an invalid value, it must be zero or greater. Defaulting to {} → CALL:processConf → EXIT",
    "log": "<log>[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT</log>"
  },
  "24a75896_1": {
    "exec_flow": "ENTRY→LOG: DEBUG: Listing status for {path}→CALL: getFileStatus→IF_FALSE: fileStatus.isDirectory()→LOG: LOG.DEBUG: Adding: rd (not a dir): {path}→CALL: add→CALL: toArray→RETURN→EXIT",
    "log": "[DEBUG] Listing status for {path} [DEBUG] Adding: rd (not a dir): {path}"
  },
  "24a75896_2": {
    "exec_flow": "ENTRY→LOG: DEBUG: Listing status for {path}→CALL: getFileStatus→IF_TRUE: fileStatus.isDirectory()→LOG: LOG.DEBUG: listStatus: doing listObjects for directory {key}→WHILE: objectSummary in objects→LOG: LOG.DEBUG: Adding: fi: {keyPath}→CALL: add→IF_LOG: LOG.isDebugEnabled()→LOG: LOG.DEBUG: listStatus: list truncated - getting next batch→EXIT",
    "log": "[DEBUG] Listing status for {path} [DEBUG] listStatus: doing listObjects for directory {key} [DEBUG] Adding: fi: {keyPath}"
  },
  "7ce58872_1": {
    "exec_flow": "ENTRY→CALL:checkPath→FOREACH:mrNodes→IF_FALSE→IF_TRUE→CALL:getModificationTime→FOREACH:mrNodes→IF_FALSE→TRY→CALL:cloneStatus→CALL:getNflyTmpPath→CALL:copy→CALL:getFs→CALL:delete→CALL:rename→TRY→CALL:setTimes→FINALLY→CALL:makeQualified→CALL:info→FOREACH_EXIT→IF_TRUE:maxMtime>0→FOREACH:mrNodes→FOREACH_EXIT→CALL:sortByDistance→FOREACH:readNodes→TRY→CALL:getFs→CALL:open→RETURN→EXIT",
    "log": "[INFO] f + \" \" + srcNode + \"->\" + dstNode + \": Failed to repair\""
  },
  "7ce58872_2": {
    "exec_flow": "ENTRY→CALL:checkPath→FOREACH:mrNodes→FOREACH_EXIT→IF_TRUE:maxMtime>0→FOREACH:mrNodes→FOREACH_EXIT→CALL:sortByDistance→FOREACH:readNodes→TRY→CALL:getFs→CALL:open→RETURN→EXIT",
    "log": "[INFO] f + \": Failed to open at \" + rNode.getFs().getUri()"
  },
  "7ce58872_3": {
    "exec_flow": "ENTRY→CALL:checkPath→IF_TRUE: client != null→IF_FALSE: !client.isConnected()→CALL: logout→CALL: disconnect→IF_TRUE: !logoutSuccess→LOG: [WARN] Logout failed while disconnecting, error code - →EXIT",
    "log": "[WARN] Logout failed while disconnecting, error code -"
  },
  "085e571e_1": {
    "exec_flow": "ENTRY→CALL:checkToken→IF_TRUE:info==null→CALL:getRealUser→LOG:LOG.WARN:{}, Token={}, err, formatTokenId(identifier)→THROW:new InvalidToken(err)→RETURN→EXIT",
    "log": "[WARN] {}, Token={}"
  },
  "085e571e_2": {
    "exec_flow": "ENTRY→CALL:checkToken→IF_FALSE:info==null→IF_TRUE:info.getRenewDate()<now→CALL:getRealUser→CALL:formatTime→CALL:formatTime→CALL:getRenewDate→CALL:getRenewDate→LOG:LOG.INFO:{}, Token={}, err, formatTokenId(identifier)→THROW:new InvalidToken(err)→RETURN→EXIT",
    "log": "[INFO] {}, Token={}"
  },
  "bdfd811b_1": {
    "exec_flow": "ENTRY→CALL:checkNotEmpty→TRY→CALL:getDoAsUser→CALL:getActualUgi→CALL:doAs→NEW:PrivilegedExceptionAction<HttpURLConnection>→CALL:createAuthenticatedURL→CALL:openConnection→CATCH:SocketTimeoutException→CALL:warn→THROW:SocketTimeoutException→EXIT",
    "log": "[WARN] Failed to connect to {}: {}"
  },
  "bdfd811b_2": {
    "exec_flow": "ENTRY→CALL:checkNotEmpty→TRY→CALL:getDoAsUser→CALL:getActualUgi→CALL:doAs→NEW:PrivilegedExceptionAction<HttpURLConnection>→CALL:createAuthenticatedURL→CALL:openConnection→CATCH:ConnectException→CALL:warn→THROW:IOException→EXIT",
    "log": "[WARN] Failed to connect to: [URL]"
  },
  "bdfd811b_3": {
    "exec_flow": "ENTRY→CALL:checkNotEmpty→TRY→IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}] → CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {} → THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "bdfd811b_4": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: jsonOutput != null→CALL: getOutputStream→CALL: writeJson→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) || conn.getResponseMessage().contains(INVALID_SIGNATURE))) || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW: DelegationTokenAuthenticatedURL.Token→IF_TRUE: authRetryCount > 0→CALL: createConnection→CALL: org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "bdfd811b_5": {
    "exec_flow": "ENTRY→TRY→IF_FALSE: jsonOutput != null→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) || conn.getResponseMessage().contains(INVALID_SIGNATURE))) || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW: DelegationTokenAuthenticatedURL.Token→IF_TRUE: authRetryCount > 0→CALL: createConnection→CALL: org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "bdfd811b_6": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → FOREACH:names → RETURN → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "bdfd811b_7": {
    "exec_flow": "ENTRY → IF_FALSE:providers.length==0 → FOR_INIT → FOR_COND:TRUE → TRY → CALL:call → CATCH:IOException → CALL:warn → CALL:shouldRetry → IF_TRUE:numFailovers>=providers.length-1 → CALL:error → THROW:ioe → EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException: [ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable"
  },
  "45d1594f_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "45d1594f_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "45d1594f_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "45d1594f_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "882d11cf_1": {
    "exec_flow": "ENTRY → IF_TRUE: service != null → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → CALL: recordLifecycleEvent → EXIT",
    "log": "<log> <template>Ignoring re-entrant call to stop()</template> <level>DEBUG</level> </log> <log> <template>Exception while notifying listeners of {}</template> <level>WARN</level> </log> <log> <template>Service {getName()} failed in state {failureState}</template> <level>INFO</level> </log> <log> <template>noteFailure</template> <level>DEBUG</level> </log> <log> <template>Service: {} entered state {}</template> <level>DEBUG</level> </log>"
  },
  "7a1cd1d1_1": {
    "exec_flow": "ENTRY→TRY→LOG:DEBUG AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}→CALL:statIncrement(CALL_APPEND)→LOG:DEBUG openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite→CALL:startTracking→CALL:client.getPathStatus→CALL:perfInfo.registerResult→IF_FALSE:parseIsDirectory(resourceType)→CALL:perfInfo.registerSuccess→IF_TRUE:isAppendBlobKey→CALL:isAppendBlobKey→CALL:maybeCreateLease→NEW:AbfsOutputStream→CALL:populateAbfsOutputStreamContext→CALL:perfInfo.close→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "ab9a1b40_1": {
    "exec_flow": "ENTRY→CALL:this.queueManager.initializeQueues→CALL:updatePlacementRules→EXCEPTION:updatePlacementRules→CATCH:Exception e→THROW:new YarnException(\"Failed to initialize queues\", e)→EXIT",
    "log": "[ERROR] Failed to updatePlacementRules"
  },
  "ab9a1b40_2": {
    "exec_flow": "ENTRY → IF_FALSE: !(scheduler instanceof CapacityScheduler) → CALL: CapacitySchedulerConfiguration:getOverrideWithQueueMappings → LOG: INFO Initialized App Name queue mappings, override: {overrideWithQueueMappings} → CALL: getCapacitySchedulerQueueManager → FOREACH: queueMappings → IF_TRUE: isStaticQueueMapping(mapping) → IF_TRUE: ifQueueDoesNotExist(queue) → IF_TRUE: queueManager.isAmbiguous(mapping.getFullPath()) → THROW: new IOException(\"mapping contains ambiguous leaf queue reference \" + mapping.getFullPath()) → IF_TRUE: newMappings.size() > 0 → LOG: INFO get valid queue mapping from app name config: {newMappings.toString()}, override: {overrideWithQueueMappings} → RETURN → EXIT → LOG: INFO Creating PlacementRule implementation: + ruleClass → CALL: ReflectionUtils:newInstance → CALL: setConfig → EXIT → CALL: getStrings → IF_FALSE → CALL: Iterators.cycle → CALL: next → CALL: getBoolean → IF_FALSE → CALL: initializeBindUsers → CALL: getTrimmed → LOG: DEBUG Usersearch baseDN: {userbaseDN} → CALL: getTrimmed → LOG: DEBUG Groupsearch baseDN: {groupbaseDN} → LOG: DEBUG Handling deprecation for all properties in config... → FOREACH: keys → LOG: DEBUG Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → FOREACH: names → CALL: substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "[INFO] Initialized App Name queue mappings, override: {overrideWithQueueMappings} [INFO] get valid queue mapping from app name config: {newMappings.toString()}, override: {overrideWithQueueMappings} [INFO] Creating PlacementRule implementation: + ruleClass [DEBUG] Usersearch baseDN: {userbaseDN} [DEBUG] Groupsearch baseDN: {groupbaseDN} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ab9a1b40_3": {
    "exec_flow": "ENTRY → CALL: readLock.lock → TRY → CALL: org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:<init> → IF_FALSE: !(scheduler instanceof CapacityScheduler) → CALL: getOverrideWithQueueMappings → LOG: INFO Initialized queue mappings, override: {overrideWithQueueMappings} → CALL: getCapacitySchedulerQueueManager → FOREACH: queueMappings → IF_TRUE: isStaticQueueMapping(mapping) → IF_FALSE: ifQueueDoesNotExist(queue) → CALL: validateAndGetQueueMapping → FOREACH_EXIT → IF_TRUE: newMappings.size() > 0 → CALL: getUserToGroupsMappingService → CALL: getConf → RETURN → CALL: unlock → EXIT",
    "log": "[INFO] Initialized queue mappings, override: {overrideWithQueueMappings}"
  },
  "ab9a1b40_4": {
    "exec_flow": "ENTRY → CALL: readLock.lock → TRY → CALL: org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:<init> → IF_FALSE: !(scheduler instanceof CapacityScheduler) → CALL: getOverrideWithQueueMappings → LOG: INFO Initialized queue mappings, override: {overrideWithQueueMappings} → CALL: getCapacitySchedulerQueueManager → FOREACH: queueMappings → IF_FALSE: isStaticQueueMapping(mapping) → CALL: validateAndGetAutoCreatedQueueMapping → FOREACH_EXIT → IF_TRUE: newMappings.size() > 0 → CALL: getUserToGroupsMappingService → CALL: getConf → RETURN → CALL: unlock → EXIT",
    "log": "[INFO] Initialized queue mappings, override: {overrideWithQueueMappings}"
  },
  "ab9a1b40_5": {
    "exec_flow": "ENTRY → CALL: addSecurityConfiguration → CALL: NEW: HdfsConfiguration → CALL: get → IF_TRUE: LOG.isDebugEnabled() → LOG: DEBUG Using NN principal: + nameNodePrincipal → CALL: set → RETURN → EXIT",
    "log": "[DEBUG] Using NN principal: + nameNodePrincipal"
  },
  "ab9a1b40_6": {
    "exec_flow": "ENTRY → NEW: KeyFieldHelper → CALL: keyFieldHelper.setKeyFieldSeparator → CALL: conf.get(java.lang.String,java.lang.String) → IF_TRUE: conf.get(\"num.key.fields.for.partition\") != null → LOG: WARN Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead → CALL: conf.getInt(java.lang.String,int) → CALL: keyFieldHelper.setKeyFieldSpec → EXIT",
    "log": "[WARN] Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead"
  },
  "604b11c6_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>FOREACH: [DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "604b11c6_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>FOREACH: [DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "f4b778eb_1": {
    "exec_flow": "<seq> ENTRY→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT→EXIT </seq>",
    "log": "[DEBUG] Exception in closing {}"
  },
  "a3ba5996_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "2cc4629f_1": {
    "exec_flow": "ENTRY→CALL:getApplicationAttemptId→CALL:getId→CALL:registerAM→ENTRY→CALL:amRegisterRequest.setHost→CALL:amRegisterRequest.setRpcPort→CALL:amRegisterRequest.setTrackingUrl→CALL:org.apache.hadoop.security.UserGroupInformation:createRemoteUser→CALL:org.apache.hadoop.security.token.Token:decodeIdentifier→CALL:addTokenIdentifier→CALL:doAs→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}]→CATCH:PrivilegedActionException→LOG:LOG.DEBUG:PrivilegedActionException as: {}→THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException→LOG:LOG.INFO:Register the application master for application {}, appId→EXIT",
    "log": "LOG.DEBUG: PrivilegedAction [as: {}][action: {}] LOG.DEBUG: PrivilegedActionException as: {} LOG.INFO: Register the application master for application {}, appId"
  },
  "2cc4629f_2": {
    "exec_flow": "ENTRY→SYNC: Token.class→CALL: getClassForIdentifier→WHILE: tokenIdentifiers.hasNext()→WHILE_COND: tokenIdentifiers.hasNext()→CALL: ServiceLoader.load→CALL: tokenIdentifiers.next→LOG: LOG.DEBUG: Added {id.getKind()}:{id.getClass()} into tokenKindMap→WHILE_EXIT→CALL: get→IF_FALSE: cls == null→CALL:newInstance→CALL:tokenIdentifier.readFields→CALL:close→RETURN→EXIT",
    "log": "LOG.DEBUG: Added {id.getKind()}:{id.getClass()} into tokenKindMap"
  },
  "fe7bf612_1": {
    "exec_flow": "ENTRY→CALL: StreamCapabilitiesPolicy.unbuffer→TRY→IF_FALSE: in instanceof StreamCapabilities && ((StreamCapabilities) in).hasCapability(StreamCapabilities.UNBUFFER)→CALL:org.apache.hadoop.fs.StreamCapabilities:hasCapability(java.lang.String)→LOG:LOG.DEBUG: in.getClass().getName() + : + does not implement StreamCapabilities + and the unbuffer capability→EXIT",
    "log": "[DEBUG] in.getClass().getName() + : + does not implement StreamCapabilities + and the unbuffer capability"
  },
  "e05f51b7_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:setDomainName</step> <step>CALL:substituteVars</step> <step>CALL:getRaw</step> <step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration, se</log>"
  },
  "2e069ec8_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CATCH: Throwable e → LOG.warn(\"Exception while notifying listeners of {}\", this, e) → EXCEPTION: serviceInit → CATCH: Exception e → CALL: noteFailure → SYNC: this → LOG.debug(\"noteFailure\", exception) → IF_TRUE: failureCause == null → ASSIGN: failureCause = exception → ASSIGN: failureState = getServiceState() → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → EXIT_SYNC → LOG.debug(\"noteFailure\", exception) → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → CALL: ServiceOperations.stopQuietly → IF_TRUE: service != null → CALL: stop → CATCH_EXCEPTION → CALL: log.warn → EXIT",
    "log": "<log>[DEBUG] Service: {getName()} entered state {getServiceState()}</log> <log>[DEBUG] Config has been overridden during init</log> <log>LOG.debug(\"noteFailure\", exception);</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>org.apache.hadoop.service.ServiceOperations:stopQuietly - WARN When stopping the service {service_name}</log>"
  },
  "2e069ec8_2": {
    "exec_flow": "ENTRY → IF_TRUE: oldState != newState → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → RETURN → EXIT",
    "log": "<log>[DEBUG] Service: {} entered state {}</log>"
  },
  "9f4e05ac_1": {
    "exec_flow": "<sequence> ENTRY→LOG:INFO:Finalizing upgrade for local dirs. →FOR_INIT→FOR_COND→CALL:org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize→IF_TRUE:!prevDir.exists()→LOG:INFO:Directory + prevDir + does not exist.→LOG:INFO:Finalize upgrade for + sd.getRoot() + is not required.→RETURN→FOR_COND→FOR_EXIT→CALL:editLog.doFinalizeOfSharedLog→CALL:checkSuperuserPrivilege→CALL:checkOperation→CALL:cpLock→CALL:writeLock→TRY→CALL:checkOperation→CALL:getFSImage→CALL:finalizeUpgrade→RETURN→IF_TRUE: (needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs)→CALL: numWriteLockLongHold.increment→IF_TRUE: (longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs)→NEW: LockHeldInfo→CALL: getStackTrace→CALL: currentThread→CALL: currentThread→CALL: record→IF_TRUE: logAction.shouldLog()→NEW: LockHeldInfo→CALL: coarseLock.writeLock().unlock→IF_TRUE: needReport→CALL: addMetric→IF_TRUE: logAction.shouldLog()→CALL: FSNamesystem.LOG.info→FINALLY→CALL:writeUnlock→CALL:cpUnlock→EXIT→CALL:logAuditEvent </sequence>",
    "log": "[INFO] Finalizing upgrade for local dirs. [INFO] Directory + prevDir + does not exist. [INFO] Finalize upgrade for + sd.getRoot() + is not required. [INFO] finalizing upgrade completed by superuser [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()} <log>HdfsAuditLogger.logAuditEvent(succeeded, ugiStr, addr, cmd, src, dst, status, CallerContext.getCurrent(), ugi, dtSecretManager)</log>"
  },
  "9f4e05ac_2": {
    "exec_flow": "<sequence> ENTRY→LOG:INFO:Finalizing upgrade for local dirs. →FOR_INIT→FOR_COND→CALL:org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize→IF_TRUE:!prevDir.exists()→LOG:INFO:Directory + prevDir + does not exist.→LOG:INFO:Finalize upgrade for + sd.getRoot() + is not required.→RETURN→FOR_COND→FOR_EXIT→CALL:editLog.doFinalizeOfSharedLog→CALL:checkSuperuserPrivilege→CALL:checkOperation→CALL:cpLock→CALL:writeLock→TRY→CALL:checkOperation→CALL:getFSImage→CALL:finalizeUpgrade→EXCEPTION→IF_TRUE: (needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs)→CALL: numWriteLockLongHold.increment→IF_TRUE: (longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs)→NEW: LockHeldInfo→CALL: getStackTrace→CALL: currentThread→CALL: currentThread→CALL: record→IF_TRUE: logAction.shouldLog()→NEW: LockHeldInfo→CALL: coarseLock.writeLock().unlock→IF_TRUE: needReport→CALL: addMetric→IF_TRUE: logAction.shouldLog()→CALL: FSNamesystem.LOG.info→FINALLY→CALL:writeUnlock→CALL:cpUnlock→EXIT→CALL:logAuditEvent </sequence>",
    "log": "[ERROR] IOException during finalizing upgrade [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()} <log>HdfsAuditLogger.logAuditEvent(succeeded, ugiStr, addr, cmd, src, dst, status, CallerContext.getCurrent(), ugi, dtSecretManager)</log>"
  },
  "9e8e8131_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception()→CALL: create→RETURN→EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "53ea9de5_1": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→CALL1:super.allocateResource→CALL2:allocateResource→IF_TRUE:!null != rmContainer && rmContainer.getNodeLabelExpression().equals(RMNodeLabelsManager.NO_LABEL) && !nodePartition.equals(RMNodeLabelsManager.NO_LABEL)→IF_TRUE:null == (rmContainers = ignorePartitionExclusivityRMContainers.get(nodePartition))→NEW:TreeSet<>→CALL:ignorePartitionExclusivityRMContainers.put→CALL:rmContainers.add→CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:updateUserResourceUsage→CALL:org.apache.hadoop.yarn.util.resource.Resources:createResource→IF_TRUE:metrics.getUserMetrics(userName) != null→CALL:getHeadroom→CALL:getLimit→CALL:getResourceLimitForActiveUsers→CALL:getLimit→CALL:getResourceLimitForActiveUsers→CALL:setAvailableResourcesToUser→IF_TRUE:LOG.isDebugEnabled→LOG:LOG.DEBUG:getQueuePath() + user= + userName + used= + queueUsage.getUsed(nodePartition) + numContainers= + numContainers + headroom = + application.getHeadroom() + user-resources= + user.getUsed()→CALL:writeLock.unlock→EXIT",
    "log": "[DEBUG] allocateResource log message if debug enabled"
  },
  "53ea9de5_2": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→CALL1:allocateResource→IF_TRUE:!rmContainer.getState().equals(RMContainerState.COMPLETED)→IF_TRUE:rmContainer.getExecutionType() == ExecutionType.GUARANTEED→CALL:writeLock.unlock→EXIT",
    "log": "<!-- No valid log sequence derived, maintained as placeholder -->"
  },
  "f4bbe449_1": {
    "exec_flow": "ENTRY → CALL: getTrimmed → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → CALL: substituteVars → LOG: Handling deprecation for (String)item → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "f4bbe449_2": {
    "exec_flow": "ENTRY -> IF_TRUE: props != null -> CALL: loadResources -> IF_TRUE: overlay != null -> FOREACH: overlay.entrySet() -> IF_TRUE: source != null -> PUT: updatingResource",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for {item}</log_entry>"
  },
  "f4bbe449_3": {
    "exec_flow": "ENTRY -> IF_TRUE: props != null -> CALL: loadResources -> IF_TRUE: loadDefaults && fullReload -> FOREACH: defaultResources -> CALL: loadResource -> FOR_INIT -> FOR_COND: i < resources.size() -> CALL: loadResource -> CALL: addTags -> IF_TRUE: overlay != null -> FOREACH: overlay.entrySet() -> IF_TRUE: source != null -> PUT: updatingResource",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for {item}</log_entry>"
  },
  "f4bbe449_4": {
    "exec_flow": "ENTRY -> IF_TRUE: props != null -> CALL: loadResources -> IF_FALSE: loadDefaults && fullReload -> FOR_INIT -> FOR_COND: i < resources.size() -> CALL: loadResource -> CALL: addTags -> IF_TRUE: overlay != null -> FOREACH: overlay.entrySet() -> IF_TRUE: source != null -> PUT: updatingResource",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for {item}</log_entry>"
  },
  "f4bbe449_5": {
    "exec_flow": "ENTRY → CALL: getTrimmed → LOG: Handling deprecation for all properties in config... → CALL: getProps → LOG: Handling deprecation for (String)item → CALL: addAll → FOREACH: keys → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "a423790c_1": {
    "exec_flow": "ENTRY → TRY → CALL:getFileController → CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogsMeta → IF_TRUE:containersLogMeta.isEmpty() → IF_TRUE:containerIdStr!=null∧nodeId!=null → CALL:println → RETURN → EXIT",
    "log": "<log>[ERROR] The container {containerIdStr} couldn't be found on the node specified: {nodeId}</log>"
  },
  "a423790c_2": {
    "exec_flow": "ENTRY → TRY → CALL:getFileController → CALL:println → RETURN → EXIT",
    "log": "<log>Entry into hasNext()</log> <log>Calling sourceHasNext()</log>"
  },
  "a423790c_3": {
    "exec_flow": "ENTRY→TRY→CALL:getFileController→CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogsMeta→IF_FALSE:containersLogMeta.isEmpty()→FOREACH:containersLogMeta→CALL:println→CALL:println→CALL:printf→CALL:println→FOREACH:logMeta→CALL:printf→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>{containerString} {string_of_equals} Per log file info header {string_of_equals_2} PER_LOG_FILE_INFO_PATTERN_PLACEHOLDER</log>"
  },
  "a423790c_4": {
    "exec_flow": "CALL:println → CALL:RMAuditLogger.logFailure → RETURN → EXIT",
    "log": "<log>[INFO] Error getting UGI </log> <log>RMAuditLogger.logFailure(\"UNKNOWN\", operation, \"UNKNOWN\", \"ClientRMService\", \"Error getting UGI\", applicationId)</log>"
  },
  "6282788b_1": {
    "exec_flow": "ENTRY → CALL:checkOpen → LOG:LOG.DEBUG:{}: masked={} → CALL:DFSOutputStream.newStreamForCreate → CALL:beginFileLease → RETURN → EXIT",
    "log": "[DEBUG] {}: masked={}"
  },
  "8e6266e5_1": {
    "exec_flow": "<step>noteFailure</step> <step>If: (exception == null)</step> <step>LOG: DEBUG - noteFailure</step> <step>Else</step> <step>If: synchronized(this)</step> <step>If: failureCause == null</step> <step>LOG: INFO - Service {getName()} failed in state {failureState}</step>",
    "log": "<log> <level>DEBUG</level> <message>noteFailure</message> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> </log>"
  },
  "8e6266e5_2": {
    "exec_flow": "<step>ServiceOperations.stopQuietly</step> <step>If: (service != null)</step> <step>ServiceOperations.stop</step> <step>LOG: WARN - When stopping the service {serviceName}</step>",
    "log": "<log> <level>WARN</level> <template>When stopping the service {serviceName}</template> </log>"
  },
  "fa6c3460_1": {
    "exec_flow": "ENTRY→CALL:addAttempt→CALL:createAttempt→CALL:setAvataar→IF_TRUE:LOG.isDebugEnabled→LOG:DEBUG:Created attempt + attempt.getID→SWITCH:attempts.size→CASE:default→CALL:put→BREAK→CALL:inProgressAttempts.add→IF_FALSE:failedAttempts.size() > 0 || reschedule→CALL:eventHandler.handle→TRY→IF_TRUE: qSize != 0 && qSize % 1000 == 0 → CALL: org.slf4j.Logger:info→LOG: Size of {getName()} event-queue is {qSize}→IF_TRUE: remCapacity < 1000→CALL: org.slf4j.Logger:info→LOG: Very low remaining capacity on {getName()} event queue: {remCapacity}→CALL: this.eventQueue.put→EXCEPTION: put→CATCH: InterruptedException e→CALL: org.slf4j.Logger:info→LOG: Interrupted. Trying to exit gracefully.→EXIT",
    "log": "<log>[DEBUG] Created attempt attempt.getID</log> <log>[INFO] Size of {getName()} event-queue is {qSize}</log> <log>[INFO] Very low remaining capacity on {getName()} event queue: {remCapacity}</log> <log>[INFO] Interrupted. Trying to exit gracefully.</log>"
  },
  "c0d48ebe_1": {
    "exec_flow": "Parent.ENTRY→[VIRTUAL_CALL]→ENTRY→LOG: Opening file: {f.toString()}→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: meta.isDirectory()→TRY→CALL: retrieve→NEW: FSDataInputStream→NEW: BufferedFSInputStream→NEW: NativeAzureFsInputStream→CALL: getLen→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Opening file: {f.toString()}</log_entry>"
  },
  "c0d48ebe_2": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "1f29fb5e_1": {
    "exec_flow": "ENTRY→IF_TRUE:!isPermissionLoaded()→TRY→CALL:loadPermissionInfoByNativeIO→EXCEPTION:loadPermissionInfoByNativeIO→CATCH:IOException ex→LOG:LOG.DEBUG:Native call failed, ex→IF_TRUE:!isPermissionLoaded()→CALL:loadPermissionInfoByNonNativeIO→EXIT",
    "log": "[DEBUG] Native call failed"
  },
  "26155c4c_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE: props != null → CALL:loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "26155c4c_2": {
    "exec_flow": "ENTRY → CALL:FailoverController.getRpcTimeoutToNewActive → LOG:Failover to HAServiceTarget successful → RETURN:0 → EXIT",
    "log": "[INFO] Failover to HAServiceTarget successful"
  },
  "26155c4c_3": {
    "exec_flow": "ENTRY → CALL:FailoverController.getRpcTimeoutToNewActive → LOG:Failover failed: ServiceFailedException → RETURN:-1 → EXIT",
    "log": "[ERROR] Failover failed: ServiceFailedException"
  },
  "8ef2a333_1": {
    "exec_flow": "WebHdfsInputStream.ENTRY→[VIRTUAL_CALL]→HdfsKMSUtil.getCryptoProtocolVersion→CALL:getCryptoCodec→CALL:decryptEncryptedDataEncryptionKey→CALL:decryptEncryptedKey→CALL:org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey→RETURN→NEW:CryptoInputStream→CALL:getMaterial→CALL:getIV→RETURN→EXIT →IF_TRUE:configName.equals(CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_AES_CTR_NOPADDING_KEY)→CALL:get(Configuration, String)→ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:getRaw→ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→RETURN→EXIT→IF_FALSE:codecString==null→FOREACH:Splitter.on(',').trimResults().omitEmptyStrings().split(codecString)→TRY→CALL:conf.getClassByName→CATCH(ClassNotFoundException)→CALL:PerformanceAdvisory.LOG.debug→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Crypto codec {} not found. [DEBUG] Decryption process started [INFO] DefaultCryptoExtension Decryption successful </log>"
  },
  "96962f64_1": {
    "exec_flow": "<sequence> ENTRY → CALL:rpcServer.checkOperation → IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.startOp → IF_TRUE:LOG.isDebugEnabled() → CALL:org.slf4j.Logger:isDebugEnabled() → LOG:DEBUG: Proxying operation: {}, methodName → CALL:rpcServer.getLocationsForPath → IF_FALSE:rpcServer.isPathAll(src) → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getFileInfo → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeSequential → IF_TRUE:ret == null → IF_TRUE:children != null && !children.isEmpty() → CALL:getMountPointDates → CALL:get → CALL:getMountPointStatus → [VIRTUAL_CALL] ENTRY→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser→CALL:ensureInitialized→IF_FALSE: (subject == null || subject.getPrincipals(User.class).isEmpty())→CALL: getLoginUser→RETURN→EXIT→CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod→FOREACH: locations→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod→CATCH→CALL:org.slf4j.Logger:error→FOREACH_EXIT→IF_TRUE: !thrownExceptions.isEmpty()→FOR_INIT→FOR_COND: i < thrownExceptions.size()→IF_TRUE: isUnavailableException(ioe)→THROW: ioe→EXIT </sequence>",
    "log": "<log_statement><![CDATA[DEBUG: Proxying operation: {}]]></log_statement> <log_statement><![CDATA[ERROR: Cannot get mount point: {e.getMessage()}]]></log_statement> <log_statement><![CDATA[LOG: getLoginUser]]></log_statement> <log_statement><![CDATA[DEBUG: Reading credentials from location {}]]></log_statement> <log_statement><![CDATA[DEBUG: Loaded {} tokens from {}]]></log_statement> <log_statement><![CDATA[INFO: Token file {} does not exist]]></log_statement> <log_statement><![CDATA[DEBUG: Failure to load login credentials]]></log_statement> <log_statement><![CDATA[DEBUG: Failed to get groups for user {}]]></log_statement> <log_statement><![CDATA[ERROR: Unexpected exception {} proxying {} to {}]]></log_statement>"
  },
  "96962f64_2": {
    "exec_flow": "ENTRY→IF_TRUE: subclusterResolver instanceof MountTableResolver→TRY→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→RETURN→EXIT",
    "log": "<log_statement><![CDATA[ERROR: Cannot get mount point]]></log_statement>"
  },
  "62ed74ed_1": {
    "exec_flow": "ENTRY→CALL: ensureInitialized→TRY→CALL: getGroups→CALL: getShortUserName→CALL: getShortUserName→CALL: org.slf4j.Logger:debug→RETURN→EXIT",
    "log": "[DEBUG] Failed to get groups for user {}"
  },
  "3c26f2cb_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {scheme} → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {property} → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \" \"+\" \"\\\"\" + scheme + \"\\\"\") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {uri}: {e.toString()} → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Bypassing cache to create filesystem {uri}</message> </log> <log> <level>DEBUG</level> <message>Looking for FS supporting {scheme}</message> </log> <log> <level>DEBUG</level> <message>looking for configuration option {property}</message> </log> <log> <level>DEBUG</level> <message>Looking in service filesystems for implementation class</message> </log> <log> <level>WARN</level> <message>Failed to initialize filesystem {uri}: {e.toString()}</message> </log> <log> <level>DEBUG</level> <message>Failed to initialize filesystem</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "3c26f2cb_2": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE: fs != null → LOG: LOGGER.DEBUG(\"Filesystem {uri} created while awaiting semaphore\") → RETURN → EXIT → IF(fs != null) → SYNC → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.DEBUG(\"Duplicate FS created for {uri}; discarding {fs}\") → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Filesystem {uri} created while awaiting semaphore</message> </log> <log> <level>DEBUG</level> <message>Duplicate FS created for {uri}; discarding {fs}</message> </log>"
  },
  "3c26f2cb_3": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {scheme} → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {property} → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \" \"+\" \"\\\"\" + scheme + \"\\\"\") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {uri}: {e.toString()} → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Bypassing cache to create filesystem {uri}</message> </log> <log> <level>DEBUG</level> <message>Looking for FS supporting {scheme}</message> </log> <log> <level>DEBUG</level> <message>looking for configuration option {property}</message> </log> <log> <level>DEBUG</level> <message>Looking in service filesystems for implementation class</message> </log> <log> <level>WARN</level> <message>Failed to initialize filesystem {uri}: {e.toString()}</message> </log> <log> <level>DEBUG</level> <message>Failed to initialize filesystem</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "a6ce11d1_1": {
    "exec_flow": "ENTRY→CALL:get→LOG: LOG.INFO: Using store: + timelineReaderClassName→TRY→IF_TRUE: TimelineReader.class.isAssignableFrom(timelineReaderClazz)→CALL: ReflectionUtils.newInstance→RETURN→EXIT",
    "log": "[INFO] Using store: + timelineReaderClassName"
  },
  "a6ce11d1_2": {
    "exec_flow": "ENTRY→CALL:get→LOG: LOG.INFO: Using store: + timelineReaderClassName→TRY→THROW: new YarnRuntimeException→EXIT",
    "log": "[INFO] Using store: + timelineReaderClassName"
  },
  "a6ce11d1_3": {
    "exec_flow": "ENTRY → CALL:addSecurityConfiguration → ENTRY → NEW:HdfsConfiguration → CALL:get → IF_TRUE: LOG.isDebugEnabled() → LOG:DEBUG Using NN principal: + nameNodePrincipal → CALL:set → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Using NN principal: + nameNodePrincipal"
  },
  "a6ce11d1_4": {
    "exec_flow": "ENTRY → CALL:getLongBytes → CALL:getFloat → CALL:org.slf4j.Logger:info → IF_TRUE: balancedPreferencePercent > 1.0 → CALL:org.slf4j.Logger:warn → IF_TRUE: balancedPreferencePercent < 0.5 → CALL:org.slf4j.Logger:warn → EXIT",
    "log": "[INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} [WARN] The value of {preference_fraction_key} is greater than 1.0 but should be in the range 0.0 - 1.0 [WARN] The value of {preference_fraction_key} is less than 0.5 so volumes with less available disk space will receive more block allocations"
  },
  "a6ce11d1_5": {
    "exec_flow": "ENTRY → CALL:getLongBytes → CALL:getFloat → CALL:org.slf4j.Logger:info → IF_FALSE: balancedPreferencePercent > 1.0 → IF_TRUE: balancedPreferencePercent < 0.5 → CALL:org.slf4j.Logger:warn → EXIT",
    "log": "[INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} [WARN] The value of {preference_fraction_key} is less than 0.5 so volumes with less available disk space will receive more block allocations"
  },
  "cbcd0db6_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length() == 0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta != null → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → RETURN → EXIT",
    "log": "<log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}].</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log> <log>[DEBUG] List objects. prefix: [{}], delimiter: [{}], maxListLength: [{}], priorLastKey: [{}].</log>"
  },
  "cbcd0db6_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length() == 0 → IF_TRUE:meta == null && !key.endsWith(\"/\") → CALL:getObjectMetadata → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT",
    "log": "<log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "cbcd0db6_3": {
    "exec_flow": "ENTRY → VIRTUAL_CALL → org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus → org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal → CALL:getFileStatus → ENTRY → CALL:getIsNamespaceEnabled → RETURN → EXIT",
    "log": "<log>[DEBUG] Getting the file status for {f}</log> <log>[DEBUG] Found the path: {f} as a file.</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "cbcd0db6_4": {
    "exec_flow": "<step> ENTRY → LOG:LOG.DEBUG:List status for path: {path} → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Adding: rd (not a dir): {path} → CALL:add → CALL:toArray → CALL:size → CALL:size → RETURN → EXIT </step>",
    "log": "<log>[DEBUG] List status for path: {path}</log> <log>[DEBUG] Adding: rd (not a dir): {path}</log>"
  },
  "cbcd0db6_5": {
    "exec_flow": "<step> ENTRY → LOG:LOG.DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom} → IF_TRUE:continuation == null || continuation.isEmpty() → IF_TRUE:startFrom != null && !startFrom.isEmpty() → CALL:getIsNamespaceEnabled → CALL:generateContinuationTokenForXns → DO_WHILE → TRY → CALL:startTracking → CALL:client.listPath → CALL:getResult → CALL:op.getResult → IF_TRUE:retrievedSchema == null → THROW:new AbfsRestOperationException → EXIT </step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "cbcd0db6_6": {
    "exec_flow": "<step> ENTRY → LOG:LOG.DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom} → IF_TRUE:continuation == null || continuation.isEmpty() → IF_FALSE:startFrom != null && !startFrom.isEmpty() → DO_WHILE → TRY → CALL:startTracking → CALL:client.listPath → CALL:getResult → CALL:op.getResult → IF_TRUE:retrievedSchema == null → THROW:new AbfsRestOperationException → EXIT </step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "cbcd0db6_7": {
    "exec_flow": "<step> ENTRY → LOG:LOG.DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom} → IF_FALSE:continuation == null || continuation.isEmpty() → DO_WHILE → TRY → CALL:startTracking → CALL:client.listPath → CALL:getResult → CALL:op.getResult → IF_TRUE:retrievedSchema == null → THROW:new AbfsRestOperationException → EXIT </step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "552d26ba_1": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:args</step> <step>TRY</step> <step>CALL:expandArgument</step> <step>Invoke PathData.expandAsGlob(arg, getConf())</step> <step>Check if items.length == 0</step> <step>Throw PathNotFoundException if true</step> <step>CATCH:IOException</step> <step>CALL:displayError</step> <step>CALL:displayWarning</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object)</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "552d26ba_2": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:args</step> <step>TRY</step> <step>CALL:expandArgument</step> <step>Invoke PathData.expandAsGlob(arg, getConf())</step> <step>Check if items.length == 0</step> <step>Throw PathNotFoundException if true</step> <step>CATCH:IOException</step> <step>CALL:displayError</step> <step>CALL:displayWarning</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String)</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Displaying error: message"
  },
  "9f7a2fbf_1": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→CALL:rpcServer.getLocationsForPath→FOREACH:results→IF_TRUE:result.hasException()→IF_TRUE:ioe instanceof FileNotFoundException→IF_FALSE:!allowPartialList→THROW:ioe→EXIT→FOREACH_EXIT→CALL:subclusterResolver.getMountPoints→IF_TRUE:children != null→FOREACH:children→CALL:getContentSummary→IF_TRUE:mountSummary != null→APPEND:summaries→FOREACH_EXIT→IF_TRUE:summaries.isEmpty() && notFoundException != null→THROW:notFoundException→EXIT→IF_FALSE:summaries.isEmpty() && notFoundException != null→CALL:aggregateContentSummary→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Cannot get content summary for mount {}: {}"
  },
  "0fa125c3_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CATCH:UnresolvedLinkException→CALL:qualifySymlinkTarget→TRY→FOR_COND: isLink→FOR_EXIT→RETURN→EXIT",
    "log": "[INFO] Resolving file system path [WARN] Unresolved link encountered"
  },
  "0fa125c3_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileContext:listStatus→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CATCH:UnresolvedLinkException→CALL:qualifySymlinkTarget→TRY→FOR_COND: isLink→FOR_EXIT→WHILE:files.hasNext→CALL:org.apache.hadoop.fs.RemoteIterator:hasNext→WHILE_COND:files.hasNext→[VIRTUAL_CALL]→CALL:org.apache.hadoop.fs.RemoteIterator:next→CALL:org.apache.hadoop.fs.FileContext:open→TRY→CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:deleteFileWithRetries→FINALLY→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object)→FOREACH_EXIT→WHILE_EXIT→RETURN→EXIT",
    "log": "[INFO] Resolving file system path [WARN] Unresolved link encountered [DEBUG] Attempting to load UUID from log file [DEBUG] Next element retrieved from RemoteIterator [INFO] Cleaning up resources [DEBUG] Exception in closing {}"
  },
  "0fa125c3_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.RemoteIterator:next→RETURN→EXIT",
    "log": "[DEBUG] Next element retrieved from RemoteIterator"
  },
  "0fa125c3_4": {
    "exec_flow": "ENTRY→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH(IOException)→CALL:org.slf4j.Logger:info→IF:retry<=fsNumRetries→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:Thread.sleep→LOOP→EXIT",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Retrying operation on FS. Retry no. x"
  },
  "0fa125c3_5": {
    "exec_flow": "ENTRY→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH(IOException)→CALL:org.slf4j.Logger:info→IF:retry>fsNumRetries→CALL:org.slf4j.Logger:info(java.lang.String, java.lang.Throwable)→THROW:e→EXIT",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Maxed out FS retries. Giving up!"
  },
  "1b665f48_1": {
    "exec_flow": "ENTRY→WHILE: !stopped.get() && !Thread.currentThread().isInterrupted()→TRY→CALL:Thread.sleep→TRY→CALL:heartbeat→EXCEPTION:heartbeat→CATCH:RMContainerAllocationException e→LOG:LOG.ERROR:Error communicating with RM: + e.getMessage(), e→RETURN→EXIT",
    "log": "[ERROR] Error communicating with RM: + e.getMessage(), e"
  },
  "1b665f48_2": {
    "exec_flow": "ENTRY→WHILE: !stopped.get() && !Thread.currentThread().isInterrupted()→TRY→CALL:Thread.sleep→TRY→CALL:heartbeat→EXCEPTION:heartbeat→LOG:LOG.ERROR:ERROR IN CONTACTING RM. →CONTINUE→CALL:getTime→CALL:getClock→CALL:executeHeartbeatCallbacks→EXCEPTION:executeHeartbeatCallbacks→CATCH:InterruptedException e→IF_TRUE:!stopped.get()→LOG:LOG.WARN:Allocated thread interrupted. Returning.→RETURN→EXIT",
    "log": "[ERROR] ERROR IN CONTACTING RM. [WARN] Allocated thread interrupted. Returning."
  },
  "851818bb_1": {
    "exec_flow": "ENTRY → IF_FALSE: ch != null → TRY → CALL: toByteArray → IF_FALSE: data != null && data.length != 0 → TRY → CALL: getChannel → IF_TRUE: ch == null → CALL: IOUtils.closeStream → ENTRY → IF_TRUE: stream != null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT → EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "0cb5b33c_1": {
    "exec_flow": "ENTRY→CALL:runWithRetries→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH:IOException→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)→IF:retry > fsNumRetries→LOG:org.slf4j.Logger:info(\"Maxed out FS retries. Giving up!\")→THROW:IOException→EXIT",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Maxed out FS retries. Giving up!"
  },
  "0cb5b33c_2": {
    "exec_flow": "ENTRY→CALL:runWithRetries→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH:IOException→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)→IF:retry <= fsNumRetries→LOG:org.slf4j.Logger:info(\"Retrying operation on FS. Retry no. \" + retry)→CALL:Thread.sleep→WHILE_COND:true",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Retrying operation on FS. Retry no. X"
  },
  "4a16decd_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapred.lib.db.DBOutputFormat$DBRecordWriter:close→CALL:org.slf4j.Logger:warn(java.lang.String)→EXIT",
    "log": "[WARN] {warning message based on warn method usage}"
  },
  "c4d818f2_1": {
    "exec_flow": "<context>org.apache.hadoop.service.AbstractService:init</context> <entry>Parent.ENTRY</entry> <flow>IF_FALSE: conf == null</flow> <flow>IF_FALSE: isInState(STATE.INITED)</flow> <sync>SYNC: stateChangeLock</sync> <flow>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</flow> <call>CALL: setConfig</call> <try>TRY</try> <call>CALL: serviceInit</call> <flow>IF_TRUE: isInState(STATE.INITED)</flow> <call>CALL: notifyListeners</call> <exit>EXIT</exit> <entry>VIRTUAL_CALL</entry> <flow>IF_TRUE:service != null</flow> <call>CALL:stop</call> <exit>EXIT</exit>",
    "log": "<log_entry> <log> <level>DEBUG</level> <template>Service: {} entered state {}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>When stopping the service {service_name}</template> </log> </log_entry>"
  },
  "c4d818f2_2": {
    "exec_flow": "<context>org.apache.hadoop.service.AbstractService:init</context> <entry>Parent.ENTRY</entry> <flow>IF_FALSE: conf == null</flow> <flow>IF_FALSE: isInState(STATE.INITED)</flow> <sync>SYNC: stateChangeLock</sync> <flow>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</flow> <call>CALL: setConfig</call> <try>TRY</try> <call>CALL: serviceInit</call> <exception>EXCEPTION: serviceInit</exception> <catch>CATCH: Exception e</catch> <call>CALL: noteFailure</call> <call_chain> <entry>VIRTUAL_CALL</entry> <flow>Child.call</flow> </call_chain> <call>CALL: ServiceOperations.stopQuietly</call> <throw>THROW: ServiceStateException.convert(e)</throw> <exit>EXIT</exit>",
    "log": "<log_entry> <log> <level>DEBUG</level> <template>Service: {} entered state {}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>When stopping the service {service_name}</template> </log> </log_entry>"
  },
  "c4d818f2_3": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log_entry> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log> </log_entry>"
  },
  "7ec9ec4e_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: i < urls.length→FOR_EXIT→FOR_INIT→FOR_COND: TRUE→IF_TRUE: index >= urls.length→IF_FALSE: indexOfLocalUrl != -1 && retry == 1→TRY→CALL: getHttpRequest→CALL: httpRequest.setHeader→CALL: client.execute→CALL: response.getStatusLine→CALL: new WasbRemoteCallException→CATCH: IOException→CALL: org.slf4j.Logger:debug→CALL: org.apache.hadoop.fs.azure.WasbRemoteCallHelper:shouldRetry(ioe,retry,url)→IF_FALSE: ioe instanceof WasbRemoteCallException && ioe.getMessage().equals(authenticationExceptionMessage)→TRY→CALL: org.apache.hadoop.io.retry.RetryPolicy:shouldRetry→IF_TRUE: isRetry || isFailoverAndRetry→LOG: LOG.DEBUG: Retrying connect to Remote service:{} Already tried {} time(s); retry policy is {}, delay {}ms.→CALL: Thread.sleep→RETURN→EXIT→IF_FALSE: isRetry || isFailoverAndRetry→LOG: LOG.DEBUG: Not retrying anymore, already retried the urls {} time(s)→THROW: new WasbRemoteCallException(url + \":\" + \"Encountered IOException while making remote call\", ioe)→EXIT",
    "log": "[DEBUG] {exceptionMessage} [ERROR] Encountered error while making remote call to {urls} retried {retry} time(s). [DEBUG] Retrying connect to Remote service:{} Already tried {} time(s); retry policy is {}, delay {}ms. [DEBUG] Not retrying anymore, already retried the urls {} time(s)"
  },
  "4c5987c8_1": {
    "exec_flow": "ENTRY→IF_TRUE:conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)→CALL:createTimelineClient→CALL:timelineClient.init→CALL:timelineClient.start→TRY→CALL:setId→CALL:setReaders→CALL:setWriters→CALL:timelineClient.putDomain→LOG:LOG.INFO:Put the timeline domain: TimelineUtils.dumpTimelineRecordtoJSON(domain)→CALL:org.apache.hadoop.yarn.client.api.TimelineClient:stop→FOR_INIT→FOR_COND:i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop→IF_FALSE:isInState(STATE.STOPPED)→SYNC:stateChangeLock→IF_FALSE:enterState(STATE.STOPPED) != STATE.STOPPED→LOG:LOG.DEBUG:Ignoring re-entrant call to stop()→CALL:notifyListeners→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH:Throwable e→LOG:LOG.WARN:Exception while notifying listeners of {}→FOR_EXIT→IF_TRUE:firstException != null→THROW:ServiceStateException.convert(firstException)→EXIT",
    "log": "[INFO] Put the timeline domain: TimelineUtils.dumpTimelineRecordtoJSON(domain) [DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "4c5987c8_2": {
    "exec_flow": "ENTRY→IF_TRUE:conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)→CALL:createTimelineClient→CALL:timelineClient.init→CALL:timelineClient.start→TRY→CALL:setId→EXCEPTION:setId→CATCH:Exception e→LOG:LOG.ERROR:Error when putting the timeline domain, e→CALL:org.apache.hadoop.yarn.client.api.TimelineClient:stop→FOR_INIT→FOR_COND:i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop→IF_FALSE:isInState(STATE.STOPPED)→SYNC:stateChangeLock→IF_TRUE:enterState(STATE.STOPPED) != STATE.STOPPED→CALL:org.apache.hadoop.service.AbstractService:enterState→IF_TRUE:oldState != newState→LOG:LOG.DEBUG:Service: {} entered state {}→CALL:recordLifecycleEvent→TRY→CALL:serviceStop→EXCEPTION:serviceStop→CATCH:Exception e→LOG:LOG.INFO:Service {} failed in state {} (exception details)→CALL:noteFailure→THROW:ServiceStateException.convert(e)→FOR_EXIT→IF_TRUE:firstException != null→THROW:ServiceStateException.convert(firstException)→EXIT",
    "log": "[ERROR] Error when putting the timeline domain [DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {} (exception details)"
  },
  "4c5987c8_3": {
    "exec_flow": "ENTRY→IF_FALSE:conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)→LOG:LOG.WARN:Cannot put the domain + domainId + because the timeline service is not enabled→RETURN→EXIT",
    "log": "[WARN] Cannot put the domain + domainId + because the timeline service is not enabled"
  },
  "79eb455a_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: App-level real-time aggregating→IF_FALSE: !isReadyToAggregate()→TRY→IF_FALSE: aggregationGroups == null || aggregationGroups.isEmpty()→CALL: addEntity→CALL: putEntitiesAsync→EXCEPTION: putEntitiesAsync→CATCH: Exception e→LOG: LOG.ERROR: Error aggregating timeline metrics, e→LOG: LOG.DEBUG: App-level real-time aggregation complete→EXIT",
    "log": "[DEBUG] App-level real-time aggregating [ERROR] Error aggregating timeline metrics [DEBUG] App-level real-time aggregation complete"
  },
  "79eb455a_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: App-level real-time aggregating→IF_FALSE: !isReadyToAggregate()→TRY→IF_FALSE: aggregationGroups == null || aggregationGroups.isEmpty()→CALL: addEntity→CALL: putEntitiesAsync→LOG: LOG.DEBUG: App-level real-time aggregation complete→EXIT",
    "log": "[DEBUG] App-level real-time aggregating [DEBUG] App-level real-time aggregation complete"
  },
  "79eb455a_3": {
    "exec_flow": "ENTRY→CALL: appAggregationExecutor.shutdown→IF_TRUE: !appAggregationExecutor.awaitTermination(10, TimeUnit.SECONDS)→LOG: LOG.INFO: App-level aggregator shutdown timed out, shutdown now.→CALL: appAggregationExecutor.shutdownNow→CALL: appAggregator.aggregate→CALL: serviceStop→EXIT",
    "log": "[INFO] App-level aggregator shutdown timed out, shutdown now."
  },
  "79eb455a_4": {
    "exec_flow": "ENTRY→CALL:cancelRenewalOrRegenerationFutureForApp→CALL:serviceStop→CALL:org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector:serviceStop()→CALL:shutdownNow→CALL:org.apache.hadoop.service.CompositeService:serviceStop()→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: getName() + \": stopping services, size=\" + numOfServicesToStop→CALL: stop→CALL: serviceStop→FOR_INIT→FOR_COND: i >= 0→CALL: org.slf4j.Logger:isDebugEnabled()→IF_TRUE: LOG.debug(\"Stopping service #\" + i + \": \" + service)→CALL: org.apache.hadoop.service.ServiceOperations:stopQuietly(LOG, service)→FOR_EXIT→IF_TRUE: firstException != null→THROW: ServiceStateException.convert(firstException)→EXIT",
    "log": "[DEBUG] getName() + \": stopping services, size=\" + numOfServicesToStop [DEBUG] Stopping service # i: service"
  },
  "5810f774_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner:setConf(org.apache.hadoop.conf.Configuration) →CALL:org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver:setConf(org.apache.hadoop.conf.Configuration) →CALL:org.apache.hadoop.mapreduce.v2.hs.webapp.MapReduceTrackingUriPlugin:setConf(org.apache.hadoop.conf.Configuration) →CALL:org.apache.hadoop.hdfs.qjournal.server.JournalNode:setConf(org.apache.hadoop.conf.Configuration)→EXIT",
    "log": "<!-- No logs in the child paths -->"
  },
  "5810f774_2": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration →CALL:getRaw→LOG:Handling deprecation for all properties in config... →CALL:getProps→FOREACH:keys→LOG:Handling deprecation for (String)item →CALL:handleDeprecation→FOREACH_EXIT →CALL:LOG_DEPRECATION.info →FOREACH:names→CALL:getProps→FOREACH_EXIT →RETURN→EXIT",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3bad7412_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead. → TRY → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → LOG:LOGGER.DEBUG: Looking for FS supporting {} → TRY → CALL:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead. [DEBUG] Looking for FS supporting {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {}"
  },
  "3bad7412_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {}, uri → CALL: createFileSystem → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "3bad7412_3": {
    "exec_flow": "ENTRY→SYNC:this→CALL:get→IF_FALSE:fs != null→TRY→CALL:creatorPermits.acquireUninterruptibly→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→SYNC:this→CALL:get→IF_TRUE:fs != null→LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri)→CALL:createFileSystem→CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)→SYNC:this→IF_TRUE:map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress()→CALL:ShutdownHookManager.get().addShutdownHook→CALL:org.apache.hadoop.conf.Configuration:getBoolean→LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs)→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose)→RETURN→EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore [DEBUG] Duplicate FS created for {}; discarding {} [DEBUG] Exception in closing {}"
  },
  "6772b2ca_1": {
    "exec_flow": "<step>ENTRY</step> <!-- The exec_flow of parent node --> <step>CALL: INSTANCE</step> <!-- Merge with the exec_flow of child node --> <step>CALL: namedCallbacks.put</step> <step>CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder</step> <step>CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource</step> <step>CALL:checkNotNull</step> <step>CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init></step> <step>CALL:put</step> <step>CALL:start</step> <step>IF_TRUE: startMBeans</step> <step>CALL: startMBeans</step> <step>CALL: org.slf4j.Logger:debug</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Registering the metrics source</log> <log>[DEBUG] Registered source + name</log>"
  },
  "43283c28_1": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {}"
  },
  "43283c28_2": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "43283c28_3": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:subject==null||subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→ENTRY→CALL:ensureInitialized→IF_FALSE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT→RETURN→EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {}"
  },
  "a270f959_1": {
    "exec_flow": "ENTRY → TRY → LOG: Getting the file status for {filePath} → CALL: getFileStatusInternal → IF_TRUE: meta != null → IF_TRUE: meta.isDirectory() → LOG: Path {} is a folder. → CALL: conditionalRedoFolderRename[ENTRY → IF_FALSE: f.getName().equals(\"\") → IF_FALSE: existsInternal(absoluteRenamePendingFile) → RETURN → EXIT] → RETURN → EXIT → IF_FALSE: meta.isDirectory() → LOG: Found the path: {} as a file. → CALL: updateFileStatusPath → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Getting the file status for {filePath} [DEBUG] Path {} is a folder. [DEBUG] Found the path: {} as a file."
  },
  "a270f959_2": {
    "exec_flow": "ENTRY → CALL:verifyDriverReady → IF_FALSE:records.isEmpty() → FOREACH:records → CALL:getPathForClass → CALL:exists → IF_TRUE:exists(recordPath) → IF_FALSE:allowUpdate → IF_TRUE:errorIfExists → LOG:LOG.ERROR:Attempt to insert record {} that already exists → CALL:addFailure → RETURN → EXIT",
    "log": "[ERROR] Attempt to insert record {} that already exists"
  },
  "a270f959_3": {
    "exec_flow": "ENTRY → CALL:verifyDriverReady → IF_FALSE:records.isEmpty() → FOREACH:records → CALL:getPathForClass → CALL:exists → IF_TRUE:exists(recordPath) → IF_FALSE:allowUpdate → IF_TRUE:errorIfExists → LOG:LOG.ERROR:Attempt to insert record {} that already exists → RETURN → EXIT",
    "log": "[ERROR] Attempt to insert record {} that already exists"
  },
  "a270f959_4": {
    "exec_flow": "ENTRY → CALL:verifyDriverReady → IF_FALSE:records.isEmpty() → FOREACH:records → CALL:getPathForClass → CALL:exists → IF_FALSE:exists(recordPath) → CALL:getWriter → TRY → CALL:serializeString → CATCH → LOG:LOG.ERROR:Cannot write {} → CALL:rename → RETURN → EXIT",
    "log": "[ERROR] Cannot write {}"
  },
  "a270f959_5": {
    "exec_flow": "ENTRY → IF_TRUE: client != null → IF_FALSE: !client.isConnected() → CALL: logout → CALL: disconnect → IF_TRUE: !logoutSuccess → LOG: LOG.WARN: Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "[WARN] Logout failed while disconnecting, error code -"
  },
  "a270f959_6": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>ENTRY</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG:DEBUG:Cannot rename the root of a filesystem</step> <step>IF_TRUE:src.isRoot()</step> <step>LOG:DEBUG:Cannot rename the root directory of a filesystem</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Cannot rename the root of a filesystem [DEBUG] Cannot rename the root directory of a filesystem"
  },
  "d44f4cdd_1": {
    "exec_flow": "ENTRY -> TRY -> IF_TRUE: isJobCompletionEvent(event.getHistoryEvent()) -> CALL: eventQueue.put -> IF_TRUE: handleTimelineEvent -> CALL: atsEventDispatcher.getEventHandler().handle -> CALL: org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event) -> EXIT",
    "log": "<!-- No logs in this path -->"
  },
  "d44f4cdd_2": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: isJobCompletionEvent(event.getHistoryEvent()) -> CALL: eventQueue.put -> IF_TRUE: handleTimelineEvent -> CALL: atsEventDispatcher.getEventHandler().handle -> CALL: org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event) -> EXIT",
    "log": "<!-- No logs in this path -->"
  },
  "d44f4cdd_3": {
    "exec_flow": "ENTRY→WHILE: iterator.hasNext()→WHILE_COND: iterator.hasNext()→CALL:iterator.next→CALL:iterator.next.getType→CALL:counterMap.containsKey→CALL:counterMap.put→CALL:counterMap.get→CALL:counterMap.put→WHILE_EXIT→FOREACH: counterMap.entrySet()→CALL:Map.Entry.getValue→CALL:Map.Entry.getKey→CALL:org.slf4j.Logger:info→FOREACH_EXIT→EXIT",
    "log": "[INFO] Event type: {entry.getKey()}, Event record counter: {num}"
  },
  "3d8baa06_1": {
    "exec_flow": "ENTRY → FOREACH: diags → FOREACH_EXIT → IF_TRUE: diagnostics.length() > diagnosticsMaxSize → CALL: delete → TRY → ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: LOG.DEBUG: storeContainerDiagnostics: containerId={}, diagnostics=, containerId, diagnostics → TRY → CALL: put → EXCEPTION: put → CATCH: DBException e → CALL: markStoreUnHealthy → THROW: new IOException(e) → EXIT",
    "log": "<log>[WARN] Unable to update diagnostics in state store for [containerId], e</log> <log>[DEBUG] storeContainerDiagnostics: containerId={}, diagnostics=</log>"
  },
  "3d8baa06_2": {
    "exec_flow": "ENTRY → FOREACH: diags → FOREACH_EXIT → IF_FALSE: diagnostics.length() > diagnosticsMaxSize → TRY → ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: LOG.DEBUG: storeContainerDiagnostics: containerId={}, diagnostics=, containerId, diagnostics → TRY → CALL: put → EXCEPTION: put → CATCH: DBException e → CALL: markStoreUnHealthy → THROW: new IOException(e) → EXIT",
    "log": "<log>[WARN] Unable to update diagnostics in state store for [containerId], e</log> <log>[DEBUG] storeContainerDiagnostics: containerId={}, diagnostics=</log>"
  },
  "3d8baa06_3": {
    "exec_flow": "ENTRY → IF_FALSE: app != null → LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "<log>[WARN] Event + event + sent to absent application + event.getApplicationID()</log>"
  },
  "069e5603_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→CALL:readFully→CALL:validatePositionedReadArgs→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→CALL:read→EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "903ea19a_1": {
    "exec_flow": "ENTRY→CALL:hasNext→CALL:requestNextBatch→WHILE:source.hasNext()→WHILE_COND:source.hasNext()→IF_TRUE:buildNextStatusBatch(source.next())→CALL:org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:next()→CALL:org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult)→[DEBUG] All entries in batch were filtered...continuing→RETURN→EXIT",
    "log": "[DEBUG] All entries in batch were filtered...continuing [DEBUG] Next element retrieved from RemoteIterator [DEBUG] Requesting next {} uploads prefix {}, next key {}, next upload id {} [DEBUG] Listing found {} upload(s) [DEBUG] New listing state: {}"
  },
  "903ea19a_2": {
    "exec_flow": "ENTRY→TRY→LOG: LOG.DEBUG: Begin parsing summary logs.→CALL: appLogs.parseSummaryLogs→IF_TRUE: !isDone()→LOG: LOG.DEBUG: Try to parse summary log for log {} in {}, appId, appDirPath→CALL: getAppState→CALL: scanForLogs→IF_FALSE: appState == AppState.UNKNOWN→FOREACH: summaryLogs→CALL: FileSystem.exists→CALL: LogInfo.parseForStore→FOREACH_EXIT→CALL: summaryLogs.removeAll→CALL: addSummaryLogReadTime→IF_FALSE: appLogs.isDone()→LOG: LOG.DEBUG: End parsing summary logs.→IF_TRUE: !doneAppPath.equals(appDirPath)→CALL: exists→IF_FALSE: !fs.exists(donePathParent)→LOG: LOG.DEBUG: Application {} is done, trying to move to done dir {}, appId, doneAppPath→IF_TRUE: !fs.rename(appDirPath, doneAppPath)→THROW: new IOException(\"Rename \" + appDirPath + \" to \" + doneAppPath + \" failed\")→EXIT",
    "log": "[DEBUG] Begin parsing summary logs. [DEBUG] Try to parse summary log for log {} in {} [DEBUG] End parsing summary logs. [DEBUG] Application {} is done, trying to move to done dir {}"
  },
  "903ea19a_3": {
    "exec_flow": "PARENT.ENTRY→[VIRTUAL_CALL]→ENTRY→CALL:listStatus→NEW:DirListingIterator→RETURN→EXIT",
    "log": "[DEBUG] Next element retrieved from RemoteIterator [DEBUG] Requesting next {} uploads prefix {}, next key {}, next upload id {} [DEBUG] Listing found {} upload(s) [DEBUG] New listing state: {}"
  },
  "509be874_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: {}: scanning directory {}, getName(), taskAttemptDir→CALL: scanDirectoryTree→LOG: LOG.INFO: {}: directory {} contained {} file(s); data size {}, getName(), taskAttemptDir, fileCount, fileDataSize→LOG: LOG.INFO: {}: Directory count = {}; maximum depth {}, getName(), dirCount, depth→CALL: addSample→RETURN→EXIT ENTRY→IF_TRUE: counter == null →LOG: LOG.DEBUG: Ignoring counter increment for unknown counter {}, key →RETURN→EXIT ENTRY→IF_FALSE: counter == null →IF_TRUE: value < 0 →LOG: LOG.DEBUG: Ignoring negative increment value {} for counter {}, value, key →RETURN→EXIT ENTRY→IF_FALSE: counter == null →IF_FALSE: value < 0 →LOG: LOG.TRACE: Incrementing counter {} by {} with final value {}, key, value, l →RETURN→EXIT",
    "log": "[INFO] {}: scanning directory {}, getName(), taskAttemptDir [INFO] {}: directory {} contained {} file(s); data size {}, getName(), taskAttemptDir, fileCount, fileDataSize [INFO] {}: Directory count = {}; maximum depth {}, getName(), dirCount, depth [DEBUG] Ignoring counter increment for unknown counter {} [DEBUG] Ignoring negative increment value {} for counter {} [TRACE] Incrementing counter {} by {} with final value {}"
  },
  "3fecfee7_1": {
    "exec_flow": "ENTRY→IF_FALSE: scheme == null && authority == null→IF_FALSE: scheme != null && authority == null→IF_TRUE: conf.getBoolean(disableCacheName, false)→LOG:LOGGER.DEBUG:Bypassing cache to create filesystem {}→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→CALL:addKVAnnotation→CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass→CALL:org.apache.hadoop.util.ReflectionUtils:newInstance→TRY→CALL:initialize→EXCEPTION:initialize→CATCH:IOException | RuntimeException e→LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString()→LOG:LOGGER.DEBUG:Failed to initialize filesystem, e→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→THROW:e→EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem"
  },
  "3fecfee7_2": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE:fs.isDirectory()→CALL:NativeFileSystemStore:getFileLength(key)→LOG:Open the file: [{f}] for reading.→NEW:FSDataInputStream→CALL:getConf→NEW:BufferedFSInputStream→NEW:CosNInputStream→RETURN→EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "3fecfee7_3": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta!=null→LOG:List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0→IF_TRUE:LOG.isDebugEnabled()→LOG:Path: [{}] is a directory. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "3fecfee7_4": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "5fdb2697_1": {
    "exec_flow": "ENTRY→CALL: writeLock.lock→TRY→IF_FALSE: !rmContext.isWorkPreservingRecoveryEnabled() || containerReports == null || (containerReports != null && containerReports.isEmpty())→FOREACH: containerReports→IF_FALSE: rmApp == null→IF_FALSE: schedulerApp == null→LOG: LOG.INFO: Recovering container + container→CALL: recoverAndCreateContainer→CALL: RMContainer.handle→CALL: SchedulerNode.recoverContainer→CALL: Queue.recoverContainer→CALL: SchedulerApplicationAttempt.recoverContainer→IF_TRUE: schedulerAttempt.getPendingRelease().remove(container.getContainerId())→CALL: RMContainer.handle→LOG: LOG.INFO: container.getContainerId() + is released by application→LOG: LOG.INFO: SchedulerAttempt + getApplicationAttemptId() + is recovering container + rmContainer.getContainerId()→FOREACH_EXIT→CALL: writeLock.unlock→EXIT",
    "log": "<log>[INFO] Recovering container</log> <log>[INFO] SchedulerAttempt + getApplicationAttemptId() + is recovering container + rmContainer.getContainerId()</log> <log>[INFO] [Container ID] is released by application</log>"
  },
  "5fdb2697_2": {
    "exec_flow": "ENTRY→CALL: writeLock.lock→TRY→IF_FALSE: !rmContext.isWorkPreservingRecoveryEnabled() || containerReports == null || (containerReports != null && containerReports.isEmpty())→FOREACH: containerReports→IF_TRUE: rmApp == null→LOG: LOG.ERROR: Skip recovering container + container + for unknown application.→CALL: killOrphanContainerOnNode→CONTINUE→FOREACH_EXIT→CALL: writeLock.unlock→EXIT",
    "log": "<log>[ERROR] Skip recovering container for unknown application.</log>"
  },
  "5fdb2697_3": {
    "exec_flow": "ENTRY->IF_FALSE: rmContainer.getState().equals(RMContainerState.COMPLETED)→CALL: allocateContainer->ENTRY->IF_TRUE: resource == null->LOG:LOG.ERROR: Invalid deduction of null resource for + rmNode.getNodeAddress()->RETURN->EXIT",
    "log": "<log>[ERROR] Invalid deduction of null resource for + rmNode.getNodeAddress()</log>"
  },
  "5fdb2697_4": {
    "exec_flow": "ENTRY->IF_FALSE: rmContainer.getState().equals(RMContainerState.COMPLETED)→CALL: allocateContainer->ENTRY->IF_FALSE: resource == null->CALL:ENTRY→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→LOG:LOG.WARN: Resource is missing: {exception_message}→FOR_EXIT→CALL:Resources.addTo->EXIT",
    "log": "<log>[WARN] Resource is missing: {exception_message}</log>"
  },
  "6f09290a_1": {
    "exec_flow": "ENTRY → CALL: Check.notEmpty → CALL: Check.notNull → CALL: Check.notNull → CALL: org.apache.hadoop.conf.Configuration:getBoolean → IF_FALSE: !conf.getBoolean(FILE_SYSTEM_SERVICE_CREATED, false) → TRY → CALL: validateNamenode → CALL: org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:getUGI → ENTRY → CALL: ensureInitialized → IF_TRUE: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → RETURN → EXIT → CALL: org.apache.hadoop.security.UserGroupInformation:doAs → LOG: Handling deprecation for all properties in config... → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CATCH: FileSystemAccessException ex → THROW: ex → EXIT → IF_TRUE: LOG.isDebugEnabled() → LOG: PrivilegedAction [as: {}][action: {}] → CALL: Subject.doAs → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "c2eb2576_1": {
    "exec_flow": "ENTRY → IF_TRUE: lister.hasNext() → CALL:org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:next() → TRY → CALL:org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → CALL:org.apache.hadoop.fs.s3a.Invoker:retry → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]) → CALL:getMultipartUploads → CALL:listIterator → CALL:hasNext → RETURN → EXIT",
    "log": "<log>[DEBUG] [{}], Requesting next {} uploads prefix {}, next key {}, next upload id {}</log> <log>[DEBUG] Listing found {} upload(s)</log> <log>[DEBUG] New listing state: {}</log> <log>[DEBUG] Next element retrieved from RemoteIterator</log>"
  },
  "c2eb2576_2": {
    "exec_flow": "ENTRY → IF_FALSE:currIterator.hasNext() → CALL:getNextIterator → CALL:fetchBatchesAsync → TRY → WHILE:listResult == null && (!isIterationComplete || !listResultQueue.isEmpty()) → WHILE_EXIT → IF_FALSE:listResult == null → IF_TRUE:listResult.isFailedListing() → THROW:listResult.getListingException() → CATCH:InterruptedException → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → THROW:IOException → EXIT",
    "log": "<log>[ERROR] Thread got interrupted: {exception}</log>"
  },
  "c2eb2576_3": {
    "exec_flow": "ENTRY → IF_FALSE:currIterator.hasNext() → CALL:getNextIterator → CALL:fetchBatchesAsync → TRY → WHILE:listResult == null && (!isIterationComplete || !listResultQueue.isEmpty()) → WHILE_EXIT → IF_FALSE:listResult == null → IF_TRUE:listResult.isFailedListing() → THROW:listResult.getListingException() → CATCH:InterruptedException → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → THROW:IOException → EXIT",
    "log": "<log>[ERROR] Thread got interrupted: {exception}</log>"
  },
  "faa85832_1": {
    "exec_flow": "ENTRY→IF_FALSE:!tarname.exists()→TRY→CALL:FileUtil.unTar→IF_TRUE:gzipped→CALL:untarCommand.append(\" gzip -dc \").append(source).append→CALL:untarCommand.append(\"cd '\").append(FileUtil.makeSecureShellPath(untarDir)).append(\"' && \").append→CALL:untarCommand.append→LOG:LOG.DEBUG:executing [{}], untarCommand→CALL:execute→IF_FALSE:exitcode!=0→IF_TRUE:!FileUtil.fullyDelete(tarname)→LOG:LOG.WARN:Failed to fully delete aliasmap archive: + tarname→EXIT",
    "log": "[DEBUG] executing [{}], untarCommand [WARN] Failed to fully delete aliasmap archive: + tarname"
  },
  "faa85832_2": {
    "exec_flow": "ENTRY→IF_FALSE:!tarname.exists()→TRY→CALL:FileUtil.unTar→IF_TRUE:gzipped→CALL:untarCommand.append(\" gzip -dc \").append(source).append→CALL:untarCommand.append(\"cd '\").append(FileUtil.makeSecureShellPath(untarDir)).append(\"' && \").append→CALL:untarCommand.append→LOG:LOG.DEBUG:executing [{}], untarCommand→CALL:execute→IF_FALSE:exitcode!=0→EXIT→IF_TRUE:!FileUtil.fullyDelete(tarname)→LOG:LOG.WARN:Failed to fully delete aliasmap archive: + tarname→EXIT",
    "log": "[DEBUG] executing [{}], untarCommand [WARN] Failed to fully delete aliasmap archive: + tarname"
  },
  "279cc90e_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>ENTRY</step> <step>IF_FALSE:isInState(STATE.STARTED)</step> <step>SYNCHRONIZE:stateChangeLock</step> <step>IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED</step> <step>TRY_BLOCK_START</step> <step>CALL:currentTimeMillis</step> <step>CALL:serviceStart</step> <step>IF_TRUE:isInState(STATE.STARTED)</step> <step>LOG: Service {} is started</step> <step>CALL:notifyListeners</step> <step>TRY_BLOCK_START:notifyListeners</step> <step>CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>TRY_BLOCK_END:notifyListeners</step> <step>CATCH_BLOCK_START:Throwable e</step> <step>LOG: Exception while notifying listeners of {}</step> <step>CALL:ServiceOperations.stopQuietly</step> <step>THROW:ServiceStateException.convert(e)</step> <step>TRY_BLOCK_END</step> <step>CATCH_BLOCK_START:Exception e</step> <step>LOG: noteFailure</step> <step>SET:failureCause = exception</step> <step>SET:failureState = getServiceState()</step> <step>LOG: Service {getName()} failed in state {failureState}</step> <step>CATCH_BLOCK_END</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<!-- Merged log sequence --> <log> <level>DEBUG</level> <message>Service {} is started</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {}</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> </log>"
  },
  "b93d033c_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→IF_TRUE:this.userPipelineMap.containsKey(user)→LOG:LOG.INFO:Request to start an already existing user: {} was received, so ignoring., user→CALL:get→RETURN→CALL: getRootInterceptor→CALL:ClientRequestInterceptor:getApplicationReport→RETURN→EXIT",
    "log": "[INFO] Request to start an already existing user: {} was received, so ignoring."
  },
  "b93d033c_2": {
    "exec_flow": "ENTRY→IF_TRUE:request == null || request.getApplicationId() == null→CALL:routerMetrics.incrAppsFailedRetrieved→ENTRY→IF_TRUE: t != null→LOG: LOG.ERROR: {errMsg}, {t}→THROW: new YarnException(errMsg, t)→EXIT→TRY→CALL:getApplicationHomeSubCluster→CALL:getClientRMProxyForSubCluster→CALL:getApplicationReport→CALL:getApplicationReport→IF_TRUE:response == null→LOG:LOG.ERROR:No response when attempting to retrieve the report of the application {request.getApplicationId()} to SubCluster {subClusterId.getId()}→CALL:routerMetrics.succeededAppsRetrieved→RETURN→EXIT",
    "log": "[ERROR] Missing getApplicationReport request or applicationId information. [ERROR] No response when attempting to retrieve the report of the application {request.getApplicationId()} to SubCluster {subClusterId.getId()}"
  },
  "b93d033c_3": {
    "exec_flow": "ENTRY→IF_TRUE: application == null→CALL: RMAuditLogger.logFailure→THROW: new ApplicationNotFoundException(\"Application with id '\" + applicationId + \"' doesn't exist in RM. Please check that the job submission was successful.\")→EXIT",
    "log": "RMAuditLogger.logFailure(callerUGI.getUserName(), operation, \"UNKNOWN\", \"ClientRMService\", \"Trying to \" + operation + \" of an absent application\", applicationId)"
  },
  "b93d033c_4": {
    "exec_flow": "ENTRY→IF_FALSE: application == null→IF_TRUE: needCheckAccess→IF_TRUE: !checkAccess(callerUGI, application.getUser(), accessType, application)→CALL: RMAuditLogger.logFailure→THROW: RPCUtil.getRemoteException(new AccessControlException(\"User \" + callerUGI.getShortUserName() + \" cannot perform operation \" + accessType.name() + \" on \" + applicationId))→EXIT",
    "log": "RMAuditLogger.logFailure(callerUGI.getShortUserName(), operation, \"User doesn't have permissions to \" + accessType.toString(), \"ClientRMService\", AuditConstants.UNAUTHORIZED_USER, applicationId)"
  },
  "e6cc4fa8_1": {
    "exec_flow": "<sequence> <condition> <if>exception == null</if> <consequence> <!-- Return if no exception is provided --> </consequence> <alternative> <synchronized> <condition> <if>failureCause == null</if> <consequence> <action>failureCause = exception</action> <action>failureState = getServiceState()</action> <log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>INFO</level> <message>Service {} failed in state {}</message> <parameters>getName(), failureState, exception</parameters> </log> </consequence> </condition> </synchronized> </alternative> </condition> </sequence>",
    "log": "<log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>INFO</level> <message>Service {} failed in state {}</message> <parameters>getName(), failureState, exception</parameters> </log>"
  },
  "e6cc4fa8_2": {
    "exec_flow": "ENTRY→IF_TRUE:service != null→CALL:stop→EXIT",
    "log": "<log> <template>org.apache.hadoop.service.ServiceOperations:stopQuietly[WARN]When stopping the service {service.getName()}</template> <parameters> <parameter>service.getName()</parameter> </parameters> </log>"
  },
  "933115ac_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:names → CALL:getProps → LOG:Handling deprecation for (String)item → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT → IF_FALSE: null==vStr → CALL: getTimeDurationHelper → ENTRY → CALL: trim → CALL: toLowerCase → IF_FALSE: null==vUnit → CALL: substring → CALL: lastIndexOf → CALL: suffix → CALL: suffix → CALL: lastIndexOf → CALL: suffix → CALL: suffix → IF_TRUE: vUnit.unit().convert(converted,returnUnit)<raw → CALL: logDeprecation → ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}"
  },
  "36359c07_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeDefaultAcl.ENTRY → CALL:checkOperation → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:getPermissionChecker → CALL:checkNameNodeSafeMode → CALL:FSDirAclOp.removeDefaultAcl → CALL:writeUnlock → EXCEPTION:AccessControlException e → CALL:logAuditEvent → THROW:e → EXIT",
    "log": "[DEBUG] Attempting operation: removeDefaultAcl [ERROR] Audit event failed: AccessControlException"
  },
  "36359c07_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeDefaultAcl.ENTRY → CALL:checkOperation → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:getPermissionChecker → CALL:checkNameNodeSafeMode → CALL:FSDirAclOp.removeDefaultAcl → CALL:writeUnlock → CALL:logSync → CALL:logAuditEvent → EXIT",
    "log": "[DEBUG] Attempting operation: removeDefaultAcl [INFO] Audit event succeeded: removeDefaultAcl"
  },
  "3f1900a0_1": {
    "exec_flow": "ENTRY → IF_FALSE: permission == null → CALL: applyUMask → CALL: org.apache.hadoop.fs.permission.FsPermission: getUMask → IF_TRUE: conf != null → LOG: Handling deprecation for all properties in config... → CALL: get → TRY → IF_TRUE: confUmask ≠ null → LOG: Handling deprecation for (String)item → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log>"
  },
  "e7893fcf_1": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkArgument→CALL:Preconditions.checkState→CALL:AddBlockOp.getInstance→LOG:LOG.DEBUG:doEditTx() op={} txid={}→CALL:org.slf4j.Logger:debug→TRY→CALL:editLogStream.write→EXCEPTION:write→CATCH:IOException ex→CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT",
    "log": "[LOG] logEdit [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "e7893fcf_2": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkArgument→CALL:Preconditions.checkState→CALL:AddBlockOp.getInstance→TRY→SYNC:this→TRY→CALL:printStatistics→WHILE:mytxid > synctxid && isSyncRunning→WHILE_COND:mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE:mytxid <= synctxid→CALL:getLastJournalledTxId→LOG:LOG.DEBUG:logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}→IF_FALSE:lastJournalledTxId <= synctxid→TRY→IF_TRUE:journalSet.isEmpty()→THROW:new IOException(\"No journals available to flush\")→CALL:org.apache.hadoop.util.ExitUtil:terminate→CALL:org.slf4j.Logger:error→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "3c169494_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG:[INFO] message → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3c169494_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "eaa9206c_1": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:org.apache.hadoop.ipc.RPC$Server:start() →LOG: [INFO] Thread.currentThread().getName(): starting →CALL: SERVER.set →CALL: connectionManager.startIdleScan →WHILE: running →WHILE_COND: running →TRY →CALL: getSelector().select →CALL: getSelector().selectedKeys →CALL: key.isValid →IF_COND: key.isValid →CALL: key.isAcceptable →IF_COND: key.isAcceptable →CALL: doAccept →CATCH: IOException →CATCH_END →OUTER_TRY_END →CATCH: OutOfMemoryError →LOG: [WARN] Out of Memory in server select →CALL: closeCurrentConnection →CALL: connectionManager.closeIdle →CALL: Thread.sleep →CATCH_END →OUTER_CATCH: Exception →CALL: closeCurrentConnection →OUTER_CATCH_END →WHILE_END →LOG: [INFO] Stopping Thread.currentThread().getName() →SYNC: this →TRY →CALL: acceptChannel.close →CALL: selector.close →CATCH: IOException →CATCH_END →SYNC_END →CALL: connectionManager.stopIdleScan →CALL: FOREACH: toArray() →CALL: org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection) →FOREACH_EXIT →EXIT →TRY →IF_TRUE: LOG.isDebugEnabled() →LOG: LOG.DEBUG:PrivilegedAction [as: {}][action: {}] →CALL: Subject.doAs →RETURN →CATCH: PrivilegedActionException →LOG: LOG.DEBUG:PrivilegedActionException as: {} →THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException →RETURN →EXIT",
    "log": "<log>[INFO] Thread.currentThread().getName(): starting</log> <log>[WARN] Out of Memory in server select</log> <log>[INFO] Stopping Thread.currentThread().getName()</log> <log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "eaa9206c_2": {
    "exec_flow": "ENTRY → LOG:[WARN] Unexpected SecurityException in Configuration → CALL:getRaw → LOG:[DEBUG] Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:[DEBUG] Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "0432b288_1": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_TRUE: user.equals(stat.getOwner()) → IF_TRUE: perm.getUserAction().implies(mode) → RETURN → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log>"
  },
  "0432b288_2": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_TRUE: user.equals(stat.getOwner()) → IF_FALSE: perm.getUserAction().implies(mode) → THROW: AccessControlException → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log>"
  },
  "0432b288_3": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_FALSE: user.equals(stat.getOwner()) → CALL: org.apache.hadoop.security.UserGroupInformation:getGroups → ENTRY→CALL: ensureInitialized→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→EXIT→TRY→CALL: getGroups→CALL: getShortUserName→CALL: getShortUserName→CALL: org.slf4j.Logger:debug→RETURN→EXIT → IF_TRUE: ugi.getGroups().contains(stat.getGroup()) → IF_TRUE: perm.getGroupAction().implies(mode) → RETURN → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log> <log> <statement>[DEBUG] Failed to get groups for user {}</statement> </log>"
  },
  "0432b288_4": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_FALSE: user.equals(stat.getOwner()) → CALL: org.apache.hadoop.security.UserGroupInformation:getGroups → ENTRY→CALL: ensureInitialized→IF_FALSE:!isInitialized()→EXIT→TRY→CALL: getGroups→CALL: getShortUserName→CALL: getShortUserName→CALL: org.slf4j.Logger:debug→RETURN→EXIT → IF_TRUE: ugi.getGroups().contains(stat.getGroup()) → IF_FALSE: perm.getGroupAction().implies(mode) → THROW: AccessControlException → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log> <log> <statement>[DEBUG] Failed to get groups for user {}</statement> </log>"
  },
  "0432b288_5": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_FALSE: user.equals(stat.getOwner()) → CALL: org.apache.hadoop.security.UserGroupInformation:getGroups → ENTRY→CALL: ensureInitialized→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→EXIT→TRY→CALL: getGroups→CALL: getShortUserName→CALL: getShortUserName→CALL: org.slf4j.Logger:debug→RETURN→EXIT → IF_FALSE: ugi.getGroups().contains(stat.getGroup()) → IF_TRUE: perm.getOtherAction().implies(mode) → RETURN → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log> <log> <statement>[DEBUG] Failed to get groups for user {}</statement> </log>"
  },
  "0432b288_6": {
    "exec_flow": "ENTRY → CALL: getFileStatus → LOG:LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: checkAccessPermissions → IF_FALSE: user.equals(stat.getOwner()) → CALL: org.apache.hadoop.security.UserGroupInformation:getGroups → ENTRY→CALL: ensureInitialized→IF_FALSE:!isInitialized()→EXIT→TRY→CALL: getGroups→CALL: getShortUserName→CALL: getShortUserName→CALL: org.slf4j.Logger:debug→RETURN→EXIT → IF_FALSE: ugi.getGroups().contains(stat.getGroup()) → IF_FALSE: perm.getOtherAction().implies(mode) → THROW: AccessControlException → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log> <log> <statement>[LOG] getLoginUser</statement> </log> <log> <statement>[DEBUG] Failed to get groups for user {}</statement> </log>"
  },
  "3c254f89_1": {
    "exec_flow": "ENTRY → TRY → CALL:getProxyUser → LOG: LOG.INFO: GET: component instances for service = {}, compNames in {}, version = {}, containerStates in {}, user = {} → CALL:getContainers → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}] → CALL: Subject.doAs → RETURN → EXIT",
    "log": "[INFO] GET: component instances for service = {}, compNames in {}, version = {}, containerStates in {}, user = {} [DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "230fa9e1_1": {
    "exec_flow": "ENTRY→CALL:this.rmContext.getDispatcher→CALL:getEventHandler→LOG:LOG.INFO: Got event {eventType} for appId {applicationId}→SWITCH: event.getType()→CASE: [APPLICATION_INIT]→LOG:LOG.INFO: Got APPLICATION_INIT for service {serviceId}→TRY→CALL:get→CALL:getServiceID→CALL:initializeApplication→CATCH:Throwable th→LOG:logWarningWhenAuxServiceThrowExceptions→BREAK→EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId} [INFO] Got APPLICATION_INIT for service {serviceId} [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_INIT"
  },
  "230fa9e1_2": {
    "exec_flow": "ENTRY→CALL:this.rmContext.getDispatcher→CALL:getEventHandler→LOG:LOG.INFO: Got event {eventType} for appId {applicationId}→SWITCH: event.getType()→CASE: [APPLICATION_STOP]→FOREACH:serviceMap.values()→TRY→CALL:stopApplication→CATCH:Throwable th→LOG:logWarningWhenAuxServiceThrowExceptions→FOREACH_EXIT→BREAK→EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId} [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_STOP"
  },
  "230fa9e1_3": {
    "exec_flow": "ENTRY→CALL:this.rmContext.getDispatcher→CALL:getEventHandler→LOG:LOG.INFO: Got event {eventType} for appId {applicationId}→SWITCH: event.getType()→CASE: [CONTAINER_INIT]→FOREACH:serviceMap.values()→TRY→CALL:initializeContainer→CATCH:Throwable th→LOG:logWarningWhenAuxServiceThrowExceptions→FOREACH_EXIT→BREAK→EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId} [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_INIT"
  },
  "230fa9e1_4": {
    "exec_flow": "ENTRY→CALL:this.rmContext.getDispatcher→CALL:getEventHandler→LOG:LOG.INFO: Got event {eventType} for appId {applicationId}→SWITCH: event.getType()→CASE: [CONTAINER_STOP]→FOREACH:serviceMap.values()→TRY→CALL:stopContainer→CATCH:Throwable th→LOG:logWarningWhenAuxServiceThrowExceptions→FOREACH_EXIT→BREAK→EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId} [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_STOP"
  },
  "230fa9e1_5": {
    "exec_flow": "ENTRY→CALL:this.rmContext.getDispatcher→CALL:getEventHandler→LOG:LOG.INFO: Got event {eventType} for appId {applicationId}→SWITCH: event.getType()→CASE: []→THROW: new RuntimeException(\"Unknown type: \" + event.getType())→EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId}"
  },
  "7055ef3e_1": {
    "exec_flow": "ENTRY→IF_FALSE: monitoring && !DefaultMetricsSystem.inMiniClusterMode()→CALL: checkNotNull→IF_FALSE: monitoring→SWITCH: initMode()→ENTRY→LOG: LOG.DEBUG: from system property: + System.getProperty(MS_INIT_MODE_KEY)→LOG: LOG.DEBUG: from environment variable: + System.getenv(MS_INIT_MODE_KEY)→CALL: valueOf→CALL: toUpperCase→CALL: name→RETURN→EXIT→CASE: [NORMAL]→TRY→CALL: start→CATCH: MetricsConfigException e→LOG: LOG.WARN: \"Metrics system not started: \" + e.getMessage()→LOG: LOG.DEBUG: \"Stacktrace: \", e→BREAK→EXIT",
    "log": "[DEBUG] from system property: [value of System.getProperty(MS_INIT_MODE_KEY)] [DEBUG] from environment variable: [value of System.getenv(MS_INIT_MODE_KEY)] [WARN] Metrics system not started: + e.getMessage() [DEBUG] Stacktrace: , e"
  },
  "ad01c01c_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] Interrupted"
  },
  "ad01c01c_2": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] IOException: errorMessage"
  },
  "ad01c01c_3": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[ERROR] Interrupted"
  },
  "ad01c01c_4": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[ERROR] IOException: errorMessage"
  },
  "c7ca52d1_1": {
    "exec_flow": "ENTRY -> CALL: rpcServer.checkOperation -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "c7ca52d1_2": {
    "exec_flow": "ENTRY -> CALL: rpcServer.checkOperation -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "c7ca52d1_3": {
    "exec_flow": "ENTRY -> CALL: rpcServer.checkOperation -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "c7ca52d1_4": {
    "exec_flow": "ENTRY -> CALL: rpcServer.checkOperation -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "d917c0e7_1": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → CALL:createFileSystem → CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC:this → IF_TRUE:map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL:ShutdownHookManager.get().addShutdownHook → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore [DEBUG] Duplicate FS created for {}; discarding {} [DEBUG] Exception in closing {}"
  },
  "d917c0e7_2": {
    "exec_flow": "ENTRY → IF_TRUE:this.conf.getBoolean(IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT) → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOG.WARN(\"Bad checksum at {position}. Skipping entries.\") → CALL:sync → CALL:org.apache.hadoop.conf.Configuration:getInt → EXIT",
    "log": "[WARN] Bad checksum at {position}. Skipping entries."
  },
  "d917c0e7_3": {
    "exec_flow": "ENTRY → IF_TRUE:val instanceof Configurable → CALL:((Configurable) val).setConf → CALL:seekToCurrentValue → IF_FALSE:!blockCompressed → CALL:deserializeValue → IF_TRUE:(valLength < 0) AND LOG.isDebugEnabled() → LOG.DEBUG:val + is a zero-length value → RETURN → EXIT",
    "log": "[DEBUG] val + is a zero-length value"
  },
  "d917c0e7_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {}, uri → CALL: createFileSystem → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "d917c0e7_5": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "ec5b13ac_1": {
    "exec_flow": "Parent.ENTRY ➝ [VIRTUAL_CALL] ➝ ENTRY ➝ IF_FALSE:LOG.isDebugEnabled() ➝ CALL:getFileStatus ➝ IF_TRUE:fileStatus.isDirectory() ➝ IF_TRUE:LOG.isDebugEnabled() ➝ LOG:LOG.DEBUG:listStatus:doing listObjects for directory+key ➝ WHILE:true ➝ WHILE_COND:true ➝ WHILE_EXIT ➝ CALL:toArray ➝ CALL:size ➝ CALL:size ➝ RETURN ➝ EXIT ➝ LOG:[INFO] message ➝ CALL:LOG_DEPRECATION.info ➝ CALL:org.slf4j.Logger:info(java.lang.String) ➝ EXIT",
    "log": "<log>[DEBUG] listStatus: doing listObjects for directory {key}</log> <log>[INFO] message</log>"
  },
  "ec5b13ac_2": {
    "exec_flow": "Parent.ENTRY ➝ [VIRTUAL_CALL] ➝ ENTRY ➝ LOG:[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()} ➝ CALL:statIncrement ➝ CALL:makeQualified ➝ TRY ➝ CALL:abfsStore.listStatus ➝ RETURN ➝ EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}</log>"
  },
  "ec5b13ac_3": {
    "exec_flow": "Parent.ENTRY ➝ [VIRTUAL_CALL] ➝ ENTRY ➝ LOG:[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()} ➝ CALL:statIncrement ➝ CALL:makeQualified ➝ TRY ➝ CALL:abfsStore.listStatus ➝ EXCEPTION:AzureBlobFileSystemException ➝ CALL:checkException ➝ RETURN ➝ EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}</log>"
  },
  "ec5b13ac_4": {
    "exec_flow": "Parent.ENTRY ➝ [VIRTUAL_CALL] ➝ ENTRY ➝ IF_TRUE:client != null ➝ IF_FALSE:!client.isConnected() ➝ CALL:logout ➝ CALL:disconnect ➝ IF_TRUE:!logoutSuccess ➝ LOG:LOG.WARN:Logout failed while disconnecting, error code - + client.getReplyCode() ➝ EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "ec5b13ac_5": {
    "exec_flow": "Parent.ENTRY ➝ [VIRTUAL_CALL] ➝ ENTRY ➝ CALL:LOG_DEPRECATION.info ➝ CALL:org.slf4j.Logger:info(java.lang.String) ➝ EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "a0bff8ed_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → Child:main(java.lang.String[]) → CALL:JobConf:init → CALL:Limits:init → CALL:UserGroupInformation:setConfiguration → CALL:SecurityUtil:setConfiguration → LOG:INFO:Updating Configuration → CALL:setConfigurationInternal → CALL:NetUtils:createSocketAddrForHost → CALL:TaskAttemptID:forName → CALL:Long:parseLong → CALL:JVMId:init → CALL:CallerContext:setCurrent → CALL:DefaultMetricsSystem:initialize → CALL:UserGroupInformation:getCurrentUser → CALL:Credentials:getAllTokens → LOG:INFO:Executing with tokens: ... → CALL:UserGroupInformation:createRemoteUser → CALL:TokenCache:getJobToken → CALL:SecurityUtil:setTokenService → CALL:UserGroupInformation:addToken → CALL:UserGroupInformation:doAs → TRY → FOR → CALL:MILLISECONDS:sleep → LOG:INFO:Sleeping for %ms before retrying again. Got null now. → CALL:TaskUmbilicalProtocol:getTask → IF → CALL:Task:getTaskID → CALL:YarnChild:configureTask → CALL:MRApps:getSystemPropertiesToLog → LOG:INFO:... → CALL:JvmMetrics:initSingleton → CALL:UserGroupInformation:createRemoteUser → CALL:UserGroupInformation:addCredentials → CALL:MRApps:setJobClassLoader → CALL:TaskLog:createLogSyncer → CALL:UserGroupInformation:doAs → CALL:setEncryptedSpillKeyIfRequired → CALL:FileSystem:get → CALL:FileSystem:setWorkingDirectory → CALL:Task:run → CATCH → LOG:ERROR:FSError from child ... → IF → CALL:TaskUmbilicalProtocol:fsError → CATCH → LOG:WARN:Exception running child ... → IF → CATCH → LOG:ERROR:Error running child ... → FINALLY → CALL:RPC:stopProxy → CALL:DefaultMetricsSystem:shutdown → CALL:TaskLog:syncLogsShutdown",
    "log": "[DEBUG] Child starting [INFO] Updating Configuration [DEBUG] PID: ... [INFO] Executing with tokens: ... [INFO] Sleeping for ...ms before retrying again. Got null now. [INFO] JVM with ID : + jvmId + asked for a task [INFO] JVM with ID: + jvmId + given task: + task.getTaskID() [WARN] Unexpected SecurityException in Configuration [ERROR] FSError from child ... [WARN] Exception running child ... [ERROR] Error running child ..."
  },
  "a0bff8ed_2": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → FOREACH:tokenFileLocation → CALL:exists → IF_TRUE → CALL:isFile → CALL:readTokenStorageFile → TRY → CALL:readTokenStorageStream → LOG:Exception reading {filename} → CALL:addCredentials → LOG:Reading credentials from location {} → LOG:Loaded {} tokens from {} → IF_FALSE → LOG:Token file {} does not exist → LOG:Failure to load login credentials → LOG:UGI loginUser: {} → EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [WARN] Null token ignored for {alias} [DEBUG] Exception reading {filename}"
  },
  "a0bff8ed_3": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "a0bff8ed_4": {
    "exec_flow": "ENTRY→CALL:setCredentials→CALL:get→CALL:fromString→RETURN→CALL:getApplicationAttemptId→LOG:APPLICATION_ATTEMPT_ID: ...→CALL:debug→CALL:setInt→CALL:setBoolean→CALL:setClass→CALL:createSecretKey→CALL:getShuffleSecretKey→IF_TRUE:scheme == null && authority == null→LOG:Bypassing cache to create filesystem {}→CALL:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→CALL:addKVAnnotation→CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass→CALL:org.apache.hadoop.util.ReflectionUtils:newInstance→TRY→CALL:initialize→RETURN→CALL:org.apache.hadoop.util.DurationInfo:close→EXIT",
    "log": "[DEBUG] APPLICATION_ATTEMPT_ID: ... [WARN] Shuffle secret missing from task credentials. Using job token secret as shuffle secret. [DEBUG] Bypassing cache to create filesystem {}"
  },
  "a0bff8ed_5": {
    "exec_flow": "ENTRY→IF_TRUE: token != null→CALL: setService→CALL: org.slf4j.Logger:isDebugEnabled()→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Acquired token + token→EXIT",
    "log": "[DEBUG] Acquired token"
  },
  "a0bff8ed_6": {
    "exec_flow": "ENTRY→IF_FALSE: token != null→CALL: org.slf4j.Logger:warn(java.lang.String)→LOG: LOG.WARN: Failed to get token for service + service→EXIT",
    "log": "[WARN] Failed to get token for service"
  },
  "8afddea7_1": {
    "exec_flow": "ENTRY→TRY→WHILE: shouldRun→CALL:NamenodeProtocol:getBlockKeys→CALL:BlockTokenSecretManager:addKeys→TRY→CATCH:IOException→CALL:Logger:error→SLEEP→WHILE_COND: shouldRun→WHILE_EXIT→EXIT",
    "log": "[ERROR] Failed to set keys [DEBUG] Proxying operation: {} [INFO] Setting block keys"
  },
  "8afddea7_2": {
    "exec_flow": "ENTRY→TRY→CATCH:InterruptedException→CALL:Logger:debug→EXIT",
    "log": "[DEBUG] InterruptedException in block key updater thread"
  },
  "8afddea7_3": {
    "exec_flow": "ENTRY→TRY→CATCH:Throwable→CALL:Logger:error→EXIT",
    "log": "[ERROR] Exception in block key updater thread"
  },
  "d9200aa5_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>Load image plan logged</step> <step>Read storage properties</step> <step>Layout version check</step> <step>Image loading</step> <step>ENTRY→CALL:Preconditions.checkState→TRY→CALL:newInputStream→CALL:toPath→CALL:toPath→CALL:IOUtils.readFully→IF_TRUE:Arrays.equals(magic, FSImageUtil.MAGIC_HEADER)→CALL:FSImageFormatProtobuf.Loader:load→CALL:IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT</step>",
    "log": "<!-- Merged log sequence --> <log>INFO: Planning to load image: {imageFile}</log> <log>DEBUG: Loading using Protobuf Loader</log> <log>DEBUG: Exception in closing {}</log> <log>INFO: Loaded FSImage in {} seconds.</log>"
  },
  "d9200aa5_2": {
    "exec_flow": "<!-- Original execution flow --> <step>ENTRY→CALL:setBlockPoolId→CALL:load→IF_FALSE:expectedMd5 != null && !expectedMd5.equals(readImageMd5)→LOG:LOG.INFO:Loaded image for txid + txId + from + curFile→CALL:setMostRecentCheckpointInfo→EXIT</step>",
    "log": "<!-- Preserved log sequence --> <log>INFO: Loaded image for txid + txId + from + curFile</log>"
  },
  "beda28bd_1": {
    "exec_flow": "ENTRY → IF_TRUE:currentBlacklistSize<failureThreshold → LOG:LOG.DEBUG:blacklist size {} is less than failure threshold ratio {} + out of total usable nodes {}, currentBlacklistSize, blacklistDisableFailureThreshold, numberOfNodeManagerHosts → CALL:newInstance → RETURN → EXIT",
    "log": "[DEBUG] blacklist size {} is less than failure threshold ratio {} out of total usable nodes {}, currentBlacklistSize, blacklistDisableFailureThreshold, numberOfNodeManagerHosts"
  },
  "beda28bd_2": {
    "exec_flow": "ENTRY → IF_FALSE:currentBlacklistSize<failureThreshold → LOG:LOG.WARN:Ignoring Blacklists, blacklist size + currentBlacklistSize + is more than failure threshold ratio + blacklistDisableFailureThreshold + out of total usable nodes + numberOfNodeManagerHosts → CALL:newInstance → RETURN → EXIT",
    "log": "[WARN] Ignoring Blacklists, blacklist size currentBlacklistSize is more than failure threshold ratio blacklistDisableFailureThreshold out of total usable nodes numberOfNodeManagerHosts"
  },
  "beda28bd_3": {
    "exec_flow": "WebAppUtils.ENTRY → YarnConfiguration.ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "beda28bd_4": {
    "exec_flow": "WebAppUtils.ENTRY → YarnConfiguration.ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "21f54d8c_1": {
    "exec_flow": "ENTRY → IF_TRUE: action == FileAction.OVERWRITE → CALL: getUMask → CALL: getReplicationFactor → CALL: getBlockSize → CALL: create → NEW: BufferedOutputStream → TRY → CALL: getInputStream → CALL: getConfiguration → CALL: seekIfRequired → LOG: LOGGER.DEBUG(\"requested seek to position {}\") → IF_FALSE: closed → IF_FALSE: n < 0 → IF_TRUE: n > contentLength → THROW: new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF) → LOG: LOGGER.DEBUG(\"Seek to position {}. Bytes skipped {}, pos, this.pos\") → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG(\"Bypassing cache to create filesystem {uri}\") → CALL: createFileSystem → WHILE: bytesRead > 0 → CALL: outStream.write → LOG: LOGGER.DEBUG(\"Writing bytes to output stream\") → CALL: updateContextStatus → LOG: LOGGER.DEBUG(\"Updating context status\") → WHILE_COND: bytesRead > 0 → WHILE_EXIT → CALL: outStream.close → LOG: LOGGER.DEBUG(\"Closing output stream\") → CALL: IOUtils.cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: LOGGER.DEBUG(\"Exception in closing {}\") → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log> <template>requested seek to position {}</template> <level>debug</level> </log> <log> <template>Seek to position {}. Bytes skipped {}, pos, this.pos</template> <level>debug</level> </log> <log> <template>Bypassing cache to create filesystem {uri}</template> <level>debug</level> </log> <log> <template>Writing bytes to output stream</template> <level>debug</level> </log> <log> <template>Updating context status</template> <level>debug</level> </log> <log> <template>Closing output stream</template> <level>debug</level> </log> <log> <template>Exception in closing {}</template> <level>debug</level> </log>"
  },
  "21f54d8c_2": {
    "exec_flow": "ENTRY → IF_FALSE: action == FileAction.OVERWRITE → CALL: getUMask → CALL: getInt → NEW: BufferedOutputStream → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN(\"Failed to initialize filesystem {}: {}, uri, e.toString()\") → LOG: LOGGER.DEBUG(\"Failed to initialize filesystem, e\") → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: LOGGER.DEBUG(\"Exception in closing {}\") → FOREACH_EXIT → THROW: e → EXIT",
    "log": "<log> <template>Failed to initialize filesystem {}: {}, uri, e.toString()</template> <level>warn</level> </log> <log> <template>Failed to initialize filesystem</template> <level>debug</level> </log> <log> <template>Exception in closing {}</template> <level>debug</level> </log>"
  },
  "21f54d8c_3": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "21f54d8c_4": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "21f54d8c_5": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "efbd0295_1": {
    "exec_flow": "ENTRY→TRY→NEW:UgiInfo→CALL:getCurrentUser→CATCH:IOException→CALL:info→NEW:UgiInfo→RETURN→EXIT",
    "log": "[INFO] Failed to get current user {}"
  },
  "efbd0295_2": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOR_EACH:names → CALL:getProps → IF_TRUE:props != null → CALL:loadResources → IF_FALSE:overlay != null → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "efbd0295_3": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:loginUser==null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser) → CALL:createLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → EXCEPTION:Read → CATCH:IOException → CALL:IOUtils.cleanupWithLogger → THROW:IOException → DO_COND:loginUser==null → DO_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] Cannot add token {}: {} [DEBUG] Loaded {} base64 tokens [DEBUG] UGI loginUser: {}"
  },
  "bd7bf76e_1": {
    "exec_flow": "ENTRY→CALL:checkOperation→CALL:FSPermissionChecker.setOperationType→TRY→CALL:getPermissionChecker→CALL:readLock→TRY→CALL:checkOperation→CALL:FSDirStatAndListingOp.isFileClosed→CALL:readUnlock→IF_TRUE:success→LOG:logAuditEvent→FOREACH: auditLoggers →IF: !(logger instanceof HdfsAuditLogger)→CALL:logger.logAuditEvent→FOREACH_EXIT→RETURN→EXIT",
    "log": "[AUDIT] Access granted for operation isFileClosed on source"
  },
  "bd7bf76e_2": {
    "exec_flow": "ENTRY→CALL:checkOperation→CALL:FSPermissionChecker.setOperationType→TRY→CALL:getPermissionChecker→CALL:readLock→EXCEPTION:readLock→CATCH:AccessControlException e→LOG:logAuditEvent→FOREACH: auditLoggers →IF: !(logger instanceof HdfsAuditLogger)→CALL:logger.logAuditEvent→FOREACH_EXIT→THROW:e→EXIT",
    "log": "[AUDIT] Access denied for operation isFileClosed on source"
  },
  "bd7bf76e_3": {
    "exec_flow": "ENTRY→CALL:coarseLock.readLock().unlock→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→EXIT",
    "log": "[INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "9a15b01f_1": {
    "exec_flow": "ENTRY → CALL:initProviderList → IF_TRUE:jobTrackAddr != null → LOG:LOG.INFO:Initializing cluster for Job Tracker= + jobTrackAddr.toString() → FOREACH:providerList → LOG:LOG.DEBUG:Trying ClientProtocolProvider : + provider.getClass().getName() → TRY → IF_TRUE:jobTrackAddr == null → CALL:org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider:create(org.apache.hadoop.conf.Configuration) → IF_TRUE:clientProtocol != null → LOG:LOG.DEBUG:Picked + provider.getClass().getName() + as the ClientProtocolProvider → BREAK → EXIT",
    "log": "[INFO] Initializing cluster for Job Tracker=... [DEBUG] Trying ClientProtocolProvider : ... [DEBUG] Picked ... as the ClientProtocolProvider"
  },
  "9a15b01f_2": {
    "exec_flow": "ENTRY → CALL:initProviderList → IF_FALSE:jobTrackAddr != null → FOREACH:providerList → LOG:LOG.DEBUG:Trying ClientProtocolProvider : + provider.getClass().getName() → TRY → IF_FALSE:jobTrackAddr == null → CALL:org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider:create(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration) → IF_TRUE:clientProtocol != null → LOG:LOG.DEBUG:Picked + provider.getClass().getName() + as the ClientProtocolProvider → BREAK → EXIT",
    "log": "[DEBUG] Trying ClientProtocolProvider : ... [DEBUG] Picked ... as the ClientProtocolProvider"
  },
  "a056a613_1": {
    "exec_flow": "ENTRY→CALL:absOrFqPath.checkNotSchemeWithRelative→CALL:absOrFqPath.checkNotRelative→TRY→CALL:defaultFS.checkPath→EXCEPTION:checkPath→CATCH:Exception e→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction[as:{}][action:{}], this, action, new Exception()→CALL:Subject.doAs→RETURN→EXIT→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→TRY→FOR_EXIT→RETURN→EXIT <step>org.apache.hadoop.fs.FileContext$26:next</step> <step>org.apache.hadoop.fs.AbstractFileSystem:getFileLinkStatus</step>",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {} [INFO] Resolving file system path [DEBUG] Attempting symlink resolution <log>INFO:start</log> <log>DEBUG:Fetching file link status</log> <log>ERROR:UnresolvedLinkException if symlink encountered</log>"
  },
  "cd4de26b_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_FALSE:fs != null → CALL:createFileSystem → CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC:this → IF_TRUE:map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL:ShutdownHookManager.get().addShutdownHook → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Duplicate FS created for {}; discarding {}"
  },
  "2023e218_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "2023e218_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:LOG_DEPRECATION.info</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "2023e218_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:loadDefaults && fullReload</step> <step>FOREACH:defaultResources</step> <step>CALL:loadResource</step> <step>FOREACH_EXIT</step> <step>FOR_INIT</step> <step>FOR_COND:i < resources.size()</step> <step>CALL:loadResource</step> <step>FOR_EXIT</step> <step>CALL:addTags</step> <step>EXIT</step>",
    "log": "<!-- Inherited Child Log Sequence -->"
  },
  "2023e218_4": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE:loadDefaults && fullReload</step> <step>FOR_INIT</step> <step>FOR_COND:i < resources.size()</step> <step>CALL:loadResource</step> <step>FOR_EXIT</step> <step>CALL:addTags</step> <step>EXIT</step>",
    "log": "<!-- Inherited Child Log Sequence -->"
  },
  "57f04549_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object)→Process with additional parameters from org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:DEBUG:Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log>INFO Configuration: Resource initialized with object.</log> <log>DEBUG Configuration: String parameter also processed.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "d7b75909_1": {
    "exec_flow": "ENTRY → CALL:obtainContext → CALL:org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite → IF_TRUE:pathStr.startsWith(\"/\") → CALL:substring → IF_TRUE:size == SIZE_UNKNOWN → FOR_INIT → FOR_COND:i < ctx.dirDF.length → CALL:confChanged → CALL:mkdirs → CALL:debug → FOR_EXIT → IF_FALSE:totalAvailable == 0 → WHILE:numDirsSearched < numDirs && returnPath == null → CALL:createPath → IF_TRUE:returnPath != null → RETURN → EXIT",
    "log": "<log>[DEBUG] mkdirs of {}={}</log>"
  },
  "d7b75909_2": {
    "exec_flow": "ENTRY → IF_TRUE:pathStr.startsWith(\"/\") → CALL:substring → IF_TRUE:size == SIZE_UNKNOWN → FOR_INIT → FOR_COND:i < ctx.dirDF.length → CALL:confChanged → CALL:get → IF_FALSE:null == newLocalDirs → CALL:getLocal → IF_TRUE:!newLocalDirs.equals(ctx.savedLocalDirs) → CALL:mkdirs → CALL:exists → CALL:checkStatus → CALL:warn → CALL:warn → RETURN → EXIT",
    "log": "<log> [DEBUG] mkdirs of {}={} [WARN] Failed to create [dirStrings[i]] [WARN] [dirStrings[i]] is not writable </log>"
  },
  "d7b75909_3": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "d7b75909_4": {
    "exec_flow": "ENTRY → LOG:Error formatting message → EXIT",
    "log": "<log>[DEBUG]Error formatting message</log>"
  },
  "123f03c7_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→TRY→CALL:org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:getTaskAttemptPath→CALL:org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:getTaskAttemptFilesystem→CALL:org.apache.hadoop.fs.FileSystem:listStatus→LOG: org.slf4j.Logger:debug→RETURN→EXIT",
    "log": "[DEBUG] {} files to commit under {}"
  },
  "123f03c7_2": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Listing status for {}, f.toString()→CALL:performAuthCheck→TRY→CALL:retrieveMetadata→IF_TRUE:meta == null→LOG:LOG.DEBUG:Did not find any metadata for path: {}, key→THROW:new FileNotFoundException(f + \" is not found\")→EXIT",
    "log": "[DEBUG] Listing status for {f} [DEBUG] Did not find any metadata for path: {key}"
  },
  "123f03c7_3": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Listing status for {}, f.toString()→CALL:performAuthCheck→TRY→CALL:retrieveMetadata→IF_FALSE:meta == null→IF_TRUE:!meta.isDirectory()→LOG:LOG.DEBUG:Found path as a file→CALL:updateFileStatusPath→RETURN→EXIT",
    "log": "[DEBUG] Listing status for {f} [DEBUG] Found path as a file"
  },
  "123f03c7_4": {
    "exec_flow": "ENTRY → LOG:LOG.INFO:Probe for needsTaskCommit({context.getTaskAttemptID()}) → RETURN → EXIT",
    "log": "[INFO] Probe for needsTaskCommit({context.getTaskAttemptID()})"
  },
  "57c4fda0_1": {
    "exec_flow": "ENTRY→TRY→CALL:updateCurrentResourceLimits→CALL:recalculateQueueUsageRatio→CALL:updateQueueStatistics→CALL:getResourceByLabel→CALL:updateConfiguredCapacityMetrics→CALL:activateApplications→FOR→CALL:computeUserLimitAndSetHeadroom→IF_FALSE: queueUser == null→IF_TRUE: userLimit == null→CALL: getUser→CALL: getResourceLimitForActiveUsers→CALL: setQueueResourceLimitsInfo→IF_TRUE: LOG.isDebugEnabled()→CALL: getUserMetrics→CALL: getHeadroom→LOG: Headroom calculation for user {}: userLimit={} queueMaxAvailRes={} consumed={} partition={}→CALL: setHeadroomProvider→CALL: setAvailableResourcesToUser→RETURN→EXIT→FINALLY→EXIT",
    "log": "[DEBUG] Headroom calculation for user {}: userLimit={} queueMaxAvailRes={} consumed={} partition={}"
  },
  "57c4fda0_2": {
    "exec_flow": "ENTRY→TRY→CALL:updateCurrentResourceLimits→CALL:recalculateQueueUsageRatio→CALL:updateQueueStatistics→CALL:getResourceByLabel→CALL:updateConfiguredCapacityMetrics→CALL:activateApplications→FOR→CALL:computeUserLimitAndSetHeadroom→IF_FALSE: queueUser == null→IF_FALSE: userLimit == null→CALL: setQueueResourceLimitsInfo→IF_TRUE: LOG.isDebugEnabled()→CALL: getUserMetrics→CALL: getHeadroom→LOG: Headroom calculation for user {}: userLimit={} queueMaxAvailRes={} consumed={} partition={}→CALL: setHeadroomProvider→CALL: setAvailableResourcesToUser→RETURN→EXIT→FINALLY→EXIT",
    "log": "[DEBUG] Headroom calculation for user {}: userLimit={} queueMaxAvailRes={} consumed={} partition={}"
  },
  "57c4fda0_3": {
    "exec_flow": "<step>updateConfiguredCapacityMetrics invoked</step> <step>setGuaranteedResources invoked when partition check is met</step>",
    "log": "<!-- No logs found in parent or child, inheriting child execution as valid path -->"
  },
  "8225bc39_1": {
    "exec_flow": "ENTRY → FOR_INIT → FOR_COND:isLink → CALL:org.apache.hadoop.fs.FileContext:getFSofPath → CALL:org.apache.hadoop.fs.FSLinkResolver:next → TRY → FOR_EXIT → RETURN → EXIT",
    "log": "<log> <level>INFO</level> <message>Resolving file system path</message> </log> <log> <level>DEBUG</level> <message>Attempting symlink resolution</message> </log>"
  },
  "8225bc39_2": {
    "exec_flow": "ENTRY → FOR_INIT → FOR_COND:isLink → CALL:org.apache.hadoop.fs.FileContext:getFSofPath → CALL:org.apache.hadoop.fs.FSLinkResolver:next → CATCH:UnresolvedLinkException → CALL:qualifySymlinkTarget → TRY → FOR_COND:isLink → FOR_EXIT → RETURN → EXIT",
    "log": "<log> <level>INFO</level> <message>Resolving file system path</message> </log> <log> <level>WARN</level> <message>Unresolved link encountered</message> </log>"
  },
  "8225bc39_3": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {uri} created while awaiting semaphore\") → RETURN → EXIT → IF(fs != null) → SYNC → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {uri}; discarding {fs}\") → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Filesystem {uri} created while awaiting semaphore</message> </log> <log> <level>DEBUG</level> <message>Duplicate FS created for {uri}; discarding {fs}</message> </log>"
  },
  "8225bc39_4": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Running DeletionTask : {}, this→IF_FALSE: null == getUser()→TRY→LOG: LOG.DEBUG: Deleting path: [{}] as user [{}], subDir, getUser()→CALL: org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:deleteAsUser→CATCH: IOException | InterruptedException e→LOG: LOG.WARN: Failed to delete as user + getUser(), e→CALL: setSuccess→CALL: deletionTaskFinished→EXIT",
    "log": "<log> <level>DEBUG</level> <message>Running DeletionTask : {}</message> </log> <log> <level>DEBUG</level> <message>Deleting path: {} as user {}</message> </log> <log> <level>WARN</level> <message>Failed to delete as user {}</message> </log>"
  },
  "8225bc39_5": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Running DeletionTask : {}, this→IF_FALSE: null == getUser()→TRY→LOG: LOG.DEBUG: Deleting path: [{}] as user [{}], subDir, getUser()→CALL: org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:deleteAsUser→CALL: deletionTaskFinished→EXIT",
    "log": "<log> <level>DEBUG</level> <message>Running DeletionTask : {}</message> </log> <log> <level>DEBUG</level> <message>Deleting path: {} as user {}</message> </log>"
  },
  "ba4e406b_1": {
    "exec_flow": "ENTRY→IF_TRUE:object instanceof Service→CALL:addService→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.debug→CALL:org.slf4j.Logger:debug→SYNC:serviceList→CALL:serviceList.add→RETURN→EXIT",
    "log": "[DEBUG] Adding service + service.getName()"
  },
  "ad2b5082_1": {
    "exec_flow": "ENTRY → IF_TRUE: NameNode.stateChangeLog.isDebugEnabled() → CALL: NameNode.stateChangeLog.debug → CALL: delete → IF_TRUE: filesRemoved < 0 → RETURN → EXIT",
    "log": "[DEBUG] DIR* NameSystem.delete: {iip.getPath()}"
  },
  "ad2b5082_2": {
    "exec_flow": "ENTRY → IF_TRUE: NameNode.stateChangeLog.isDebugEnabled() → CALL: NameNode.stateChangeLog.debug → CALL: delete → IF_FALSE: filesRemoved < 0 → CALL: fsd.getEditLog().logDelete → CALL: incrDeletedFileCount → CALL: removeLeasesAndINodes → IF_TRUE: NameNode.stateChangeLog.isDebugEnabled() → CALL: NameNode.stateChangeLog.debug → RETURN → EXIT",
    "log": "[DEBUG] DIR* NameSystem.delete: {iip.getPath()} [DEBUG] DIR* Namesystem.delete: {iip.getPath()} is removed"
  },
  "ad2b5082_3": {
    "exec_flow": "ENTRY → IF_TRUE: iip.length() < 1 || iip.getLastINode() == null → IF_TRUE: NameNode.stateChangeLog.isDebugEnabled() → CALL: NameNode.stateChangeLog.debug → RETURN → EXIT",
    "log": "[DEBUG] DIR* FSDirectory.unprotectedDelete: failed to remove ${iip.getPath()} because it does not exist"
  },
  "04218268_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → CALL: serviceStop → FINALLY → terminationNotification.set(true) → SYNC: terminationNotification → terminationNotification.notifyAll() → CALL: notifyListeners → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [WARN] Exception while notifying listeners of {}"
  },
  "04218268_2": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → CALL: notifyListeners → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "86eb8044_1": {
    "exec_flow": "ENTRY→IF_TRUE: policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null→CALL: applyRevisionConstraint→IF_TRUE: revisionId != null→LOG: [DEBUG] Restricting metadata request to version {}, revisionId→CALL: withVersionId→RETURN→EXIT",
    "log": "[DEBUG] Restricting metadata request to version {}"
  },
  "86eb8044_2": {
    "exec_flow": "ENTRY→IF_TRUE: policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null→CALL: applyRevisionConstraint→IF_FALSE: revisionId != null→LOG: [DEBUG] No version ID to use as a constraint→RETURN→EXIT",
    "log": "[DEBUG] No version ID to use as a constraint"
  },
  "86eb8044_3": {
    "exec_flow": "ENTRY→IF_TRUE: policy.getMode() == ChangeDetectionPolicy.Mode.Server && revisionId != null→CALL: applyRevisionConstraint→LOG: [DEBUG] Unable to restrict HEAD request to etag; will check later→RETURN→EXIT",
    "log": "[DEBUG] Unable to restrict HEAD request to etag; will check later"
  },
  "86eb8044_4": {
    "exec_flow": "ENTRY→CALL:policy.getRevisionId→LOG:debug→CALL:policy.getSource→CALL:policy.isRequireVersion→IF_TRUE→THROW:NoVersionAttributeException→EXIT",
    "log": "[DEBUG] Copy result {policy.getSource()}: {newRevisionId}"
  },
  "86eb8044_5": {
    "exec_flow": "ENTRY→CALL:policy.getRevisionId→LOG:debug→CALL:policy.getSource→CALL:policy.isRequireVersion→IF_FALSE→EXIT",
    "log": "[DEBUG] Copy result {policy.getSource()}: {newRevisionId}"
  },
  "172e3351_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→IF_FALSE: chain != null AND chain.getRootInterceptor() != null→CALL:org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline→SYNC: this.userPipelineMap→IF_TRUE: this.userPipelineMap.containsKey(user)→LOG: [INFO] Request to start an already existing user: {} was received, so ignoring.→CALL: get→RETURN→EXIT",
    "log": "[INFO] Request to start an already existing user: {} was received, so ignoring."
  },
  "172e3351_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→IF_FALSE: chain != null AND chain.getRootInterceptor() != null→CALL:org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline→SYNC: this.userPipelineMap→IF_FALSE: this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: [INFO] Initializing request processing pipeline for application for the user: {}→CALL: interceptorChain.init→CALL: chainWrapper.init→CALL: this.userPipelineMap.put→RETURN→EXIT",
    "log": "[INFO] Initializing request processing pipeline for application for the user: {}"
  },
  "172e3351_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→IF_FALSE: chain != null AND chain.getRootInterceptor() != null→CALL:org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline→SYNC: this.userPipelineMap→IF_FALSE: this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: [INFO] Initializing request processing pipeline for application for the user: {}→EXCEPTION: info→CATCH: Exception e→LOG: [ERROR] Init ClientRequestInterceptor error for user: {}→THROW: e→EXIT",
    "log": "[INFO] Initializing request processing pipeline for application for the user: {} [ERROR] Init ClientRequestInterceptor error for user: {}"
  },
  "7bc0d5d4_1": {
    "exec_flow": "ENTRY→CALL:trim→CALL:toLowerCase→IF_TRUE:null==vUnit→CALL:unitFor→IF_TRUE:vUnit.unit().convert(converted,returnUnit)<raw→CALL:logDeprecation→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}"
  },
  "7bc0d5d4_2": {
    "exec_flow": "ENTRY→CALL:trim→CALL:toLowerCase→IF_FALSE:null==vUnit→CALL:substring→CALL:lastIndexOf→CALL:suffix→CALL:suffix→CALL:lastIndexOf→CALL:suffix→CALL:suffix→IF_TRUE:vUnit.unit().convert(converted,returnUnit)<raw→CALL:logDeprecation→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}"
  },
  "b7339aba_1": {
    "exec_flow": "ENTRY → CALL: Thread.setDefaultUncaughtExceptionHandler → LOG: LOG.INFO: createStartupShutdownMessage → IF_TRUE: SystemUtils.IS_OS_UNIX → TRY → CALL: SignalLogger.INSTANCE.register → CATCH: Throwable t → LOG: LOG.WARN: failed to register any UNIX signal loggers: → CALL: ShutdownHookManager.get().addShutdownHook → NEW: JobHistoryServer → NEW: JobConf → CALL: YarnConfiguration.<init> → CALL: GenericOptionsParser.<init> → IF_TRUE: line.hasOption(\"fs\") → CALL: FileSystem.setDefaultUri → CALL: getLocal → CALL: makeQualified → CALL: getFileStatus → CALL: isDebugEnabled → CALL: UserGroupInformation.getCurrentUser → CALL: addCredentials → CALL: readTokenStorageFile → CALL: set → CALL: debug → IF_TRUE: line.hasOption(\"conf\") → FOREACH: values → CALL: addResource → FOREACH_EXIT → CALL: setBoolean → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → RETURN → EXIT → CALL: jobHistoryServer.start → EXCEPTION: start → CATCH: Throwable t → LOG: LOG.ERROR: Error starting JobHistoryServer → CALL: ExitUtil.terminate → VIRTUAL_CALL: CompositeServiceShutdownHook.run → LOG: LOG.WARN: When stopping the service {serviceName} → RETURN → EXIT → IF_FALSE: SystemUtils.IS_OS_UNIX → CALL: ShutdownHookManager.get().addShutdownHook → RETURN → EXIT → IF_FALSE: isInState(STATE.STARTED) → SYNC: stateChangeLock → IF_TRUE: stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL: currentTimeMillis → CALL: serviceStart → IF_TRUE: isInState(STATE.STARTED) → CALL: debug → TRY → SYNC: this → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → RETURN → EXIT → CALL: stopQuietly",
    "log": "<log_entry> <level>INFO</level> <template>createStartupShutdownMessage</template> </log_entry> <log_entry> <level>WARN</level> <template>failed to register any UNIX signal loggers:</template> </log_entry> <log_entry> <level>DEBUG</level> <template>setting conf tokensFile: [fileName value]</template> </log_entry> <log_entry> <level>ERROR</level> <template>Error starting JobHistoryServer</template> </log_entry> <log_entry> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log_entry> <log_entry> <level>WARN</level> <template>When stopping the service {serviceName}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Config has been overridden during init</template> </log_entry>"
  },
  "8bc1f5e0_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL: getNameserviceForBlockPoolId → CALL: invokeSingle → RETURN → EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials"
  },
  "2f72f671_1": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: [DEBUG] PrivilegedAction [as: {}][action: {}] → CALL: Subject.doAs → IF_TRUE: loginUser==null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser==null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String) item → CALL: handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "2f72f671_2": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: Refresh request received for nameservices: {conf.get(DFSConfigKeys.DFS_NAMESERVICES)} → CALL: DFSUtil.getNNServiceRpcAddressesForCluster → TRY_CATCH: IOException → LOG: Unable to get NameNode addresses. → IF_TRUE: newAddressMap==null || newAddressMap.isEmpty() → THROW: IOException → SYNC: refreshNamenodesLock → CALL: doRefreshNamenodes → EXIT",
    "log": "[INFO] Refresh request received for nameservices: {conf.get(DFSConfigKeys.DFS_NAMESERVICES)} [WARN] Unable to get NameNode addresses."
  },
  "34d2d7c5_1": {
    "exec_flow": "ENTRY→LOG:LOG.INFO:Removing RMDelegationToken_ + identifier.getSequenceNumber()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:deleteFileWithRetries→ENTRY→CALL:runWithRetries→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH:IOException→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)→IF:retry ≤ fsNumRetries→LOG:org.slf4j.Logger:info(\"Retrying operation on FS. Retry no. X\")→CALL:Thread.sleep→WHILE_COND:true→RETURN→EXIT→EXIT",
    "log": "[INFO] Removing RMDelegationToken_{sequenceNumber} [INFO] Deleting file with retries [INFO] Exception while executing an FS operation. [INFO] Retrying operation on FS. Retry no. X"
  },
  "619c74c0_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:executeOnlyOnce → LOG:LOG.DEBUG(\"Delete path {} - recursive {}\", path, recursive) → LOG:LOG.DEBUG(\"Type = {}\", status.isFile() ? File : (status.isEmptyDirectory() == Tristate.TRUE ? Empty Directory : Directory)) → IF_TRUE:status.isDirectory() → LOG:LOG.DEBUG(\"delete: Path is a directory: {}\", path) → IF_FALSE:!key.endsWith(\"/\") AND status.isEmptyDirectory() == Tristate.FALSE → CALL:deleteDirectoryTree → LOG:LOG.DEBUG(\"Deleted {} objects\", filesDeleted) → EXIT",
    "log": "[DEBUG] Delete path {} - recursive {} [DEBUG] Type = {} [DEBUG] delete: Path is a directory: {} [DEBUG] Deleted {} objects"
  },
  "619c74c0_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:executeOnlyOnce → LOG:LOG.DEBUG(\"Delete path {} - recursive {}\", path, recursive) → LOG:LOG.DEBUG(\"Type = {}\", status.isFile() ? File : (status.isEmptyDirectory() == Tristate.TRUE ? Empty Directory : Directory)) → IF_FALSE:status.isDirectory() → LOG:LOG.DEBUG(\"deleting simple file {}\", path) → CALL:deleteObjectAtPath → LOG:LOG.DEBUG(\"Deleted {} objects\", filesDeleted) → EXIT",
    "log": "[DEBUG] Delete path {} - recursive {} [DEBUG] Type = {} [DEBUG] deleting simple file {} [DEBUG] Deleted {} objects"
  },
  "619c74c0_3": {
    "exec_flow": "ENTRY→TRY→CALL:invokeTrackingDuration→IF(executed.getAndSet(true))→CATCH:IllegalStateException→CALL:org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→RETURN→EXIT",
    "log": "[DEBUG] Exception in closing {} [DEBUG] drain or abort reason {} remaining={} abort={} [DEBUG] draining {} bytes [DEBUG] read {} bytes [DEBUG] Drained stream of {} bytes [DEBUG] drained fewer bytes than expected; {} remaining [DEBUG] Closing stream [DEBUG] When closing {} stream for {}, will abort the stream [DEBUG] Aborting stream {} [WARN] When aborting {} stream after failing to close it for {} [DEBUG] Stream {} aborted: {}; remaining={}"
  },
  "329a0e55_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → TRY → CALL:faultInjectorEventHook.beforeFileIo → CALL:Storage.nativeCopyFileUnbuffered → CALL:profilingEventHook.afterFileIo → EXCEPTION:afterFileIo → CATCH:Exception e → CALL:onFailure → ENTRY → CALL:volumeChecker.checkVolume → CALL:lambda$0 → EXIT",
    "log": "[WARN] checkDiskErrorAsync callback got {} failed volumes: {} [DEBUG] checkDiskErrorAsync: no volume failures detected"
  },
  "329a0e55_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → TRY → CALL:faultInjectorEventHook.beforeFileIo → CALL:Storage.nativeCopyFileUnbuffered → EXCEPTION:nativeCopyFileUnbuffered → CATCH:Exception e → CALL:onFailure → ENTRY → CALL:volumeChecker.checkVolume → CALL:lambda$0 → EXIT",
    "log": "[WARN] checkDiskErrorAsync callback got {} failed volumes: {} [DEBUG] checkDiskErrorAsync: no volume failures detected"
  },
  "329a0e55_3": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → TRY → CALL:faultInjectorEventHook.beforeFileIo → CALL:Storage.nativeCopyFileUnbuffered → CALL:profilingEventHook.afterFileIo → EXCEPTION:afterFileIo → CATCH:Exception e → CALL:onFailure → ENTRY → CALL:volumeChecker.checkVolume → CALL:lambda$0 → EXIT",
    "log": "[WARN] checkDiskErrorAsync callback got {} failed volumes: {} [DEBUG] checkDiskErrorAsync: no volume failures detected"
  },
  "329a0e55_4": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_FALSE: srcFile == null → IF_FALSE: destFile == null → IF_FALSE: srcFile.exists() == false → IF_FALSE: srcFile.isDirectory() → IF_FALSE: srcFile.getCanonicalPath().equals(destFile.getCanonicalPath()) → IF_TRUE: parentFile != null → IF_FALSE: !parentFile.mkdirs() && !parentFile.isDirectory() → IF_TRUE: destFile.exists() → IF_FALSE: FileUtil.canWrite(destFile) == false → IF_FALSE: destFile.delete() == false → TRY → CALL: NativeIO.copyFileUnbuffered → IF_FALSE: srcFile.length() != destFile.length() → IF_TRUE: preserveFileDate → IF_TRUE: destFile.setLastModified(srcFile.lastModified()) == false → EXIT",
    "log": "[DEBUG] Failed to preserve last modified date from '{srcFile}' to '{destFile}'"
  },
  "44599c28_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "44599c28_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ef55eadd_1": {
    "exec_flow": "<step>Check if directory/file exists</step> <step>Check if user is superuser</step> <step>Acquire read lock</step> <step>Check user permission against cache pool owner</step> <step>Check group permission</step> <step>Check other permissions</step> <step>Permission denied - AccessControlException thrown</step> <step>Release read lock</step>",
    "log": "<log> <template>Permission denied while accessing pool {PoolName}: user {User} does not have {Access} permissions.</template> <parameters> <parameter name=\"PoolName\" value=\"{pool.getPoolName()}\" /> <parameter name=\"User\" value=\"{getUser()}\" /> <parameter name=\"Access\" value=\"{access.toString()}\" /> </parameters> </log>"
  },
  "c1e53863_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus</step> <step>IF_FALSE:key.length()==0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE:meta!=null</step> <step>IF_TRUE:meta.isFile()</step> <step>LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}]</step> <step>CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "c1e53863_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus</step> <step>IF_FALSE:key.length()==0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE:meta!=null</step> <step>IF_FALSE:meta.isFile()</step> <step>LOG:LOG.DEBUG:Path: [{}] is a dir. COS key: [{}]</step> <step>CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "c1e53863_3": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus</step> <step>IF_FALSE:key.length()==0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE:meta!=null</step> <step>LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list</step> <step>IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG:LOG.DEBUG:Path: [{}] is a directory. COS key: [{}]</step> <step>CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "c1e53863_4": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus</step> <step>IF_FALSE:key.length()==0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE:meta!=null</step> <step>LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list</step> <step>IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0</step> <step>IF_FALSE:LOG.isDebugEnabled()</step> <step>CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path."
  },
  "c1e53863_5": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getTrashRoots</step> <step>CALL:org.apache.hadoop.fs.FilterFileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileStatus</step> <step>CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus</step> <step>IF_FALSE:key.length()==0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE:meta!=null</step> <step>LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list</step> <step>IF_FALSE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0</step> <step>THROW:FileNotFoundException</step> <step>EXIT</step>",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path."
  },
  "ed018473_1": {
    "exec_flow": "<exec_flow_node>Parent.ENTRY</exec_flow_node> <exec_flow_node>[VIRTUAL_CALL]</exec_flow_node> <exec_flow_node>Child paths</exec_flow_node>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Open the file: [{f}] for reading.</log>"
  },
  "ed018473_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:[INFO] Open the file: [{f}] for reading. → LOG:[DEBUG] Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[INFO] Open the file: [{f}] for reading.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "ed018473_3": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isFile() → LOG:[DEBUG] Path: [{}] is a dir. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log>"
  },
  "ed018473_4": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG:[DEBUG] List COS key: [{}] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0 → IF_TRUE:LOG.isDebugEnabled() → LOG:[DEBUG] Path: [{}] is a directory. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "85644758_1": {
    "exec_flow": "ENTRY→CALL:getWrapped→CALL:abortTask→IF_TRUE:hasOutputPath()→CALL:progress→CALL:getFileSystem→CALL:delete→IF_TRUE:!fs.delete(taskAttemptPath, true)→LOG:LOG.WARN:Could not delete + taskAttemptPath→EXIT",
    "log": "LOG.warn(\"Could not delete \" + taskAttemptPath)"
  },
  "85644758_2": {
    "exec_flow": "ENTRY→CALL:getWrapped→CALL:abortTask→IF_FALSE:hasOutputPath()→LOG:LOG.WARN:Output Path is null in abortTask()→EXIT",
    "log": "LOG.warn(\"Output Path is null in abortTask()\")"
  },
  "85644758_3": {
    "exec_flow": "ENTRY→CALL:enterCommitter→TRY→CALL:AbortTaskStage.apply→EXEC_SEQ:CALL:executeOnlyOnce→CALL:progress→CALL:getStageConfig().enterStage→LOG:LOG.INFO:{}:Executing Stage {},getName(),stageName→CALL:createTracker→CALL:getIOStatistics→TRY→CALL:executeStage→EXCEPTION→LOG:LOG.ERROR:{}:Stage {} failed: after {}, {},getName(),stageName,OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis())→LOG:LOG.DEBUG:{}:Stage failure:,getName(),e→CALL:progress→CALL:getStageConfig().exitStage→CALL:org.apache.hadoop.fs.statistics.DurationTracker:close()→EXIT→CALL:createManifestStoreOperations→CALL:org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug→CALL:org.slf4j.Logger:isDebugEnabled→CALL:org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString→CALL:org.slf4j.Logger:debug→EXIT→CALL:updateCommonContextOnCommitterExit→EXIT",
    "log": "[INFO] {}: Executing Stage {}, getName(), stageName [ERROR] {}: Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString() [DEBUG] {}: Stage failure:, getName(), e [DEBUG] Committer Statistics"
  },
  "85644758_4": {
    "exec_flow": "ENTRY→CALL:currentAuditContext()→IF_TRUE: LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→CALL: org.slf4j.Logger:trace→LOG: LOG.TRACE: Remove context entry {}, PARAM_JOB_ID→CALL: evaluatedEntries.remove→RETURN→CALL:currentAuditContext()→IF_TRUE: LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→CALL: org.slf4j.Logger:trace→LOG: LOG.TRACE: Remove context entry {}, CONTEXT_ATTR_TASK_ATTEMPT_ID→CALL: evaluatedEntries.remove→RETURN→EXIT",
    "log": "[TRACE] Remove context entry {PARAM_JOB_ID}, [TRACE] Remove context entry {CONTEXT_ATTR_TASK_ATTEMPT_ID}"
  },
  "defc2ed2_1": {
    "exec_flow": "ENTRY→CALL:incrementWriteOps→CALL:storageStatistics.incrementOpCounter→CALL:applyUMask→CALL:run→NEW:FsPathBooleanRunner→NEW:PermissionParam→CALL:getMasked→NEW:UnmaskedPermissionParam→CALL:getUnmasked→RETURN→EXIT",
    "log": "[DEBUG] Incremented write operations count [DEBUG] Incremented storage statistics OpCounter [INFO] Applying UMask [DEBUG] FsPathBooleanRunner is running"
  },
  "defc2ed2_2": {
    "exec_flow": "ENTRY → IF_TRUE: permission == null → CALL:getDefault → CALL:applyUMask → CALL:getUMask → CALL:getConf → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "defc2ed2_3": {
    "exec_flow": "ENTRY → IF_FALSE: permission == null → CALL:applyUMask → CALL:getUMask → CALL:getConf → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "57444d89_1": {
    "exec_flow": "ENTRY→IF_FALSE: target >= getFileLength()→CALL:maybeRegisterBlockRefresh→CALL:closeCurrentBlockReaders→WHILE:true→WHILE_COND:true→CALL:getBlockAt→CALL:chooseDataNode→TRY→CALL:getBlockReader→EXCEPTION:IOException→CALL:checkInterrupted→IF_TRUE:ex instanceof InvalidEncryptionKeyException→CALL:info→CALL:clearDataEncryptionKey→WHILE_CONTINUE",
    "log": "[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to ... [DEBUG] Clearing encryption key"
  },
  "57444d89_2": {
    "exec_flow": "ENTRY→IF_FALSE: target >= getFileLength()→CALL:maybeRegisterBlockRefresh→CALL:closeCurrentBlockReaders→WHILE:true→WHILE_COND:true→CALL:getBlockAt→CALL:chooseDataNode→TRY→CALL:getBlockReader→EXCEPTION:IOException→CALL:checkInterrupted→IF_FALSE:ex instanceof InvalidEncryptionKeyException→IF_FALSE:tokenRefetchNeeded→CALL:warn→CALL:addToLocalDeadNodes→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→CALL: deadNodes.put→WHILE_CONTINUE",
    "log": "[WARN] Failed to connect to ... for file ... for block ..., add to deadNodes and continue. [DEBUG] Add {} to local dead nodes, previously was {}."
  },
  "57444d89_3": {
    "exec_flow": "ENTRY→CALL:maybeRegisterBlockRefresh→SYNC:infoLock→IF_TRUE:targetBlockIdx<0→CALL:getInsertIndex→IF_TRUE:!useCache→CALL:dfsClient.getLocatedBlocks→IF_FALSE:newBlocks==null||newBlocks.locatedBlockCount()==0→IF_FALSE:offset>=locatedBlocks.getFileLength()→CALL:setLocatedBlocksFields→CALL:get→RETURN→EXIT",
    "log": "[DEBUG] Failed to getReplicaVisibleLength from datanode {} for block {}"
  },
  "57444d89_4": {
    "exec_flow": "ENTRY→IF_TRUE: Thread.currentThread().isInterrupted() && (e instanceof ClosedByInterruptException || e instanceof InterruptedIOException)→CALL:DFSClient.LOG.debug→THROW:e→EXIT",
    "log": "[DEBUG] The reading thread has been interrupted."
  },
  "57444d89_5": {
    "exec_flow": "ENTRY→IF_FALSE: blockReader == null→TRY→CALL: blockReader.close→EXCEPTION: close→CATCH: IOException e→CALL: DFSClient.LOG.error(java.lang.String,java.lang.Throwable)→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[ERROR] error closing blockReader"
  },
  "57444d89_6": {
    "exec_flow": "ENTRY→IF_FALSE: closed → CALL:IOUtilsClient.cleanupWithLogger→IF_TRUE:(slowReadBuff != null)→CALL:bufferPool.returnBuffer→IF_TRUE:(checksumBuff != null)→CALL:bufferPool.returnBuffer→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close→CATCH→IF(log != null AND log.isDebugEnabled())→CALL:log.isDebugEnabled→CALL:log.debug→FOREACH_EXIT→CALL: unref → CALL: org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:unref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica) → CALL: freeDataBufIfExists → CALL: freeChecksumBufIfExists → IF_TRUE: metrics != null → CALL: collectThreadLocalStates→EXIT",
    "log": "[DEBUG] Exception in closing closeable [TRACE] close(filename={}, block={})"
  },
  "57444d89_7": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → CALL:Preconditions.checkNotNull → IF_FALSE:peer.isClosed() → IF_FALSE:capacity <= 0 → CALL:putInternal → EXIT",
    "log": "<!-- Child log sequence retained as parent has no logs -->"
  },
  "57444d89_8": {
    "exec_flow": "ENTRY→WHILE: true →WHILE_COND: true →CALL:getBestNodeDNAddrPair →IF_FALSE: result != null →CALL:refetchLocations →IF_FALSE→IF_TRUE→CALL:DFSClient.LOG.info→CALL:DFSClient.LOG.info→TRY→CALL:DFSClient.LOG.warn→CATCH→CALL:Thread.currentThread().interrupt→THROW:InterruptedIOException→EXIT",
    "log": "[INFO] No node available for {blockInfo} [INFO] Could not obtain {block.getBlock()} from any node: {errMsg}. Will get new block locations from namenode and retry... [WARN] DFS chooseDataNode: got #{failures + 1} IOException, will wait for {waitTime} msec."
  },
  "57444d89_9": {
    "exec_flow": "ENTRY→WHILE: true →WHILE_COND: true →CALL:getBestNodeDNAddrPair →IF_FALSE: result != null →CALL:refetchLocations →IF_FALSE→IF_FALSE→CALL:DFSClient.LOG.info→TRY→CALL:DFSClient.LOG.warn→CATCH→CALL:Thread.currentThread().interrupt→THROW:InterruptedIOException→EXIT",
    "log": "[INFO] Could not obtain {block.getBlock()} from any node: {errMsg}. Will get new block locations from namenode and retry... [WARN] DFS chooseDataNode: got #{failures + 1} IOException, will wait for {waitTime} msec."
  },
  "57444d89_10": {
    "exec_flow": "ENTRY→IF_TRUE: dfsClient.getConf().isReadUseCachePriority()→IF_TRUE: cachedLocs != null→FOR_INIT→FOR_COND: i < cachedLocs.length→FOR_EXIT→IF_FALSE: chosenNode == null",
    "log": "[DEBUG] Connecting to datanode {dnAddr}"
  },
  "57444d89_11": {
    "exec_flow": "ENTRY→IF_FALSE: dfsClient.getConf().isReadUseCachePriority()→IF_TRUE: chosenNode == null && nodes != null→FOR_INIT→FOR_COND: i < nodes.length→FOR_EXIT→IF_FALSE: chosenNode == null",
    "log": "[DEBUG] Connecting to datanode {dnAddr}"
  },
  "57444d89_12": {
    "exec_flow": "ENTRY→CALL:DFSClient.LOG.warn→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "[WARN] No live nodes contain block"
  },
  "83bdc96e_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf.isEmpty() → IF_TRUE: filter != null → CALL: org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin → ENTRY → LOG: LOG.DEBUG: Class name for prefix {} is {}, prefix, clsName → IF_FALSE: clsName == null → CALL:getClassName → TRY → CALL:forName → CALL:newInstance → CALL:init → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Class name for prefix {} is {}"
  },
  "83bdc96e_2": {
    "exec_flow": "ENTRY → IF_FALSE: conf.isEmpty() → IF_TRUE: filter != null → CALL: org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin → ENTRY → LOG: LOG.DEBUG: Class name for prefix {} is {}, prefix, clsName → IF_FALSE: clsName == null → CALL:getClassName → TRY → CALL:forName → CALL:newInstance → CALL:init → EXCEPTION:init → CATCH:Exception e → THROW:new MetricsConfigException(\"Error creating plugin: \" + clsName, e) → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Class name for prefix {} is {}"
  },
  "83bdc96e_3": {
    "exec_flow": "ENTRY → IF_FALSE: conf.isEmpty() → IF_TRUE: filter != null → CALL: org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin → ENTRY → LOG: LOG.DEBUG: Class name for prefix {} is {}, prefix, clsName → IF_FALSE: clsName == null → CALL:getClassName → TRY → CALL:forName → EXCEPTION:forName → CATCH:Exception e → THROW:new MetricsConfigException(\"Error creating plugin: \" + clsName, e) → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Class name for prefix {} is {}"
  },
  "83bdc96e_4": {
    "exec_flow": "ENTRY → IF_FALSE: conf.isEmpty() → IF_TRUE: filter != null → CALL: org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin → ENTRY → LOG: LOG.DEBUG: Class name for prefix {} is {}, prefix, clsName → IF_TRUE: clsName == null → RETURN → EXIT → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Class name for prefix {} is {}"
  },
  "83bdc96e_5": {
    "exec_flow": "ENTRY → IF_FALSE: conf.isEmpty() → IF_TRUE: filter != null → CALL: org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin → ENTRY → IF_FALSE: pluginLoader != null → IF_FALSE: purls == null → IF_TRUE: len > 0 → TRY → FOREACH: jars → LOG: [DEBUG] Parsing URL for {} → FOREACH_EXIT → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled → LOG: [DEBUG] Using plugin jars: {} → CALL: doPrivileged → NEW: PrivilegedAction<ClassLoader> → NEW: URLClassLoader → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Parsing URL for {} [DEBUG] Using plugin jars: {}"
  },
  "6faed93e_1": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: conf == null</step> <step>IF_FALSE: isInState(STATE.INITED)</step> <step>SYNC: stateChangeLock</step> <step>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</step> <step>CALL: setConfig</step> <step>TRY</step> <step>CALL: serviceInit</step> <step>IF_TRUE: isInState(STATE.INITED)</step> <step>CALL: notifyListeners</step> <step>CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)</step> <step>CALL: noteFailure</step> <step>CALL: recordLifecycleEvent</step> <step>CALL: ServiceOperations.stopQuietly</step> <step>EXIT</step> <step>ENTRY→IF_TRUE</step> <step>CALL: org.slf4j.Logger:debug(java.lang.String)</step> <step>CALL: setConfig</step> <step>EXIT</step> <step>ENTRY</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>TRY</step> <step>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>CATCH: Throwable e</step> <step>CALL: LOG.WARN: Exception while notifying listeners of {}, this, e</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Service: {} entered state {}</log> <log>[DEBUG] Config has been overridden during init</log> <log>[DEBUG] noteFailure</log> <log>[INFO] Service {} failed in state {}</log> <log message=\"When stopping the service {service.getName()}\" level=\"warn\"/> <log>[WARN] Exception while notifying listeners of {}</log>"
  },
  "4a53dc70_1": {
    "exec_flow": "<ENTRY>org.apache.hadoop.mapred.ResourceMgrDelegate:getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)</ENTRY> <VIRTUAL_CALL>org.apache.hadoop.yarn.client.api.YarnClient:getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)</VIRTUAL_CALL> <TRY> <CALL>getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)</CALL> <CALL>org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)</CALL> <CALL>setApplicationAttemptId</CALL> <CALL>org.apache.hadoop.yarn.api.ApplicationClientProtocol:getContainers</CALL> <CALL>containersForAttempt.addAll</CALL> <TRY> <CALL>getContainerReportFromHistory</CALL> <IF_FALSE>null != containersListFromAHS && containersListFromAHS.size() > 0</IF_FALSE> </TRY> <RETURN/> </TRY> <EXIT/>",
    "log": "[WARN] Got an error while fetching container report from ATSv2"
  },
  "88a3d16e_1": {
    "exec_flow": "ENTRY → IF:hsr!=null → CALL:getCallerUserGroupInformation → IF:callerUGI==null → LOG:LOG.ERROR:Unable to obtain user name, user not authenticated → RETURN → EXIT ELSE:CALL:createRemoteUser → IF:callerUGI==null → LOG:LOG.ERROR:Unable to obtain user name, user not authenticated → RETURN → EXIT ELSE:LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}] → CALL:doAs → [VIRTUAL_CALL] → IF_TRUE: overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → TRY → CALL: handleDeprecation → FOREACH: names → CALL: getProps → CALL: substituteVars → FOREACH_EXIT → LOG: Handling deprecation for all properties in config... → CATCH: IOException → THROW: new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe) → EXIT",
    "log": "[ERROR] Unable to obtain user name, user not authenticated [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] Handling deprecation for all properties in config..."
  },
  "88a3d16e_2": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→IF_TRUE:!isInitNodeLabelStoreInProgress()→CALL:checkRemoveFromClusterNodeLabelsOfQueue →CALL:cloneNodeMap→CALL:removeFromClusterNodeLabels→CALL:updateResourceMappings →CALL:writeLock.unlock→EXIT",
    "log": "[INFO] Resource mappings updated"
  },
  "88a3d16e_3": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→IF_FALSE:!isInitNodeLabelStoreInProgress()→CALL:cloneNodeMap →CALL:removeFromClusterNodeLabels→CALL:updateResourceMappings→CALL:writeLock.unlock→EXIT",
    "log": "[INFO] Resource mappings updated"
  },
  "1be68f7e_1": {
    "exec_flow": "ENTRY → IF_TRUE: fields == null || fields.isEmpty() → CALL:add → CALL:mergeFilters → CALL:doGetUri → IF_TRUE: resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode() → LOG: LOG.ERROR: msg → CALL: org.slf4j.Logger:error → THROW: new IOException(msg) → EXIT",
    "log": "[DEBUG] Fields parameter is empty, defaulting to INFO [ERROR] Response from the timeline reader server is ..."
  },
  "1be68f7e_2": {
    "exec_flow": "ENTRY → IF_FALSE: fields == null || fields.isEmpty() → CALL:add → CALL:mergeFilters → CALL:doGetUri → IF_TRUE: resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode() → LOG: LOG.ERROR: msg → CALL: org.slf4j.Logger:error → THROW: new IOException(msg) → EXIT",
    "log": "[DEBUG] Fields parameter provided [ERROR] Response from the timeline reader server is ..."
  },
  "50cedbd1_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.crypto.key.KeyProviderExtension:getMetadata→RETURN→EXIT →CALL:checkNotEmpty→CALL:createURL→TRY→CALL:getDoAsUser→CALL:getActualUgi →LOG:PrivilegedAction [as: {}][action: {}]→NEW:PrivilegedExceptionAction<HttpURLConnection> →CALL:createAuthenticatedURL→CALL:openConnection→CALL:getActualUgi →CALL:setUseCaches→CALL:setRequestMethod →IF_FALSE:method.equals(HTTP_POST)||method.equals(HTTP_PUT) →CALL:configureConnection→RETURN→CALL:call→CALL:parseJSONMetadata→EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "50cedbd1_2": {
    "exec_flow": "ENTRY→IF_FALSE:providers.length==0→FOR_INIT→FOR_COND:TRUE→TRY→CALL:call→CATCH:IOException →CALL:warn→CALL:shouldRetry→IF_TRUE:numFailovers>=providers.length-1→CALL:error→THROW:ioe→EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException:"
  },
  "50cedbd1_3": {
    "exec_flow": "ENTRY→IF_FALSE:providers.length==0→FOR_INIT→FOR_COND:TRUE→TRY→CALL:call→CATCH:IOException →CALL:shouldRetry→IF_TRUE:numFailovers>=providers.length-1→CALL:error→THROW:ioe→EXIT",
    "log": "[ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable"
  },
  "50cedbd1_4": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:jsonOutput != null→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) || conn.getResponseMessage().contains(INVALID_SIGNATURE))) || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED →IF_TRUE:LOG.isDebugEnabled()→LOG:[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW:DelegationTokenAuthenticatedURL.Token →IF_TRUE:authRetryCount > 0→CALL:createConnection→CALL:call→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "3dd388a1_1": {
    "exec_flow": "ENTRY → IF_FALSE: name == null || name.isEmpty() → IF_TRUE: YarnConfiguration.RM_CONFIGURATION_FILES.contains(name) || YarnConfiguration.NM_CONFIGURATION_FILES.contains(name) → CALL: getConfResourceAsInputStream → ENTRY → TRY → IF_FALSE: url == null → CALL: org.slf4j.Logger:info(java.lang.String) → CALL: openStream → RETURN → IF_TRUE: services != null → FOREACH: services → CALL: org.apache.hadoop.conf.Configuration:get → CALL: org.apache.hadoop.security.authorize.AccessControlList:<init> → CALL: org.apache.hadoop.util.MachineList:<init> → FOREACH_EXIT→EXIT",
    "log": "[INFO] found resource name at url"
  },
  "3dd388a1_2": {
    "exec_flow": "ENTRY → IF_FALSE: name == null || name.isEmpty() → IF_TRUE: YarnConfiguration.RM_CONFIGURATION_FILES.contains(name) || YarnConfiguration.NM_CONFIGURATION_FILES.contains(name) → NEW: Path → CALL: org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path) → IF_TRUE: !fs.exists(filePath) → CALL: org.slf4j.Logger:info(java.lang.String) → LOG: [INFO] {filePath} not found → RETURN → IF_FALSE: services != null → EXIT",
    "log": "[INFO] {filePath} not found"
  },
  "3dd388a1_3": {
    "exec_flow": "ENTRY→IF_TRUE: services != null→FOREACH: services→CALL:org.apache.hadoop.conf.Configuration:get→CALL:org.apache.hadoop.security.authorize.AccessControlList:<init>→CALL:org.apache.hadoop.util.MachineList:<init>→FOREACH_EXIT→EXIT",
    "log": "<!-- Merged log sequence remains empty as original logs are empty -->"
  },
  "e6c9ffb7_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:org.apache.hadoop.fs.RemoteIterator:hasNext() → IF_TRUE: batchIterator.hasNext() → CALL:requestNextBatch → IF_TRUE: lister.hasNext() → CALL:org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:next() → CALL:getMultipartUploads → CALL:listIterator → CALL:hasNext → RETURN → EXIT",
    "log": "<log>[DEBUG] Next element retrieved from RemoteIterator</log> <log>[DEBUG] Requesting next {} uploads prefix {}, next key {}, next upload id {}</log> <log>[DEBUG] Listing found {} upload(s)</log> <log>[DEBUG] New listing state: {}</log>"
  },
  "dececf1a_1": {
    "exec_flow": "ENTRY→TRY→CALL:processQueue→EXCEPTION:processQueue→CATCH:Throwable t→CALL:terminate→ CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→ CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→ CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "27813cb7_1": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→CALL:stateMachine.doTransition→IF_TRUE:oldState!=getState()→CALL:LOG.info→CALL:writeLock.unlock→EXIT",
    "log": "[INFO] [CompInstanceId] Transitioned from [oldState] to [getState] on [eventType] event"
  },
  "27813cb7_2": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→CATCH:InvalidStateTransitionException e→CALL:org.slf4j.Logger:error→IF_TRUE:oldState!=getState()→CALL:org.slf4j.Logger:info→CALL:writeLock.unlock→LOG:LOG.INFO: Stopping resource-monitoring for {}, containerId→CALL:updateContainerMetrics→CALL:trackingContainers.remove→EXIT",
    "log": "[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState [INFO] [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType() [INFO] Stopping resource-monitoring for {}"
  },
  "27813cb7_3": {
    "exec_flow": "ENTRY→IF_FALSE: LOG.isDebugEnabled()→CALL: writeLock.lock→TRY→TRY→CALL: stateMachine.doTransition→IF_TRUE: oldState != getState()→IF_TRUE: getState() == TaskAttemptStateInternal.FAILED →LOG: LOG.INFO: attemptId + transitioned from state + oldState + to + getState() + , event type is + event.getType() + and nodeId= + nodeId→CALL: writeLock.unlock→EXIT",
    "log": "[INFO] attemptId + transitioned from state + oldState + to + getState() + , event type is + event.getType() + and nodeId= + nodeId"
  },
  "70535190_1": {
    "exec_flow": "ENTRY → CALL: noteFailure → IF_TRUE: exception == null → RETURN → IF_FALSE: exception == null → SYNC: this → IF_TRUE: failureCause == null → SET: failureCause = exception → SET: failureState = getServiceState() → LOG: Service {} failed in state {} → EXIT",
    "log": "<log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>INFO</level> <message>Service {} failed in state {}</message> <parameters>getName(), failureState, exception</parameters> </log>"
  },
  "70535190_2": {
    "exec_flow": "ENTRY → IF_TRUE → CALL: org.slf4j.Logger:debug(java.lang.String) → CALL: setConfig → EXIT",
    "log": "<log> <location>org.apache.hadoop.service.AbstractService:serviceInit</location> <level>DEBUG</level> <message>Config has been overridden during init</message> </log>"
  },
  "70535190_3": {
    "exec_flow": "ENTRY → IF_TRUE: oldState != newState → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → RETURN → EXIT",
    "log": "<log> <location>org.apache.hadoop.service.AbstractService:enterState</location> <level>DEBUG</level> <message>Service: {} entered state {}</message> <parameters>getName(), getServiceState()</parameters> </log>"
  },
  "70535190_4": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL:globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log> <location>org.apache.hadoop.service.AbstractService:notifyListeners</location> <level>WARN</level> <message>Exception while notifying listeners of {}</message> <parameters>this, e</parameters> </log>"
  },
  "9a3d0fb0_1": {
    "exec_flow": "ENTRY→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config... →FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation →FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "9a3d0fb0_2": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:getTrimmed→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "9a3d0fb0_3": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→IF_TRUE:props != null→CALL:loadResources→IF_TRUE:loadDefaults && fullReload→FOREACH:defaultResources→CALL:loadResource→FOREACH_EXIT→FOR_INIT→FOR_COND:i < resources.size()→CALL:loadResource→FOR_EXIT→CALL:addTags→IF_TRUE:overlay != null→CALL:putAll→IF_TRUE:backup != null→FOREACH:overlay.entrySet()→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [INFO] message"
  },
  "9a3d0fb0_4": {
    "exec_flow": "ENTRY→IF_TRUE:(subject==null || subject.getPrincipals(User.class).isEmpty())→CALL:getLoginUser→ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→ENTRY→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials"
  },
  "9a3d0fb0_5": {
    "exec_flow": "ENTRY→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG:LOG.DEBUG:Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "9a3d0fb0_6": {
    "exec_flow": "ENTRY→NEW:HashSet<String>→NEW:HashSet<String>→FOREACH:userGroupStrings→FOREACH_EXIT→IF_TRUE:!allAllowed→IF_TRUE:userGroupStrings.length >= 1 && userGroupStrings[0] != null→CALL:getTrimmedStringCollection→IF_TRUE:userGroupStrings.length == 2 && userGroupStrings[1] != null→CALL:getTrimmedStringCollection→CALL:groupsMapping.cacheGroupsAdd→TRY→CALL:impl.cacheGroupsAdd→EXCEPTION:IOException→CATCH:IOException→LOG:LOG.WARN:Error caching groups, e→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "[WARN] Error caching groups"
  },
  "9c989bd2_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register</step> <step>CALL: namedCallbacks.put</step> <step>CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder</step> <step>CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource</step> <step>CALL:checkNotNull</step> <step>CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init></step> <step>CALL:put</step> <step>CALL:start</step> <step>IF_TRUE: startMBeans</step> <step>CALL: startMBeans</step> <step>CALL: org.slf4j.Logger:debug</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Registering the metrics source</log> <log>[DEBUG] Registered source + name</log>"
  },
  "69d2be7b_1": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: fileName != null → IF_FALSE: file.exists() → LOG: LOG.DEBUG: Missing ip list file : + fileName → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "[DEBUG] Missing ip list file : + fileName"
  },
  "69d2be7b_2": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: fileName != null → IF_TRUE: file.exists() → TRY → WHILE: (line = bufferedReader.readLine()) ≠ null → WHILE_COND: (line = bufferedReader.readLine()) ≠ null → WHILE_EXIT → THROW: IOException → LOG: LOG.ERROR: ioe.toString() → CALL: org.slf4j.Logger:error(java.lang.String) → EXIT",
    "log": "[ERROR] ioe.toString()"
  },
  "69d2be7b_3": {
    "exec_flow": "ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "69d2be7b_4": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → RETURN → EXIT → CALL:getTrimmed(java.lang.String) → RETURN",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "4a6eb72d_1": {
    "exec_flow": "ENTRY→IF_TRUE:azureAuthorization && authorizer != null→LOG:DEBUG: Getting the file status for {filePath}→CALL:getFileStatusInternal→IF_TRUE:meta != null→IF_TRUE:meta.isDirectory()→LOG:DEBUG: Path {} is a folder.→CALL:conditionalRedoFolderRename→ENTRY→IF_FALSE:f.getName().equals(\"\")→IF_FALSE:existsInternal(absoluteRenamePendingFile)→RETURN→EXIT→RETURN→EXIT→IF_FALSE:meta.isDirectory()→LOG:DEBUG: Found the path: {} as a file.→CALL:updateFileStatusPath→RETURN→EXIT→CALL:getUri→CALL:getWorkingDirectory→CALL:getUri→CALL:getWorkingDirectory→CALL:makeQualified→CALL:getUri→CALL:getWorkingDirectory→CALL:getUri→CALL:getWorkingDirectory→CALL:getOwnerForPath→TRY→CALL:retrieveMetadata→CATCH:IOException→CALL:checkForAzureStorageException→IF_FALSE:isFileNotFoundException→LOG:ERROR:Could not retrieve owner information for path - absolutePath→THROW:IOException→EXIT",
    "log": "<log> <template>Getting the file status for {filePath}</template> <level>DEBUG</level> <parameters> <parameter name=\"filePath\" value=\"f.toString()\" /> </parameters> </log> <log> <template>Path {} is a folder.</template> <level>DEBUG</level> <parameters> <parameter name=\"filePath\" value=\"f.toString()\" /> </parameters> </log> <log> <template>Found the path: {} as a file.</template> <level>DEBUG</level> <parameters> <parameter name=\"filePath\" value=\"f.toString()\" /> </parameters> </log> <log> <template>Could not retrieve owner information for path - absolutePath</template> <level>ERROR</level> <parameters/> </log> <log> <template>AzureBlobFileSystem.getFileStatus path: {}</template> <level>DEBUG</level> <parameters/> </log> <log> <template>Failed to get groups for user {}</template> <level>DEBUG</level> <parameters/> </log>"
  },
  "aa51c106_1": {
    "exec_flow": "ENTRY → CALL:obtainContext → CALL:org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead → CALL:confChanged → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → IF_TRUE:pathStr.startsWith(\"/\") → CALL:substring → WHILE:numDirsSearched < numDirs → WHILE_COND:numDirsSearched < numDirs → CALL:exists → IF_TRUE:ctx.localFS.exists(file) → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}].</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "aa51c106_2": {
    "exec_flow": "ENTRY → CALL:obtainContext → CALL:org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead → CALL:confChanged → IF_TRUE:pathStr.startsWith(\"/\") → CALL:substring → WHILE:numDirsSearched < numDirs → WHILE_COND:numDirsSearched < numDirs → WHILE_EXIT → THROW:DiskErrorException → EXIT",
    "log": "<log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "1831a9da_1": {
    "exec_flow": "<![CDATA[ ENTRY → TRY → CALL: toJson → CALL: org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled() → FOREACH: yarnApp.getComponents() → CALL: org.apache.hadoop.conf.Configuration:get(java.lang.String) → CALL: org.apache.hadoop.yarn.conf.YarnConfiguration:useHttps(org.apache.hadoop.conf.Configuration) → CALL: org.apache.hadoop.security.UserGroupInformation:getCurrentUser() → CALL: org.apache.hadoop.yarn.service.utils.HttpUtil:connect(java.lang.String) → SYNC: UserGroupInformation.class → IF_TRUE: (response.getStatus() == ClientResponse.Status.OK.getStatusCode()) → EXIT → LOG: YARN sysfs synchronized. ]]>",
    "log": "<![CDATA[ [ERROR] Fail to sync service spec: {e} [INFO] YARN sysfs synchronized. [DEBUG] Creating new Groups object [DEBUG] Authorization: Negotiate {} ]]>"
  },
  "12e0e693_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → THROW: e → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log>"
  },
  "12e0e693_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "b4e82c1e_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Reloading placement policy from allocation config→IF_FALSE: confElement == null || !confElement.hasChildNodes()→FOR_INIT→FOR_COND: i < elements.getLength()→IF_TRUE: node instanceof Element && node.getNodeName().equalsIgnoreCase(\"rule\")→LOG: LOG.DEBUG: Creating new rule: {}, name→CALL: createRule→IF_TRUE: child != null→CALL: getParentRuleElement→CALL: getParentRule→LOG: LOG.DEBUG: Creating new parent rule: {parent.getAttribute(\"name\")}→CALL: parentRule.createRule→TRY→IF_FALSE: !(scheduler instanceof CapacityScheduler)→CALL:CapacitySchedulerConfiguration:getOverrideWithQueueMappings→LOG: LOG.INFO: Initialized App Name queue mappings, override: {overrideWithQueueMappings}→CALL:getCapacitySchedulerQueueManager→FOREACH:queueMappings→IF_TRUE: isStaticQueueMapping(mapping)→IF_TRUE: ifQueueDoesNotExist(queue)→IF_TRUE: queueManager.isAmbiguous(mapping.getFullPath())→THROW:new IOException(\"mapping contains ambiguous leaf queue reference \" + mapping.getFullPath())→EXIT→CALL: parentRule.initialize→EXCEPTION: initialize→CATCH: IOException ioe→THROW: new AllocationConfigurationException(\"Parent Rule initialisation failed with exception\", ioe)→IF_FALSE: name.equalsIgnoreCase(\"nestedUserQueue\") && parentRule == null→CALL: updateRuleSet→EXIT→IF_FALSE: newRules.isEmpty()→LOG: LOG.DEBUG: Placement rule order check→FOR_INIT→FOR_COND: i < newTerminalState.size() - 1→FOR_EXIT→IF_FALSE: !newTerminalState.get(newTerminalState.size() - 1)→LOG: LOG.DEBUG: Initialising new rule set→TRY→FOREACH: newRules→CALL: org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementRule:initialize→FOREACH_EXIT→CALL: fs.getRMContext().getQueuePlacementManager().updateRules→LOG: LOG.DEBUG: PlacementManager active with new rule set→EXIT",
    "log": "[DEBUG] Reloading placement policy from allocation config [DEBUG] Creating new rule: {} [DEBUG] Creating new parent rule: {parent.getAttribute(\"name\")} [INFO] Initialized App Name queue mappings, override: {overrideWithQueueMappings} [DEBUG] Placement rule order check [DEBUG] Initialising new rule set [DEBUG] PlacementManager active with new rule set"
  },
  "97352b21_1": {
    "exec_flow": "<seq> ENTRY→IF_FALSE: null == instrumentation →IF_FALSE: null == uri →IF_FALSE: null == conf →CALL: org.apache.hadoop.conf.Configuration:getBoolean →CALL: org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:<init> →CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:configureAzureStorageSession →CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:createAzureStorageSession →CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getDirectorySet →LOG: Handling deprecation for all properties in config... →FOR_INIT →FOR_COND: i < resources.size() →LOG: Handling deprecation for (String)item →CALL: loadResource →FOR_EXIT →LOG: Page blob directories: {} →CALL: org.apache.hadoop.conf.Configuration:get →CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getDirectorySet →LOG: Handling deprecation for all properties in config... →LOG: Handling deprecation for (String)item →CALL: org.apache.hadoop.conf.Configuration:get →CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getDirectorySet →CALL: org.apache.hadoop.conf.Configuration:get →CALL: org.slf4j.Logger:warn →CALL: org.slf4j.Logger:debug →CALL: org.apache.hadoop.conf.Configuration:getBoolean →CALL: org.slf4j.Logger:info →EXIT </seq>",
    "log": "[DEBUG] Page blob directories: {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unable to initialize HBase root as an atomic rename directory. [DEBUG] Atomic rename directories: {} [INFO] {} configured as false. Blob metadata will be treated case insensitive."
  },
  "97352b21_2": {
    "exec_flow": "ENTRY→TRY→CALL:getPasswordString→CALL:validateStorageAccountKey→EXCEPTION:validateStorageAccountKey→CATCH:IOException ioe→LOG:WARN:Unable to get key for accountName from credential providers. ioe, ioe → RETURN → EXIT",
    "log": "[WARN] Unable to get key for accountName from credential providers. ioe, ioe"
  },
  "97352b21_3": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:org.slf4j.Logger:error→THROW:UnsupportedOperationException→EXIT",
    "log": "[ERROR] createBlobClient is an invalid operation in SAS Key Mode"
  },
  "97352b21_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.azure.SASKeyGeneratorInterface:getContainerSASUri→CATCH→CALL:org.slf4j.Logger:error→THROW:StorageException→EXIT",
    "log": "Encountered SASKeyGeneration exception while generating SAS Key for container : [name] inside Storage account : [storageAccount]"
  },
  "97352b21_5": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Connecting to Azure storage in Secure Mode→IF_TRUE: !(this.storageInteractionLayer instanceof SecureStorageInterfaceImpl)→THROW: new AssertionError(\"connectToAzureStorageInSecureMode() should be called only for SecureStorageInterfaceImpl instances\")→EXIT",
    "log": "[DEBUG] Connecting to Azure storage in Secure Mode"
  },
  "97352b21_6": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Connecting to Azure storage in Secure Mode→IF_FALSE: !(this.storageInteractionLayer instanceof SecureStorageInterfaceImpl)→CALL: ((SecureStorageInterfaceImpl) this.storageInteractionLayer).setStorageAccountName→CALL: getContainerReference→CALL: getDirectoryReference→EXIT",
    "log": "[DEBUG] Connecting to Azure storage in Secure Mode"
  },
  "97352b21_7": {
    "exec_flow": "ENTRY→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config... →FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation →FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "97352b21_8": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:getTrimmed→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "48e08c99_1": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:subject==null || subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {}"
  },
  "48e08c99_2": {
    "exec_flow": "ENTRY → IF_TRUE: !isInitNodeLabelStoreInProgress() → CALL:checkRemoveFromClusterNodeLabelsOfQueue → CALL:cloneNodeMap → CALL:removeFromClusterNodeLabels → CALL:updateResourceMappings → FOREACH:before.entrySet() → FOREACH_EXIT → FOREACH:after.entrySet() → FOREACH_EXIT → FOREACH:allNMs → IF_TRUE:(oldNM = getNMInNodeSet(nodeId, before, true)) != null → IF_FALSE:oldLabels.isEmpty() → FOREACH:oldLabels → CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:removeNode → CALL:org.apache.hadoop.yarn.util.resource.Resources:subtractFrom → CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:addNode → CALL:org.apache.hadoop.yarn.util.resource.Resources:addTo → IF_TRUE:rmContext != null && rmContext.getDispatcher() != null → CALL:org.apache.hadoop.yarn.event.EventHandler:handle → EXIT → CALL:writeLock.unlock → EXIT",
    "log": "<log> <level>INFO</level> <message>Resource mappings updated</message> </log> <log> <level>WARN</level> <message>Resource is missing:</message> </log> <log> <level>INFO</level> <message>[SERVICE] Transitioned from {} to {} on {} event., oldState, getState(), event.getType()</message> </log> <log> <level>INFO</level> <message>Node labels removed from cluster</message> </log>"
  },
  "48e08c99_3": {
    "exec_flow": "ENTRY → IF_FALSE:!nodeLabelsEnabled → CALL:normalizeLabels → CALL:checkRemoveFromClusterNodeLabels → CALL:internalRemoveFromClusterNodeLabels → ENTRY → FOREACH:nodeCollections.entrySet() → FOREACH_EXIT → FOREACH:labelsToRemove → FOREACH_EXIT → IF_TRUE:null != dispatcher → CALL:dispatcher.getEventHandler().handle → CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event) → LOG:LOG.INFO:Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ] → EXIT",
    "log": "<log> <level>INFO</level> <message>Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ]</message> </log>"
  },
  "0b4aff89_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE: props != null → CALL:loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "a9f62f44_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "a9f62f44_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "a9f62f44_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "a9f62f44_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "eb974b0c_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG.debug(\"Config has been overridden during init\");</step> <step>setConfig(conf);</step> <step>LOG.debug(\"noteFailure\", exception);</step> <step>if (exception == null) { return; }</step> <step> <sync> <condition>failureCause == null</condition> <steps> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> </steps> </sync> </step> <step>Service: {getName()} entered state {getServiceState()}</step> <step>LOG.warn(\"Exception while notifying listeners of {}\", this, e);</step>",
    "log": "<log> <level>DEBUG</level> <message>Config has been overridden during init</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log> <log> <level>DEBUG</level> <message>Service: {getName()} entered state {getServiceState()}</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {this}</message> </log>"
  },
  "eb974b0c_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:service != null</step> <step>CALL:stop</step> <step>CATCH:Exception</step> <step>LOG.warn(\"When stopping the service {service.getName()}\")</step> <step>EXIT</step>",
    "log": "<log> <level>WARN</level> <template>When stopping the service {service.getName()}</template> </log>"
  },
  "e6285809_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config... →CALL:getProps→CALL:addAll→FOREACH:keys →LOG:Handling deprecation for (String)item →CALL:handleDeprecation→FOREACH_EXIT →CALL:LOG_DEPRECATION.info →CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration, se</log> <log>[INFO] message</log>"
  },
  "e956a209_1": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN) → CALL: ProxyUsers.authorize → CALL: authorize → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Successfully authorized + connectionContext → CALL: rpcMetrics.incrAuthorizationSuccesses → EXIT",
    "log": "[DEBUG] Successfully authorized"
  },
  "e956a209_2": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN) → CALL: authorize → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Successfully authorized + connectionContext → CALL: rpcMetrics.incrAuthorizationSuccesses → EXIT",
    "log": "[DEBUG] Successfully authorized"
  },
  "e956a209_3": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN) → CALL: ProxyUsers.authorize → CALL: authorize → EXCEPTION: authorize → CATCH: AuthorizationException ae → LOG: LOG.INFO: Connection from {this} for protocol {connectionContext.getProtocol()} is unauthorized for user {user} → CALL: rpcMetrics.incrAuthorizationFailures → THROW: new FatalRpcServerException(RpcErrorCodeProto.FATAL_UNAUTHORIZED, ae) → EXIT",
    "log": "[INFO] Connection from {this} for protocol {connectionContext.getProtocol()} is unauthorized for user {user}"
  },
  "e956a209_4": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: user != null && user.getRealUser() != null && (authMethod != AuthMethod.TOKEN) → CALL: authorize → EXCEPTION: authorize → CATCH: AuthorizationException ae → LOG: LOG.INFO: Connection from {this} for protocol {connectionContext.getProtocol()} is unauthorized for user {user} → CALL: rpcMetrics.incrAuthorizationFailures → THROW: new FatalRpcServerException(RpcErrorCodeProto.FATAL_UNAUTHORIZED, ae) → EXIT",
    "log": "[INFO] Connection from {this} for protocol {connectionContext.getProtocol()} is unauthorized for user {user}"
  },
  "8511cc09_1": {
    "exec_flow": "ENTRY → IF_FALSE: meta == null → CALL: generateKey → CALL: rollNewVersion (LoadBalancingKMSClientProvider) → TRY → CALL: doOp → CALL: warn → CALL: error → CALL: invalidateCache → RETURN → EXIT → CALL: checkNotNull → TRY → CALL: rollNewVersionInternal → CALL: checkNotEmpty → IF_TRUE: material != null → CALL: jsonMaterial.put → CALL: createConnection → CALL: setRequestProperty → TRY → IF_TRUE: jsonOutput != null → CALL: getOutputStream → CALL: writeJson → IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage() → NEW: DelegationTokenAuthenticatedURL.Token → IF_TRUE: authRetryCount > 0 → CALL: createConnection → CALL: call → RETURN → EXIT",
    "log": "<log>[WARN] KMS provider at [{}] threw an IOException: </log> <log>[ERROR] Aborting since the Request has failed with all KMS providers (depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable</log> <log>[WARN] Failed to connect to: [URL]</log> <log>[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()</log>"
  },
  "f20dfbe1_1": {
    "exec_flow": "ENTRY → CALL:addCustomerProvidedKeyHeaders → CALL:requestHeaders.add → CALL:requestHeaders.add → CALL:abfsUriQueryBuilder.addQuery → CALL:appendSASTokenToQuery → TRY → CALL: urlEncode → CALL: append → CALL: append → CALL: append → TRY → NEW: URL → CALL: toString → ENTRY → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:getClientLatency() → IF_TRUE: latencyHeader != null && !latencyHeader.isEmpty() → CALL: requestHeaders.add → LOG: [DEBUG] First execution of REST operation - {} → WHILE: !executeHttpOperation(retryCount, tracingContext) → WHILE_COND: !executeHttpOperation(retryCount, tracingContext) → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int, org.apache.hadoop.fs.azurebfs.utils.TracingContext) → LOG: [DEBUG] Retrying REST operation {}. RetryCount = {} → WHILE_EXIT → IF_FALSE: status < HTTP_CONTINUE → IF_FALSE: status >= HttpURLConnection.HTTP_BAD_REQUEST → LOG: [TRACE] {} REST operation complete → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → RETURN → EXIT",
    "log": "[DEBUG] Adding default headers [INFO] Executing operation with tracing [DEBUG] Unexpected error. [DEBUG] First execution of REST operation - {} [DEBUG] Retrying REST operation {}. RetryCount = {} [TRACE] {} REST operation complete"
  },
  "803a4bcb_1": {
    "exec_flow": "ENTRY→IF_TRUE:!isPermissionLoaded()&&NativeIO.isAvailable()→TRY →CALL:loadPermissionInfoByNativeIO→EXCEPTION:loadPermissionInfoByNativeIO →CATCH:IOException ex→LOG:LOG.DEBUG: Native call failed, ex →IF_TRUE:!isPermissionLoaded()→CALL:loadPermissionInfoByNonNativeIO →[VIRTUAL_CALL]→org.apache.hadoop.fs.FileUtil:execCommand→EXIT",
    "log": "[DEBUG] Native call failed <log> Class:FileUtil[INFO]Executing command on file </log>"
  },
  "803a4bcb_2": {
    "exec_flow": "ENTRY→IF_TRUE:!isPermissionLoaded()&&NativeIO.isAvailable()→TRY →CALL:loadPermissionInfoByNativeIO→EXCEPTION:loadPermissionInfoByNativeIO →CATCH:IOException ex→LOG:LOG.DEBUG: Native call failed, ex →IF_FALSE:!isPermissionLoaded()→EXIT",
    "log": "[DEBUG] Native call failed"
  },
  "6c3ec65c_1": {
    "exec_flow": "ENTRY → CALL:handleNewContainers → CALL:writeLock.lock → TRY → IF_TRUE: !getApplicationAttemptId().equals(rmContainer.getApplicationAttemptId()) && !liveContainers.containsKey(id) → LOG:LOG.INFO:recovered container + id + from previous attempt + rmContainer.getApplicationAttemptId() → CALL:recoveredPreviousAttemptContainers.add → CALL:liveContainers.put → CALL:rmContainer.getAllocatedResource → IF_TRUE: rmContainer.getExecutionType() == ExecutionType.OPPORTUNISTIC → CALL:this.attemptOpportunisticResourceUsage.incUsed → LOG:LOG.DEBUG:Processing {} of type {}, event.getContainerId(), event.getType() → CALL:writeLock.lock → TRY → CALL:stateMachine.doTransition → IF_TRUE: oldState != getState() → LOG:LOG.INFO:event.getContainerId() + Container Transitioned from + oldState + to + getState() → IF_FALSE: rmContainer.isRemotelyAllocated() → CALL:writeLock.unlock → CALL:allocate → CALL:recordFactory.newRecordInstance → CALL:dsResp.setAllocateResponse → CALL:dsResp.setNodesForScheduling → RETURN → EXIT",
    "log": "[INFO] recovered container + id + from previous attempt + rmContainer.getApplicationAttemptId() [DEBUG] Processing {} of type {}, event.getContainerId(), event.getType() [INFO] event.getContainerId() + Container Transitioned from + oldState + to + getState()"
  },
  "6c3ec65c_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils:authorizeRequest() → TRY → CALL:getCurrentUser → TRY → CALL:selectAMRMTokenIdentifier → IF_FALSE:appTokenIdentifier==null → IF_FALSE:!tokenFound → RETURN → CALL:this.amLivelinessMonitor.receivedPing → IF_FALSE:lock == null → SYNC:lock → IF_FALSE:!hasApplicationMasterRegistered(appAttemptId) → IF_FALSE:AMRMClientUtils.getNextResponseId(request.getResponseId()) == lastResponse.getResponseId() → IF_FALSE:request.getResponseId() != lastResponse.getResponseId() → CALL:this.amsProcessingChain.allocate → CALL:org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:getAMRMTokenKeyId() → CALL:org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:createAndGetAMRMToken(org.apache.hadoop.yarn.api.records.ApplicationAttemptId) → CALL:setResponseId → CALL:setAllocateResponse → LOG:INFO:The AMRMToken has been rolled-over. Send new AMRMToken back to application → RETURN → EXIT",
    "log": "[INFO] The AMRMToken has been rolled-over. Send new AMRMToken back to application"
  },
  "fdcc847a_1": {
    "exec_flow": "ENTRY → CALL: gatherPossiblyRunnableAppLists → WHILE: iter.hasNext() → CALL: canAppBeRunnable → CALL: trackRunnableApp → CALL: FSLeafQueue.addApp → CALL: noLongerPendingApps.add → CONDITION: noLongerPendingApps.size() >= maxRunnableApps → BREAK → FOREACH: noLongerPendingApps → CALL: appSched.getQueue → CALL: removeNonRunnableApp → LOG.error → FOREACH_EXIT → CALL: updateAppsRunnability(java.util.List,int) → EXIT",
    "log": "<log>[ERROR] Can't make app runnable that does not already exist in queue as non-runnable: [appSched]. This should never happen.</log> <log>[ERROR] Waiting app [appSched] expected to be in usersNonRunnableApps, but was not. This should never happen.</log>"
  },
  "fdcc847a_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:getAuthenticationMethod</step> <step>LOG:DefaultRuleAssigned</step> <step>SWITCH:SecurityUtil.getAuthenticationMethod(conf)</step> <step>CASE:KERBEROS</step> <step>CASE:KERBEROS_SSL</step> <step>TRY_CATCH</step> <step>CATCH:Exception</step> <step>THROW:IllegalArgumentException</step> <step>CASE:default</step> <step>ASSIGN:defaultRule</step> <step>CALL:setRules</step> <step>CALL:setRuleMechanism</step> <step>CALL_SITE_EXIT</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:getProps</step> <step>FOREACH:names</step> <step>CALL:substituteVars</step> <step>FOREACH_EXIT</step> <step>CALL:getLong</step> <step>CALL:getBoolean</step> <step>IF_TRUE:!(groups instanceof TestingGroups)</step> <step>CALL:getUserToGroupsMappingService</step> <step>CALL:isDebugEnabled</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG:Creating new Groups object</step> <step>NEW:Groups</step> <step>CALL:<init></step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[INFO] DefaultRuleAssigned</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log>"
  },
  "ba39f7dc_1": {
    "exec_flow": "ENTRY→CALL:remove→CALL:org.apache.hadoop.yarn.nodelabels.AbstractLabel:removeNode(org.apache.hadoop.yarn.api.records.Resource)→ENTRY→CALL: Resources.subtractFrom→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT→EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "ccd99f34_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: adding {}({})->{}#{}→CALL: addController→IF_TRUE: defaultViewNeeded→VIRTUAL_CALL: addDefaultView→IF_TRUE: controllerName.endsWith(\"Controller\")→CALL: substring→CALL: length→CALL: length→CALL: find→ENTRY→LOG: LOG.DEBUG: trying: {}, className→TRY→IF_TRUE: cls.isAssignableFrom(found)→LOG: LOG.DEBUG: found {}, className→RETURN→EXIT→CALL: getName→CALL: getPackage→CALL: join→EXIT",
    "log": "[DEBUG] adding {}({})->{}#{} [DEBUG] trying: {} [DEBUG] found {}"
  },
  "ccd99f34_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: adding {}({})->{}#{}→CALL: addController→IF_TRUE: defaultViewNeeded→VIRTUAL_CALL: addDefaultView→IF_TRUE: controllerName.endsWith(\"Controller\")→CALL: substring→CALL: length→CALL: length→CALL: find→ENTRY→LOG: LOG.DEBUG: trying: {}, className→TRY→IF_FALSE: cls.isAssignableFrom(found)→LOG: LOG.WARN: found a {} but it's not a {}, className, cls.getName()→CATCH: ClassNotFoundException e→RETURN→EXIT→CALL: getName→CALL: getPackage→CALL: join→EXIT",
    "log": "[DEBUG] adding {}({})->{}#{} [DEBUG] trying: {} [WARN] found a {} but it's not a {}"
  },
  "9cf2c903_1": {
    "exec_flow": "ENTRY→CALL:checkNNStartup→CALL:namesystem.getListing→IF_TRUE:files != null→CALL:metrics.incrGetListingOps→CALL:metrics.incrFilesInGetListingOps→LOG:logAuditEvent→RETURN→EXIT",
    "log": "[INFO] Audit event for listStatus operation allowed on src"
  },
  "1c3e3a2d_1": {
    "exec_flow": "ENTRY → CALL:makeQualified → CALL:getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL:getDelegationToken → IF_TRUE: token != null → CALL:authParams.add → CALL:encodeToUrlString → CALL:toArray → RETURN → EXIT → CALL:getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT",
    "log": "[DEBUG] Delegation token encoded [INFO] Returning authentication parameters [TRACE] url={}"
  },
  "e1f4174a_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→CALL:org.apache.hadoop.conf.Configuration:write→CALL:org.apache.hadoop.io.GenericWritable:write→IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Starting write operation [WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "e1f4174a_2": {
    "exec_flow": "ENTRY→CALL:UTF8.writeString→CALL:writeInt→IF_TRUE:s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort→ CALL:writeChars → RETURN → EXIT",
    "log": "[WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "e1f4174a_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.conf.Configuration:write→CALL:org.apache.hadoop.io.GenericWritable:write→IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_TRUE: len > 0xffff → THROW: new IOException(\"string too long!\") → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "[DEBUG] Starting write operation [WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "e1f4174a_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.conf.Configuration:write→CALL:org.apache.hadoop.io.GenericWritable:write→IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "[DEBUG] Starting write operation [WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "b8352b9d_1": {
    "exec_flow": "<sequence> ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT </sequence>",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "b8352b9d_2": {
    "exec_flow": "<sequence> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT </sequence>",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "f8e7dd4d_1": {
    "exec_flow": "ENTRY→CALL:getRMStateStoreEventHandler→CALL:handle→ENTRY→CALL:writeLock.lock→TRY→CALL:stateMachine.doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→LOG:LOG.ERROR:[SERVICE]: Invalid event {1} at {2}, event.getType(), oldState, e→IF_TRUE:oldState!=getState()→LOG:LOG.INFO:[SERVICE] Transitioned from {} to {} on {} event., oldState, getState(), event.getType()→CALL:writeLock.unlock→EXIT",
    "log": "<log>[ERROR] [SERVICE]: Invalid event {1} at {2}, event.getType(), oldState, e</log> <log>[INFO] [SERVICE] Transitioned from {} to {} on {} event.</log> <log_entry level=\"INFO\" template=\"Notify AM launcher launched: {containerId}\"/>"
  },
  "f8e7dd4d_2": {
    "exec_flow": "ENTRY→CALL:getRMStateStoreEventHandler→CALL:handle→CALL:((EventHandler<TaskAttemptEvent>) attempt).handle→SWITCH:event.getType()→CASE:[UPDATE_CONTAINER]→IF_FALSE:event instanceof UpdateContainerSchedulerEvent→LOG:LOG.ERROR:Unknown event type on UpdateContainer:{event.getType()}→BREAK→EXIT",
    "log": "<log>[ERROR] Unknown event type on UpdateContainer: {event.getType()}</log>"
  },
  "f8e7dd4d_3": {
    "exec_flow": "ENTRY→CALL:getRMStateStoreEventHandler→CALL:handle→CALL:((EventHandler<TaskAttemptEvent>) attempt).handle→SWITCH:event.getType()→CASE:[]→LOG:LOG.ERROR:Unknown event arrived at ContainerScheduler:{event.toString()}→EXIT",
    "log": "<log>[ERROR] Unknown event arrived at ContainerScheduler: {event.toString()}</log>"
  },
  "f8e7dd4d_4": {
    "exec_flow": "ENTRY→CALL:getRMStateStoreEventHandler→CALL:handle→IF_TRUE: rmApp != null→TRY→CALL: rmApp.handle→EXCEPTION: handle→CATCH: Throwable t→LOG:LOG.ERROR:Error in handling event type + event.getType() + for application + appID, t→EXIT",
    "log": "<log>[ERROR] Error in handling event type + event.getType() + for application + appID, t</log>"
  },
  "829536eb_1": {
    "exec_flow": "<step>Get possible locations of a path in the federated cluster</step> <step>Check if the location is a mount point</step> <step>Check if the path is in a read-only mount point</step> <step>Verify quota if necessary</step> <step>Filter disabled subclusters</step> <step>If rpcMonitor is not null, proxy operation via rpcMonitor</step> <step>Invoke concurrent calls to remote locations</step> <step>Handle any sub-cluster invocation timeouts or errors</step> <entry_point>org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamespaces()</entry_point> <virtual_call_chain> <virtual_call>org.apache.hadoop.hdfs.server.federation.store.MembershipStore:getNamespaceInfo()</virtual_call> </virtual_call_chain>",
    "log": "<log>Operation not allowed because the path is a mount point or there are sub-mount points</log> <log>Cannot find locations for path</log> <log>Check path quota</log> <log>Filter disabled subclusters</log> <log>[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out</log> <log>Log from parent node</log> <log>Log from child node</log>"
  },
  "afe5eb4d_1": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→TRY→SYNC:stateChangeLock→IF_TRUE:!isInState(STATE.STARTED)→CALL:noteFailure→CALL:stopQuietly→CATCH:ServiceStateException→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [DEBUG] noteFailure [INFO] Service {getName()} failed in state {failureState} [WARN] When stopping the service {service.getName()}"
  },
  "7d4e20cd_1": {
    "exec_flow": "ENTRY → LOG.INFO: Exiting... → TRY → CALL: killComponent → LOG.WARN: Interrupted waiting for + component → IF_FALSE: monitor == null → CALL: monitor.getRemainingJobs → IF_FALSE: remainingJobs.isEmpty() → LOG.INFO: Killing running jobs... → FOR: JobStats stats : remainingJobs → CALL: stats.getJob → CALL: Job:isComplete → IF_FALSE: job.isComplete → CALL: Job.killJob → LOG.INFO: Killed {jobName} ({jobID}) → LOG.INFO: Done.",
    "log": "[INFO] Exiting... [WARN] Interrupted waiting for + component [INFO] Killing running jobs... [INFO] Killed {jobName} ({jobID}) [INFO] Done."
  },
  "7d4e20cd_2": {
    "exec_flow": "ENTRY → LOG.INFO: Exiting... → TRY → CALL: killComponent → LOG.WARN: Interrupted waiting for + component → IF_FALSE: monitor == null → CALL: monitor.getRemainingJobs → IF_FALSE: remainingJobs.isEmpty() → LOG.INFO: Killing running jobs... → FOR: JobStats stats : remainingJobs → CALL: stats.getJob → CALL: Job:isComplete → IF_TRUE: job.isComplete → CALL: Job:isSuccessful → IF_TRUE: job.isSuccessful → CALL: monitor.onSuccess(job)",
    "log": "[INFO] Exiting... [WARN] Interrupted waiting for + component [INFO] Killing running jobs..."
  },
  "7d4e20cd_3": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapreduce.Job:getJobName → LOG.INFO: {job.getJobName()} ({job.getJobID()}) failure → EXIT",
    "log": "[INFO] {job.getJobName()} ({job.getJobID()}) failure"
  },
  "7d4e20cd_4": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapreduce.Job:getJobName → CALL:org.apache.hadoop.mapreduce.Job:getJobID → LOG.INFO: job.getJobName() + \" (\" + job.getJobID() + \")\" + \" success\" → EXIT",
    "log": "[INFO] job.getJobName() + \" (\" + job.getJobID() + \")\" + \" success\""
  },
  "7d4e20cd_5": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapred.ClientCache:getClient(org.apache.hadoop.mapreduce.JobID)→CALL:org.apache.hadoop.mapred.ClientServiceDelegate:getJobStatus(org.apache.hadoop.mapreduce.JobID)→IF_FALSE:status==null→IF_FALSE:status.getState()!=JobStatus.State.RUNNING→CALL:org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)→TRY→CALL:org.apache.hadoop.mapred.ClientServiceDelegate:killJob(org.apache.hadoop.mapreduce.JobID)→EXCEPTION:IOException io→CATCH:IOException io→LOG:LOG.DEBUG:Error when checking for application status, io→IF_TRUE:status!=null&&!isJobInTerminalState(status)→CALL:org.apache.hadoop.mapred.YARNRunner:killApplication→EXIT",
    "log": "[DEBUG] Error when checking for application status"
  },
  "e0a3513a_1": {
    "exec_flow": "<entry>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus</entry> <call>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal</call> <sequence> <log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <if condition=\"key.length() != 0\"> <try> <call>retrieveMetadata</call> <if condition=\"meta != null\"> <if condition=\"meta.isDirectory()\"> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <call>conditionalRedoFolderRename</call> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </if> <else> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log> <call>updateFileStatusPath</call> <return>EXIT</return> </else> </if> </try> </if> <else> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </else> </sequence>",
    "log": "<log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log>"
  },
  "057c9cef_1": {
    "exec_flow": "ENTRY→IF_TRUE:!addr.isUnresolved() && addr.getAddress().isAnyLocalAddress()→TRY→NEW:InetSocketAddress→CALL:getLocalHost→CALL:getPort→RETURN→EXIT→[VIRTUAL_CALL]→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_TRUE: elapsedMs >= slowLookupThresholdMs→CALL: org.slf4j.Logger:warn(java.lang.String)→IF_TRUE: staticHost != null→CALL: getByAddress→CALL: getAddress→CALL: getAddress→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "057c9cef_2": {
    "exec_flow": "ENTRY→IF_TRUE:!addr.isUnresolved() && addr.getAddress().isAnyLocalAddress()→TRY→NEW:InetSocketAddress→CALL:getLocalHost→CALL:getPort→RETURN→EXIT→[VIRTUAL_CALL]→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_FALSE: elapsedMs >= slowLookupThresholdMs→IF_TRUE: LOG.isTraceEnabled()→CALL: org.slf4j.Logger:trace(java.lang.String)→IF_TRUE: staticHost != null→CALL: getByAddress→CALL: getAddress→CALL: getAddress→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[TRACE] Name lookup for + hostname + took + elapsedMs + ms."
  },
  "109ee647_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: refCount= + refCount→IF_FALSE: refCount <= 0→IF_FALSE: --refCount > 0→IF_TRUE: monitoring→TRY→CALL: stop→CALL: allSources.clear→CALL: allSinks.clear→CALL: clear→CALL: namedCallbacks.clear→IF_FALSE: mbeanName != null→LOG: LOG.INFO: prefix + metrics system shutdown complete.→RETURN→EXIT",
    "log": "[DEBUG] refCount= + refCount [INFO] metrics system shutdown complete."
  },
  "109ee647_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: refCount= + refCount→IF_TRUE: refCount <= 0→LOG: LOG.DEBUG: Redundant shutdown, new Throwable()→RETURN→EXIT",
    "log": "[DEBUG] refCount= + refCount [DEBUG] Redundant shutdown"
  },
  "109ee647_3": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: refCount= + refCount→IF_FALSE: refCount <= 0→IF_FALSE: --refCount > 0→IF_FALSE: monitoring→CALL: allSources.clear→CALL: allSinks.clear→CALL: clear→CALL: namedCallbacks.clear→IF_TRUE: mbeanName != null→CALL: MBeans.unregister→LOG: LOG.INFO: prefix + metrics system shutdown complete.→RETURN→EXIT",
    "log": "[DEBUG] refCount= + refCount [INFO] metrics system shutdown complete."
  },
  "109ee647_4": {
    "exec_flow": "ENTRY→IF_TRUE:!monitoring && !DefaultMetricsSystem.inMiniClusterMode()→LOG:warn:prefix + metrics system not yet started!, new MetricsException(Illegal stop)→RETURN→EXIT",
    "log": "[WARN] prefix + metrics system not yet started!, new MetricsException(Illegal stop)"
  },
  "109ee647_5": {
    "exec_flow": "ENTRY→IF_FALSE:!monitoring && !DefaultMetricsSystem.inMiniClusterMode()→IF_TRUE:!monitoring→LOG:info:prefix + metrics system stopped (again)→RETURN→EXIT",
    "log": "[INFO] prefix + metrics system stopped (again)"
  },
  "109ee647_6": {
    "exec_flow": "ENTRY→IF_FALSE:!monitoring && !DefaultMetricsSystem.inMiniClusterMode()→IF_FALSE:!monitoring→ITERATE_CALLBACKS:Callback preStop()→LOG:info:Stopping + prefix + metrics system...→CALL:stopTimer→CALL:stopSources→CALL:stopSinks→CALL:clearConfigs→LOG:info:prefix + metrics system stopped.→ITERATE_CALLBACKS:Callback postStop()→EXIT",
    "log": "[INFO] Stopping + prefix + metrics system... [INFO] prefix + metrics system stopped."
  },
  "08590612_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Starting SchedulingMonitor= + getName()→CALL: newSingleThreadScheduledExecutor→NEW: ThreadFactory→NEW: Thread→CALL: setName→CALL: getName→CALL: getName→NEW: ThreadFactory→NEW: Thread→CALL: setName→CALL: getName→CALL: getName→CALL: schedulePreemptionChecker→CALL:scheduleAtFixedRate→NEW: PolicyInvoker→TRY→CATCH→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[INFO] Starting SchedulingMonitor= + getName() [ERROR] Exception raised while executing preemption checker, skip this run..., exception="
  },
  "c537f57d_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {} [DEBUG] noteFailure [INFO] Service {} failed in state {} [DEBUG] Service: {} entered state {}"
  },
  "e3681b6a_1": {
    "exec_flow": "FSDirStatAndListingOp:isFileClosed → VIRTUAL_CALL → ENTRY → CALL:resolveComponents → IF_TRUE: remainder > 0 → CALL: copyOf → CALL: System.arraycopy → IF_TRUE: NameNode.LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → CALL: NameNode.LOG.debug → CALL: org.slf4j.Logger:debug → RETURN → EXIT",
    "log": "[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)]"
  },
  "9b34390d_1": {
    "exec_flow": "ENTRY→CALL:decodeIdentifier→IF_FALSE:cls == null→CALL:newInstance→CALL:tokenIdentifier.readFields→CALL:close→RETURN→EXIT",
    "log": "[DEBUG] Added {}:{} into tokenKindMap [DEBUG] Cannot find class for token kind {}"
  },
  "8adfbe68_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→LOG:Handling deprecation for + (String)item→FOREACH:names→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→RETURN→NEW:SaslRpcClient→CALL:<init>→CALL:getTicket→CALL:getProtocol→CALL:getAddress→CALL:saslConnect→SWITCH:method→CASE:[KERBEROS]→CALL:ugi.getRealAuthenticationMethod→IF_TRUE:!= AuthMethod.KERBEROS→LOG:client isn't using kerberos→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] client isn't using kerberos"
  },
  "32a35775_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "32a35775_2": {
    "exec_flow": "ENTRY→WHILE_COND→WHILE_EXIT→CALL:lock→TRY→IF_TRUE:!closed→CALL:peerServer.close→IF_TRUE:datanode.shutdownForUpgrade→CALL:restartNotifyPeers→LOG:INFO - Shutting down DataXceiverServer before restart→CALL:waitAllPeers→CALL:closeAllPeers→EXIT",
    "log": "[INFO] Shutting down DataXceiverServer before restart"
  },
  "32a35775_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.net.TcpPeerServer:close()→TRY→CALL:serverSocket.close→EXCEPTION:close→CATCH:IOException→CALL:org.slf4j.Logger:error→EXIT",
    "log": "[ERROR] error closing TcpPeerServer: , e"
  },
  "32a35775_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.net.unix.DomainSocket:close→EXCEPTION:close→CATCH:IOException e→CALL:org.slf4j.Logger:error→EXIT",
    "log": "[ERROR] error closing DomainPeerServer: , e"
  },
  "32a35775_5": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Closing all peers.→CALL: lock→TRY→CALL: peers.keySet().forEach→CALL: IOUtils.closeQuietly→CALL: clear→CALL: peersXceiver.clear→CALL: setDataNodeActiveXceiversCount→CALL: this.noPeers.signalAll→CALL: unlock→EXIT",
    "log": "[INFO] Closing all peers."
  },
  "32a35775_6": {
    "exec_flow": "ENTRY→CALL:lock→TRY→WHILE:!peers.isEmpty()→WHILE_COND:!peers.isEmpty()→THROW:InterruptedException→CALL:org.slf4j.Logger:debug→CALL:unlock→RETURN→EXIT",
    "log": "[DEBUG] Interrupted waiting for peers to close"
  },
  "9b9803a8_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_TRUE:loginUser==null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser) → CALL:createLoginUser → CALL:loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND:loginUser==null → DO_EXIT → RETURN → EXIT → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addTags → IF_TRUE:overlay != null → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT → CALL:checkOperation → CALL:readLock → TRY → CALL:checkOperation → CALL:getBlockManager → CALL:getBlocksWithLocations → CALL:getDatanode → IF_TRUE:node==null → CALL:blockLog.warn → THROW:new HadoopIllegalArgumentException(\"Datanode \" + datanode + \" not found.\") → RETURN → CALL:readUnlock → IF_TRUE:needReport → CALL:addMetric → CALL:readLockHeldTimeStampNanos.remove → IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs → CALL:timeStampOfLastReadLockReportMs.compareAndSet → CALL:numReadLockLongHold.increment → CALL:longestReadLockHeldInfo.get → CALL:longestReadLockHeldInfo.compareAndSet → CALL:timer.monotonicNow → CALL:numReadLockWarningsSuppressed.incrementAndGet → EXIT → TRY → CALL:getBlocks(datanode, size, minBlockSize) → CALL:RouterRpcServer:getBlocks → CALL:rpcServer.checkOperation → FOREACH:map.entrySet() → FOREACH_EXIT → IF_FALSE:nsId != null → RETURN → EXIT → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] Loaded {} base64 tokens</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[WARN] Null token ignored for {}</log> <log>[WARN] BLOCK* getBlocks: Asking for blocks from an unrecorded node {datanode}</log> <log>[INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}</log> <log>[DEBUG] Starting the block retrieval process</log>"
  },
  "e59be84f_1": {
    "exec_flow": "ENTRY→CALL:super.stop→CALL:mountd.stop→IF_TRUE:boundPort != port→LOG:LOG.INFO:The bound port is X, different with configured port Y→FOR_INIT→FOR_COND:vers <= highProgVersion→CALL:PortmapRequest.create→TRY→CALL:registrationClient.run→CATCH:IOException e→LOG:LOG.ERROR:request + failure with + host + : + port + , portmap entry: + mapEntry→THROW:new RuntimeException(request + \" failure\", e)→FOR_EXIT→EXIT",
    "log": "[INFO] The bound port is X, different with configured port Y [ERROR] Registration failure with host:port, portmap entry: mapEntry"
  },
  "e59be84f_2": {
    "exec_flow": "ENTRY→IF_TRUE:udpBoundPort > 0→CALL:rpcProgram.unregister→LOG:LOG.INFO:The bound port is X, different with configured port Y→RETURN→IF_TRUE:tcpBoundPort > 0→CALL:rpcProgram.unregister→LOG:LOG.INFO:The bound port is X, different with configured port Y→RETURN→IF_TRUE:udpServer != null→CALL:udpServer.shutdown→RETURN→IF_TRUE:tcpServer != null→CALL:tcpServer.shutdown→RETURN→EXIT",
    "log": "[INFO] The bound port is X, different with configured port Y [INFO] The bound port is X, different with configured port Y"
  },
  "6359e699_1": {
    "exec_flow": "ENTRY→CALL:getLocalDirsPathPermissionsMap→FOREACH:pathPermissionMap.entrySet()→TRY→CALL:getFileStatus→CALL:getKey→CALL:getKey→IF_TRUE:!status.getPermission().equals(entry.getValue())→LOG:LOG.WARN:msg→THROW:new YarnRuntimeException(msg)→EXIT",
    "log": "<log>[WARN] \"Could not carry out resource dir checks for \" + localDir + \", which was marked as good\"</log> <log>[INFO] Resolving file system path</log> <log>[DEBUG] Attempting symlink resolution</log>"
  },
  "6359e699_2": {
    "exec_flow": "ENTRY→CALL:getLocalDirsPathPermissionsMap→FOREACH:pathPermissionMap.entrySet()→TRY→CALL:getFileStatus→CATCH:IOException→LOG:LOG.WARN:msg→THROW:new YarnRuntimeException(msg)→EXIT",
    "log": "<log>[WARN] \"Could not carry out resource dir checks for \" + localDir + \", which was marked as good\"</log> <log>[INFO] Resolving file system path</log> <log>[WARN] Unresolved link encountered</log>"
  },
  "6359e699_3": {
    "exec_flow": "ENTRY→IF_TRUE:conf != null→CALL:getLocalDirsPathPermissionsMap→TRY→IF_TRUE:confUmask != null→CALL:getUMask→NEW:UmaskParser→NEW:FsPermission→RETURN→EXIT",
    "log": "<log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log>"
  },
  "0f66b3b2_1": {
    "exec_flow": "ENTRY → CALL: updateBlockForPipeline → CALL: rpcServer.checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "0f66b3b2_2": {
    "exec_flow": "ENTRY → CALL: updateBlockForPipeline → CALL: rpcServer.checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "0f66b3b2_3": {
    "exec_flow": "ENTRY → CALL: updateBlockForPipeline → CALL: rpcServer.checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "0f66b3b2_4": {
    "exec_flow": "ENTRY → CALL: updateBlockForPipeline → CALL: rpcServer.checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "1c999122_1": {
    "exec_flow": "<step>LOG.info(\"Planning to load image: \" + imageFile);</step> <step>storage.readProperties(sdForProperties, startupOption);</step> <step> <condition>NameNodeLayoutVersion.supports(LayoutVersion.Feature.TXID_BASED_LAYOUT, getLayoutVersion())</condition> <exec_flow> <step>setBlockPoolId</step> <step>newLoader → load</step> <step>IF_FALSE: expectedMd5 == null || expectedMd5.equals(readImageMd5) → THROW: IOException(\"Image file \" + curFile + \" is corrupt with MD5 checksum of \" + readImageMd5 + \" but expecting \" + expectedMd5)</step> <step>getLoadedImageTxId</step> <step>LOG: [INFO] Loaded image for txid {txId} from {curFile}</step> <step>setMostRecentCheckpointInfo</step>",
    "log": "<log>LOG.info(\"Planning to load image: \" + imageFile);</log> <log>[INFO] Loaded image for txid {txId} from {curFile}</log> <log>[INFO] Loaded FSImage in {} seconds.</log> <log>[ERROR] This is a rare failure scenario!!!</log> <log>[ERROR] Image checkpoint time X > edits checkpoint time Y</log> <log>[ERROR] Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.</log> <log>[DEBUG] Performing recovery in + latestNameSD + and + latestEditsSD</log> <log>[DEBUG] End of the phase: {}</log>"
  },
  "1c999122_2": {
    "exec_flow": "<step>String md5 = storage.getDeprecatedProperty(NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY);</step> <step> <condition>md5 == null</condition> <exec_flow> <step>THROW: new InconsistentFSStateException(sdForProperties.getRoot(), \"Message digest property \" + NNStorage.DEPRECATED_MESSAGE_DIGEST_PROPERTY + \" not set for storage directory \" + sdForProperties.getRoot())</step>",
    "log": "<log>[DEBUG] Name checkpoint time is newer than edits, not loading edits.</log>"
  },
  "c06edda0_1": {
    "exec_flow": "ENTRY→IF_FALSE: monitoring && !DefaultMetricsSystem.inMiniClusterMode()→CALL: checkNotNull→IF_FALSE: monitoring→SWITCH: initMode()→CASE: [NORMAL]→TRY→CALL: start→CATCH: MetricsConfigException e→LOG: LOG.WARN: Metrics system not started: + e.getMessage() → LOG: LOG.DEBUG: Stacktrace: , e → BREAK→EXIT ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "[WARN] Metrics system not started: + e.getMessage() [DEBUG] Stacktrace: , e [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] prefix + metrics system started (again) [DEBUG] Registering the metrics source [DEBUG] Registered source + name"
  },
  "c06edda0_2": {
    "exec_flow": "ENTRY→CALL:namedCallbacks.put→CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink→CALL:org.slf4j.Logger:warn→CALL:org.slf4j.Logger:debug→EXIT",
    "log": "[WARN] some warn message here [DEBUG] some debug message here"
  },
  "c06edda0_3": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:opening listeners:{}, listeners→FOREACH:listeners→IF_FALSE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→IF_TRUE:portRanges != null && port != 0→CALL:bindForPortRange→RETURN→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] opening listeners: {}"
  },
  "c06edda0_4": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:opening listeners:{}, listeners→FOREACH:listeners→IF_FALSE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→IF_FALSE:portRanges != null && port != 0→CALL:bindForSinglePort→RETURN→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] opening listeners: {}"
  },
  "c06edda0_5": {
    "exec_flow": "ENTRY→WHILE: true→WHILE_COND: true→TRY→CALL: close→CALL: open→CALL: org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→RETURN→WHILE_EXIT→EXIT",
    "log": "[INFO] Jetty bound to port"
  },
  "c06edda0_6": {
    "exec_flow": "ENTRY→TRY→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→EXCEPTION:bindListener→CATCH:IOException ex→FOREACH:portRanges→IF_TRUE:port==startPort→CONTINUE→EXIT",
    "log": "[INFO] Jetty bound to port"
  },
  "c06edda0_7": {
    "exec_flow": "ENTRY→TRY→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→EXCEPTION:bindListener→CATCH:IOException ex→FOREACH:portRanges→IF_FALSE:port==startPort→CALL:Thread.sleep→CALL:setPort→TRY→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→EXCEPTION:bindListener→CATCH:IOException ex→IF_TRUE:!(ex instance of BindException) && !(ex.getCause() instance of BindException)→THROW:ex→EXIT",
    "log": "[INFO] Jetty bound to port [INFO] Jetty bound to port"
  },
  "c06edda0_8": {
    "exec_flow": "ENTRY→TRY→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→EXCEPTION:bindListener→CATCH:IOException ex→FOREACH:portRanges→IF_FALSE:port==startPort→CALL:Thread.sleep→CALL:setPort→TRY→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→RETURN→EXIT",
    "log": "[INFO] Jetty bound to port [INFO] Jetty bound to port"
  },
  "0f651bbc_1": {
    "exec_flow": "ENTRY→CALL:multiplyAndRound→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→IF_COND: roundingDirection→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→FOR_ITERATION→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:..."
  },
  "0f651bbc_2": {
    "exec_flow": "ENTRY→CALL:multiplyAndRound→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_ITERATION→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:..."
  },
  "a32e481f_1": {
    "exec_flow": "ENTRY → IF_TRUE:pathStr.startsWith(\"/\") → CALL:substring → IF_TRUE:size == SIZE_UNKNOWN → FOR_INIT → FOR_COND:i < ctx.dirDF.length → CALL:confChanged → CALL:mkdirs → CALL:debug → FOR_EXIT → IF_FALSE:totalAvailable == 0 → WHILE:numDirsSearched < numDirs && returnPath == null → CALL:createPath → IF_TRUE:returnPath != null → RETURN → EXIT",
    "log": "<log>[DEBUG] mkdirs of {}={}</log>"
  },
  "a32e481f_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: abfsStore.getFileStatus → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "a32e481f_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: abfsStore.getFileStatus → CATCH: AzureBlobFileSystemException → CALL: checkException → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "cba7fac1_1": {
    "exec_flow": "<seq> ENTRY→CALL:org.apache.hadoop.conf.Configuration:get→TRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→CALL:hostRestrictingAuthorizationFilter.init→CALL:loadRuleMap→IF_FALSE:ruleString == null || ruleString.equals(\"\")→IF_FALSE:!splits.keySet().equals(Collections.singleton(3))→FOREACH:splits.get(3)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→FOREACH_EXIT→EXIT </seq>",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Loaded rule: user: {}, network/bits: {} path: {}"
  },
  "c960fab8_1": {
    "exec_flow": "<entry>org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$1:run()</entry> <virtual_call>org.slf4j.Logger:info(java.lang.String)</virtual_call> <entry>org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:getContainerEventProcessor(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)</entry> <log>LOG.INFO:Processing Event [event] for Container [containerId]</log> <log>LOG.INFO:Container [containerId] is already stopped or failed</log>",
    "log": "[INFO] Processing Event [event] for Container [containerId] [INFO] Container [containerId] is already stopped or failed"
  },
  "c960fab8_2": {
    "exec_flow": "<entry>org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:getContainerEventProcessor(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent)</entry> <log>LOG.INFO:Processing Event [event] for Container [containerId]</log> <call>handle</call> <conditional_call isCompletelyDone(container)=true> <call>remove</call> </conditional_call>",
    "log": "[INFO] Processing Event [event] for Container [containerId]"
  },
  "0b8f11b4_1": {
    "exec_flow": "ENTRY→LOG:DEBUG: Received nodePublishVolume call, request: {}→LOG:DEBUG: Translate to CSI proto message: {}→CALL:csiClient.nodePublishVolume→CALL:org.apache.hadoop.yarn.csi.client.CsiClientImpl:nodePublishVolume→TRY→CALL:createNodeBlockingStub→CALL:nodePublishVolume→CALL:CsiGrpcClient:close→TRY→CALL:awaitTermination→EXCEPTION:awaitTermination→CATCH:InterruptedException→CALL:LOG.error→RETURN→EXIT",
    "log": "[DEBUG] Received nodePublishVolume call, request: {} [DEBUG] Translate to CSI proto message: {} [INFO] Node publish volume request initiated [ERROR] IOException encountered during node publish volume [ERROR] Failed to gracefully shutdown gRPC communication channel in 5 seconds"
  },
  "0b8f11b4_2": {
    "exec_flow": "ENTRY→LOG:DEBUG: Received nodePublishVolume call, request: {}→LOG:DEBUG: Translate to CSI proto message: {}→CALL:csiClient.nodePublishVolume→[EXCEPTION:IOException]→EXIT",
    "log": "[DEBUG] Received nodePublishVolume call, request: {} [DEBUG] Translate to CSI proto message: {} [ERROR] IOException encountered during node publish volume"
  },
  "59e28389_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationToken→ENTRY→CALL:rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {}, methodName→CALL: opCategory.set→IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→RETURN→CALL:this.securityManager.getDelegationToken→ENTRY→LOG: LOG.DEBUG: Generate delegation token with renewer→TRY→IF_TRUE: !isAllowedDelegationTokenOp()→CALL: isAllowedDelegationTokenOp()→THROW: new IOException(\"Delegation Token can be issued only \" + \"with kerberos or web authentication\")→CALL: isAllowedDelegationTokenOp()→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName} [DEBUG] Generate delegation token with renewer"
  },
  "59e28389_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationToken→ENTRY→CALL:rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {}, methodName→CALL: opCategory.set→IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: checkSafeMode→CALL:this.securityManager.getDelegationToken→ENTRY→LOG: LOG.DEBUG: Generate delegation token with renewer→TRY→IF_FALSE: !isAllowedDelegationTokenOp()→CALL: isAllowedDelegationTokenOp()→IF_TRUE: dtSecretManager == null || !dtSecretManager.isRunning()→CALL: isRunning()→LOG: LOG.WARN: trying to get DT with no secret manager running→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName} [DEBUG] Generate delegation token with renewer [WARN] trying to get DT with no secret manager running"
  },
  "59e28389_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationToken→ENTRY→CALL:rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {}, methodName→CALL: opCategory.set→IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: checkSafeMode→CALL:this.securityManager.getDelegationToken→ENTRY→LOG: LOG.DEBUG: Generate delegation token with renewer→TRY→IF_FALSE: !isAllowedDelegationTokenOp()→CALL: isAllowedDelegationTokenOp()→IF_FALSE: dtSecretManager == null || !dtSecretManager.isRunning()→IF_TRUE: ugi.getRealUser() != null→CALL: getRemoteUser()→CALL: getUserName→CALL: getRealUser→CALL: getUserName→NEW: DelegationTokenIdentifier→CALL:<init>→NEW: Token<DelegationTokenIdentifier>→CALL:<init>→CALL: toStringStable→CALL: logAuditEvent→LOG: [DEBUG] Operation: getDelegationToken Status: true TokenId: tokenId→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName} [DEBUG] Generate delegation token with renewer [DEBUG] Operation: getDelegationToken Status: true TokenId: tokenId"
  },
  "59e28389_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationToken→ENTRY→CALL:rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {}, methodName→CALL: opCategory.set→IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: checkSafeMode→CALL:this.securityManager.getDelegationToken→ENTRY→LOG: LOG.DEBUG: Generate delegation token with renewer→TRY→IF_FALSE: !isAllowedDelegationTokenOp()→CALL: isAllowedDelegationTokenOp()→IF_FALSE: dtSecretManager == null || !dtSecretManager.isRunning()→IF_FALSE: ugi.getRealUser() != null→CALL: getRemoteUser()→CALL: getUserName→NEW: DelegationTokenIdentifier→CALL:<init>→NEW: Token<DelegationTokenIdentifier>→CALL:<init>→CALL: toStringStable→CALL: logAuditEvent→LOG: [DEBUG] Operation: getDelegationToken Status: true TokenId: tokenId→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName} [DEBUG] Generate delegation token with renewer [DEBUG] Operation: getDelegationToken Status: true TokenId: tokenId"
  },
  "74b1ea89_1": {
    "exec_flow": "ENTRY → CALL: refreshSuperUserGroupsConfiguration → CALL: org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshSuperUserGroupsConfiguration → SYNC: this.userPipelineMap → IF_FALSE: chain != null && chain.getRootInterceptor() != null → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: [INFO] Initializing request processing pipeline for user: {} → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → EXCEPTION: createRequestInterceptorChain → CATCH: Exception e → LOG: [ERROR] Init RMAdminRequestInterceptor error for user: {} → THROW: e → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → LOG: [WARN] Unexpected SecurityException in Configuration → EXIT → RETURN → EXIT",
    "log": "<log> [INFO] Initializing request processing pipeline for user: {} [ERROR] Init RMAdminRequestInterceptor error for user: {} [WARN] Unexpected SecurityException in Configuration </log>"
  },
  "74b1ea89_2": {
    "exec_flow": "ENTRY→IF_TRUE:!isRMActive()→VIRTUAL_CALL→[VIRTUAL_CALL]→IF_TRUE:LOG.isWarnEnabled()→LOG:LOG.WARN:createFailureLog→CALL:throwStandbyException→EXIT",
    "log": "<log> [DEBUG] ResourceManager is not active. Can not [message] </log>"
  },
  "74b1ea89_3": {
    "exec_flow": "ENTRY→IF_TRUE: LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→EXIT",
    "log": "<log> [INFO] createSuccessLog(user, operation, target, null, null, null, null) </log>"
  },
  "76b4f275_1": {
    "exec_flow": "ENTRY→TRY→CALL:setRequestMethod→CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:makeQualified→EXCEPTION:setRequestMethod→CATCH:Exception ex→THROW:new IOException(ex)→EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "76b4f275_2": {
    "exec_flow": "ENTRY→TRY→CALL:setRequestMethod→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→IF_TRUE:method.equals(HTTP_POST) || method.equals(HTTP_PUT)→CALL:setDoOutput→RETURN→EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "76b4f275_3": {
    "exec_flow": "ENTRY→TRY→CALL:setRequestMethod→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→CALL:org.apache.hadoop.security.UserGroupInformation:doAs→IF_FALSE:method.equals(HTTP_POST) || method.equals(HTTP_PUT)→RETURN→EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "90f50e0b_1": {
    "exec_flow": "ENTRY → CALL:checkNNStartup → CALL:completeFile → CALL:NameNode.stateChangeLog.debug → CALL:checkBlock → CALL:org.apache.hadoop.hdfs.server.namenode.FSDirectory:resolvePath → CALL:completeFileInternal → TRY → CALL: getLastINode → CALL: checkLease → IF_FALSE: !fsn.checkFileProgress(src, pendingFile, false) → CALL: commitOrCompleteLastBlock → IF_FALSE: !fsn.checkFileProgress(src, pendingFile, true) → CALL: addCommittedBlocksToPending → CALL: finalizeINodeFileUnderConstruction → IF_TRUE:success → CALL:NameNode.stateChangeLog.info → RETURN → EXIT",
    "log": "[DEBUG] DIR* NameSystem.completeFile: ${src} for ${holder} [INFO] DIR* completeFile: ${src} is closed by ${holder}"
  },
  "90f50e0b_2": {
    "exec_flow": "ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "52bbc88e_1": {
    "exec_flow": "<!-- 由于没有提供具体的执行流合并逻辑，这里无法给出具体内容 -->",
    "log": "本节点日志序列为本节点源代码信息为 本节点日志序列为本节点源代码信息为{'source_code': 'public static <K, V> void writePartitionFile(JobConf job, Sampler<K, V> sampler) throws IOException, ClassNotFoundException, InterruptedException {\\n writePartitionFile(Job.getInstance(job), sampler);\\n}', 'file_path': 'hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/InputSampler.java'} 本节点日志序列为本节点源代码信息为 本节点日志序列为本节点源代码信息为{'source_code': 'protected synchronized Properties getProps() {\\n if (properties == null) {\\n properties = new Properties();\\n loadProps(properties, 0, true);\\n }\\n return properties;\\n}', 'file_path': 'hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java'}"
  },
  "52bbc88e_2": {
    "exec_flow": "ENTRY→CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY→CALL:get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→EXIT",
    "log": "[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "52bbc88e_3": {
    "exec_flow": "<step> <method>org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials)</method> <code>addAll(other, false);</code> </step> <step> <method>org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)</method> <code>private void addAll(Credentials other, boolean overwrite) { for (Map.Entry<Text, byte[]> secret : other.secretKeysMap.entrySet()) { Text key = secret.getKey(); if (!secretKeysMap.containsKey(key) || overwrite) { secretKeysMap.put(key, secret.getValue()); } } for (Map.Entry<Text, Token<?>> token : other.tokenMap.entrySet()) { Text key = token.getKey(); if (!tokenMap.containsKey(key) || overwrite) { addToken(key, token.getValue()); } } }</code> </step>",
    "log": "<log>org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials)本节点日志序列为本节点源代码信息为{'source_code': '/**\\n * Copy all of the credentials from one credential object into another.\\n * Existing secrets and tokens are not overwritten.\\n * @param other the credentials to copy\\n */\\npublic void mergeAll(Credentials other) {\\n addAll(other, false);\\n}', 'file_path': 'hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Credentials.java'}</log> <log>org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)本节点日志序列为本节点源代码信息为{'source_code': 'private void addAll(Credentials other, boolean overwrite) {\\n for (Map.Entry<Text, byte[]> secret : other.secretKeysMap.entrySet()) {\\n Text key = secret.getKey();\\n if (!secretKeysMap.containsKey(key) || overwrite) {\\n secretKeysMap.put(key, secret.getValue());\\n }\\n }\\n for (Map.Entry<Text, Token<?>> token : other.tokenMap.entrySet()) {\\n Text key = token.getKey();\\n if (!tokenMap.containsKey(key) || overwrite) {\\n addToken(key, token.getValue());\\n }\\n }\\n}', 'file_path': 'hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Credentials.java'}</log>"
  },
  "52bbc88e_4": {
    "exec_flow": "ENTRY -> CALL: ensureInitialized -> IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() -> CALL: getLoginUser -> RETURN -> EXIT",
    "log": "[LOG] getLoginUser"
  },
  "52bbc88e_5": {
    "exec_flow": "ENTRY -> CALL: ensureInitialized -> IF_FALSE: subject == null || subject.getPrincipals(User.class).isEmpty() -> NEW: UserGroupInformation -> RETURN -> EXIT",
    "log": ""
  },
  "91b042bd_1": {
    "exec_flow": "<entry>ENTRY→CALL:startTimelineReaderWebApp→CALL:getWebAppBindURL→CALL:getTrimmed→CALL:handleDeprecation→FOREACH:names→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→RETURN→EXIT</entry> <call> <name>CALL:startTimelineReaderWebApp</name> </call> <call> <name>CALL:getWebAppBindURL</name> </call> <call> <name>CALL:getTrimmed</name> </call> <call> <name>CALL:handleDeprecation</name> </call> <foreach> <name>FOREACH:names</name> </foreach> <call> <name>CALL:getProps</name> </call> <call> <name>CALL:substituteVars</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<log>org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)</log>"
  },
  "91b042bd_2": {
    "exec_flow": "<entry>ENTRY→CALL:startTimelineReaderWebApp→CALL:getWebAppBindURL→CALL:getTrimmed→CALL:handleDeprecation→FOREACH:names→CALL:getProps→FOREACH_EXIT→RETURN→EXIT</entry> <call> <name>CALL:startTimelineReaderWebApp</name> </call> <call> <name>CALL:getWebAppBindURL</name> </call> <call> <name>CALL:getTrimmed</name> </call> <call> <name>CALL:handleDeprecation</name> </call> <foreach> <name>FOREACH:names</name> </foreach> <call> <name>CALL:getProps</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<!-- 由于没有提供具体的日志序列合并信息，这里无法详细填写 -->"
  },
  "91b042bd_3": {
    "exec_flow": "<!-- 这里需要根据具体的合并逻辑来填充优化后的执行流结构，由于没有详细的合并逻辑，暂时无法给出具体内容 -->",
    "log": "<!-- 这里需要根据具体的合并逻辑来填充合并后的日志序列，由于没有详细的合并逻辑，暂时无法给出具体内容 -->"
  },
  "91b042bd_4": {
    "exec_flow": "<!-- 这里需要根据具体的合并逻辑来填充优化后的执行流结构，由于没有详细的合并逻辑，暂时无法给出具体内容 -->",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "91b042bd_5": {
    "exec_flow": "ENTRY→CALL:namedCallbacks.put→CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink→CALL:checkNotNull→CALL:newSink→CALL:put→CALL:start→CALL:org.slf4j.Logger:warn→CALL:org.slf4j.Logger:debug→LOG:LOG.INFO: Registered sink {name}→EXIT",
    "log": "[WARN] some warn message here [DEBUG] some debug message here [INFO] Registered sink {name}"
  },
  "91b042bd_6": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:LOG.DEBUG:opening listeners: {}, listeners→FOREACH:listeners→IF_TRUE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→CONTINUE→EXIT</step> <step>TRY</step> <step>CALL:bindListener</step> <step>EXCEPTION:bindListener</step> <step>CATCH:IOException ex</step> <step>FOREACH:portRanges</step> <step>IF_TRUE:port==startPort</step> <step>CONTINUE</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] opening listeners: {}</log> <log>[INFO] Jetty bound to port </log>"
  },
  "91b042bd_7": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:LOG.DEBUG:opening listeners: {}, listeners→FOREACH:listeners→IF_FALSE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→IF_TRUE:portRanges != null && port != 0→CALL:bindForPortRange→RETURN→FOREACH_EXIT→EXIT</step> <step>TRY</step> <step>CALL:bindListener</step> <step>EXCEPTION:bindListener</step> <step>CATCH:IOException ex</step> <step>FOREACH:portRanges</step> <step>IF_FALSE:port==startPort</step> <step>CALL:Thread.sleep</step> <step>CALL:setPort</step> <step>TRY</step> <step>CALL:bindListener</step> <step>EXCEPTION:bindListener</step> <step>CATCH:IOException ex</step> <step>IF_TRUE:!(ex instance of BindException) &&!(ex.getCause() instance of BindException)</step> <step>THROW:ex</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] opening listeners: {}</log> <log>[INFO] Jetty bound to port </log>"
  },
  "91b042bd_8": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:LOG.DEBUG:opening listeners: {}, listeners→FOREACH:listeners→IF_FALSE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→IF_FALSE:portRanges != null && port != 0→CALL:bindForSinglePort→RETURN→FOREACH_EXIT→EXIT</step> <step>TRY</step> <step>CALL:bindListener</step> <step>EXCEPTION:bindListener</step> <step>CATCH:IOException ex</step> <step>FOREACH:portRanges</step> <step>IF_FALSE:port==startPort</step> <step>CALL:Thread.sleep</step> <step>CALL:setPort</step> <step>TRY</step> <step>CALL:bindListener</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] opening listeners: {}</log> <log>[INFO] Jetty bound to port </log>"
  },
  "91b042bd_9": {
    "exec_flow": "<seq>ENTRY→WHILE: true→WHILE_COND: true→TRY→CALL: bindListener→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→EXCEPTION: bindListener→CATCH: IOException ex→IF_TRUE: port == 0 ||!findPort→THROW: constructBindException(listener, ex)→EXIT</seq>",
    "log": "<log>[INFO] Jetty bound to port </log>"
  },
  "91b042bd_10": {
    "exec_flow": "<seq>ENTRY→WHILE: true→WHILE_COND: true→TRY→CALL: bindListener→CALL:close→CALL:open→CALL:org.slf4j.Logger:info→LOG.INFO: Jetty bound to port + listener.getLocalPort()→RETURN→WHILE_EXIT→EXIT</seq>",
    "log": "<log>[INFO] Jetty bound to port </log>"
  },
  "91b042bd_11": {
    "exec_flow": "<entry>ENTRY</entry> <call>Preconditions.checkArgument</call> <call>Preconditions.checkArgument</call> <call>trim</call> <if> <condition>deprecations.getDeprecatedKeyMap().isEmpty() == true</condition> <true> <call>getProps</call> </true> </if> <call>getOverlay().setProperty</call> <call>getProps().setProperty</call> <if> <condition>!isDeprecated(name) == false</condition> <true> <call>putIntoUpdatingResource</call> <if> <condition>altNames != null == true</condition> <true> <call>org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String)</call> <foreach>altNames</foreach> </true> </if> </true> </if> <exit>EXIT</exit>",
    "log": "<!-- No explicit log statements in the code -->"
  },
  "91b042bd_12": {
    "exec_flow": "<entry>ENTRY</entry> <call>Preconditions.checkArgument</call> <call>Preconditions.checkArgument</call> <call>trim</call> <if> <condition>deprecations.getDeprecatedKeyMap().isEmpty() == true</condition> <true> <call>getProps</call> </true> </if> <call>getOverlay().setProperty</call> <call>getProps().setProperty</call> <if> <condition>!isDeprecated(name) == true</condition> <true> <call>handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)</call> <foreach>names</foreach> </true> </if> <exit>EXIT</exit>",
    "log": "<!-- No explicit log statements in the code -->"
  },
  "91b042bd_13": {
    "exec_flow": "<entry>ENTRY</entry> <call>getBoolean:YarnConfiguration.TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED,YarnConfiguration.TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED_DEFAULT</call> <if> <condition>(enableCorsFilter == true)</condition> <true> <call>setBoolean</call> </true> </if> <call>get</call> <call>TimelineServerUtils.setTimelineFilters</call> <exit>EXIT</exit>",
    "log": "<!-- No logs in this path -->"
  },
  "91b042bd_14": {
    "exec_flow": "<entry>ENTRY</entry> <call>getBoolean:YarnConfiguration.TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED,YarnConfiguration.TIMELINE_SERVICE_HTTP_CROSS_ORIGIN_ENABLED_DEFAULT</call> <if> <condition>(enableCorsFilter == false)</condition> <true> <call>get</call> <call>TimelineServerUtils.setTimelineFilters</call> </true> </if> <exit>EXIT</exit>",
    "log": "<!-- No logs in this path -->"
  },
  "91b042bd_15": {
    "exec_flow": "<entry>ENTRY → CALL:handleDeprecation → CALL:getProps → FOREACH: names → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT</entry> <call> <name>CALL:handleDeprecation</name> </call> <call> <name>CALL:getProps</name> </call> <foreach> <name>FOREACH: names</name> </foreach> <call> <name>CALL:substituteVars</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<log> <log_entry>org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)</log_entry> </log>"
  },
  "91b042bd_16": {
    "exec_flow": "<entry>ENTRY → CALL:handleDeprecation → CALL:getProps → FOREACH: names → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT</entry> <call> <name>CALL:handleDeprecation</name> </call> <call> <name>CALL:getProps</name> </call> <foreach> <name>FOREACH: names</name> </foreach> <call> <name>CALL:substituteVars</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<log> <log_entry>org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)</log_entry> </log>"
  },
  "91b042bd_17": {
    "exec_flow": "<entry>ENTRY → CALL:loadProps(properties, 0, true) → RETURN</entry> <call> <name>CALL:loadProps(properties, 0, true)</name> </call> <return>RETURN</return>",
    "log": "<log> <log_entry>org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)</log_entry> </log>"
  },
  "91b042bd_18": {
    "exec_flow": "<!-- 这里需要根据具体的合并逻辑来填充优化后的执行流结构，由于没有详细的合并逻辑，暂时无法给出具体内容 -->",
    "log": "<log> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> </log> <log> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> </log> <log> <log_entry>[INFO] message</log_entry> </log>"
  },
  "91b042bd_19": {
    "exec_flow": "<entry>ENTRY→CALL:getBoolean→CALL:getTrimmed→CALL:handleDeprecation→FOREACH:names→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→RETURN→EXIT</entry> <call> <name>CALL:getBoolean</name> </call> <call> <name>CALL:getTrimmed</name> </call> <call> <name>CALL:handleDeprecation</name> </call> <foreach> <name>FOREACH:names</name> </foreach> <call> <name>CALL:getProps</name> </call> <call> <name>CALL:substituteVars</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<log>org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)</log>"
  },
  "91b042bd_20": {
    "exec_flow": "<entry>ENTRY→CALL:getBoolean→CALL:getTrimmed→CALL:handleDeprecation→FOREACH:names→CALL:getProps→FOREACH_EXIT→RETURN→EXIT</entry> <call> <name>CALL:getBoolean</name> </call> <call> <name>CALL:getTrimmed</name> </call> <call> <name>CALL:handleDeprecation</name> </call> <foreach> <name>FOREACH:names</name> </foreach> <call> <name>CALL:getProps</name> </call> <foreach_exit>FOREACH_EXIT</foreach_exit> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<!-- 由于没有提供具体的日志序列合并信息，这里无法详细填写 -->"
  },
  "91b042bd_21": {
    "exec_flow": "<!-- 这里需要根据具体的合并逻辑来填充优化后的执行流结构，由于没有详细的合并逻辑，暂时无法给出具体内容 -->",
    "log": "<!-- 这里需要根据具体的合并逻辑来填充合并后的日志序列，由于没有详细的合并逻辑，暂时无法给出具体内容 -->"
  },
  "91b042bd_22": {
    "exec_flow": "<!-- 这里需要根据具体的合并逻辑来填充优化后的执行流结构，由于没有详细的合并逻辑，暂时无法给出具体内容 -->",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "91b042bd_23": {
    "exec_flow": "<entry>org.apache.hadoop.yarn.server.util.timeline.TimelineServerUtils:setTimelineFilters(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Set)</entry> <call>ignoreInitializers.add</call> <call>ignoreInitializers.add</call> <foreach>parts</foreach> <call>trim</call> <if_true>ignoreInitializers.contains(filterInitializer) || filterInitializer.isEmpty()</if_true> <continue/> <foreach_exit/> <call>addAll</call> <log>[INFO] Filter initializers set for timeline service: + actualInitializers</log> <call>org.apache.hadoop.conf.Configuration:set</call> <exit>org.apache.hadoop.yarn.server.util.timeline.TimelineServerUtils:setTimelineFilters(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Set)</exit> <call>org.slf4j.Logger:info(java.lang.String)</call> <exit>org.slf4j.Logger:info(java.lang.String)</exit>",
    "log": "[INFO] Filter initializers set for timeline service: + actualInitializers"
  },
  "35c66ad8_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:clearNodeSetForAttempt</step> <step>Thread.sleep(500)</step> <step>Catch InterruptedException</step> <step>EventHandler.Handle(RMAppAttemptEvent)</step> <step>ENTRY→LOG: LOG.INFO: Storing attempt: AppId: + getAppAttemptId().getApplicationId() + AttemptId: + getAppAttemptId() + MasterContainer: + masterContainer→CALL: org.slf4j.Logger:info(java.lang.String)→CALL: rmContext.getStateStore().storeNewApplicationAttempt→CALL: RMAppAttemptMetrics:getAggregateAppResourceUsage→CALL: ApplicationAttemptStateData:newInstance→CALL: getRMStateStoreEventHandler().handle→EXIT</step>",
    "log": "<!-- Merged log sequence --> <log> <class>org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM</class> <method>clearNodeSetForAttempt</method> <level>INFO</level> <template>Clear node set for {attemptId}</template> </log> <log> <class>Parent</class> <method>ENTRY</method> <level>INFO</level> <template>initAndGetCustomResources initializing custom resource metrics.</template> </log> <log> <class>org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl</class> <method>run()</method> <level>WARN</level> <template>Interrupted while waiting to resend the ContainerAllocated Event.</template> </log> <log> <class>rmContext.getStateStore().storeNewApplicationAttempt</class> <method>info()</method> <level>INFO</level> <template>Storing attempt: AppId: <placeholder> AttemptId: <placeholder> MasterContainer: <placeholder></template> </log>"
  },
  "ff04e74a_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\") → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT → ENTRY → SYNC:this → CALL:get → IF_FALSE: fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_FALSE: fs != null → CALL:createFileSystem → CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC:this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL:ShutdownHookManager.get().addShutdownHook → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log> <log>[DEBUG] Filesystem {} created while awaiting semaphore</log> <log>[DEBUG] Duplicate FS created for {}; discarding {}</log> <log>[DEBUG] No summary directory set in {}</log> <log>[DEBUG] Summary directory set to {}</log> <log>[DEBUG] Report already exists: {}</log> <log>[INFO] Job summary saved to {}</log> <log>[DEBUG] Failed to save summary to {}</log>"
  },
  "f0d5e23d_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable) → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] Unexpected SecurityException in Configuration"
  },
  "f0d5e23d_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable) → LOG:LOG.DEBUG→CALL:statIncrement → CALL:trailingPeriodCheck→CALL:makeQualified → TRY→EXCEPTION:createFile→CATCH:AzureBlobFileSystemException → CALL:checkException→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}"
  },
  "f0d5e23d_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable) → TRY→CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory()→IF_FALSE:!overwrite → CALL:org.slf4j.Logger:debug→NEW:FSDataOutputStream → NEW:CosNOutputStream→CALL:getConf→RETURN→EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS."
  },
  "fe30fe64_1": {
    "exec_flow": "ENTRY → IF_TRUE:stagingDirFS.exists → LOG:LOG.INFO:Copying fromPath.toString() to toPath.toString() → CALL:org.apache.hadoop.fs.FileSystem:delete → CALL:org.apache.hadoop.fs.FileUtil:copy → CALL:doneDirFS.setPermission → IF_TRUE:copied → LOG:LOG.INFO:Copied from: fromPath.toString() to done location: toPath.toString() → RETURN → EXIT",
    "log": "<log>[INFO] Copying fromPath to toPath</log> <log>[INFO] Copied from: fromPath to done location: toPath</log>"
  },
  "fe30fe64_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Getting the file status for {}, f.toString() → IF_FALSE:key.length()==0 → TRY → CALL:retrieveMetadata → IF_TRUE:meta != null → IF_FALSE:meta.isDirectory() → LOG:LOG.DEBUG:Found the path: {} as a file., f.toString() → CALL:updateFileStatusPath → RETURN → EXIT",
    "log": "<log>[DEBUG] Getting the file status for {}</log> <log>[DEBUG] Found the path: {} as a file.</log>"
  },
  "fe30fe64_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:AzureBlobFileSystem.getFileStatus path: {} → CALL:statIncrement → CALL:makeQualified → TRY → CALL:getFileStatus → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "fe30fe64_4": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:AzureBlobFileSystem.getFileStatus path: {} → CALL:statIncrement → CALL:makeQualified → TRY → CALL:getFileStatus → CATCH → CALL:checkException → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "fe30fe64_5": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → LOG:Handling deprecation for (String)item → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "fe30fe64_6": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Getting the file status for {}, f.toString() → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta != null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[DEBUG] Getting the file status for {}, [DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "bf43b503_1": {
    "exec_flow": "ENTRY → TRY → LOG: [INFO] createStartupShutdownMessage(classname, hostname, args) → IF_TRUE: SystemUtils.IS_OS_UNIX → CALL: SignalLogger.INSTANCE.register → CATCH: Throwable t → LOG: [WARN] failed to register any UNIX signal loggers: , t → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.slf4j.Logger:warn(\"When stopping the service {service.getName()}\") → CALL:terminate → CALL: org.slf4j.Logger:info(\"Service {getName()} failed in state {failureState}\") → CALL: org.slf4j.Logger:debug(\"noteFailure\") → EXCEPTION: processGeneralOptions → EXIT",
    "log": "<log>[INFO] createStartupShutdownMessage(classname, hostname, args)</log> <log>[WARN] failed to register any UNIX signal loggers: , t</log> <log>[WARN] When stopping the service {service.getName()}</log> <log>[INFO] Service {getName()} failed in state {failureState}</log> <log>[DEBUG] noteFailure</log>"
  },
  "e6bfc96b_1": {
    "exec_flow": "ENTRY→CALL:exists→CALL:CreateFlag.validate→IF_TRUE:pathExists && flag.contains(CreateFlag.APPEND)→CALL:append→RETURN→EXIT→CALL:statIncrement→CALL:trailingPeriodCheck→CALL:resolve→IF_FALSE:path.length<=1→IF_FALSE:root.isLink()→LOG:DEBUG→CALL:Preconditions.checkState→CALL:tryResolveInRegexMountpoint→IF_TRUE:resolveResult!=null→RETURN→EXIT→CALL:getUriPath→CALL:create→RETURN→EXIT→CALL:trackDurationAndSpan",
    "log": "<log>[DEBUG] Getting the file status for {filePath}</log> <log>[DEBUG] Path {filePath} is a folder.</log> <log>[DEBUG] Found the path: {filePath} as a file.</log> <log>[DEBUG] Creating a new file: [{}] in COS.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Overwriting file {}</log> <log>[INFO] message</log>"
  },
  "e6bfc96b_2": {
    "exec_flow": "ENTRY→TRY→CALL:setLogEnabled→CALL:getObjectMetadata(request)→EXCEPTION:getObjectMetadata→CATCH:OSSException osse→CALL:LOG.debug→RETURN→EXIT→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→LOG:DEBUG→CATCH:FileNotFoundException→CALL:getMultipartSizeProperty→CALL:org.apache.hadoop.conf.Configuration:getLong→IF_TRUE:partSize<MULTIPART_MIN_SIZE→LOG:WARN→RETURN→TEXT:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→EXIT",
    "log": "<log>[DEBUG] Creating a new file: [{}] in COS.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log> <log>[DEBUG] Overwriting file {}</log> <log>[WARN] {} must be at least 100 KB; configured value is {}</log>"
  },
  "e6bfc96b_3": {
    "exec_flow": "ENTRY → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite → CALL:startTracking → CALL:client.getPathStatus → CALL:perfInfo.registerResult → IF_FALSE:parseIsDirectory(resourceType) → CALL:perfInfo.registerSuccess → IF_TRUE:isAppendBlobKey → CALL:isAppendBlobKey → CALL:maybeCreateLease → NEW:AbfsOutputStream → CALL:populateAbfsOutputStreamContext → CALL:perfInfo.close → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "e6bfc96b_4": {
    "exec_flow": "ENTRY → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite → CALL:startTracking → CALL:client.getPathStatus → CALL:perfInfo.registerResult → IF_FALSE:parseIsDirectory(resourceType) → CALL:perfInfo.registerSuccess → IF_FALSE:isAppendBlobKey → NEW:AbfsOutputStream → CALL:populateAbfsOutputStreamContext → CALL:perfInfo.close → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "e6bfc96b_5": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_TRUE: meta == null→THROW: new FileNotFoundException(f.toString())→EXIT",
    "log": "<log>[DEBUG] Opening file: {} for append</log>"
  },
  "e6bfc96b_6": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_TRUE: meta.isDirectory()→THROW: new FileNotFoundException(f.toString() + \" is a directory not a file.\")→EXIT",
    "log": "<log>[DEBUG] Opening file: {} for append</log>"
  },
  "e6bfc96b_7": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: meta.isDirectory()→IF_TRUE: store.isPageBlobKey(key)→THROW: new IOException(\"Append not supported for Page Blobs\")→EXIT",
    "log": "<log>[DEBUG] Opening file: {} for append</log>"
  },
  "e6bfc96b_8": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: meta.isDirectory()→IF_FALSE: store.isPageBlobKey(key)→TRY→CALL: retrieveAppendStream→NEW: FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] Opening file: {} for append</log>"
  },
  "55108320_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:ensureInitialized→IF_FALSE:subject == null || subject.getPrincipals(User.class).isEmpty()→NEW:UserGroupInformation→RETURN→SYNC:this.userPipelineMap→IF_FALSE:this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: LOG.INFO: Initializing request processing pipeline for application for the user: {}, user→CALL: interceptorChain.init→CALL: chainWrapper.init→CALL: this.userPipelineMap.put→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object [INFO] Initializing request processing pipeline for application for the user: {}"
  },
  "55108320_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.security.UserGroupInformation:ensureInitialized→IF_FALSE:subject == null || subject.getPrincipals(User.class).isEmpty()→NEW:UserGroupInformation→RETURN→SYNC:this.userPipelineMap→IF_FALSE:this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: LOG.INFO: Initializing request processing pipeline for application for the user: {}, user→EXCEPTION: info→CATCH: Exception e→LOG: LOG.ERROR: Init ClientRequestInterceptor error for user: +user, e→THROW: e→EXIT",
    "log": "[DEBUG] Creating new Groups object [INFO] Initializing request processing pipeline for application for the user: {} [ERROR] Init ClientRequestInterceptor error for user: {}"
  },
  "55108320_3": {
    "exec_flow": "<entry>AbstractClientRequestInterceptor.ENTRY</entry> <call>VIRTUAL_CALL</call> <child>DefaultClientRequestInterceptor.getResourceProfiles</child>",
    "log": "<!-- No logs in parent, inheriting from child -->"
  },
  "55180392_1": {
    "exec_flow": "ENTRY → CALL: clearAllPendingReduceRequests → CALL: multiply → CALL: org.apache.hadoop.yarn.util.resource.Resources:multiplyTo → CALL: org.apache.hadoop.yarn.util.resource.Resources:multiplyAndRound → FOR_INIT → CALL: org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes() → FOR_COND: i < maxLength → TRY → CALL: org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) → CALL: org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) → CATCH: ResourceNotFoundException → LOG.warn(Resource is missing) → CALL: org.apache.hadoop.yarn.api.records.Resource:setResourceValue(int,long) → FOR_BODY → FOR_UPDATE → FOR_COND: i < maxLength → EXCEPTION: ResourceNotFoundException → LOG.warn(Resource is missing) → FOR_UPDATE → FOR_COND: i < maxLength → FOR_EXIT → CALL: Resources.multiply → LOG.info(\"Going to preempt \" + toPreempt + \" due to lack of space for maps\") → CALL: assignedRequests.preemptReduce → IF_TRUE: rmApp != null → IF_TRUE: rmAppAttempt != null → TRY → LOG: [DEBUG] Processing {event.getNodeId()} of type {event.getType()} → CALL: rmAppAttempt.handle → CATCH: Throwable t → LOG: [ERROR] Error in handling event type for applicationAttempt → LOG: [ERROR] Can't handle this event at current state → LOG: [ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState → IF_TRUE: oldState != getState() → LOG: [INFO] nodeId + Node Transitioned from + oldState + to + getState() → EXIT",
    "log": "[WARN] Resource is missing [INFO] Going to preempt {toPreempt} due to lack of space for maps [DEBUG] Processing {event.getNodeId()} of type {event.getType()} [ERROR] Error in handling event type for applicationAttempt [ERROR] Can't handle this event at current state [ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState [INFO] nodeId + Node Transitioned from + oldState + to + getState()"
  },
  "e0f76ab6_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → CALL:org.apache.hadoop.hdfs.DFSOutputStream:hflush() → CALL:dfsClient.checkOpen → CALL:checkClosed → TRY → SYNC:this → CALL:flushBuffer → CALL:org.slf4j.Logger:debug → CALL:enqueueCurrentPacket → CALL:getStreamer().waitForAckedSeqno → CALL:dfsClient.namenode.fsync → CALL:getStreamer().setHflush → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] DFSClient flush(): bytesCurBlock={}, lastFlushOffset={}, createNewBlock={}"
  },
  "e0f76ab6_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → LOG: LOG.DEBUG: {} waiting for ack for: {}, this, seqno → TRY → SYNC: dataQueue → WHILE: !streamerClosed → CALL: checkClosed → EXCEPTION: checkClosed → CATCH: ClosedChannelException cce → LOG: LOG.DEBUG: Closed channel exception, cce → IF_FALSE: duration > dfsclientSlowLogThresholdMs → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] {} waiting for ack for: {} [DEBUG] Closed channel exception"
  },
  "e0f76ab6_3": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → LOG: LOG.DEBUG: {} waiting for ack for: {}, this, seqno → TRY → SYNC: dataQueue → WHILE: !streamerClosed → CALL: checkClosed → IF_TRUE: duration > dfsclientSlowLogThresholdMs → LOG: LOG.WARN: Slow waitForAckedSeqno took {}ms (threshold={}ms). File being written: {}, block: {}, Write pipeline datanodes: {}. , duration, dfsclientSlowLogThresholdMs, src, block, nodes → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] {} waiting for ack for: {} [WARN] Slow waitForAckedSeqno took {}ms (threshold={}ms). File being written: {}, block: {}, Write pipeline datanodes: {}."
  },
  "e0f76ab6_4": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → LOG: LOG.DEBUG: {} waiting for ack for: {}, this, seqno → TRY → SYNC: dataQueue → WHILE: !streamerClosed → CALL: checkClosed → IF_FALSE: duration > dfsclientSlowLogThresholdMs → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] {} waiting for ack for: {}"
  },
  "e0f76ab6_5": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → CALL:checkState → CALL:uploadBlockAsync → LOG:LOG.DEBUG:Writing block # {}, blockCount → TRY → CALL:uploadBlockAsync → CALL:clearActiveBlock → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Writing block # {}"
  },
  "ffd05e0a_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "2f3659ea_1": {
    "exec_flow": "ENTRY→FOREACH:app.containers.values()→IF_TRUE:node!=null→TRY→CALL:((EventHandler<RMNodeEvent>)node).handle→EXCEPTION:handle→CATCH:Throwable t→LOG:ERROR:Error in handling event type + event.getType() + for node + nodeId, t→FOREACH_EXIT→EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for node + nodeId, t"
  },
  "016dbb30_1": {
    "exec_flow": "ENTRY→IF_TRUE: finished > 0 && started > 0→IF_FALSE: elapsed >= 0→CALL: org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Finished time + finished + is ahead of started time + started"
  },
  "016dbb30_2": {
    "exec_flow": "ENTRY→IF_FALSE: finished > 0 && started > 0→IF_TRUE: isRunning→IF_FALSE: elapsed >= 0→CALL: org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Current time + current + is ahead of started time + started"
  },
  "016dbb30_3": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}]→CATCH: PrivilegedActionException→LOG: LOG.DEBUG: PrivilegedActionException as: {}→THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "50b028e7_1": {
    "exec_flow": "ENTRY → CALL: checkAcls → CALL: checkRMStatus → TRY → CALL: getConfigurationInputStream → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE: meta != null → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE: listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → RETURN → EXIT",
    "log": "[INFO] Refreshing nodes resources [DEBUG] Configuration loaded [INFO] Nodes checked and resources updated [DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}]. [DEBUG] Path: [{}] is a file. COS key: [{}] [DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}] [INFO] Audit log for success"
  },
  "50b028e7_2": {
    "exec_flow": "ENTRY → VIRTUAL_CALL: getTrimmed → LOG: Unexpected SecurityException in Configuration → EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "50b028e7_3": {
    "exec_flow": "ENTRY → CALL: getBoolean → CALL: handleDeprecation → LOG: Handling deprecation for all properties in config... → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "50b028e7_4": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isInfoEnabled() → CALL: org.slf4j.Logger:isInfoEnabled() → CALL: org.slf4j.Logger:info(java.lang.String) → CALL: createSuccessLog(user, operation, target, null, null, null, null) → EXIT",
    "log": "[INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "50b028e7_5": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → EXCEPTION: createRequestInterceptorChain → CATCH: Exception e → LOG: LOG.ERROR: Init RMAdminRequestInterceptor error for user: + user, e → THROW: e → EXIT",
    "log": "[INFO] Initializing request processing pipeline for user: {} [ERROR] Init RMAdminRequestInterceptor error for user: {}"
  },
  "171c56d3_1": {
    "exec_flow": "<step>checkSpillException()</step> <step>[VIRTUAL_CALL] → reportFatalError()</step> <step>ENTRY→TRY→CALL:getSpillFileForWrite→CALL:create→FOR_INIT→FOR_COND:i < partitions→CALL:wrapIfNecessary→CALL:<init>→CALL:append→CALL:close→CALL:cryptoPadding→FOR_EXIT→IF_FALSE:totalIndexCacheMemory >= indexCacheMemoryLimit→CALL:indexCacheList.add→CALL:size→IF_TRUE:out != null→CALL:close→IF_FALSE:partitionOut != null→EXIT</step>",
    "log": "<log_entry> <source>checkSpillException()</source> <log_template>[Error] Task {taskID} failed : {exception_description}</log_template> </log_entry>"
  },
  "171c56d3_2": {
    "exec_flow": "ENTRY -> CALL:progress -> CALL:checkSpillException -> IF_FALSE:key.getClass() != keyClass -> IF_FALSE:value.getClass() != valClass -> IF_FALSE:partition < 0 || partition >= partitions -> CALL:spillSingleRecord -> CALL:org.slf4j.Logger:info -> RETURN -> EXIT",
    "log": "<log_entry> <source>collect()</source> <log_template>[INFO] Record too large for in-memory buffer</log_template> </log_entry>"
  },
  "171c56d3_3": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapred.MapTask$MapOutputBuffer:collect → RETURN → EXIT",
    "log": "<log_entry> <source>collect()</source> <log_template>[INFO] Collection by MapOutputBuffer started</log_template> </log_entry>"
  },
  "171c56d3_4": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:collect → RETURN → EXIT",
    "log": "<log_entry> <source>collect()</source> <log_template>[INFO] Collection by NativeMapOutputCollectorDelegator started</log_template> </log_entry>"
  },
  "171c56d3_5": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapred.MapTask$DirectMapOutputCollector:collect → RETURN → EXIT",
    "log": "<log_entry> <source>collect()</source> <log_template>[INFO] Collection by DirectMapOutputCollector started</log_template> </log_entry>"
  },
  "7d07ebb0_1": {
    "exec_flow": "ENTRY→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "7d07ebb0_2": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:getTrimmed→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "7d07ebb0_3": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "8fec6406_1": {
    "exec_flow": "ENTRY→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→ENTRY→TRY→LOG:LOG.DEBUG:openFileForWrite→CALL:startTracking→CALL:client.getPathStatus→CALL:perfInfo.registerResult→CALL:perfInfo.registerSuccess→IF_TRUE:isAppendBlobKey→CALL:isAppendBlobKey→CALL:maybeCreateLease→NEW:AbfsOutputStream→CALL:populateAbfsOutputStreamContext→CALL:perfInfo.close→RETURN→EXIT→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "8fec6406_2": {
    "exec_flow": "ENTRY→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→ENTRY→TRY→LOG:LOG.DEBUG:openFileForWrite→CALL:startTracking→CALL:client.getPathStatus→CALL:perfInfo.registerResult→CALL:perfInfo.registerSuccess→IF_TRUE:isAppendBlobKey→CALL:isAppendBlobKey→CALL:maybeCreateLease→NEW:AbfsOutputStream→CALL:populateAbfsOutputStreamContext→CALL:perfInfo.close→RETURN→EXIT→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "8fec6406_3": {
    "exec_flow": "ENTRY→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→ENTRY→TRY→LOG:LOG.DEBUG:openFileForWrite→CALL:startTracking→CALL:client.getPathStatus→CALL:perfInfo.registerResult→CALL:perfInfo.registerSuccess→IF_FALSE:isAppendBlobKey→NEW:AbfsOutputStream→CALL:populateAbfsOutputStreamContext→CALL:perfInfo.close→RETURN→EXIT→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}</log> <log>[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite</log>"
  },
  "2bdf9c93_1": {
    "exec_flow": "ENTRY -> IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 -> IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock -> CALL: getDatanodeDescriptor -> CALL: get -> IF_TRUE: storageTypes == null -> CALL: getRequiredStorageTypes -> LOG: LOG.TRACE: storageTypes={}, storageTypes -> TRY -> IF_FALSE: requiredStorageTypes.size() == 0 -> CALL: chooseTargetInOrder -> RETURN -> EXIT",
    "log": "[TRACE] storageTypes={}, storageTypes"
  },
  "2bdf9c93_2": {
    "exec_flow": "ENTRY -> IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 -> IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock -> CALL: getDatanodeDescriptor -> CALL: get -> IF_FALSE: storageTypes == null -> LOG: LOG.TRACE: storageTypes={}, storageTypes -> TRY -> IF_TRUE: requiredStorageTypes.size() == 0 -> THROW: new NotEnoughReplicasException -> LOG: LOG.TRACE: message, e -> LOG: LOG.WARN: message + e.getMessage() -> EXIT",
    "log": "[TRACE] storageTypes={}, storageTypes [TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e [WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()"
  },
  "5c7d8008_1": {
    "exec_flow": "ENTRY → TRY → CALL:toBoolean → CATCH:TrileanConversionException → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext) → CATCH:AbfsRestOperationException → CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode() → CALL:org.slf4j.Logger:debug(java.lang.String) → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close() → ENTRY → [VIRTUAL_CALL] → LOG: LOG.DEBUG: createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {} → CALL:createDefaultHeaders → IF_TRUE: isFile → CALL: addCustomerProvidedKeyHeaders → IF_TRUE: !overwrite → CALL: requestHeaders.add → IF_FALSE: permission != null && !permission.isEmpty() → IF_FALSE: umask != null && !umask.isEmpty() → IF_FALSE: eTag != null && !eTag.isEmpty() → CALL: abfsUriQueryBuilder.addQuery → IF_FALSE: isAppendBlob → CALL: appendSASTokenToQuery → TRY → CALL: IOStatisticsBinding.trackDurationOfInvocation → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:lambda$execute$0 → CATCH: AzureBlobFileSystemException ex → IF_TRUE: !op.hasResult() → THROW: ex → CALL: perfInfo.registerResult(op.getResult()).registerSuccess → CALL: maybeCreateLease → ENTRY → [VIRTUAL_CALL] → LOG: LOG.DEBUG: Attempting to acquire lease on {}, retry {}, path, numRetries → IF_FALSE: future != null && !future.isDone() → CALL: schedule → CALL: acquireLease → CALL: addCallback → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsLease:lambda$0(tracingContext) → EXIT → RETURN → EXIT",
    "log": "<log> [DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {} </log> <log> [DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call </log> <log> [DEBUG] Get root ACL status </log> <log> [DEBUG] IOStatisticsBinding track duration started </log> <log> [TRACE] Fetch SAS token for {operation} on {path} </log> <log> [TRACE] SAS token fetch complete for {operation} on {path} </log> <log> [DEBUG] First execution of REST operation - {} </log> <log> [DEBUG] Retrying REST operation {}. RetryCount = {} </log> <log> [DEBUG] Unexpected error. </log> <log> [TRACE] {} REST operation complete </log> <log> [DEBUG] IOStatisticsBinding track duration completed </log> <log> [DEBUG] Attempting to acquire lease on {}, retry {} </log>"
  },
  "5c7d8008_2": {
    "exec_flow": "ENTRY → TRY → CALL:createPath → EXCEPTION → CALL:getPathStatus → RETURN → CALL:createPath → RETURN → EXIT",
    "log": "<log> [DEBUG] IOStatisticsBinding track duration started </log> <log> [TRACE] Fetch SAS token for {operation} on {path} </log> <log> [TRACE] SAS token fetch complete for {operation} on {path} </log> <log> [DEBUG] First execution of REST operation - {} </log> <log> [DEBUG] Retrying REST operation {}. RetryCount = {} </log> <log> [DEBUG] IOStatisticsBinding track duration completed </log>"
  },
  "5c7d8008_3": {
    "exec_flow": "ENTRY → IF_FALSE:!enabled → IF_TRUE:isValidInstant(perfInfo.getAggregateStart())&&perfInfo.getAggregateCount()>0 → CALL:recordClientLatency → ENTRY → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:String.format → CALL:offerToQueue → IF_TRUE:LOG.isDebugEnabled() → CALL:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Queued latency info [{} ms]: {}, elapsed, latencyDetails → RETURN → EXIT → EXIT",
    "log": "<log> [DEBUG] Queued latency info [{} ms]: {} </log>"
  },
  "91dd532e_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a file. COS key: {key}</template> </log_entry>"
  },
  "91dd532e_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a dir. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a dir. COS key: {key}</template> </log_entry>"
  },
  "91dd532e_3": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0 → LOG:LOG.DEBUG:Path: [{}] is a directory. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>List COS key: {key} to check the existence of the path.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a directory. COS key: {key}</template> </log_entry>"
  },
  "6eff6773_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→IF_TRUE: loadDefaults && fullReload→FOREACH: defaultResources→CALL: loadResource→FOREACH_EXIT→FOR_INIT→FOR_COND: i < resources.size()→CALL: loadResource→FOR_EXIT→CALL: addTags→EXIT→ENTRY→CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY→CALL:get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→EXIT",
    "log": "<!-- Merged log sequence --> [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "d3f5cf35_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT → LOG: [DEBUG] Incrementing stat CALL_OPEN → IF: condition=\"!q.isRoot()\" THEN → LOG: LOG.debug(\"Stripping trailing '/' from {}\", q) → LOG: [INFO] File qualified → LOG: [INFO] File opened for read → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Incrementing stat CALL_OPEN</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Stripping trailing '/' from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>File qualified</template> </log_entry> <log_entry> <level>INFO</level> <template>File opened for read</template> </log_entry>"
  },
  "d3f5cf35_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Opening file: {}, f.toString() → CALL: performAuthCheck → TRY → CALL: retrieveMetadata → IF_TRUE: meta == null THEN → THROW: new FileNotFoundException(f.toString()) → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Opening file: {f.toString()}</template> </log_entry>"
  },
  "d3f5cf35_3": {
    "exec_flow": "ENTRY → IF: condition=\"!fs.isDirectory()\" THEN → CALL: NativeFileSystemStore:getFileLength(key) → LOG: LOG.info(\"Open the file: [{}] for reading.\", f) → RETURN: FSDataInputStream → CALL: getConf → NEW: BufferedFSInputStream → NEW: CosNInputStream → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Open the file: [{f}] for reading.</template> </log_entry>"
  },
  "8972bb2e_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:clientConf.setMaxConnections</step> <step>CALL:clientConf.setProtocol</step> <step>CALL:clientConf.setMaxErrorRetry</step> <step>CALL:clientConf.setConnectionTimeout</step> <step>CALL:clientConf.setSocketTimeout</step> <step>CALL:clientConf.setUserAgent</step> <step>CALL:org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>IF_FALSE:StringUtils.isNotEmpty(proxyHost)</step> <step>CALL:org.apache.hadoop.conf.Configuration:getInt</step> <step>IF_TRUE:proxyPort >= 0</step> <step>CALL:LOG.error</step> <step>THROW:new IllegalArgumentException</step>",
    "log": "<log>[ERROR] Proxy error: proxyPort set without proxyHost</log>"
  },
  "8972bb2e_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.conf.Configuration:getLong</step> <step>IF_TRUE:partSize < MULTIPART_MIN_SIZE</step> <step>CALL:LOG.WARN</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN]: {} must be at least 100 KB; configured value is {}</log>"
  },
  "8972bb2e_3": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.conf.Configuration:getLong</step> <step>IF_FALSE:partSize < MULTIPART_MIN_SIZE</step> <step>IF_TRUE:partSize > Integer.MAX_VALUE</step> <step>CALL:LOG.WARN</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN]: oss: {} capped to ~2.14GB(maximum allowed size with current output mechanism)</log>"
  },
  "8972bb2e_4": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:props != null</step> <step>CALL:loadResources</step> <step>IF_FALSE:overlay != null</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>WARN: Unexpected SecurityException in Configuration</log>"
  },
  "8972bb2e_5": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:LOG_DEPRECATION.info</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "8972bb2e_6": {
    "exec_flow": "<step>ENTRY</step> <step>FOR_LOOP:newNames</step> <step>GET:deprecatedKey</step> <step>IF_TRUE:deprecatedKey != null && !getProps().containsKey(newName)</step> <step>IF_TRUE:deprecatedValue != null</step> <step>SET_PROPERTY:newName</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "7f73ecf2_1": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "<log> <template>Filesystem {} created while awaiting semaphore</template> <level>debug</level> </log>"
  },
  "7f73ecf2_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE:props != null → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → CALL:addTags → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log> <template>Handling deprecation for all properties in config...</template> <level>debug</level> </log> <log> <template>Handling deprecation for (String)item</template> <level>debug</level> </log> <log> <template>message</template> <level>info</level> </log>"
  },
  "7f73ecf2_3": {
    "exec_flow": "ENTRY → SYNC:this → CALL:createDistCacheDirectory → WHILE:(jobStory = jsp.getNextJob()) != null → CALL:org.apache.hadoop.tools.rumen.JobStoryProducer:getNextJob → WHILE_COND:(jobStory = jsp.getNextJob()) != null → IF:jobStory.getOutcome() == Pre21JobHistoryConstants.Values.SUCCESS → CALL:org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:updateHDFSDistCacheFilesList(org.apache.hadoop.tools.rumen.JobStory) → WHILE_EXIT → CALL:close → CALL:writeDistCacheFilesList → RETURN → EXIT",
    "log": "<log> <template>Number of HDFS based distributed cache files to be generated is ${fileCount}. Total size of HDFS based distributed cache files to be generated is ${byteCount}.</template> <level>info</level> </log> <log> <template>Missing ${fileCount} distributed cache files under the directory ${distCachePath} that are needed for gridmix to emulate distributed cache load. Either use -generate option to generate distributed cache data along with input data OR disable distributed cache emulation by configuring '${DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE}' to false.</template> <level>error</level> </log>"
  },
  "7f73ecf2_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs→LOG: LOG.DEBUG: AzureBlobFileSystem.mkdirs path: {} permissions: {}→CALL: statIncrement→CALL: trailingPeriodCheck→IF_TRUE: parentFolder == null→RETURN→EXIT",
    "log": "<log> <template>AzureBlobFileSystem.mkdirs path: {} permissions: {}</template> <level>debug</level> </log>"
  },
  "770aa241_1": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Processing [EVENT_ID] of type [EVENT_TYPE]→TRY→CALL:writeLock.lock→TRY→CALL:getStateMachine().doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→LOG:LOG.ERROR:Can't handle this event at current state, e→CALL:addDiagnostic→CALL:eventHandler.handle→CALL:writeLock.unlock→EXIT",
    "log": "[DEBUG] Processing [EVENT_ID] of type [EVENT_TYPE] [ERROR] Can't handle this event at current state, e"
  },
  "770aa241_2": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[REQUEST_RESOURCE_LOCALIZATION]→SWITCH:req.getVisibility()→CASE:[APPLICATION]→SYNC:privLocalizers→IF_TRUE:localizer != null && localizer.killContainerLocalizer.get()→LOG:LOG.INFO:New + event.getType() + localize request for + locId + , remove old private localizer.→CALL:privLocalizers.remove→CALL:localizer.interrupt→BREAK→EXIT",
    "log": "[INFO] New REQUEST_RESOURCE_LOCALIZATION localize request for locId, remove old private localizer."
  },
  "770aa241_3": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[REQUEST_RESOURCE_LOCALIZATION]→SWITCH:req.getVisibility()→CASE:[APPLICATION]→SYNC:privLocalizers→IF_FALSE:localizer != null && localizer.killContainerLocalizer.get()→IF_TRUE:null == localizer→IF_TRUE:recentlyCleanedLocalizers.containsKey(locId)→LOG:LOG.INFO:Skipping localization request for recently cleaned + localizer + locId + resource: + req.getResource()→BREAK→EXIT",
    "log": "[INFO] Skipping localization request for recently cleaned localizer locId resource: req.getResource()"
  },
  "770aa241_4": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Processing [EVENT_ID] of type [EVENT_TYPE]→TRY→CALL:writeLock.lock→TRY→CALL:getStateMachine().doTransition→IF_TRUE:oldState != getInternalState()→LOG:LOG.INFO:jobId + Job Transitioned from + oldState + to + getInternalState()→CALL:rememberLastNonFinalState→CALL:writeLock.unlock→EXIT",
    "log": "[DEBUG] Processing [EVENT_ID] of type [EVENT_TYPE] [INFO] [JOBID] Job Transitioned from [OLD_STATE] to [NEW_STATE]"
  },
  "770aa241_5": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[]→LOG:LOG.WARN:Invalid shutdown event + event.getType() + . Ignoring.→EXIT",
    "log": "[WARN] Invalid shutdown event + event.getType() + . Ignoring."
  },
  "770aa241_6": {
    "exec_flow": "ENTRY→IF_FALSE:instance == null→TRY→CALL:handle→EXCEPTION:handle→CATCH:Throwable t→LOG:LOG.ERROR:instance.getCompInstanceId() + \": Error in handling event type \" + event.getType(), t→EXIT",
    "log": "[ERROR] instance.getCompInstanceId() + \": Error in handling event type \" + event.getType(), t"
  },
  "6b08165a_1": {
    "exec_flow": "ENTRY → CALL:getUserName → NEW:URL → CALL:setRequestMethod → CALL:connect → CALL:AuthenticatedURL.extractToken → ENTRY → IF_FALSE: respCode == HttpURLConnection.HTTP_OK || respCode == HttpURLConnection.HTTP_CREATED || respCode == HttpURLConnection.HTTP_ACCEPTED → IF_FALSE: respCode == HttpURLConnection.HTTP_NOT_FOUND → LOG: LOG.TRACE: Setting token value to null ({token}), resp={respCode} → CALL: set → THROW: new AuthenticationException(\"Authentication failed\" + \", URL: \" + conn.getURL() + \", status: \" + conn.getResponseCode() + \", message: \" + conn.getResponseMessage()) → EXIT",
    "log": "[TRACE] Setting token value to null ({token}), resp={respCode}"
  },
  "32ff90f1_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_TRUE: editLogWasOpen→CALL: editLog.endCurrentLogSegment→LOG: LOG.INFO: Ending log segment + curSegmentTxId + , + getLastWrittenTxId()→CALL: Preconditions.checkState→IF_TRUE: writeEndTxn→CALL: logEdit→CALL: logSyncAll→CALL: printStatistics→CALL: Preconditions.checkArgument→TRY→CALL: journalSet.finalizeLogSegment→CATCH: IOException e→EXIT→IF_TRUE: !addToCheckpointing(imageTxId)→THROW: new IOException(\"FS image is being downloaded from another NN at txid \" + imageTxId)→EXIT",
    "log": "[INFO] Save namespace ... [INFO] Ending log segment"
  },
  "32ff90f1_2": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_TRUE: editLogWasOpen→CALL: editLog.endCurrentLogSegment→LOG: LOG.INFO: Ending log segment + curSegmentTxId + , + getLastWrittenTxId()→CALL: Preconditions.checkState→IF_TRUE: writeEndTxn→CALL: logEdit→CALL: logSyncAll→CALL: printStatistics→CALL: Preconditions.checkArgument→TRY→CALL: journalSet.finalizeLogSegment→EXIT→IF_FALSE: !addToCheckpointing(imageTxId)→TRY→TRY→CALL: saveFSImageInAllDirs→IF_FALSE: !source.isRollingUpgrade()→CALL: updateStorageVersion→IF_TRUE: editLogWasOpen→CALL: startLogSegmentAndWriteHeaderTxn→CALL: writeTransactionIdFileToStorage→CALL: removeFromCheckpointing→CALL: updateNameDirSize→IF_TRUE: exitAfterSave.get()→LOG: LOG.ERROR: NameNode process will exit now... The saved FsImage <nnf> is potentially corrupted.→CALL: terminate→EXIT",
    "log": "[INFO] Save namespace ... [INFO] Ending log segment [ERROR] NameNode process will exit now... The saved FsImage <nnf> is potentially corrupted."
  },
  "32ff90f1_3": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_FALSE: editLogWasOpen→IF_TRUE: !addToCheckpointing(imageTxId)→THROW: new IOException(\"FS image is being downloaded from another NN at txid \" + imageTxId)→EXIT",
    "log": "[INFO] Save namespace ..."
  },
  "32ff90f1_4": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_FALSE: editLogWasOpen→IF_FALSE: !addToCheckpointing(imageTxId)→TRY→TRY→CALL: saveFSImageInAllDirs→IF_TRUE: !source.isRollingUpgrade()→CALL: updateStorageVersion→IF_FALSE: editLogWasOpen→CALL: removeFromCheckpointing→CALL: updateNameDirSize→IF_FALSE: exitAfterSave.get()→EXIT",
    "log": "[INFO] Save namespace ..."
  },
  "32ff90f1_5": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_FALSE: editLogWasOpen→IF_FALSE: !addToCheckpointing(imageTxId)→TRY→TRY→CALL: saveFSImageInAllDirs→IF_FALSE: !source.isRollingUpgrade()→IF_FALSE: editLogWasOpen→CALL: removeFromCheckpointing→CALL: updateNameDirSize→IF_FALSE: exitAfterSave.get()→EXIT",
    "log": "[INFO] Save namespace ..."
  },
  "2c8d4232_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→P2-C1:ENTRY→TRY→CALL:UserGroupInformation.getCurrentUser→[if_true (ensureInitialized succeeded AND (subject == null || subject.getPrincipals(User.class).isEmpty()))]→CALL:getLoginUser→CALL:warn→CALL:makeQualified→NEW:Path→RETURN→EXIT→RETURN→EXIT",
    "log": "[LOG] getLoginUser [WARN] Unable to get user name. Fall back to system property user.name"
  },
  "ba141f60_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem:getLocal → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_FALSE:fs != null → CALL:createFileSystem → LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT → CALL:org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker:schedule → FOREACH_EXIT → IF_FALSE: maxVolumeFailuresTolerated >= dataDirs.size() → FOREACH: futures.entrySet() → TRY → SWITCH: result → CASE: [DEGRADED] → LOG: org.slf4j.Logger:warn(\"StorageLocation {} appears to be degraded.\", location) → BREAK → CASE: [FAILED] → LOG: org.slf4j.Logger:warn(\"StorageLocation {} detected as failed.\", location) → CALL: failedLocations.add → CALL: goodLocations.remove → BREAK → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Duplicate FS created for {}; discarding {} [WARN] StorageLocation {location} appears to be degraded. [WARN] StorageLocation {location} detected as failed."
  },
  "ba141f60_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem:getLocal → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_FALSE:fs != null → CALL:createFileSystem → LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT → CALL:org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker:schedule → FOREACH_EXIT → IF_FALSE: maxVolumeFailuresTolerated >= dataDirs.size() → FOREACH: futures.entrySet() → TRY → SWITCH: result → CASE: [FAILED] → LOG: org.slf4j.Logger:warn(\"StorageLocation {} detected as failed.\", location) → CALL: failedLocations.add → CALL: goodLocations.remove → BREAK → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Duplicate FS created for {}; discarding {} [WARN] StorageLocation {location} detected as failed."
  },
  "4b07fef7_1": {
    "exec_flow": "Parent.ENTRY → LOGGER.warn(\"\\\"local\\\" is a deprecated filesystem name. Use \\\"file:///\\\" instead.\") Parent.ENTRY → LOGGER.warn(\"\\\"\" + name + \"\\\" is a deprecated filesystem name. Use \\\"hdfs://\\\" + name + \\\"/\\\" instead.\")",
    "log": "<log>[WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead.</log> <log>[WARN] \"<name>\" is a deprecated filesystem name. Use \"hdfs://<name>/\" instead.</log>"
  },
  "4b07fef7_2": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "799b974c_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE:props != null → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → CALL:addTags → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → ENTRY→CALL:checkAcls→CALL:checkRMStatus→TRY→CALL:writeLock.lock→TRY→ENTRY→IF_FALSE:!nodeLabelsEnabled→IF_FALSE:null == labels || labels.isEmpty()→CALL:normalizeNodeLabels→CALL:checkExclusivityMatch→FOREACH:labels→CALL:NodeLabelUtil.checkAndThrowLabelName→FOREACH_EXIT→FOREACH:labels→CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:<init>(org.apache.hadoop.yarn.api.records.NodeLabel)→FOREACH_EXIT→IF_TRUE:null != dispatcher && !newLabels.isEmpty()→CALL:dispatcher.getEventHandler().handle→CALL:org.apache.hadoop.yarn.event.EventHandler:handle→LOG:LOG.INFO:Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ]→CALL:writeLock.unlock→EXIT→CALL:rm.getRMContext().getNodeLabelManager().addToClusterNodeLabels→IF_TRUE:LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ] [INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "799b974c_2": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→ENTRY→IF_FALSE:!nodeLabelsEnabled→IF_FALSE:null == labels || labels.isEmpty()→CALL:normalizeNodeLabels→CALL:checkExclusivityMatch→FOREACH:labels→CALL:NodeLabelUtil.checkAndThrowLabelName→FOREACH_EXIT→FOREACH:labels→CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:<init>(org.apache.hadoop.yarn.api.records.NodeLabel)→FOREACH_EXIT→IF_TRUE:null != dispatcher && !newLabels.isEmpty()→CALL:dispatcher.getEventHandler().handle→CALL:org.apache.hadoop.yarn.event.EventHandler:handle→LOG:LOG.INFO:Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ]→CALL:writeLock.unlock→EXIT",
    "log": "[INFO] Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ]"
  },
  "799b974c_3": {
    "exec_flow": "ENTRY→LOG:LOG.WARN:Exception + msg, exception→CALL:org.slf4j.Logger:warn→IF_TRUE:LOG.isWarnEnabled()→CALL:org.slf4j.Logger:isWarnEnabled()→CALL:RMAuditLogger.logFailure→LOG:LOG.WARN:createFailureLog→CALL:org.slf4j.Logger:warn(java.lang.String)→CALL:RPCUtil.getRemoteException→RETURN→EXIT",
    "log": "[WARN] Exception + msg [WARN] createFailureLog"
  },
  "799b974c_4": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→ENTRY→IF_TRUE: !nodeLabelsEnabled→LOG:LOG.ERROR:NODE_LABELS_NOT_ENABLED_ERR→THROW:new IOException(NODE_LABELS_NOT_ENABLED_ERR)→EXIT→CALL:writeLock.unlock→EXIT",
    "log": "[ERROR] NODE_LABELS_NOT_ENABLED_ERR"
  },
  "799b974c_5": {
    "exec_flow": "ENTRY→IF_TRUE:!isRMActive()→CALL:RMAuditLogger.logFailure→IF_TRUE:LOG.isWarnEnabled()→CALL:org.slf4j.Logger:isWarnEnabled()→LOG:LOG.WARN:createFailureLog→CALL:org.slf4j.Logger:warn(java.lang.String)→CALL:throwStandbyException→EXIT",
    "log": "[DEBUG] ResourceManager is not active. Can not [message] [WARN] createFailureLog"
  },
  "799b974c_6": {
    "exec_flow": "ENTRY→CALL:verifyAdminAccess→TRY→CALL:getCurrentUser→IF_TRUE:!authorizer.isAdmin(user)→LOG:LOG.WARN:User + user.getShortUserName() + doesn’t have permission + to call ' + method + '→CALL:RMAuditLogger.logFailure→THROW:new AccessControlException(\"User \" + user.getShortUserName() + \" doesn’t have permission\" + \" to call ' + method + '\")→RETURN→EXIT",
    "log": "[WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'"
  },
  "799b974c_7": {
    "exec_flow": "ENTRY→CALL:verifyAdminAccess→TRY→CALL:getCurrentUser→IF_FALSE:!authorizer.isAdmin(user)→IF_TRUE:LOG.isTraceEnabled()→LOG:LOG.TRACE:method + invoked by user + user.getShortUserName()→RETURN→EXIT",
    "log": "[TRACE] ${method} invoked by user ${user.getShortUserName()}"
  },
  "799b974c_8": {
    "exec_flow": "ENTRY→CALL:verifyAdminAccess→TRY→CALL:getCurrentUser→CATCH→LOG:LOG.WARN:Couldn’t get current user→CALL:RMAuditLogger.logFailure→THROW:IOException→RETURN→EXIT",
    "log": "[WARN] Couldn't get current user"
  },
  "7e9c6100_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] Interrupted [DEBUG] Displaying error: message with object1, object2"
  },
  "7e9c6100_2": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] IOException: errorMessage [DEBUG] Displaying error: message with object1, object2"
  },
  "7e9c6100_3": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[ERROR] Interrupted [DEBUG] Displaying error: message with object1, object2"
  },
  "7e9c6100_4": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[ERROR] IOException: errorMessage [DEBUG] Displaying error: message with object1, object2"
  },
  "262b6352_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [WARN] Unexpected SecurityException in Configuration [INFO] message"
  },
  "262b6352_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[INFO] message"
  },
  "34fad456_1": {
    "exec_flow": "ENTRY → TRY → CALL:getReturnProtoType → TRY → CALL:getValue → CALL:getDefaultInstanceForType → CALL:getDefaultInstanceForType → CALL:isTraceEnabled → IF_TRUE:LOG.isTraceEnabled() → CALL:trace → LOG:LOG.TRACE:Thread.currentThread().getId() + \": Response <- \" + remoteId + \": \" + method.getName() + \" {\" + TextFormat.shortDebugString(returnMessage) + \"} → RETURN → EXIT",
    "log": "[TRACE] Thread.currentThread().getId() + \": Response <- \" + remoteId + \": \" + method.getName() + \" {\" + TextFormat.shortDebugString(returnMessage) + \"}\""
  },
  "06a6762f_1": {
    "exec_flow": "<flow> <step>Parent.ENTRY</step> <step>VIRTUAL_CALL</step> <step>ENTRY</step> <step>IF_TRUE: rpcMonitor != null</step> <step>CALL: rpcMonitor.startOp</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>CALL: org.slf4j.Logger:isDebugEnabled()</step> <step>LOG: LOG.DEBUG: Proxying operation: {}, methodName</step> <step>CALL: opCategory.set</step> <step>IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ</step> <step>RETURN</step> <step>EXIT</step> </flow>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "06a6762f_2": {
    "exec_flow": "<flow> <step>Parent.ENTRY</step> <step>VIRTUAL_CALL</step> <step>ENTRY</step> <step>IF_TRUE: rpcMonitor != null</step> <step>CALL: rpcMonitor.startOp</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>CALL: org.slf4j.Logger:isDebugEnabled()</step> <step>LOG: LOG.DEBUG: Proxying operation: {}, methodName</step> <step>CALL: opCategory.set</step> <step>IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ</step> <step>CALL: checkSafeMode</step> <step>EXIT</step> </flow>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "06a6762f_3": {
    "exec_flow": "<flow> <step>Parent.ENTRY</step> <step>VIRTUAL_CALL</step> <step>ENTRY</step> <step>IF_FALSE: rpcMonitor != null</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>CALL: org.slf4j.Logger:isDebugEnabled()</step> <step>LOG: LOG.DEBUG: Proxying operation: {}, methodName</step> <step>CALL: opCategory.set</step> <step>IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ</step> <step>RETURN</step> <step>EXIT</step> </flow>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "06a6762f_4": {
    "exec_flow": "<flow> <step>Parent.ENTRY</step> <step>VIRTUAL_CALL</step> <step>ENTRY</step> <step>IF_FALSE: rpcMonitor != null</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>CALL: org.slf4j.Logger:isDebugEnabled()</step> <step>LOG: LOG.DEBUG: Proxying operation: {}, methodName</step> <step>CALL: opCategory.set</step> <step>IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ</step> <step>CALL: checkSafeMode</step> <step>EXIT</step> </flow>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "fc07f888_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → FOREACH_EXIT → RETURN → EXIT → S3AFileSystem:ENTRY → [CHECK] → [SYNCHRONIZED_BLOCK] → [ASSIGN] → RETURN → S3AFileSystem:EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "eec053ae_1": {
    "exec_flow": "ENTRY → CALL:getInterceptorChain → CALL:getRootInterceptor → CALL:refreshQueues → RETURN → EXIT",
    "log": "<log>[INFO] createSuccessLog(user, operation, target, null, null, null, null)</log> <log>[WARN] Exception + msg</log> <log>[WARN] createFailureLog</log> <log>[INFO] message</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[INFO] Cleaning up resources</log> <log>[INFO] Request to start an already existing user: {} was received, so ignoring.</log>"
  },
  "eec053ae_2": {
    "exec_flow": "ENTRY → CALL:getInterceptorChain → IF_FALSE:chain!=null && chain.getRootInterceptor()!=null → CALL:initializePipeline → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating new Groups object</log> <log>[INFO] Initializing request processing pipeline for user: {}</log>"
  },
  "eec053ae_3": {
    "exec_flow": "ENTRY→CALL:rm.getRMContext().getScheduler().reinitialize→IF_TRUE:rSystem != null→CALL:rSystem.reinitialize→ENTRY→IF_TRUE:LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→EXIT",
    "log": "<log>[WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'</log>"
  },
  "ca10b22d_1": {
    "exec_flow": "AbfsPerfInfo.ENTRY → [VIRTUAL_CALL] → AbfsPerfTracker.ENTRY → IF_FALSE:!enabled → IF_TRUE:isValidInstant(perfInfo.getAggregateStart())&&perfInfo.getAggregateCount()>0 → CALL:recordClientLatency → CALL:isValidInstant → CALL:isValidInstant → CALL:between → CALL:isValidInstant → CALL:isValidInstant → CALL:between → CALL:String.format → CALL:offerToQueue → IF_TRUE:LOG.isDebugEnabled() → CALL:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Queued latency info [{} ms]: {}, elapsed, latencyDetails → EXIT",
    "log": "[DEBUG] Queued latency info [{} ms]: {}"
  },
  "042bf368_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE:!isCentralizedNodeLabelConfiguration → LOG:LOG.ERROR:msg → THROW:new IOException(msg) → EXIT",
    "log": "Error when invoke method=%s because centralized node label configuration is not enabled."
  },
  "042bf368_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE:!nodeLabelsEnabled → LOG:LOG.ERROR:NODE_LABELS_NOT_ENABLED_ERR → THROW:new IOException(NODE_LABELS_NOT_ENABLED_ERR) → EXIT",
    "log": "[ERROR] NODE_LABELS_NOT_ENABLED_ERR"
  },
  "042bf368_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL:writeLock.lock → TRY → IF_TRUE:effectiveModifiedLabelMappings.isEmpty() → LOG:LOG.INFO:No Modified Node label Mapping to replace → RETURN → CALL:writeLock.unlock → EXIT",
    "log": "[INFO] No Modified Node label Mapping to replace"
  },
  "6af2ed98_1": {
    "exec_flow": "ENTRY → CALL:rpcServer.checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → CALL:invokeAtAvailableNs → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "69699aae_1": {
    "exec_flow": "ENTRY→FOREACH:args→TRY→CALL:processArgument→CATCH(IOException)→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "69699aae_2": {
    "exec_flow": "ENTRY→FOREACH:args→TRY→CALL:processArgument→CATCH(IOException)→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "00f0a6f5_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:org.apache.hadoop.yarn.webapp.view.HtmlBlock:render → CALL:org.slf4j.Logger:debug → EXIT",
    "log": "[DEBUG] Html block rendering initiated"
  },
  "945906f2_1": {
    "exec_flow": "ENTRY → CALL:normalizeHostNames → IF_FALSE:names.isEmpty() → CALL:getUncachedHosts → ENTRY → IF_NULL: map → IF_TRUE:StringUtils.isBlank(filename) → LOG:WARN NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured. → RETURN → CALL:org.apache.hadoop.net.DNSToSwitchMapping:resolve → IF_FALSE:scriptName == null → CALL:runResolveCommand → IF_TRUE:output != null → WHILE:allSwitchInfo.hasMoreTokens() → IF_TRUE:m.size() != names.size() → LOG:ERROR Script {scriptName} returned {m.size()} values when {names.size()} were expected. → RETURN → EXIT",
    "log": "[WARN] NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured. [ERROR] Script {scriptName} returned {m.size()} values when {names.size()} were expected."
  },
  "945906f2_2": {
    "exec_flow": "ENTRY → IF_TRUE: map == null → IF_TRUE: StringUtils.isBlank(filename) → LOG:WARN NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured. → RETURN → IF_TRUE: map == null → LOG:WARN Failed to read topology table. DEFAULT_RACK will be used for all nodes. → NEW:HashMap<String, String> → FOREACH:names → FOREACH_EXIT → RETURN → EXIT",
    "log": "[WARN] NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured. [WARN] Failed to read topology table. DEFAULT_RACK will be used for all nodes."
  },
  "945906f2_3": {
    "exec_flow": "ENTRY → IF_FALSE: map == null → IF_FALSE: StringUtils.isBlank(filename) → TRY → IF_TRUE: line does not have two columns → LOG:WARN Line does not have two columns. Ignoring. {line} → WHILE_EXIT → RETURN → FOREACH:names → FOREACH_EXIT → RETURN → EXIT",
    "log": "[WARN] Line does not have two columns. Ignoring. {line}"
  },
  "945906f2_4": {
    "exec_flow": "ENTRY → IF_FALSE: map == null → IF_FALSE: StringUtils.isBlank(filename) → TRY → EXCEPTION_OCCURRED → IF_TRUE: Exception → LOG:WARN {filename} cannot be read. → RETURN → FOREACH:names → FOREACH_EXIT → RETURN → EXIT",
    "log": "[WARN] {filename} cannot be read."
  },
  "44b48834_1": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Processing {event.getContainerId()} of type {event.getType()} → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → CATCH: InvalidStateTransitionException → LOG: LOG.ERROR: Can't handle this event at current state → CALL: onInvalidStateTransition → ENTRY → CALL: org.slf4j.Logger:error(java.lang.String) → EXIT → CALL: writeLock.unlock → EXIT",
    "log": "[DEBUG] Processing {event.getContainerId()} of type {event.getType()} [ERROR] Invalid event {rmContainerEventType} on container {this.getContainerId()}"
  },
  "44b48834_2": {
    "exec_flow": "ENTRY → TRY → CALL: rmContainer.handle → IF_TRUE: reservedContainers == null → NEW: HashMap<NodeId, RMContainer> → CALL: this.reservedContainers.put → CALL: reservedContainers.put → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "[DEBUG] Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource"
  },
  "44b48834_3": {
    "exec_flow": "ENTRY → TRY → CALL: rmContainer.handle → IF_FALSE: reservedContainers == null → CALL: reservedContainers.put → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "[DEBUG] Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource"
  },
  "2ff9e6cf_1": {
    "exec_flow": "<![CDATA[ ENTRY→IF_TRUE: conf.getBoolean(DFSConfigKeys.DFS_MOVER_KEYTAB_ENABLED_KEY, DFSConfigKeys.DFS_MOVER_KEYTAB_ENABLED_DEFAULT)→CALL:Configuration.getBoolean→LOG:LOG.INFO: Keytab is configured, will login using keytab.→CALL:UserGroupInformation.setConfiguration→CALL:Configuration.get→CALL:NetUtils.createSocketAddr→TRY → CALL:org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → IF_TRUE: staticHost != null → CALL: getByAddress → CALL: getAddress → CALL: getAddress → IF_TRUE: elapsedMs >= slowLookupThresholdMs → CALL: org.slf4j.Logger:warn(java.lang.String) → NEW: InetSocketAddress → RETURN → EXIT→CALL:SecurityUtil.login→EXIT ]]>",
    "log": "<log_entry> <level>INFO</level> <template>Keytab is configured, will login using keytab.</template> </log_entry> <log_entry> <level>WARN</level> <template>Slow name lookup for + hostname + . Took + elapsedMs + ms.</template> </log_entry>"
  },
  "2ff9e6cf_2": {
    "exec_flow": "<![CDATA[ ENTRY→IF_TRUE: conf.getBoolean(DFSConfigKeys.DFS_MOVER_KEYTAB_ENABLED_KEY, DFSConfigKeys.DFS_MOVER_KEYTAB_ENABLED_DEFAULT)→CALL:Configuration.getBoolean→LOG:LOG.INFO: Keytab is configured, will login using keytab.→CALL:UserGroupInformation.setConfiguration→CALL:Configuration.get→CALL:NetUtils.createSocketAddr→TRY → CALL:org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → IF_TRUE: elapsedMs < slowLookupThresholdMs → IF_TRUE: LOG.isTraceEnabled() → CALL: org.slf4j.Logger:trace(java.lang.String) → IF_TRUE: staticHost != null → CALL: getByAddress → CALL: getAddress → CALL: getAddress → NEW: InetSocketAddress → RETURN → EXIT→CALL:SecurityUtil.login→EXIT ]]>",
    "log": "<log_entry> <level>INFO</level> <template>Keytab is configured, will login using keytab.</template> </log_entry> <log_entry> <level>TRACE</level> <template>Name lookup for + hostname + took + elapsedMs + ms.</template> </log_entry>"
  },
  "70a9b48f_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "127253d8_1": {
    "exec_flow": "<sequence> ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → CALL: substituteVars → FOREACH_EXIT → RETURN → EXIT → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for {item} → CALL: handleDeprecation → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG: Unexpected SecurityException in Configuration → CATCH: SecurityException → EXIT </sequence>",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "127253d8_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "127253d8_3": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → TRY → CALL: retrieveMetadata → IF_TRUE: meta != null → IF_TRUE: meta.isDirectory() → LOG: [DEBUG] Path {} is a folder., f.toString() → CALL: conditionalRedoFolderRename → RETURN → EXIT",
    "log": "<log>[DEBUG] Path {} is a folder., f.toString()</log>"
  },
  "127253d8_4": {
    "exec_flow": "ENTRY → TRY → LOG: [WARN] Unexpected SecurityException in Configuration → CATCH: SecurityException → CALL: connect → METHOD: getFileStatus → RETURN: FileStatus → LOG: [DEBUG] Handling deprecation for keys → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "0ec12555_1": {
    "exec_flow": "<entry> <name>org.apache.hadoop.fs.RemoteIterator</name> <method>next</method> </entry> <virtual_call> ENTRY → IF_TRUE: reader.next(NullWritable.get(), readBack) → IF_TRUE: val instanceof Configurable → CALL: ((Configurable) val).setConf → CALL: org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration) → CALL: getStrings → IF_FALSE → CALL: Iterators.cycle → CALL: next → CALL: getBoolean → IF_TRUE → CALL: loadSslConf → CALL: initializeBindUsers → CALL: getTrimmed → LOG:DEBUG [Usersearch baseDN: {userbaseDN}] → CALL: getTrimmed → LOG:DEBUG [Groupsearch baseDN: {groupbaseDN}] → CALL: get → CALL: get → CALL: get → CALL: get → CALL: contains → CALL: contains → CALL: get → CALL: isEmpty → CALL: get → CALL: get → CALL: getInt → CALL: setTimeLimit → CALL: isEmpty → CALL: getClass → IF_TRUE → CALL: getName → IF_FALSE → ASSIGN: ldapCtxFactoryClassName → CALL:seekToCurrentValue → IF_TRUE: !blockCompressed → CALL: deserializeValue → IF_TRUE: valIn.read() > 0 → LOG.INFO [available bytes: + valIn.available()] → THROW: new IOException(val + \" read \" + (valBuffer.getPosition() - keyLength) + \" bytes, should read \" + (valBuffer.getLength() - keyLength)) → EXIT </virtual_call>",
    "log": "<log>[DEBUG] Usersearch baseDN: {userbaseDN}</log> <log>[DEBUG] Groupsearch baseDN: {groupbaseDN}</log> <log>[INFO] available bytes: + valIn.available()</log>"
  },
  "0ec12555_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.RemoteIterator:next → RETURN → EXIT",
    "log": "<log>[DEBUG] Next element retrieved from RemoteIterator</log>"
  },
  "0ec12555_3": {
    "exec_flow": "ENTRY → TRY → LOG: DEBUG [Loading service definition from FS: {}] → EXCEPTION: debug → CATCH: IOException e → LOG: INFO [Error while loading service definition from FS: {}] → RETURN → EXIT",
    "log": "<log>[DEBUG] Loading service definition from FS: {}</log> <log>[INFO] Error while loading service definition from FS: {}</log>"
  },
  "4afd4735_1": {
    "exec_flow": "ENTRY→LOG:DEBUG List status for path: {path}→CALL:getFileStatus→IF_TRUE:fileStatus.isDirectory()→IF_TRUE:LOG.isDebugEnabled()→LOG:DEBUG listStatus: doing listObjects for directory {key}→WHILE:true→WHILE_COND:true→WHILE_EXIT→CALL:toArray→CALL:size→CALL:size→RETURN→EXIT",
    "log": "[DEBUG] List status for path: {path} [DEBUG] listStatus: doing listObjects for directory {key}"
  },
  "4afd4735_2": {
    "exec_flow": "ENTRY→LOG:DEBUG AzureBlobFileSystem.listStatus path: {f.toString()}→CALL:statIncrement→CALL:makeQualified→TRY→CALL:abfsStore.listStatus→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}"
  },
  "4afd4735_3": {
    "exec_flow": "ENTRY→LOG:DEBUG AzureBlobFileSystem.listStatus path: {f.toString()}→CALL:statIncrement→CALL:makeQualified→TRY→CALL:abfsStore.listStatus→EXCEPTION:AzureBlobFileSystemException→CALL:checkException→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}"
  },
  "4afd4735_4": {
    "exec_flow": "ENTRY→IF_TRUE:key.length() > 0→CALL:getFileStatus→IF_TRUE:fileStatus.isFile()→LOG:[DEBUG] Path: [{path}] is a file. COS key: [{key}]→RETURN→EXIT",
    "log": "[DEBUG] Path: [{path}] is a file. COS key: [{key}]"
  },
  "4afd4735_5": {
    "exec_flow": "ENTRY→IF_TRUE:key.length() > 0→CALL:getFileStatus→IF_FALSE:fileStatus.isFile()→LOG:[DEBUG] Path: [{path}] is a dir. COS key: [{key}]→CALL:newDirectory→RETURN→EXIT",
    "log": "[DEBUG] Path: [{path}] is a dir. COS key: [{key}]"
  },
  "1e6d5121_1": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds DO_EXIT → RETURN → EXIT → LOG: [WARN] Unexpected SecurityException in Configuration → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → LOG: [DEBUG] Handling deprecation for all properties in config... LOG: [DEBUG] Making dir: [{}] in COS LOG: [DEBUG] Path: [{}] is a file. COS key: [{}] LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}] LOG: [DEBUG] List COS key: [{}] to check the existence of the path. LOG: [DEBUG] The Path: [{}] does not exist. EXIT → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → LOG: [DEBUG] Handling deprecation for all properties in config... CALL: getProps → CALL: addAll → FOREACH: keys → LOG: [DEBUG] Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → EXIT IF_FALSE: !isInitialized() → VIRTUAL_CALL → org.apache.hadoop.mapred.ResourceMgrDelegate:getStagingAreaDir()",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Making dir: [{}] in COS</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] List COS key: [{}] to check the existence of the path.</log_entry> <log_entry>[DEBUG] The Path: [{}] does not exist.</log_entry> <log_entry>org.apache.hadoop.mapred.ResourceMgrDelegate:getStagingAreaDir: dir={path}</log_entry>"
  },
  "1e6d5121_2": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → VIRTUAL_CALL → CALL: findSubVariable → CALL: getProperty → CALL: getenv → CALL: getRaw → LOG: [WARN] Unexpected SecurityException in Configuration → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: [DEBUG] Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → LOG: [DEBUG] Handling deprecation for all properties in config... EXIT",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "deb932e5_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.fs.azure.SimpleKeyProvider:getStorageAccountKey → CATCH:IllegalAccessException | InvalidConfigurationValueException e → LOG:[DEBUG] Failure to retrieve storage account key for accountName, e → THROW:new KeyProviderException(\"Failure to initialize configuration for \" + accountName + \" key =\\\"\" + key + \"\\\" + \": \" + e, e) → NEW: AbfsConfiguration → IF_FALSE: command == null → TRY → CALL: execCommand → [EXCEPTION: IOException] → THROW: new KeyProviderException(ex) → EXIT",
    "log": "<log_entry>[DEBUG] Failure to retrieve storage account key for accountName, e</log_entry>"
  },
  "deb932e5_2": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.fs.azure.SimpleKeyProvider:getStorageAccountKey → CATCH:IOException ioe → LOG:[WARN] Unable to get key for accountName from credential providers. ioe, ioe → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unable to get key for accountName from credential providers. ioe, ioe</log_entry>"
  },
  "69251edd_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)→CALL: init→TRY→CALL: getTimelineReaderManager→LOG:Unexpected SecurityException in Configuration→CALL: getEntity→LOG:Handling deprecation for all properties in config...→CALL: createTimelineReaderContext→CALL: createTimelineDataToRetrieve→CALL: checkAccessForGenericEntity→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL: METRICS.addGetEntitiesLatency→LOG: LOG.INFO: Processed URL + url + (Took + latency + ms.)→HANDLE_EXCEPTION(CONSOLIDATED)→EXIT",
    "log": "<log_entry>[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] Processed URL + url + (Took + latency + ms.)</log_entry>"
  },
  "efd23439_1": {
    "exec_flow": "<sequence> ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT </sequence>",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "efd23439_2": {
    "exec_flow": "<optimized_flow> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT </optimized_flow>",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "efd23439_3": {
    "exec_flow": "<optimized_flow> ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT </optimized_flow>",
    "log": "<log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "efd23439_4": {
    "exec_flow": "<seq>ENTRY→CALL:getStrings→IF_FALSE→CALL:Iterators.cycle→CALL:next→CALL:getBoolean→IF_TRUE→CALL:loadSslConf→CALL:initializeBindUsers→CALL:getTrimmed→LOG:debug→CALL:getTrimmed→LOG:debug→CALL:get→CALL:get→CALL:get→CALL:get→CALL:contains→CALL:contains→CALL:get→CALL:isEmpty→CALL:get→CALL:get→CALL:getInt→CALL:setTimeLimit→CALL:isEmpty→CALL:getClass→IF_TRUE→CALL:getName→IF_FALSE→ASSIGN:ldapCtxFactoryClassName</seq>",
    "log": "<log> [DEBUG] Usersearch baseDN: {userbaseDN} [DEBUG] Groupsearch baseDN: {groupbaseDN} </log>"
  },
  "13717171_1": {
    "exec_flow": "ENTRY→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "13717171_2": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "13717171_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] message"
  },
  "8cdbb54a_1": {
    "exec_flow": "ENTRY→IF_TRUE: zst == null→NEW: ZoneSubmissionTracker→CALL: setSubmissionDone→CALL: batchService.submit→CALL: org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:<init>→CALL: addTask→SYNC: this→CALL: put→EXIT",
    "log": "[INFO] Processing batched re-encryption for zone {}, batch size {}, start: {}, zoneNodeId, batch.size(), batch.getFirstFilePath()"
  },
  "8cdbb54a_2": {
    "exec_flow": "ENTRY→IF_FALSE: zst == null→CALL: setSubmissionDone→CALL: batchService.submit→CALL: org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:<init>→CALL: addTask→SYNC: this→CALL: put→EXIT",
    "log": "[WARN] Failed to re-encrypt one batch of {} edeks, start: {}"
  },
  "8cdbb54a_3": {
    "exec_flow": "ENTRY→IF_FALSE: zst == null→CALL: setSubmissionDone→CALL: batchService.submit→CALL: org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:<init>→CALL: addTask→SYNC: this→CALL: put→EXIT",
    "log": "[INFO] Completed re-encrypting one batch of {} edeks from KMS, time consumed: {}, start: {}, result, batch.size(), kmsSW.stop(), batch.getFirstFilePath()"
  },
  "c193b8f4_1": {
    "exec_flow": "ENTRY→CALL:getRegistrySecurity→CALL:digest→CALL:toDigestId→CALL:ALL→CALL:addDigestACL→IF_TRUE: secureRegistry→IF_TRUE: LOG.isDebugEnabled→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: LOG.DEBUG: Added ACL {}, aclToString(acl)→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→CALL: digestACLs.add→RETURN→EXIT",
    "log": "[DEBUG] Added ACL {}, aclToString(acl)"
  },
  "c193b8f4_2": {
    "exec_flow": "ENTRY→CALL:getRegistrySecurity→CALL:digest→CALL:toDigestId→CALL:ALL→CALL:addDigestACL→IF_FALSE: secureRegistry→IF_TRUE: LOG.isDebugEnabled→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: LOG.DEBUG: Ignoring added ACL - registry is insecure{}, aclToString(acl)→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→RETURN→EXIT",
    "log": "[DEBUG] Ignoring added ACL - registry is insecure{}, aclToString(acl)"
  },
  "eaa2ebc4_1": {
    "exec_flow": "<!-- Optimized execution flow structure from both parent and child --> ENTRY→CALL:stop→ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock →IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e →LOG: LOG.debug(\"noteFailure\", e)→SYNC: this →CHECK: if (failureCause == null) →ASSIGN: failureCause = e →ASSIGN: failureState = getServiceState() →LOG: LOG.info(\"Service {} failed in state {}\", getName(), failureState, e) →THROW: ServiceStateException.convert(e)→CALL: notifyListeners →CATCH: Throwable e →LOG: LOG.WARN: Exception while notifying listeners of {}, this, e →EXIT",
    "log": "<!-- Merged log sequence from parent and child --> LOG.debug(\"noteFailure\", exception) LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) [WARN] Exception while notifying listeners of {}"
  },
  "eaa2ebc4_2": {
    "exec_flow": "ENTRY→CALL:stop→ENTRY→IF_TRUE: oldState != newState →CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) →CALL: recordLifecycleEvent→RETURN→EXIT",
    "log": "LOG.debug(\"Service: {} entered state {}\", getName(), getServiceState())"
  },
  "725a8ee6_1": {
    "exec_flow": "ENTRY→TRY→CALL:ensureInitialized→IF_FALSE:subject==null||subject.getPrincipals(User.class).isEmpty()→NEW:UserGroupInformation→RETURN→TRY→CALL:getCurrentUser→CATCH→LOG:LOG.WARN:Couldn’t get current user→CALL:RMAuditLogger.logFailure→THROW:IOException→EXIT",
    "log": "[WARN] Couldn't get current user"
  },
  "eeb39bae_1": {
    "exec_flow": "ENTRY→FOREACH:configuredNodelabels→CALL:getConfiguredNodeLabels→LOG:LOG.DEBUG:capacityConfigType is '{}' for queue {},capacityConfigType,getQueuePath()→IF_FALSE:this.capacityConfigType.equals(CapacityConfigType.NONE)→CALL:validateAbsoluteVsPercentageCapacityConfig→IF_FALSE:!maxResource.equals(Resources.none())&&Resources.greaterThan(resourceCalculator,clusterResource,minResource,maxResource)→IF_TRUE:parent!=null→IF_TRUE:Resources.greaterThan(resourceCalculator,clusterResource,parentMaxRes,Resources.none())→IF_TRUE:Resources.greaterThan(resourceCalculator,clusterResource,maxResource,parentMaxRes)→THROW:new IllegalArgumentException(\"Max resource configuration \"+maxResource+\" is greater than parents max value:\"+parentMaxRes+\" in queue:\"+getQueuePath())→EXIT",
    "log": "[DEBUG] capacityConfigType is '{}' for queue {} [DEBUG] capacityConfigType is updated as '{}' for queue {} [DEBUG] Updating absolute resource configuration for queue:{} as minResource={} and maxResource={}"
  },
  "5f19ebe1_1": {
    "exec_flow": "ENTRY → CALL: decodeIdentifier → IF_FALSE: tokenKindMap == null → CALL: get → IF_TRUE: cls == null → LOG: LOG.DEBUG: Cannot find class for token kind {}, kind → RETURN → EXIT",
    "log": "[DEBUG] Cannot find class for token kind {}"
  },
  "c49d760d_1": {
    "exec_flow": "ENTRY → TRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.slf4j.Logger:debug → NEW:FSDataOutputStream → NEW:CosNOutputStream → CALL:getConf → RETURN → EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS."
  },
  "c49d760d_2": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_FALSE:!overwrite → CALL:org.slf4j.Logger:debug → NEW:FSDataOutputStream → NEW:CosNOutputStream → CALL:getConf → RETURN → EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS. [DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "c49d760d_3": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.apache.hadoop.fs.FileSystem:create → RETURN → EXIT",
    "log": "[ERROR] Failed to create file: {fileToCreate} at fallback : {linkedFallbackFs.getUri()}"
  },
  "c49d760d_4": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:status.isDirectory() → IF_FALSE:!overwrite → LOG:debug → CATCH:FileNotFoundException → CALL:getMultipartSizeProperty → NEW:FSDataOutputStream → NEW:AliyunOSSBlockOutputStream → CALL:getConf → NEW:SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "[DEBUG] Overwriting file {} [WARN] {} must be at least 100 KB; configured value is {}"
  },
  "c49d760d_5": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:status.isDirectory() → IF_FALSE:!overwrite → LOG:debug → CALL:getMultipartSizeProperty → NEW:FSDataOutputStream → NEW:AliyunOSSBlockOutputStream → CALL:getConf → NEW:SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "[DEBUG] Overwriting file {}"
  },
  "09eb6a21_1": {
    "exec_flow": "ENTRY→CALL: incrementPutStartStatistics→TRY→CALL: org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier→CALL: incrementPutCompletedStatistics→RETURN→EXIT→ENTRY→CALL:Preconditions.checkArgument→DO_WHILE→TRY→IF_TRUE:retryCount > 0→LOG:LOG.DEBUG:retry #{}, retryCount→CALL:apply→RETURN→EXIT",
    "log": "[DEBUG] PUT start {} bytes [INFO] Incrementing put start statistics [DEBUG] Tracking duration of supplier [DEBUG] PUT completed success=true; {} bytes [INFO] Incrementing put completed statistics [DEBUG] retry #{}"
  },
  "09eb6a21_2": {
    "exec_flow": "ENTRY→CALL: incrementPutStartStatistics→TRY→CALL: org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier→EXCEPTION: incrementPutCompletedStatistics→CATCH: AmazonClientException e→CALL: incrementPutCompletedStatistics→THROW: e→EXIT→ENTRY→CALL:Preconditions.checkArgument→DO_WHILE→TRY→CATCH:InterruptedException→LOG:LOG.WARN:exception in retry processing→EXIT",
    "log": "[DEBUG] PUT start {} bytes [INFO] Incrementing put start statistics [DEBUG] Tracking duration of supplier [DEBUG] PUT completed success=false; {} bytes [ERROR] Incrementing put completed statistics with exception [WARN] {}: exception in retry processing"
  },
  "09eb6a21_3": {
    "exec_flow": "ENTRY→CALL:execute→CALL:executeOnlyOnce→CALL:org.apache.hadoop.fs.s3a.impl.MkdirOperation:execute()→TRY→CALL:invokeTrackingDuration→CALL:drainOrAbortHttpStream→RETURN→EXIT",
    "log": "[DEBUG] drain or abort reason {} remaining={} abort={} [DEBUG] drained fewer bytes than expected; {} remaining [DEBUG] Closing stream"
  },
  "afa8d4a4_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner:setConf(org.apache.hadoop.conf.Configuration)→CALL:org.apache.hadoop.hdfs.server.namenode.SingleUGIResolver:setConf(org.apache.hadoop.conf.Configuration)→CALL:org.apache.hadoop.mapreduce.v2.hs.webapp.MapReduceTrackingUriPlugin:setConf(org.apache.hadoop.conf.Configuration)→CALL:org.apache.hadoop.hdfs.qjournal.server.JournalNode:setConf(org.apache.hadoop.conf.Configuration)→EXIT",
    "log": "[WARN] Bad checksum at {position}. Skipping entries. [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [WARN] Serialization class not found: , e [WARN] Error caching groups [INFO] message"
  },
  "55275570_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.ConfigurableResource:getResource(org.apache.hadoop.yarn.api.records.Resource)→CALL:org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)→IF_TRUE:!Resources.equals(maxResource, result)→LOG:LOG.WARN:String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)→RETURN→EXIT",
    "log": "[WARN] String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)"
  },
  "55275570_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)→TRY→LOG:LOG.WARN:Resource is missing: + ye.getMessage()→CONTINUE→RETURN→EXIT",
    "log": "[WARN] Resource is missing: + ye.getMessage()"
  },
  "b0f9eed0_1": {
    "exec_flow": "ENTRY→CALL: readLock.lock→TRY→CALL: getApplications→FOREACH→CALL: org.apache.hadoop.yarn.api.records.Resource:newInstance→CALL: getUser→TRY→CALL: writeLock.lock→CALL: getResourceLimitForActiveUsers→CALL: get→IF_FALSE: isRecomputeNeeded(schedulingMode, nodePartition, true)→CALL: writeLock.unlock→CALL: Resources.multiplyAndNormalizeDown→IF_TRUE: user != null→CALL: setUserResourceLimit→LOG: [DEBUG] userLimit is fetched. userLimit={}, userSpecificUserLimit={}, schedulingMode={}, partition={}→FOREACH→CALL: org.apache.hadoop.yarn.util.resource.Resources:subtract→CALL:clone→CALL:subtractFrom→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→LOG: [WARN] Resource is missing: {exception_message}→FOR_EXIT→CALL: org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax→CALL: org.apache.hadoop.yarn.util.resource.Resources:subtract→CALL: org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax→CALL: org.apache.hadoop.yarn.util.resource.Resources:componentwiseMin→CALL: org.apache.hadoop.yarn.util.resource.Resources:addTo→CALL: org.apache.hadoop.yarn.util.resource.Resources:subtractFrom→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] userLimit is fetched. userLimit={}, userSpecificUserLimit={}, schedulingMode={}, partition={} [WARN] Resource is missing: {exception_message}"
  },
  "b0f9eed0_2": {
    "exec_flow": "ENTRY→CALL: readLock.lock→TRY→CALL: getApplications→FOREACH→CALL: getUser→CALL: Resources.subtract→IF: deductReservedFromPending→CALL: Resources.subtract→CALL: Resources.componentwiseMax→CALL: Resources.componentwiseMin→CALL: Resources.addTo→CALL: Resources.subtractFrom→FOREACH_EXIT→RETURN→EXIT",
    "log": "<!-- Assume this path had no logs hence no sequence is merged -->"
  },
  "f4efc0c9_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:getCurrentUser</step> <step>LOG:DefaultRuleAssigned</step> <step>TRY_CATCH</step> <step>CATCH:Exception</step> <step>THROW:IllegalArgumentException</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>LOG:Handling deprecation for (String)item</step> <step>FOREACH_EXIT</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG:Creating new Groups object</step> <step>NEW:Groups</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[INFO] DefaultRuleAssigned [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object"
  },
  "de963ef5_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "de963ef5_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "9f4d0b65_1": {
    "exec_flow": "ENTRY→CALL:subtract→CALL:clone→CALL:subtractFrom→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "fceee016_1": {
    "exec_flow": "<step> <name>ENTRY</name> </step> <step> <name>org.apache.hadoop.mapred.JobConf:<init>(org.apache.hadoop.conf.Configuration)</name> </step> <step> <name>org.apache.hadoop.mapred.FileInputFormat:setInputPaths</name> <details> Set the array of Paths as the list of inputs for the map-reduce job. </details> </step> <step> <name>org.apache.hadoop.mapred.JobConf:getWorkingDirectory</name> <details> Get the current working directory for the default file system. </details> </step> <step> <name>org.apache.hadoop.mapred.JobConf:set</name> <details> Set the configuration property in the JobConf. </details> </step> <step> <name>CALL:setClassLoader</name> </step> <step> <name>CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)</name> </step> <step> <name>IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null</name> </step> <step> <name>CALL:warn</name> </step> <step> <name>LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY</name> </step> <step> <name>CALL:get(JobConf.MAPRED_TASK_ULIMIT)</name> </step> <step> <name>IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null</name> </step> <step> <name>CALL:warn</name> </step> <step> <name>LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)</name> </step> <step> <name>CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)</name> </step> <step> <name>IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null</name> </step> <step> <name>CALL:warn</name> </step> <step> <name>LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)</name> </step> <step> <name>CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)</name> </step> <step> <name>IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null</name> </step> <step> <name>CALL:warn</name> </step> <step> <name>LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)</name> </step> <step> <name>RETURN</name> </step> <step> <name>EXIT</name> </step>",
    "log": "[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "fceee016_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → RETURN → EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "b71577a7_1": {
    "exec_flow": "Parent.ENTRY → IF_FALSE:name == null → CALL:P2-C1 → IF_TRUE:output != null → WHILE:allSwitchInfo.hasMoreTokens() → WHILE_COND:allSwitchInfo.hasMoreTokens() → WHILE_EXIT → RETURN → EXIT",
    "log": "[WARN] Exception running {s}, {e}"
  },
  "b71577a7_2": {
    "exec_flow": "Parent.ENTRY → IF_FALSE:name == null → CALL:P3-C1 → IF_FALSE:output != null → RETURN → EXIT",
    "log": "[WARN] Exception running {s}, {e}"
  },
  "1933aece_1": {
    "exec_flow": "ENTRY → CALL:checkNotSchemeWithRelative → CALL:checkNotSchemeWithRelative → CALL:checkDest → CALL:getFileStatus → IF_FALSE:fs.isDirectory() → CALL:FutureDataInputStreamBuilder:build → IF_TRUE:props != null → CALL:handleDeprecation → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE:overlay != null → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] message [WARN] Unexpected SecurityException in Configuration [DEBUG] Exception in closing {} [DEBUG] Handling deprecation for all properties in config [DEBUG] Handling deprecation for (String)item"
  },
  "1933aece_2": {
    "exec_flow": "ENTRY → FOR_INIT → FOR_COND:isLink → CALL:org.apache.hadoop.fs.FileContext:getFSofPath → CALL:org.apache.hadoop.fs.FSLinkResolver:next → TRY → FOR_EXIT → RETURN → EXIT",
    "log": "[INFO] Resolving file system path [DEBUG] Attempting symlink resolution [INFO] Resolving file system path [DEBUG] Attempting symlink resolution"
  },
  "1933aece_3": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→TRY→FOR_EXIT→RETURN→EXIT",
    "log": "[INFO] Resolving file system path [DEBUG] Attempting symlink resolution"
  },
  "dd5f693b_1": {
    "exec_flow": "ENTRY→CALL:stop→ENTRY→IF_FALSE:isInState(STATE.STOPPED)→SYNC:stateChangeLock→IF_FALSE:enterState(STATE.STOPPED)!=STATE.STOPPED→LOG:LOG.DEBUG:Ignoring re-entrant call to stop()→CALL:notifyListeners→ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH:Throwable e→LOG:LOG.WARN:Exception while notifying listeners of {},this,e→EXIT→EXIT→EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "dd5f693b_2": {
    "exec_flow": "ENTRY→CALL:stop→ENTRY→IF_FALSE:isInState(STATE.STOPPED)→SYNC:stateChangeLock→IF_TRUE:enterState(STATE.STOPPED)!=STATE.STOPPED→TRY→CALL:serviceStop→FINALLY→terminationNotification.set(true)→SYNC:terminationNotification→terminationNotification.notifyAll()→CALL:notifyListeners→ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→EXIT→EXIT→EXIT",
    "log": "[DEBUG] Service: {} entered state {}"
  },
  "59ff3406_1": {
    "exec_flow": "ENTRY→CALL:get→IF_TRUE:null==vStr→CALL:trim→CALL:toLowerCase→IF_CONDITION1:null==vUnit→CALL:unitFor→IF_CONDITION2:vUnit.unit().convert(converted, returnUnit)<raw→CALL:logDeprecation→RETURN→EXIT",
    "log": "<log>[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}</log>"
  },
  "59ff3406_2": {
    "exec_flow": "ENTRY→CALL:get→IF_FALSE:null==vStr→CALL:getTimeDurationHelper→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "59ff3406_3": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "887a26fd_1": {
    "exec_flow": "ENTRY → TRY → CALL:replaceBlock → IF_TRUE:traceScope != null → CALL:traceScope.close → LOG:[WARN] Not able to start → CALL:sendResponse → RETURN → EXIT",
    "log": "<log>[WARN] Not able to start</log>"
  },
  "887a26fd_2": {
    "exec_flow": "ENTRY → TRY → CALL:replaceBlock → IF_FALSE:traceScope != null → INLINE: (No trace scope closure) → CALL:updateCurrentThreadName → CALL:checkAccess → IF_FALSE:!dataXceiverServer.balanceThrottler.acquire() → TRY → IF_TRUE:proxySource.equals(datanode.getDatanodeId()) → CALL:datanode.data.moveBlockAcrossStorage → LOG:[INFO] Moved block from StorageType to StorageType → CALL:sendResponse → FINALLY → EXIT",
    "log": "<log>[INFO] Moved block from StorageType to StorageType</log>"
  },
  "887a26fd_3": {
    "exec_flow": "ENTRY → IF_FALSE:bpos != null → LOG:[ERROR] Cannot find BPOfferService for reporting block received + for bpid={} → EXIT",
    "log": "<log>[ERROR] Cannot find BPOfferService for reporting block received + for bpid={}</log>"
  },
  "887a26fd_4": {
    "exec_flow": "ENTRY → IF_FALSE:socket == null || endpoint == null || timeout < 0 → IF_TRUE:localAddr != null → TRY → IF_TRUE:ch == null → CALL:connect → IF_TRUE:socket.getLocalPort() == socket.getPort() && socket.getLocalAddress().equals(socket.getInetAddress()) → LOG:[INFO] Detected a loopback TCP socket, disconnecting it → CALL:close → THROW:new ConnectException(\"Localhost targeted connection resulted in a loopback. No daemon is listening on the target port.\") → EXIT",
    "log": "<log>[INFO] Detected a loopback TCP socket, disconnecting it</log>"
  },
  "887a26fd_5": {
    "exec_flow": "ENTRY → TRY → CALL:OpReadBlockProto.parseFrom → CALL:continueTraceSpan → CALL:readBlock → IF_TRUE:traceScope != null → CALL:traceScope.close → CATCH:IOException → IF_FALSE:!(ioe instanceof SocketTimeoutException) → CALL:handleBadBlock → IF_FALSE:!isBadBlock → IF_FALSE:!fromScanner && blockScanner.isEnabled() → TRY → CALL:reportBadBlocks → EXCEPTION:reportBadBlocks → CATCH:IOException → LOG:WARN:report bad block {} failed, block, ie → EXIT",
    "log": "<log>[TRACE] \"{}: Ignoring exception while serving {} to {}\", dnR, block, remoteAddress, ignored</log> <log>[WARN] report bad block {} failed, block, ie</log>"
  },
  "887a26fd_6": {
    "exec_flow": "ENTRY → TRY → CALL: copyBlock → IF_TRUE: traceScope!=null → CALL: traceScope.close → CALL: updateCurrentThreadName → CALL: checkAccess → IF_TRUE: datanode.data.getPinning(block) → LOG: LOG.INFO: Not able to copy block ... because it's pinned → CALL: sendResponse → RETURN → EXIT",
    "log": "<log>[INFO] Not able to copy block ... because it's pinned</log>"
  },
  "887a26fd_7": {
    "exec_flow": "ENTRY → TRY → CALL: copyBlock → IF_TRUE: traceScope!=null → CALL: traceScope.close → CALL: updateCurrentThreadName → CALL: checkAccess → IF_FALSE: datanode.data.getPinning(block) → IF_TRUE: !dataXceiverServer.balanceThrottler.acquire() → LOG: LOG.INFO: Not able to copy block ... because threads quota exceeded → CALL: sendResponse → RETURN → EXIT",
    "log": "<log>[INFO] Not able to copy block ... because threads quota exceeded</log>"
  },
  "887a26fd_8": {
    "exec_flow": "ENTRY → TRY → CALL: copyBlock → IF_TRUE: traceScope!=null → CALL: traceScope.close → CALL: updateCurrentThreadName → CALL: checkAccess → IF_FALSE: datanode.data.getPinning(block) → IF_FALSE: !dataXceiverServer.balanceThrottler.acquire() → CALL: BlockSender:<init> → CALL: getOutputStream → CALL: writeSuccessWithChecksumInfo → CALL: Time.monotonicNow → TRY → CALL: FsTracer.get → CALL: doSendBlock → RETURN → CALL: scope.close → CALL: BlockSender:sendBlock → CALL: Time.monotonicNow → CALL: DataNodeMetrics.incrBytesRead → CALL: DataNodeMetrics.incrBlocksRead → CALL: DataNodeMetrics.incrTotalReadTime → CALL: DFSUtil:addTransferRateMetric → LOG: LOG.INFO: Copied ... to ... → CALL: IOUtils.closeStream → CALL: IOUtils.closeStream → CALL: DataNodeMetrics.addCopyBlockOp → EXIT",
    "log": "<log>[INFO] Copied ... to ...</log>"
  },
  "887a26fd_9": {
    "exec_flow": "ENTRY → TRY → CALL: copyBlock → IF_TRUE: traceScope!=null → CALL: traceScope.close → CALL: updateCurrentThreadName → CALL: checkAccess → IF_FALSE: datanode.data.getPinning(block) → IF_FALSE: !dataXceiverServer.balanceThrottler.acquire() → CALL: BlockSender:<init> → CALL: getOutputStream → CALL: writeSuccessWithChecksumInfo → CALL: Time.monotonicNow → TRY → CALL: FsTracer.get → CALL: doSendBlock → EXCEPTION: IOException → CALL: scope.close → LOG: LOG.INFO: opCopyBlock ... received exception ... → CALL: DataXceiver:incrDatanodeNetworkErrors → CALL: DataNode:handleBadBlock → CALL: IOUtils.closeStream → CALL: IOUtils.closeStream → EXIT",
    "log": "<log>[INFO] opCopyBlock ... received exception ...</log>"
  },
  "d0df5e40_1": {
    "exec_flow": "ENTRY → CALL:inspectStorageDirs → FOREACH:inspector.getFoundImages() → TRY → CALL:renameImageFileInDir → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:renaming + fromFile.getAbsolutePath() + \" to \" + toFile.getAbsolutePath() → CALL:org.slf4j.Logger:debug(java.lang.String) → IF_FALSE:!fromFile.renameTo(toFile) → IF_TRUE:renameMD5 → CALL:MD5FileUtils.renameMD5File → IF_FALSE:!fromFile.exists() → TRY → CALL:readLine → IF_TRUE:md5Line==null → CALL:trim → CALL:IOUtils.cleanupWithLogger → IF_TRUE:!matcher.matches() → THROW:new IOException(\"Invalid MD5 file \" + md5File + \": the content \\\"\" + md5Line + \"\\\" does not match the expected pattern.\") → EXIT → CALL:saveMD5File → CALL:write → CALL:close → IF_TRUE:LOG.isDebugEnabled() → IF_TRUE:!fromFile.delete() → LOG:LOG.DEBUG:Saved MD5 ${digestString} to ${md5File} → LOG:LOG.WARN: deleting + fromFile.getAbsolutePath() + FAILED → CATCH:IOException → CALL:warn → TRY_END → FOREACH_EXIT → IF_TRUE:al != null → CALL:reportErrorsOnDirectories → FOREACH:sds → CALL:reportErrorsOnDirectory → FOREACH_EXIT → EXIT",
    "log": "[WARN] Unable to rename checkpoint in [StorageDirectory]: IOException [DEBUG] renaming [fromFile.getAbsolutePath()] to [toFile.getAbsolutePath()] [DEBUG] Saved MD5 ${digestString} to ${md5File} [WARN] deleting + fromFile.getAbsolutePath() + FAILED"
  },
  "d0df5e40_2": {
    "exec_flow": "ENTRY → FOREACH:threads → WHILE:thread.isAlive() → TRY → CALL:Thread:join → CATCH:InterruptedException → CALL:org.slf4j.Logger:error(java.lang.String) → TRY_EXIT → WHILE_EXIT → FOREACH_EXIT → EXIT",
    "log": "[ERROR] Caught interrupted exception while waiting for thread {thread.getName()} to finish. Retrying join"
  },
  "d0df5e40_3": {
    "exec_flow": "ENTRY → CALL:ShutdownHookManager.get().addShutdownHook → TRY → CALL:saveFSImage → EXCEPTION:saveFSImage → CATCH:SaveNamespaceCancelledException snce → LOG:LOG.INFO:Cancelled image saving for + sd.getRoot() + : + snce.getMessage() → EXIT",
    "log": "[INFO] Cancelled image saving for + sd.getRoot() + : + snce.getMessage()"
  },
  "d0df5e40_4": {
    "exec_flow": "NNStorageRetentionManager.ENTRY <!-- Virtual call --> NNStorageRetentionManager$StoragePurger.purgeImage",
    "log": "<log> <class>org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger</class> <method>purgeImage</method> <level>INFO</level> <template>Purging old image {image}</template> </log>"
  },
  "9eea9283_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → RETURN → EXIT → CALL:getTrimmed(java.lang.String) → RETURN → AzureNativeFileSystemStore:getBlobReference.ENTRY → IF_FALSE: isPageBlobKey(aKey) → CALL: getBlockBlobReference → TRY → CALL: sasKeyGenerator.getRelativeBlobSASUri → EXCEPTION: SASKeyGenerationException → CATCH: SASKeyGenerationException sasEx → LOG: LOG.ERROR: errorMsg → THROW: new StorageException(SAS_ERROR_CODE, errorMsg, sasEx) → CALL: setStreamMinimumReadSizeInBytes → CALL: setWriteBlockSizeInBytes → RETURN → EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<!-- Merged log sequence --> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>ERROR</level> <template>Encountered SASKeyGeneration exception while generating SAS Key for relativePath : {relativePath} inside container : {getName()} Storage account : {storageAccount}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "66f118f7_1": {
    "exec_flow": "ENTRY→CALL:create→CALL:contains→CALL:contains→CALL:create→RETURN→CALL:getInt→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "66f118f7_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE: containsColon(f) → CALL: performAuthCheck → CALL: createInternal → RETURN → EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "66f118f7_3": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_FALSE:!overwrite → CALL:org.slf4j.Logger:debug → NEW:FSDataOutputStream → NEW:CosNOutputStream → CALL:getConf → RETURN → EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS."
  },
  "f4797439_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.FileSystem:listStatus→EXCEPTION→CALL:org.slf4j.Logger:error→RETURN→EXIT",
    "log": "[ERROR] Cannot get children for {}"
  },
  "f4797439_2": {
    "exec_flow": "ENTRY→CALL:getUriPath→CALL:resolve→IF_TRUE:!res.isInternalDir()→CALL:listStatus→FOREACH:statusLst→CALL:fixFileStatus→CALL:getChrootedPath→FOREACH_EXIT→CALL:checkPathIsSlash→FOREACH:theInternalDir.getChildren().entrySet()→IF_TRUE:inode.isLink()→IF_FALSE:showMountLinksAsSymlinks→IF_TRUE:\"\".equals(linkedPath)→TRY→CALL:org.apache.hadoop.fs.FileSystem:getFileStatus→CALL:linkStatuses.add→EXCEPTION:add→CATCH:FileNotFoundException ex→LOG:LOG.WARN:Cannot get one of the children's( + path + ) target path( + link.getTargetFileSystem().getUri() + ) file status., ex→THROW:ex→RETURN→EXIT",
    "log": "[DEBUG] Listing status for {f} [DEBUG] List status for path: {path} [DEBUG] listStatus: doing listObjects for directory {key} [WARN] Cannot get one of the children's( + path + ) target path( + link.getTargetFileSystem().getUri() + ) file status., ex"
  },
  "8622d4e0_1": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:PUT {} bytes to {}→CALL:Preconditions.checkArgument→DO_WHILE→TRY→CATCH:IOException|SdkBaseException→CALL:translateException→CALL:shouldRetry→IF_TRUE:shouldRetry→CALL:onFailure→THREAD:SLEEP→INCREMENT:retryCount→EXIT",
    "log": "[DEBUG] PUT {} bytes to {} [DEBUG] retry #{}"
  },
  "8622d4e0_2": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:PUT {} bytes to {}→CALL:Preconditions.checkArgument→IF_TRUE→CALL:keyToQualifiedPath→ENTRY→WHILE:!path.isRoot()→WHILE_COND:!path.isRoot()→WHILE_EXIT→TRY→CALL:removeKeys→EXCEPTION:removeKeys→CATCH:AmazonClientException|IOException e→CALL:errorIgnored→IF_TRUE:LOG.isDebugEnabled→FOREACH:keysToRemove→LOG:LOG.DEBUG:While deleting keys{}→FOREACH_EXIT→EXIT→EXIT",
    "log": "[DEBUG] PUT {} bytes to {} [DEBUG] Finished write to {}, len {}. etag {}, version {}, key, length, eTag, versionId [DEBUG] While deleting keys {} [DEBUG] PUT completed success={}; {} bytes"
  },
  "8622d4e0_3": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:PUT {} bytes to {}→CALL:Preconditions.checkArgument→IF_FALSE→EXIT",
    "log": "[DEBUG] PUT {} bytes to {} [DEBUG] Finished write to {}, len {}. etag {}, version {}, key, length, eTag, versionId [DEBUG] PUT completed success={}; {} bytes"
  },
  "8622d4e0_4": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:PUT start {} bytes→CALL:incrementWriteOperations→CALL:incrementGauge→IF_TRUE:bytes > 0→CALL:incrementGauge→EXIT",
    "log": "[DEBUG] PUT start {} bytes"
  },
  "8622d4e0_5": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:PUT start {} bytes→CALL:incrementWriteOperations→CALL:incrementGauge→IF_FALSE:bytes > 0→EXIT",
    "log": "[DEBUG] PUT start {} bytes"
  },
  "ada1702f_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for +(String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ada1702f_2": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta!=null→LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_FALSE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0→THROW:FileNotFoundException→EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path."
  },
  "ada1702f_3": {
    "exec_flow": "ENTRY→CALL:connect→IF_TRUE:client!=null→IF_FALSE:!client.isConnected()→CALL:logout→CALL:disconnect→IF_TRUE:!logoutSuccess→LOG:LOG.WARN:Logout failed while disconnecting, error code - +client.getReplyCode()→EXIT",
    "log": "[WARN] Logout failed while disconnecting, error code -"
  },
  "ada1702f_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus→IF_FALSE:fs.isDirectory()→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:getFileLength(key)→LOG:LOG.INFO:Open the file: [{f}] for reading.→NEW:FSDataInputStream→CALL:getConf→NEW:BufferedFSInputStream→NEW:CosNInputStream→RETURN→EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Opening file: {f.toString()} [INFO] f + \" \" + srcNode + \"->\" + dstNode + \": Failed to repair\" [INFO] f + \": Failed to open at \" + rNode.getFs().getUri()"
  },
  "ada1702f_5": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for +(String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ada1702f_6": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "0c0da8f6_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY CALL: writeLock TRY IF_TRUE: inode == null CALL: writeUnlock CALL: getEditLog().logSync EXIT IF_FALSE: inode == null IF_TRUE: xaf == null CALL: writeUnlock CALL: getEditLog().logSync EXIT IF_FALSE: xaf == null IF_TRUE: spsXAttr != null CALL: FSDirSatisfyStoragePolicyOp.removeSPSXattr CALL: writeUnlock CALL: getEditLog().logSync EXIT IF_FALSE: spsXAttr != null CALL: writeUnlock CALL: getEditLog().logSync EXIT CALL: beginTransaction LOG:DEBUG:doEditTx() op={} txid={} TRY CALL: editLogStream.writeRaw CALL: endTransaction EXIT CALL: printStatistics IF_FALSE: lastPrintTime + 60000 > now && !force LOG:INFO:Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()> WHILE: mytxid > synctxid && isSyncRunning WHILE_EXIT IF_FALSE: mytxid <= synctxid CALL: getLastJournalledTxId LOG:DEBUG:logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} IF_FALSE: lastJournalledTxId <= synctxid TRY IF_TRUE: journalSet.isEmpty() THROW: new IOException(\"No journals available to flush\") CALL: org.apache.hadoop.util.ExitUtil:terminate CALL: org.slf4j.Logger:error CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger EXIT",
    "log": "<!-- Merged log sequence --> [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()> [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions} [DEBUG] doEditTx() op={} txid={}"
  },
  "b54c0062_1": {
    "exec_flow": "ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → CALL: getDatanodeDescriptor → CALL: get → IF_TRUE: storageTypes == null → CALL: getRequiredStorageTypes → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_FALSE: requiredStorageTypes.size() == 0 → CALL: chooseTargetInOrder → RETURN → EXIT",
    "log": "[TRACE] storageTypes={}"
  },
  "b54c0062_2": {
    "exec_flow": "ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → CALL: getDatanodeDescriptor → CALL: get → IF_TRUE: storageTypes == null → CALL: getRequiredStorageTypes → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_TRUE: requiredStorageTypes.size() == 0 → THROW: new NotEnoughReplicasException → LOG: LOG.TRACE: message, e → LOG: LOG.WARN: message + e.getMessage() → EXIT",
    "log": "[TRACE] storageTypes={} [TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e [WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()"
  },
  "b54c0062_3": {
    "exec_flow": "ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_TRUE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → CALL: getDatanodeDescriptor → CALL: get → IF_FALSE: storageTypes == null → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_FALSE: requiredStorageTypes.size() == 0 → CALL: chooseTargetInOrder → RETURN → EXIT",
    "log": "[TRACE] storageTypes={}"
  },
  "b54c0062_4": {
    "exec_flow": "ENTRY → IF_FALSE: numOfReplicas == 0 || clusterMap.getNumOfLeaves() == 0 → IF_FALSE: (writer == null || !(writer instanceof DatanodeDescriptor)) && !newBlock → IF_TRUE: storageTypes == null → CALL: getRequiredStorageTypes → LOG: LOG.TRACE: storageTypes={}, storageTypes → TRY → IF_TRUE: requiredStorageTypes.size() == 0 → THROW: new NotEnoughReplicasException → LOG: LOG.TRACE: message, e → LOG: LOG.WARN: message + e.getMessage() → EXIT",
    "log": "[TRACE] storageTypes={} [TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e [WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()"
  },
  "3611da13_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3611da13_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE: loadDefaults && fullReload</step> <step>FOREACH: defaultResources</step> <step>CALL: loadResource</step> <step>FOREACH_EXIT</step> <step>FOR_INIT</step> <step>FOR_COND: i < resources.size()</step> <step>CALL: loadResource</step> <step>FOR_EXIT</step> <step>CALL: addTags</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3611da13_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: loadDefaults && fullReload</step> <step>FOR_INIT</step> <step>FOR_COND: i < resources.size()</step> <step>CALL: loadResource</step> <step>FOR_EXIT</step> <step>CALL: addTags</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3611da13_4": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: loadDefaults && fullReload</step> <step>FOR_INIT</step> <step>FOR_COND: i < resources.size()</step> <step>CALL: loadResource</step> <step>FOR_EXIT</step> <step>CALL: addTags</step> <step>CALL: org.slf4j.Logger:info(java.lang.String)</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "950be9fc_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → IF_TRUE: overlay != null → CALL:putAll → IF_FALSE: backup != null → CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) → IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null → CALL:warn → LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY → CALL:get(JobConf.MAPRED_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null → CALL:warn → LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) → CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null → CALL:warn → LOG: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) → CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null → CALL:warn → LOG: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)",
    "log": "<log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)</message> </log>"
  },
  "25ca0151_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkArgument→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append(\"recycle: array.length=\").append→IF_TRUE:array.length == byteArrayLength→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append(\", freeQueueSize=\").append→LOG:logDebugMessage→RETURN→EXIT",
    "log": "[DEBUG] recycle: array.length=, freeQueueSize="
  },
  "25ca0151_2": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkArgument→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append(\"recycle: array.length=\").append→IF_FALSE:array.length == byteArrayLength→CALL:org.apache.hadoop.hdfs.util.ByteArrayManager:access$1()→CALL:recycle→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append(\", freeQueueSize=\").append→LOG:logDebugMessage→RETURN→EXIT",
    "log": "[DEBUG] recycle: array.length=, freeQueueSize="
  },
  "25ca0151_3": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkArgument→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append(\", \").append→CALL:notify→IF_TRUE:numAllocated < 0→IF_TRUE:freeQueue.size() < maxAllocated - numAllocated→IF_TRUE:LOG.isDebugEnabled()→CALL:DEBUG_MESSAGE.get().append→CALL:freeQueue.offer→CALL:size→RETURN→EXIT",
    "log": "[DEBUG] , freeQueue.offer"
  },
  "b268d1bf_1": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL] → Child paths</step> <step>ENTRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser</step> <step>IF_FALSE:locations.isEmpty</step> <step>IF_FALSE:locations.size() == 1 && timeOutMs <= 0</step> <step>FOREACH:locations</step> <step>CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod</step> <step>CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice</step> <step>CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:lambda$0</step> <step>FOREACH_EXIT</step> <step>TRY</step> <step>CALL:invokeAll</step> <step>FOR_INIT</step> <step>FOR_COND:i < futures.size()</step> <step>CALL:org.slf4j.Logger:error</step> <step>FOR_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "b268d1bf_2": {
    "exec_flow": "<sequence> ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "b268d1bf_3": {
    "exec_flow": "<sequence> ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "b268d1bf_4": {
    "exec_flow": "<sequence> ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "b268d1bf_5": {
    "exec_flow": "<sequence> ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "c208cd3f_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → FOREACH_EXIT → CALL: noteFailure → CALL: recordLifecycleEvent → CALL: ServiceOperations.stopQuietly → EXIT",
    "log": "<log>[DEBUG] Service: {} entered state {}</log> <log>[DEBUG] Config has been overridden during init</log> <log>[DEBUG] noteFailure</log> <log>[INFO] Service {} failed in state {}</log> <log>[WARN] Exception while notifying listeners of {}</log>"
  },
  "c208cd3f_2": {
    "exec_flow": "ENTRY → IF_TRUE: oldState != newState → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → RETURN → EXIT",
    "log": "<log>[DEBUG] Service: {} entered state {}</log>"
  },
  "4955572b_1": {
    "exec_flow": "ENTRY→CALL: checkOperation→CALL: FSPermissionChecker.setOperationType→TRY→CALL: writeLock→CALL: getPermissionChecker→CALL: checkNameNodeSafeMode→CALL: FSDirAttrOp.setOwner→CALL: resolveComponents→CALL: byteArray2PathString→RETURN→EXIT→CALL: writeUnlock→IF_TRUE: needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs→IF_TRUE: logAction.shouldLog()→CALL: FSNamesystem.LOG.info→CALL: logAuditEvent (true)→FOREACH: auditLoggers →IF: logger instanceof HdfsAuditLogger→CALL: appendClientPortToCallerContextIfAbsent→CALL: hdfsLogger.logAuditEvent→FOREACH_EXIT→EXIT",
    "log": "[INFO] Audit Event: setOwner successful for src FSNamesystem.LOG.info(\"\\tNumber of suppressed write-lock reports: {}\" + \"\\n\\tLongest write-lock held at {} for {}ms via {}\" + \"\\n\\tTotal suppressed write-lock held time: {}\") <log> <template>logSetOwner({path}, {username}, {group})</template> <params> <path>{iip.getPath()}</path> <username>{username}</username> <group>{group}</group> </params> </log>"
  },
  "4955572b_2": {
    "exec_flow": "ENTRY→CALL: checkOperation→CALL: FSPermissionChecker.setOperationType→TRY→CALL: writeLock→CALL: getPermissionChecker→CALL: checkNameNodeSafeMode→CALL: FSDirAttrOp.setOwner→CALL: resolveComponents→CALL: byteArray2PathString→RETURN→EXIT→CALL: writeUnlock→IF_TRUE: needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs→IF_TRUE: logAction.shouldLog()→CALL: FSNamesystem.LOG.info→CALL: logAuditEvent (true)→FOREACH: auditLoggers →IF: !(logger instanceof HdfsAuditLogger)→CALL: logger.logAuditEvent→FOREACH_EXIT→EXIT",
    "log": "[INFO] Audit Event: setOwner successful for src FSNamesystem.LOG.info(\"\\tNumber of suppressed write-lock reports: {}\" + \"\\n\\tLongest write-lock held at {} for {}ms via {}\" + \"\\n\\tTotal suppressed write-lock held time: {}\") <log> <template>logSetOwner({path}, {username}, {group})</template> <params> <path>{iip.getPath()}</path> <username>{username}</username> <group>{group}</group> </params> </log>"
  },
  "4955572b_3": {
    "exec_flow": "ENTRY→CALL: checkOperation→CALL: FSPermissionChecker.setOperationType→TRY→CALL: writeLock→CALL: getPermissionChecker→CALL: checkNameNodeSafeMode→EXCEPTION: AccessControlException→CATCH: AccessControlException e→CALL: logAuditEvent (false)→FOREACH: auditLoggers →IF: logger instanceof HdfsAuditLogger→CALL: appendClientPortToCallerContextIfAbsent→CALL: hdfsLogger.logAuditEvent→FOREACH_EXIT→THROW: e→EXIT",
    "log": "[WARN] Audit Event: setOwner failed for src"
  },
  "4955572b_4": {
    "exec_flow": "ENTRY→CALL: checkOperation→CALL: FSPermissionChecker.setOperationType→TRY→CALL: writeLock→CALL: getPermissionChecker→CALL: checkNameNodeSafeMode→EXCEPTION: AccessControlException→CATCH: AccessControlException e→CALL: logAuditEvent (false)→FOREACH: auditLoggers →IF: !(logger instanceof HdfsAuditLogger)→CALL: logger.logAuditEvent→FOREACH_EXIT→THROW: e→EXIT",
    "log": "[WARN] Audit Event: setOwner failed for src"
  },
  "592197e6_1": {
    "exec_flow": "ENTRY→CALL:WebApps.$for(new HelloWorld()).at(8888).inDevMode→CALL:start →ENTRY→IF_TRUE:ui2Context!=null→CALL:addFiltersForNewContext→CALL:httpServer.addHandlerAtFront→TRY→CALL:httpServer.start →LOG:INFO:Web app {name} started at {httpServer.getConnectorAddress(0).getPort()}→RETURN→EXIT",
    "log": "[INFO] Web app {name} started at {httpServer.getConnectorAddress(0).getPort()}"
  },
  "592197e6_2": {
    "exec_flow": "ENTRY→CALL:WebApps.$for(new HelloWorld()).at(8888).inDevMode→CALL:start →ENTRY→IF_FALSE:ui2Context!=null→TRY→CALL:httpServer.start →LOG:INFO:Web app {name} started at {httpServer.getConnectorAddress(0).getPort()}→RETURN→EXIT",
    "log": "[INFO] Web app {name} started at {httpServer.getConnectorAddress(0).getPort()}"
  },
  "3c9c446b_1": {
    "exec_flow": "ENTRY→CALL:PerformanceAdvisory.LOG.debug→CALL:doDecode→FOR_INIT→FOR_COND:i < decodingState.outputs.length→FOR_EXIT→EXIT",
    "log": "[DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs"
  },
  "72e60429_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: [INFO] message → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "72e60429_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "72e60429_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- No logs; inherited from child path -->"
  },
  "72e60429_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- No logs; inherited from child path -->"
  },
  "72e60429_5": {
    "exec_flow": "ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- No logs -->"
  },
  "72e60429_6": {
    "exec_flow": "ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- No logs -->"
  },
  "d5a7fbab_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:checkPathsForReservedRaw → CALL:setVerifyChecksum → TRY → CALL:openFile → CALL:opt → CALL:build → IF(target.exists && (target.stat.isDirectory() || !overwrite)) → THROW:PathExistsException → CALL:copyStreamToTarget → TRY → CALL:writeStreamToFile → TRY → CALL:create → CALL:IOUtils.copyBytes → IF_TRUE:direct=false → CALL:deleteOnExit → CALL:IOUtils.closeStream → IF_TRUE:stream!=null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c!=null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger!=null) → CALL:org.slf4j.Logger:debug → FOREACH_EXIT → EXIT(TRY) → IF(!direct) → CALL:targetFs.rename → FINALLY → CALL:targetFs.close → CALL:preserveAttributes → FINALLY → CALL:closeStream → EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "001ae59b_1": {
    "exec_flow": "ENTRY→SYNC: queueResourceLimitsInfo→CALL:getQueueCurrentLimit→CALL:getClusterResource→IF_TRUE:requestedPartitions.isEmpty() || (requestedPartitions.size() == 1 && requestedPartitions.contains(RMNodeLabelsManager.NO_LABEL))→CALL:getHeadroom→IF_TRUE:headroom.getMemorySize() < 0→CALL:setMemorySize→RETURN→EXIT→ENTRY→IF_TRUE: !initializedResources→SYNC: ResourceUtils.class→IF_TRUE: !initializedResources→IF_FALSE: resConf == null→CALL: addResourcesFileToConf→CALL: initializeResourcesMap→CALL: size→CALL: size→EXIT",
    "log": "<!-- No explicit logs identified so left empty since parent has no logs -->"
  },
  "001ae59b_2": {
    "exec_flow": "ENTRY→SYNC: queueResourceLimitsInfo→CALL:getQueueCurrentLimit→CALL:getClusterResource→IF_TRUE:requestedPartitions.isEmpty() || (requestedPartitions.size() == 1 && requestedPartitions.contains(RMNodeLabelsManager.NO_LABEL))→CALL:getHeadroom→IF_FALSE:headroom.getMemorySize() < 0→RETURN→EXIT→ENTRY→IF_TRUE: !initializedResources→SYNC: ResourceUtils.class→IF_FALSE: !initializedResources→CALL: size→CALL: size→EXIT",
    "log": "<!-- No explicit logs identified so left empty since parent has no logs -->"
  },
  "001ae59b_3": {
    "exec_flow": "ENTRY→SYNC: queueResourceLimitsInfo→CALL:getQueueCurrentLimit→CALL:getClusterResource→IF_FALSE:requestedPartitions.isEmpty() || (requestedPartitions.size() == 1 && requestedPartitions.contains(RMNodeLabelsManager.NO_LABEL))→CALL:newInstance→FOREACH:requestedPartitions→CALL:queue.getHeadroom→CALL:Resources:addTo→FOREACH_EXIT→IF_TRUE:headroom.getMemorySize() < 0→CALL:setMemorySize→RETURN→EXIT→ENTRY→IF_FALSE: !initializedResources→CALL: size→CALL: size→EXIT",
    "log": "<!-- No explicit logs identified so left empty since parent has no logs -->"
  },
  "001ae59b_4": {
    "exec_flow": "ENTRY→CALL:equals→CALL:getQueueMaxResource→CALL:Resources.subtractNonNegative→CALL:Resources.componentwiseMin→CALL:Resources.subtract→CALL:Resources.roundDown→CALL:labelManager.getResourceByLabel→CALL:csContext.getClusterResourceUsage→CALL:Resources.subtract→CALL:Resources.min→RETURN→EXIT",
    "log": "<!-- No logs since both parent and child nodes have no relevant logs -->"
  },
  "001ae59b_5": {
    "exec_flow": "ENTRY → FOR_INIT → CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes() → FOR_COND: i < maxLength → TRY → CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) → CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) → CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue(int,long) → FOR_BODY → FOR_UPDATE → FOR_COND: i < maxLength → EXCEPTION:ResourceNotFoundException → LOG.warn → FOR_UPDATE → FOR_COND: i < maxLength → FOR_EXIT → RETURN → EXIT",
    "log": "[WARN] Resource is missing:"
  },
  "6854773d_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → ENTRY(openFileForWrite) → TRY → LOG:LOG.DEBUG: openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), qualifiedPath, false → CALL:startTracking → CALL:client.getPathStatus → CALL:perfInfo.registerResult → IF_TRUE:parseIsDirectory(resourceType) → THROW:new AbfsRestOperationException → CALL:perfInfo.close(openFileForWrite) → EXCEPTION → CALL:checkException → RETURN → EXIT",
    "log": "[DEBUG] Opening file: {} for append [DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize} [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), qualifiedPath, false"
  },
  "6854773d_2": {
    "exec_flow": "ENTRY→CALL:makeQualified→NEW:Path→CALL:getHomeDirectory→TRY→CALL:getCurrentUser→[EXCEPTION:getCurrentUser]→CALL:warn→CALL:getPath→CALL:toUri→RETURN→EXIT",
    "log": "[WARN] Unable to get user name. Fall back to system property user.name"
  },
  "6854773d_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:getFileStatus →IF_FALSE:key.length() == 0 →CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata →IF_TRUE:meta!=null→IF_TRUE:meta.isFile() →LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] →CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "6854773d_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:getFileStatus →IF_FALSE:key.length() == 0 →CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata →IF_TRUE:meta!=null→IF_FALSE:meta.isFile() →LOG:LOG.DEBUG:Path: [{}] is a dir. COS key: [{}] →CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "[DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "5710c76f_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config... →CALL:getProps→CALL:addAll→FOREACH:keys →LOG:Handling deprecation for (String)item→CALL:handleDeprecation →FOREACH_EXIT→CALL:getProps→CALL:substituteVars →LOG:Unexpected SecurityException in Configuration →CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw →RETURN→FOREACH_EXIT→RETURN→EXIT →CALL:getHttpAddress→CALL:HAUtil.getConfForOtherNodes→NEW:ArrayList →CALL:size→FOREACH:confForActive→CALL:getHttpAddress→CALL:checkAddress →CALL:Preconditions.checkArgument→FOREACH_EXIT→CALL:checkAddress →CALL:Preconditions.checkArgument→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "5710c76f_2": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "f96f457f_1": {
    "exec_flow": "ENTRY→FOREACH:args→TRY→CALL:expandArgument→CATCH:IOException→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "42868d45_1": {
    "exec_flow": "ENTRY→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→ENTRY→CALL:getAuthenticationMethod→IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()→TRY→CALL:HadoopKerberosName.setConfiguration→EXCEPTION:setConfiguration→CATCH:IOException→THROW:new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe)→EXIT→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG:LOG.DEBUG: Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "c841ccaf_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: Getting the file status for {filePath} → CALL: getFileStatusInternal → IF_TRUE: meta != null → IF_TRUE: meta.isDirectory() → LOG: Path {filePath} is a folder. → CALL: conditionalRedoFolderRename → ENTRY → IF_FALSE: filePath.getName().equals(\"\") → IF_FALSE: existsInternal(absoluteRenamePendingFile) → RETURN → EXIT → IF_FALSE: meta.isDirectory() → LOG: Found the path: {filePath} as a file. → CALL: updateFileStatusPath → RETURN → EXIT → CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {} → THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "<log>[DEBUG] Getting the file status for {filePath}</log> <log>[DEBUG] Path {filePath} is a folder.</log> <log>[DEBUG] Found the path: {filePath} as a file.</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "c841ccaf_2": {
    "exec_flow": "ENTRY → IF_TRUE: client != null → IF_FALSE: !client.isConnected() → CALL: logout → CALL: disconnect → IF_TRUE: !logoutSuccess → LOG: LOG.WARN: Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code -</log>"
  },
  "c841ccaf_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL:statIncrement → CALL:makeQualified → TRY → CALL:abfsStore.getFileStatus → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "89aa6d6f_1": {
    "exec_flow": "ENTRY → CALL:SkylineStore:getEstimation → IF_TRUE:result==null → CALL:SkylineStore:getHistory → ENTRY → CALL:inputValidator.validate → CALL:readLock.lock → TRY → IF_FALSE:pipelineId.equals(\"*\") → IF_TRUE:runId.equals(\"*\") → FOREACH:skylineStore.entrySet() → FOREACH_EXIT → IF_FALSE:result.size() > 0 → LOG:LOGGER.WARN:Trying to getHistory non-existing resource skylines for {}. → RETURN → EXIT",
    "log": "[WARN] Trying to getHistory non-existing resource skylines for {}."
  },
  "89aa6d6f_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:validate → IF_TRUE: (jobHistory == null) || (jobHistory.size() == 0) → CALL:org.slf4j.Logger:error(java.lang.String) → THROW: new InvalidInputException(\"Job ResourceSkyline history\", \"invalid\") → EXIT → IF_FALSE: (jobHistory == null) || (jobHistory.size() == 0) → IF_TRUE: timeInterval ≤ 0 → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Object) → THROW: new InvalidInputException(\"Solver timeInterval\", \"non-positive\") → EXIT → CALL:org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:aggregateSkylines → CALL:getJobLen → CALL:org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:getDiscreteSkyline → CALL:generateOverAllocationConstraints → CALL:generateUnderAllocationConstraints → CALL:generateObjective → CALL:minimise → CALL:org.apache.hadoop.yarn.api.records.Resource:newInstance → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation:addInterval → CALL:predictionSkylineStore:addEstimation → EXIT",
    "log": "[ERROR] Job resource skyline history is invalid, please try again with valid resource skyline history. [ERROR] Solver timeInterval {} is invalid, please specify a positive value., timeInterval [DEBUG] time interval: {}, container: {}."
  },
  "89aa6d6f_3": {
    "exec_flow": "ENTRY → FOREACH: jobHistory.entrySet() → CALL:mergeSkyline → FOREACH_EXIT → IF_TRUE: numJobs < minJobRuns → LOG: LOGGER.ERROR: Solver requires job resource skyline history for at least {} runs, but it only receives history info for {} runs., minJobRuns, numJobs → THROW: new InvalidInputException(\"Job ResourceSkyline history\", \"containing less job runs\" + \" than \" + minJobRuns) → EXIT",
    "log": "[ERROR] Solver requires job resource skyline history for at least {} runs, but it only receives history info for {} runs."
  },
  "6383a49c_1": {
    "exec_flow": "ENTRY -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CALL: Subject.doAs -> RETURN -> EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "ede31a95_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→TRY→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}]→CALL:rmProxy.getRMAddress→CALL:getProxy→EXCEPTION:IOException→CALL:Logger:error→EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [ERROR] Unable to create proxy to the ResourceManager"
  },
  "886865be_1": {
    "exec_flow": "<seq> ENTRY→CALL:get→CALL:AccessControlList:<init>→RETURN→EXIT </seq>",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[DEBUG] Creating new Groups object</log>"
  },
  "3706ef36_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_TRUE:meta.isDirectory()→LOG:LOG.DEBUG:Path: [{}] is a directory. COS key: [{}]→RETURN→EXIT",
    "log": "[DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "3706ef36_2": {
    "exec_flow": "ENTRY→DO_WHILE→TRY→CALL:getFileStatus→THROW:FileNotFoundException→CALL:debug→EXIT",
    "log": "[DEBUG] The Path: [{}] does not exist."
  },
  "a223487f_1": {
    "exec_flow": "<call_chain> <virtual_call> <entry>org.apache.hadoop.hdfs.server.mover.Mover$Processor:chooseTarget(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List,org.apache.hadoop.hdfs.server.balancer.Matcher)</entry> <exec_path> ENTRY → IF_TRUE: moveExecutor == null → IF_TRUE: nThreads > 0 → CALL: initMoveExecutor → IF_TRUE: moveExecutor == null → LOG: [WARN] No mover threads available: skip moving + p → CALL: targetDn.removePendingBlock → CALL: p.proxySource.removePendingBlock → RETURN → EXIT </exec_path> </virtual_call> <entry>org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoveReplica(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List)</entry> <exec_path> ENTRY→IF_TRUE: chooseTargetInSameNode(db, source, targetTypes)→RETURN→EXIT </exec_path> <entry>org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:addPendingMove</entry> <virtual_call> org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup:access$3 </virtual_call> <entry>org.apache.hadoop.hdfs.server.balancer.Dispatcher:executePendingMove</entry> <exec_path> IF_TRUE: moveExecutor == null → IF_TRUE: nThreads > 0 → CALL: initMoveExecutor → IF_TRUE: moveExecutor == null → LOG: [WARN] No mover threads available: skip moving + p → CALL: targetDn.removePendingBlock → CALL: p.proxySource.removePendingBlock → RETURN → EXIT </exec_path> </call_chain>",
    "log": "[WARN] No mover threads available: skip moving + p"
  },
  "3818ffaa_1": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_TRUE: !excludedNodes.contains(ret)→LOG: LOG.DEBUG: Chosen node {} from first random, ret→RETURN→EXIT",
    "log": "[DEBUG] Choosing data node [DEBUG] nthValidToReturn is {} [DEBUG] Chosen node {} from first random [INFO] Chosen node: ..."
  },
  "3818ffaa_2": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_FALSE: !excludedNodes.contains(ret)→FOR_INIT→FOR_COND: i < totalInScopeNodes→FOR_EXIT→IF_TRUE: ret == null && lastValidNode != null→LOG: LOG.ERROR: BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}→RETURN→EXIT",
    "log": "[DEBUG] Choosing data node [DEBUG] nthValidToReturn is {} [ERROR] BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={} [INFO] Chosen node: ..."
  },
  "3818ffaa_3": {
    "exec_flow": "ENTRY→IF_FALSE: totalInScopeNodes < availableNodes→IF_FALSE: excludedNodes == null || excludedNodes.isEmpty()→LOG: LOG.DEBUG: nthValidToReturn is {}, nthValidToReturn→IF_FALSE: !excludedNodes.contains(ret)→FOR_INIT→FOR_COND: i < totalInScopeNodes→FOR_EXIT→IF_FALSE: ret == null && lastValidNode != null→RETURN→EXIT",
    "log": "[DEBUG] Choosing data node [DEBUG] nthValidToReturn is {} [INFO] Chosen node: ..."
  },
  "fd2fcb1f_1": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:writeLock.lock → TRY → WHILE → CALL:swapContainer → CALL:removeFromOutstandingUpdate → CALL:updateContainerAndNMToken → TRY → LOG: Error trying to assign container token and NM token to an updated container → CALL: stateMachine.doTransition → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: Can't handle this event at current state, e → LOG: Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState → IF_TRUE: oldState != getState() → LOG: nodeId + Node Transitioned from + oldState + to + getState() → CALL: writeLock.unlock → WHILE_EXIT → WHILE → CALL:asyncContainerRelease → LOG: Processing {} of type {}, event.getNodeId(), event.getType() → CALL: this.rmContext.getDispatcher → CALL: getEventHandler → CALL: org.apache.hadoop.yarn.event.EventHandler:handle → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: Can't handle this event at current state, e → LOG: Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState → IF_TRUE: oldState != getState() → LOG: nodeId + Node Transitioned from + oldState + to + getState() → CALL: writeLock.unlock → WHILE_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Processing {} of type {} [ERROR] Can't handle this event at current state [ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState [INFO] nodeId + Node Transitioned from + oldState + to + getState() [ERROR] Error trying to assign container token and NM token to an updated container"
  },
  "9c118f0e_1": {
    "exec_flow": "ENTRY→TRY→CALL:DelegationTokenRenewerRunnable→CALL:DelegationTokenRenewer.this.handleAppSubmitEvent→CATCH:Throwable t→LOG:LOG.WARN:Unable to add the application to the delegation token renewer., t→EXIT",
    "log": "[WARN] Unable to add the application to the delegation token renewer."
  },
  "30f9b110_1": {
    "exec_flow": "ENTRY → FOREACH: runningContainers.values() → FOREACH_EXIT → SYNC: amContainerList → FOREACH: amContainerList → CALL: csList.add → SYNC: completedContainerList → FOREACH: completedContainerList → LOG: LOG.DEBUG: NodeManager {} completed container ({})., node.getNodeID(), cId → CALL: org.slf4j.Logger:debug → SYNC: releasedContainerList → FOREACH: releasedContainerList → LOG: LOG.DEBUG: NodeManager {} released container ({})., node.getNodeID(), cId → RETURN → SYNC: completedContainerList → FOREACH: completedContainerList → LOG: LOG.DEBUG: Container {} has completed, cs.getId() → CALL: nodeHeartbeat → LOG: LOG.DEBUG: NodeManager {} releases an AM ({})., node.getNodeID(), containerId or LOG: LOG.DEBUG: NodeManager {} releases a container ({})., node.getNodeID(), containerId → EXIT",
    "log": "[DEBUG] NodeManager {} completed container ({}). [DEBUG] NodeManager {} released container ({}). [DEBUG] Container {} has completed. [DEBUG] NodeManager {} releases an AM ({}) or [DEBUG] NodeManager {} releases a container ({})."
  },
  "30f9b110_2": {
    "exec_flow": "ENTRY → IF_TRUE:isAMContainerRunning → CALL:processResponseQueue → CALL:sendContainerRequest → SYNC:Token.class → CALL:getClassForIdentifier → WHILE:tokenIdentifiers.hasNext() → WHILE_COND:tokenIdentifiers.hasNext() → CALL:ServiceLoader.load → CALL:tokenIdentifiers.next → LOG:LOG.DEBUG:Added {id.getKind()}:{id.getClass()} into tokenKindMap → WHILE_EXIT → CALL:get → IF_FALSE:cls == null → CALL:newInstance → CALL:tokenIdentifier.readFields → CALL:close → CALL:checkStop → EXIT",
    "log": "[DEBUG] Added {id.getKind()}:{id.getClass()} into tokenKindMap"
  },
  "30f9b110_3": {
    "exec_flow": "ENTRY → IF_TRUE:!responseQueue.isEmpty() → CALL:processResponseQueue → CALL:responseQueue.take() → IF:!response.getCompletedContainersStatuses().isEmpty() → FOR:response.getCompletedContainersStatuses() → IF_TRUE:assignedStreams.containsKey(containerId) → CALL:LOG.debug → CALL:pendingStreams.add → ELSE IF:amContainer.getId().equals(containerId) → IF:cs.getExitStatus()==ContainerExitStatus.SUCCESS → CALL:LOG.info → ELSE → CALL:LOG.info → IF_FALSE → WHILE_EXIT → EXIT",
    "log": "[DEBUG] Application {} has one streamer finished ({}). [INFO] Application {} goes to finish. [INFO] Application {}'s AM is going to be killed. Waiting for rescheduling..."
  },
  "bef42d36_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → IF_TRUE: conf == null → THROW: ServiceStateException(\"Cannot initialize service \" + getName() + \": null configuration\") → IF_FALSE: conf == null → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → SET: config = conf → TRY → CALL: serviceInit(config) → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners() → IF_FALSE: isInState(STATE.INITED) → EXIT → IF_FALSE: enterState(STATE.INITED) != STATE.INITED → EXIT → CATCH: Exception e → CALL: noteFailure(e) → CALL: ServiceOperations.stopQuietly(LOG, this) → THROW: ServiceStateException.convert(e)",
    "log": "<!-- Merged log sequence --> <log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <location>org.apache.hadoop.service.AbstractService:noteFailure</location> <level>INFO</level> <message>Service {} failed in state {}</message> <parameters>getName(), failureState, exception</parameters> </log>"
  },
  "bef42d36_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → IF_TRUE: conf == null → THROW: ServiceStateException(\"Cannot initialize service \" + getName() + \": null configuration\") → IF_FALSE: conf == null → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.INITED) != STATE.INITED → EXIT → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<!-- Merged log sequence --> <log> <location>org.apache.hadoop.service.AbstractService:notifyListeners</location> <level>WARN</level> <message>Exception while notifying listeners of {}</message> <parameters>this, e</parameters> </log>"
  },
  "5e4b3ef2_1": {
    "exec_flow": "ENTRY→LOG:LOG.INFO:createStartupShutdownMessage→IF_FALSE:SystemUtils.IS_OS_UNIX→CALL:ShutdownHookManager.get().addShutdownHook→CALL:LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log> <level>INFO</level> <template>createStartupShutdownMessage</template> </log> <log> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log> <log> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log>"
  },
  "5e4b3ef2_2": {
    "exec_flow": "ENTRY → CALL:Thread.setDefaultUncaughtExceptionHandler → CALL:StringUtils.startupShutdownMessage → TRY → NEW:TimelineReaderServer → CALL:ShutdownHookManager.get().addShutdownHook → CALL:timelineReaderServer.init → ENTRY → IF_FALSE:conf == null → IF_FALSE:isInState(STATE.INITED) → SYNC:stateChangeLock → IF_TRUE:enterState(STATE.INITED) != STATE.INITED → TRY → CALL:serviceInit → IF_TRUE:isInState(STATE.INITED) → ENTRY → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL:globalListeners.notifyListeners → CATCH:Throwable e → LOG:[WARN] Exception while notifying listeners of this, e → EXIT → CALL:timelineReaderServer.start() → ENTRY → IF_FALSE:isInState(STATE.STARTED) → SYNC:stateChangeLock → IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL:currentTimeMillis → CALL:serviceStart → IF_TRUE:isInState(STATE.STARTED) → CALL:debug → CALL:notifyListeners → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL:globalListeners.notifyListeners → EXIT → RETURN → EXIT",
    "log": "<log> <level>ERROR</level> <template>Error starting TimelineReaderWebServer</template> </log> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "5e4b3ef2_3": {
    "exec_flow": "ENTRY → CALL:Thread.setDefaultUncaughtExceptionHandler → CALL:StringUtils.startupShutdownMessage → TRY → NEW:TimelineReaderServer → CALL:ShutdownHookManager.get().addShutdownHook → CALL:timelineReaderServer.init → ENTRY → IF_FALSE:conf == null → IF_FALSE:isInState(STATE.INITED) → SYNC:stateChangeLock → IF_TRUE:enterState(STATE.INITED) != STATE.INITED → ENTRY → IF_TRUE:oldState != newState → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL:recordLifecycleEvent → RETURN → EXIT → TRY → CALL:serviceInit → IF_TRUE:isInState(STATE.INITED) → ENTRY → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL:globalListeners.notifyListeners → CATCH:Throwable e → LOG:[WARN] Exception while notifying listeners of this, e → EXIT → CALL:timelineReaderServer.start() → EXCEPTION:serviceStart → CATCH:Exception → CALL:noteFailure → CALL:debug → CALL:ServiceOperations.stopQuietly → THROW:ServiceStateException.convert(e) → EXIT",
    "log": "<log> <level>WARN</level> <template>When stopping the service {}</template> </log> <log> <level>INFO</level> <template>Service {} failed in state {}</template> </log> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "7de21d5d_1": {
    "exec_flow": "ENTRY → LOG:Initializing S3AFileSystem for {} → IF_TRUE:LOG.isTraceEnabled() → LOG:Filesystem for {} created; fs.s3a.impl.disable.cache = {} → CALL:propagateBucketOptions → CALL:ProviderUtils.excludeIncompatibleCredentialProviders → IF_TRUE:delegationTokensEnabled → LOG:Using delegation tokens → IF_TRUE:!configuredArn.isEmpty() → LOG:Using AccessPoint ARN \"{}\" for bucket {} → ELSE_IF:conf.getBoolean(AWS_S3_ACCESSPOINT_REQUIRED, false) → LOG:Access Point usage is required because \"{}\" is enabled, but not configured for the bucket: {} → THROW:PathIOException → CALL:setUri → CALL:setConf → IF_TRUE:delegationTokensEnabled → LOG:Client Side Encryption enabled: {} → CALL:invoke → CALL:createAWSCredentialProvider → IF_TRUE:accessPoint != null && useListV1 → LOG:!V1 list configured in fs.s3a.list.version. This is not supported in by access points. Upgrading to V2 → CALL:checkNoS3Guard → IF_TRUE:!blockOutputEnabled → LOG:The \"slow\" output stream is no longer supported → CALL:bindAWSClient → CALL:initTransferManager → IF_TRUE:owner == null → LOG:Using current user as owner → FOREACH:deleteException.getErrors() → LOG:Bulk delete operation failed to delete all objects; failure count = {errors.size()} → FOREACH_EXIT → IF_TRUE:ACCESS_DENIED.equals(exitCode) → CALL:initCause → NEW:AccessDeniedException → CALL:toString → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Initializing S3AFileSystem for {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Filesystem for {} created; fs.s3a.impl.disable.cache = {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Using delegation tokens</template> </log_entry> <log_entry> <level>INFO</level> <template>Using AccessPoint ARN \"{}\" for bucket {}</template> </log_entry> <log_entry> <level>WARN</level> <template>Access Point usage is required because \"{}\" is enabled, but not configured for the bucket: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Client Side Encryption enabled: {}</template> </log_entry> <log_entry> <level>WARN</level> <template>The \"slow\" output stream is no longer supported</template> </log_entry> <log_entry> <level>INFO</level> <template>Bulk delete operation failed to delete all objects; failure count = {errors.size()}</template> </log_entry>"
  },
  "7de21d5d_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for {item} → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "7de21d5d_3": {
    "exec_flow": "ENTRY → LOG:Stripping trailing '/' from {q} → CALL:makeQualified → IF:!q.isRoot() → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for {item} → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Stripping trailing '/' from {q}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "7de21d5d_4": {
    "exec_flow": "ENTRY→IF_TRUE: transfers != null→TRY→CALL:shutdownNow→CALL:HadoopExecutors.shutdown→CALL:HadoopExecutors.shutdown→CALL:HadoopExecutors.shutdown→IF_TRUE:futurePool != null→CALL:futurePool.shutdown→LOG:cleanupWithLogger→CALL:closeAutocloseables→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>When shutting down</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Gracefully shutting down executor service {}. Waiting max {} {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Executor service has not shutdown yet. Forcing. Will wait up to an additional {} {} for shutdown</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Succesfully shutdown executor service</template> </log_entry> <log_entry> <level>ERROR</level> <template>Unable to shutdown executor service after timeout {} {}</template> </log_entry> <log_entry> <level>ERROR</level> <template>Interrupted while attempting to shutdown</template> </log_entry>"
  },
  "7de21d5d_5": {
    "exec_flow": "ENTRY→IF_TRUE: log == null→FOREACH: closeables→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→TRY→CALL:java.lang.AutoCloseable:close→CATCH→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Closing {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "7de21d5d_6": {
    "exec_flow": "ENTRY→IF_FALSE: log == null→FOREACH: closeables→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→TRY→CALL:java.lang.AutoCloseable:close→CATCH→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Closing {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "ba7dad69_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_TRUE: elapsedMs >= slowLookupThresholdMs→CALL: org.apache.hadoop.conf.Configuration:setSocketAddr→CALL: org.slf4j.Logger:warn(java.lang.String)→IF_FALSE: staticHost != null→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "ba7dad69_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_TRUE: elapsedMs >= slowLookupThresholdMs→CALL: org.apache.hadoop.conf.Configuration:setSocketAddr→CALL: org.slf4j.Logger:warn(java.lang.String)→IF_TRUE: staticHost != null→CALL: getByAddress→CALL: getAddress→CALL: getAddress→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "ba7dad69_3": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_FALSE: elapsedMs >= slowLookupThresholdMs→IF_TRUE: LOG.isTraceEnabled()→CALL: org.apache.hadoop.conf.Configuration:setSocketAddr→CALL: org.slf4j.Logger:trace(java.lang.String)→IF_TRUE: staticHost != null→CALL: getByAddress→CALL: getAddress→CALL: getAddress→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[TRACE] Name lookup for + hostname + took + elapsedMs + ms."
  },
  "ba7dad69_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.security.SecurityUtil:getByName→IF_TRUE: logSlowLookups || LOG.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→IF_FALSE: elapsedMs >= slowLookupThresholdMs→IF_TRUE: LOG.isTraceEnabled()→CALL: org.apache.hadoop.conf.Configuration:setSocketAddr→CALL: org.slf4j.Logger:trace(java.lang.String)→IF_FALSE: staticHost != null→NEW: InetSocketAddress→RETURN→EXIT",
    "log": "[TRACE] Name lookup for + hostname + took + elapsedMs + ms."
  },
  "4b249edd_1": {
    "exec_flow": "ENTRY→CALL:setBlockPoolId→CALL:load→IF_FALSE:expectedMd5 != null && !expectedMd5.equals(readImageMd5)→LOG:LOG.INFO:Loaded image for txid + txId + from + curFile→CALL:setMostRecentCheckpointInfo→EXIT",
    "log": "[INFO] Loaded image for txid + txId + from + curFile"
  },
  "4b249edd_2": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkState→TRY→CALL:newInputStream→CALL:toPath→CALL:toPath→CALL:IOUtils.readFully→IF_TRUE:Arrays.equals(magic, FSImageUtil.MAGIC_HEADER)→CALL:FSImageFormatProtobuf.Loader:load→CALL:IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Loading using Protobuf Loader [DEBUG] Exception in closing {} [INFO] Loaded FSImage in {} seconds."
  },
  "4b249edd_3": {
    "exec_flow": "ENTRY→IF_FALSE: !md5File.exists()→CALL:readStoredMd5→TRY→CALL:readLine→IF_TRUE:md5Line==null→CALL:trim→CALL:IOUtils.cleanupWithLogger→IF_TRUE:!matcher.matches()→THROW:new IOException(\"Invalid MD5 file \" + md5File + \": the content \\\"\" + md5Line + \"\\\" does not match the expected pattern.\")→EXIT",
    "log": "[ERROR] Error reading md5 file at {md5File}"
  },
  "4b249edd_4": {
    "exec_flow": "ENTRY→IF_FALSE: !md5File.exists()→CALL:readStoredMd5→TRY→CALL:readLine→IF_TRUE:md5Line==null→CALL:trim→CALL:IOUtils.cleanupWithLogger→IF_FALSE:!matcher.matches()→RETURN→EXIT",
    "log": "[INFO] Matcher matches and returns result"
  },
  "a71283fc_1": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: fileName == null || fileName.trim().length() == 0 → LOG:The machine list file path is not specified in the configuration → TRY → CALL:get → TRY → CALL:newBufferedReader → WHILE → LOG:Loading node into resolver: {nodeName} --> {subClusterId} → LOG:Loading rack into resolver: {rackName} --> {subClusterId} → CALL:readLine → ELSE → LOG:Skipping malformed line in machine list: {line} → CLOSE → LOG:Successfully loaded file {fileName} → LOG:Handling deprecation for all properties in config... → LOG:Handling deprecation for (String)item → EXIT",
    "log": "<log_entry>[INFO] The machine list file path is not specified in the configuration</log_entry> <log_entry>[ERROR] Failed to parse file fileName</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] Successfully loaded file {fileName}</log_entry> <log_entry>[DEBUG] Loading node into resolver: {nodeName} --> {subClusterId}</log_entry> <log_entry>[DEBUG] Loading rack into resolver: {rackName} --> {subClusterId}</log_entry> <log_entry>[WARN] Skipping malformed line in machine list: {line}</log_entry>"
  },
  "7e6d1552_1": {
    "exec_flow": "ENTRY→TRY→CALL:readTotalProcessJiffies→CALL:cpuTimeTracker.updateElapsedJiffies→EXCEPTION:updateElapsedJiffies→CATCH:YarnException e→LOG:LOG.WARN:Failed to parse + pid, e→CALL:getMemorySize→IF_TRUE:memswStat.exists()→CALL:getMemorySize→EXIT",
    "log": "[WARN] Failed to parse + pid, e"
  },
  "7e6d1552_2": {
    "exec_flow": "ENTRY→CALL:ProcfsBasedProcessTree:updateProcessTree→CALL:CGroupsResourceCalculator:updateProcessTree→TRY→CALL:readTotalProcessJiffies→CALL:cpuTimeTracker.updateElapsedJiffies→EXCEPTION:updateElapsedJiffies→CATCH:YarnException e→LOG:LOG.WARN:Failed to parse + pid, e→CALL:getMemorySize→IF_FALSE:memswStat.exists()→LOG:LOG.DEBUG:Swap cgroups monitoring is not compiled into the kernel {}, memswStat.getAbsolutePath()→EXIT",
    "log": "[WARN] Failed to parse + pid, e [DEBUG] Swap cgroups monitoring is not compiled into the kernel {}"
  },
  "7e6d1552_3": {
    "exec_flow": "ENTRY→IF_TRUE: taskProcessId != null→CALL:org.apache.hadoop.yarn.util.WindowsBasedProcessTree:getAllProcessInfoFromShell()→EXCEPTION:execute→CATCH:IOException e→LOG:LOG.ERROR:StringUtils.stringifyException(e)→CALL: processTree.clear→EXIT",
    "log": "LOG.error(StringUtils.stringifyException(e))"
  },
  "52cbc62b_1": {
    "exec_flow": "ENTRY→CALL:requestContainerUpdate→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkNotNull→LOG:LOG.INFO:Requesting Container update : + container= + container + , + updateType= + updateContainerRequest.getContainerUpdateType() + , + targetCapability= + updateContainerRequest.getCapability() + , + targetExecType= + updateContainerRequest.getExecutionType→IF_TRUE:updateContainerRequest.getCapability() != null && updateContainerRequest.getExecutionType() == null→CALL:validateContainerResourceChangeRequest→CALL:org.apache.hadoop.yarn.api.records.Container:getResource()→IF_TRUE:change.get(container.getId()) == null→CALL:put→IF_TRUE:pendingChange.get(container.getId()) == null→CALL:pendingChange.put→EXIT",
    "log": "[INFO] Requesting Container update : container=<container>, updateType=<updateType>, targetCapability=<targetCapability>, targetExecType=<targetExecType>"
  },
  "52cbc62b_2": {
    "exec_flow": "ENTRY→CALL:requestContainerUpdate→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkNotNull→LOG:LOG.INFO:Requesting Container update : + container= + container + , + updateType= + updateContainerRequest.getContainerUpdateType() + , + targetCapability= + updateContainerRequest.getCapability() + , + targetExecType= + updateContainerRequest.getExecutionType→IF_FALSE:updateContainerRequest.getCapability() != null && updateContainerRequest.getExecutionType() == null→IF_TRUE:updateContainerRequest.getExecutionType() != null && updateContainerRequest.getCapability() == null→CALL:validateContainerExecTypeChangeRequest→CALL:org.apache.hadoop.yarn.api.records.UpdateContainerRequest:getCapability()→IF_TRUE:change.get(container.getId()) == null→CALL:put→IF_FALSE:pendingChange.get(container.getId()) == null→CALL:pendingChange.get(container.getId()).setValue→EXIT",
    "log": "[INFO] Requesting Container update : container=<container>, updateType=<updateType>, targetCapability=<targetCapability>, targetExecType=<targetExecType>"
  },
  "57f4e675_1": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>CALL:</step> <step>LOG:[INFO] createStartupShutdownMessage(classname, hostname, args)</step> <step>IF_FALSE:SystemUtils.IS_OS_UNIX</step> <step>CALL:ShutdownHookManager.get().addShutdownHook</step> <step>LOG:secureLogin</step> <step>CALL:getNameNodeConnector</step> <step>CALL:init</step> <step>CALL:start</step> <step>IF_TRUE:sps != null</step> <step>CALL:join</step> <step>IF_TRUE:nnc != null</step> <step>CALL:close</step> <step>CATCH:IOException</step> <step>LOG_WARN:Failed to delete {idPath}</step> <step>END_CATCH</step> <step>EXIT</step>",
    "log": "<log>[INFO] createStartupShutdownMessage(classname, hostname, args)</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Failed to delete {idPath}</log>"
  },
  "57f4e675_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.security.UserGroupInformation:setConfiguration</step> <step>CALL:org.apache.hadoop.conf.Configuration:getBoolean</step> <step>DESCRIPTION:Evaluate boolean property from configuration</step> <step>CALL:org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>DESCRIPTION:Retrieve trimmed property value</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>EXIT</step> <step>CALL:Configuration.get</step> <step>CALL:NetUtils.createSocketAddr</step> <step>CALL:SecurityUtil.login</step> <step>ENTRY</step> <step>CALL:org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled</step> <step>IF_FALSE</step> <step>CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String)</step> <step>IF_FALSE</step> <step>CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)</step> <step>CALL:org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)</step> <step>CALL:UserGroupInformation.loginUserFromKeytab</step> <step>IF_FALSE:!isSecurityEnabled</step> <step>CALL:loginUserFromKeytabAndReturnUGI</step> <step>IF_FALSE:isKerberosKeyTabLoginRenewalEnabled</step> <step>CALL:setLoginUser</step> <step>CALL:LOG.INFO:Login successful for user {} using keytab file {}. Keytab auto renewal enabled : {}</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Login successful for user {} using keytab file {}. Keytab auto renewal enabled : {}</log>"
  },
  "57f4e675_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: serviceMode == StoragePolicySatisfierMode.NONE</step> <step>LOG: INFO: Starting {} StoragePolicySatisfier., StringUtils.toLowerCase(serviceMode.toString())</step> <step>NEW: Daemon</step> <step>CALL: storagePolicySatisfierThread.setName</step> <step>CALL: storagePolicySatisfierThread.start</step> <step>CALL: this.storageMovementsMonitor.start</step> <step>CALL: this.storageMovementNeeded.activate</step> <step>LOG: Handling deprecation for all properties in config...</step> <step>CALL: getProps</step> <step>CALL: addAll</step> <step>FOREACH: keys</step> <step>LOG: Handling deprecation for (String)item</step> <step>CALL: handleDeprecation</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[INFO] Starting {} StoragePolicySatisfier.</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "8fe66893_1": {
    "exec_flow": "ENTRY → CALL:makeQualified → CALL:getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL:getDelegationToken → IF_TRUE: token != null → CALL:authParams.add → CALL:encodeToUrlString → CALL:toArray → RETURN → EXIT → CALL:getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT",
    "log": "[DEBUG] Delegation token encoded [INFO] Returning authentication parameters [TRACE] url={}"
  },
  "6cd008c2_1": {
    "exec_flow": "ENTRY→FOREACH:before.entrySet()→FOREACH_EXIT→FOREACH:after.entrySet()→FOREACH_EXIT→FOREACH:allNMs→IF_TRUE:(oldNM = getNMInNodeSet(nodeId, before, true)) != null→IF_FALSE:oldLabels.isEmpty()→FOREACH:oldLabels→CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:removeNode→CALL:org.apache.hadoop.yarn.util.resource.Resources:subtractFrom→FOREACH_EXIT→CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:addNode→CALL:org.apache.hadoop.yarn.util.resource.Resources:addTo→FOREACH_EXIT→IF_TRUE:rmContext != null && rmContext.getDispatcher() != null→CALL:org.apache.hadoop.yarn.event.EventHandler:handle→LOG: [INFO] Resource mappings updated→EXIT",
    "log": "[INFO] Resource mappings updated"
  },
  "6cd008c2_2": {
    "exec_flow": "ENTRY→FOREACH:before.entrySet()→FOREACH_EXIT→FOREACH:after.entrySet()→FOREACH_EXIT→FOREACH:allNMs→FOREACH_EXIT→IF_TRUE:rmContext != null && rmContext.getDispatcher() != null→CALL:org.apache.hadoop.yarn.event.EventHandler:handle→LOG: [INFO] Resource mappings updated→EXIT",
    "log": "[INFO] Resource mappings updated"
  },
  "6cd008c2_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.resource.Resources:subtractFrom→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "fbc7ec9e_1": {
    "exec_flow": "ENTRY→FOREACH: args→TRY→CALL:PathData:expandAsGlob→CALL:glob→NEW:Globber→CALL:build→EXCEPTION→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "fbc7ec9e_2": {
    "exec_flow": "ENTRY→FOREACH: args→TRY→CALL:PathData:expandAsGlob→CALL:glob→NEW:Globber→CALL:build→EXCEPTION→CALL:displayError→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String)→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "0deaa34c_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:(overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()) → TRY → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → LOG:Handling deprecation for all properties in config... → CATCH:IOException → THROW:new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe) → EXIT → IF_TRUE:subject==null||subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → CALL:debug → RETURN → EXIT",
    "log": "<log_entry> <level>WARN</level> <template>User {user.getShortUserName()} doesn't have permission to call '{method}'</template> </log_entry> <log_entry> <level>WARN</level> <template>Couldn't get current user</template> </log_entry> <log_entry> <level>INFO</level> <template>HS Admin: {method} invoked by user {user.getShortUserName()}</template> </log_entry> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[LOG] getLoginUser</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log>"
  },
  "4aad136f_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Listing status for {}, f.toString()→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: !meta.isDirectory()→CALL: listWithErrorHandling→IF_FALSE: renamed→CALL: conditionalRedoFolderRenames→IF_TRUE: key.equals(\"/\")→FOREACH: listing→IF_TRUE: fileMetadata.isDirectory()→IF_TRUE: fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)→CONTINUE→EXIT",
    "log": "<log> <template>[DEBUG] Listing status for {f}</template> </log>"
  },
  "4aad136f_2": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta==null→LOG:DEBUG: List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0→LOG.isDebugEnabled()→LOG:DEBUG: Path: [{}] is a directory. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "<log> <template>[DEBUG] List COS key: [{}] to check the existence of the path.</template> </log> <log> <template>[DEBUG] Path: [{}] is a directory. COS key: [{}]</template> </log>"
  },
  "4aad136f_3": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:DEBUG: Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log> <template>[DEBUG] Path: [{}] is a file. COS key: [{}]</template> </log>"
  },
  "4aad136f_4": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT→CALL:listStatus→RETURN",
    "log": "<log> <template>[DEBUG] Handling deprecation for all properties in config...</template> </log> <log> <template>[DEBUG] Handling deprecation for (String)item</template> </log> <log> <template>[INFO] message</template> </log>"
  },
  "460c8550_1": {
    "exec_flow": "ENTRY → CALL: org.slf4j.Logger:debug → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → ENTRY(openFileForWrite) → TRY → LOG: LOG.DEBUG: openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), qualifiedPath, false → CALL: startTracking → CALL: client.getPathStatus → CALL: perfInfo.registerResult → IF_TRUE: parseIsDirectory(resourceType) → THROW: new AbfsRestOperationException → CALL: perfInfo.close(openFileForWrite) → EXCEPTION → CALL: checkException → RETURN → EXIT",
    "log": "[DEBUG] Opening file: {} for append [DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize} [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), qualifiedPath, false"
  },
  "5d7d0052_1": {
    "exec_flow": "ENTRY → TRY → CALL:setLogEnabled → CALL:getObjectMetadata(request) → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT",
    "log": "<log> <statement>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</statement> </log>"
  },
  "5d7d0052_2": {
    "exec_flow": "Get file status path input → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → TRY → CALL: makeQualified → CALL: abfsStore.getFileStatus → CATCH: AzureBlobFileSystemException → CALL: checkException → EXIT",
    "log": "<log> <statement>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</statement> </log>"
  },
  "5d7d0052_3": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isDirectory() → LOG:LOG.DEBUG:Found the path: {} as a file., f.toString() → CALL:updateFileStatusPath → RETURN → EXIT",
    "log": "<log> <statement>LOG.debug(\"Getting the file status for {}\", f.toString())</statement> </log> <log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log>"
  },
  "7615563f_1": {
    "exec_flow": "ENTRY→CALL:validatePath→CALL:zkList→CALL:checkServiceLive→TRY→CALL:org.slf4j.Logger:isDebugEnabled→IF_TRUE:org.slf4j.Logger:isDebugEnabled→LOG:org.slf4j.Logger:debug→CALL:GetChildrenBuilder.forPath→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] ls {}"
  },
  "8017953b_1": {
    "exec_flow": "<step>LOG.debug(\"Config has been overridden during init\");</step> <step>setConfig(conf);</step> <step>LOG.debug(\"noteFailure\", exception);</step> <step>if (exception == null) { return; }</step> <step> <sync> <condition>failureCause == null</condition> <steps> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> </steps> </sync> </step> <step>Service: {getName()} entered state {getServiceState()}</step> <step>LOG.warn(\"Exception while notifying listeners of {}\", this, e);</step>",
    "log": "<log> <level>DEBUG</level> <message>Config has been overridden during init</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log> <log> <level>DEBUG</level> <message>Service: {getName()} entered state {getServiceState()}</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {}</message> <exception>{this}</exception> </log>"
  },
  "10356710_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <sequence> <call>org.apache.hadoop.fs.azure.security.RemoteWasbDelegationTokenManager:getDelegationToken</call> <virtual_call>[VIRTUAL_CALL]</virtual_call> <call>org.apache.hadoop.fs.azure.security.TokenUtils:toDelegationToken</call> <virtual_call>[VIRTUAL_CALL]</virtual_call> <call>org.apache.hadoop.fs.azure.security.TokenUtils:toToken</call> </sequence>",
    "log": "<!-- Merged log sequence --> <log> <level>DEBUG</level> <template>Read url string param - {urlString}</template> </log>"
  },
  "10356710_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→TRY→CALL:mapReader→CALL:readValue→CALL:org.slf4j.Logger:debug→CONDITIONAL→CALL:org.slf4j.Logger:error→THROW:IOException→EXIT",
    "log": "<!-- Merged log sequence --> <log> <level>DEBUG</level> <template>JSON Parsing exception: {e.getMessage()} while parsing {jsonString}</template> </log> <log> <level>ERROR</level> <template>Internal Server Error was encountered while making a request</template> </log>"
  },
  "10356710_3": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→CALL:getCurrentUser⟩→IF_TRUE:connectUgi != null→CALL:queryParams.add→IF_TRUE:!alwaysRequiresKerberosAuth && delegationToken != null→CALL:getDelegationToken →ENTRY→IF_TRUE:this.delegationToken == null→FOREACH:userGroupInformation.getTokens()→IF_TRUE:iterToken.getKind().equals(WasbDelegationTokenIdentifier.TOKEN_KIND)→LOG:LOG.DEBUG: {} token found in cache : {}, WasbDelegationTokenIdentifier.TOKEN_KIND, iterToken→BREAK→EXIT→CALL:org.apache.hadoop.security.token.Token:encodeToUrlString()→CALL:queryParams.add→IF_TRUE:delegationToken == null→CALL:connectUgi.checkTGTAndReloginFromKeytab →TRY→CALL:doAs→NEW:PrivilegedExceptionAction⟩→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception()→CALL:Subject.doAs→RETURN→EXIT",
    "log": "<!-- Merged log sequence --> <log> <level>DEBUG</level> <template>{} token found in cache : {} - WasbDelegationTokenIdentifier.TOKEN_KIND, iterToken</template> </log> <log> <level>DEBUG</level> <template>PrivilegedAction [as: {}][action: {}]</template> </log>"
  },
  "10356710_4": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→CALL:ensureInitialized→IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→RETURN→EXIT",
    "log": "<!-- Merged log sequence --> <log> <level>LOG</level> <template>getLoginUser</template> </log>"
  },
  "f8d8d517_1": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[INFO] Operation category checked [DEBUG] Proxying operation: {} [INFO] Invoke single method executed"
  },
  "97f8c761_1": {
    "exec_flow": "ENTRY→FOREACH: journalSyncersById.values()→[VIRTUAL_CALL]→CALL:org.apache.hadoop.fs.FileUtil:fullyDelete→CALL:delete→CALL:deleteImpl→ENTRY→LOG: [WARN] null file argument.→RETURN→EXIT→RETURN→EXIT→FOREACH_EXIT→IF_TRUE: rpcServer != null→CALL: JournalNodeRpcServer:stop()→IF_TRUE: httpServer != null→TRY→CALL: JournalNodeHttpServer:stop()→CATCH: IOException ioe→LOG: [WARN] Unable to stop HTTP server for ...→FOREACH: journalsById.values()→CALL: IOUtils:cleanupWithLogger→FOREACH_EXIT→CALL: DefaultMetricsSystem:shutdown→IF_TRUE: journalNodeInfoBeanName != null→[VIRTUAL_CALL]→MBeans:unregister→EXIT",
    "log": "<log>[WARN] null file argument.</log> <log>[WARN] Unable to stop HTTP server for ...</log> <log>[ERROR] Error while stopping web app context for webapp + webAppContext.getDisplayName(), e</log>"
  },
  "97f8c761_2": {
    "exec_flow": "ENTRY→FOREACH: journalSyncersById.values()→[VIRTUAL_CALL]→CALL:org.apache.hadoop.fs.FileUtil:fullyDelete→CALL:delete→CALL:deleteImpl→ENTRY→IF_FALSE: f == null→IF_FALSE: wasDeleted→IF_TRUE: doLog && ex→LOG: [WARN] Failed to delete file or dir [ + f.getAbsolutePath() + ]: it still exists.→RETURN→EXIT→RETURN→EXIT→FOREACH_EXIT→IF_TRUE: rpcServer != null→CALL: JournalNodeRpcServer:stop()→IF_TRUE: httpServer != null→TRY→CALL: JournalNodeHttpServer:stop()→CATCH: IOException ioe→LOG: [WARN] Unable to stop HTTP server for ...→FOREACH: journalsById.values()→CALL: IOUtils:cleanupWithLogger→FOREACH_EXIT→CALL: DefaultMetricsSystem:shutdown→IF_TRUE: journalNodeInfoBeanName != null→[VIRTUAL_CALL]→MBeans:unregister→EXIT",
    "log": "<log>[WARN] Failed to delete file or dir [ + f.getAbsolutePath() + ]: it still exists.</log> <log>[WARN] Unable to stop HTTP server for ...</log> <log>[ERROR] Error while stopping web app context for webapp + webAppContext.getDisplayName(), e</log>"
  },
  "97f8c761_3": {
    "exec_flow": "ENTRY → IF_FALSE:this.configurationChangeMonitor.isPresent() → FOREACH:listeners → TRY → CALL:ServerConnector.close → CATCH:Exception e → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → CALL:addMultiException → FOREACH_EXIT → TRY → CALL:secretProvider.destroy → CALL:webAppContext.clearAttributes → CALL:webAppContext.stop → TRY → CALL:webServer.stop → IF_TRUE:exception != null → CALL:ifExceptionThrow → EXIT",
    "log": "<log>[ERROR] Error while stopping listener for webapp + webAppContext.getDisplayName(), e</log>"
  },
  "becbb075_1": {
    "exec_flow": "ENTRY → IF_FALSE → CALL: writeLock.lock → TRY → WHILE → CALL: swapContainer → CALL: removeFromOutstandingUpdate → CALL: updateContainerAndNMToken → WHILE_EXIT → WHILE → CALL: asyncContainerRelease → ENTRY → LOG: LOG.DEBUG: Processing {event.getNodeId()} of type {event.getType()} → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getState() → LOG: LOG.INFO: {nodeId} Node Transitioned from {oldState} to {getState()} → CALL: writeLock.unlock → WHILE_EXIT → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Processing {event.getNodeId()} of type {event.getType()} [INFO] {nodeId} Node Transitioned from {oldState} to {getState()}"
  },
  "becbb075_2": {
    "exec_flow": "ENTRY → IF_FALSE → CALL: writeLock.lock → TRY → WHILE → CALL: swapContainer → CALL: removeFromOutstandingUpdate → CALL: updateContainerAndNMToken → WHILE_EXIT → WHILE → CALL: asyncContainerRelease → ENTRY → IF_TRUE: rmApp != null → IF_TRUE: rmAppAttempt != null → TRY → CALL: rmAppAttempt.handle → CALL: handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent) → EXCEPTION: handle → CATCH: Throwable t → LOG: LOG.ERROR: Error in handling event type {event.getType()} for applicationAttempt {appAttemptId} → WHILE_EXIT → RETURN → EXIT → EXIT",
    "log": "[ERROR] Error in handling event type {event.getType()} for applicationAttempt {appAttemptId}"
  },
  "becbb075_3": {
    "exec_flow": "ENTRY → IF_FALSE → CALL: writeLock.lock → TRY → WHILE → CALL: swapContainer → CALL: removeFromOutstandingUpdate → CALL: updateContainerAndNMToken → WHILE_EXIT → WHILE → CALL: asyncContainerRelease → ENTRY → TRY → TRY → CALL: stateMachine.doTransition → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: LOG.ERROR: [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState), e → IF_TRUE: oldState != getState() → LOG: LOG.INFO: [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType() → WRITELOCK_UNLOCK → WHILE_EXIT → RETURN → EXIT → EXIT",
    "log": "[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState [INFO] [COMPONENT {}] Transitioned from {} to {} on {} event."
  },
  "becbb075_4": {
    "exec_flow": "ENTRY → IF_FALSE → CALL: writeLock.lock → TRY → WHILE → CALL: swapContainer → CALL: removeFromOutstandingUpdate → CALL: updateContainerAndNMToken → WHILE_EXIT → WHILE → CALL: asyncContainerRelease → ENTRY → CALL: this.writeLock.lock → TRY → LOG: LOG.DEBUG: Processing {} of type {}, resourcePath, event.getType() → TRY → CALL: doTransition → CALL: getType → CALL: getType → CALL: handle → EXCEPTION: InvalidStateTransitionException → LOG: LOG.ERROR: Can't handle this event at current state → CALL: this.writeLock.unlock → EXIT → WHILE_EXIT → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Processing {} of type {} [ERROR] Can't handle this event at current state"
  },
  "9eac52bb_1": {
    "exec_flow": "ENTRY→CALL:init→CALL:getInterceptorChain→CALL:submitApplication→CALL:getRootInterceptor→CALL:submitApplication→IF_FALSE→TRY→CALL:fromString→FOR_INIT→FOR_COND→TRY→CALL:RMWebAppUtil.createAppSubmissionContext→CALL:policyFacade.getHomeSubcluster→LOG:org.slf4j.Logger.info→CALL:FederationStateStoreFacade.addApplicationHomeSubCluster→TRY→IF_TRUE: flushCache && isCachingEnabled()→LOG:LOG.INFO→CALL:remove→CALL:getSubClusters→CALL:getSubCluster→RETURN→EXIT",
    "log": "[INFO] submitApplication appId {} try #{} on SubCluster {} [INFO] Flushing subClusters from cache and rehydrating from store, most likely on account of RM failover."
  },
  "cd583080_1": {
    "exec_flow": "<context>org.apache.hadoop.service.AbstractService:init</context> <entry>Parent.ENTRY</entry> <flow>IF_FALSE: conf == null</flow> <flow>IF_FALSE: isInState(STATE.INITED)</flow> <sync>SYNC: stateChangeLock</sync> <flow>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</flow> <call>CALL: setConfig</call> <try>TRY</try> <call>CALL: serviceInit</call> <flow>IF_TRUE: isInState(STATE.INITED)</flow> <call>CALL: notifyListeners</call> <exit>EXIT</exit> <entry>VIRTUAL_CALL</entry> <flow>IF_TRUE: service != null</flow> <call>CALL: stop</call> <exit>EXIT</exit> <try>TRY</try> <call>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</call> <call>CALL: globalListeners.notifyListeners</call> <catch>CATCH: Throwable e</catch> <log>LOG: WARN: Exception while notifying listeners of {}, this, e</log> <exit>EXIT</exit>",
    "log": "<log_entry> <log> <level>DEBUG</level> <template>Service: {} entered state {}</template> </log> </log_entry> <log_entry> <log> <level>DEBUG</level> <template>Config has been overridden during init</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>When stopping the service {service_name}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log> </log_entry> <log_entry> <log> <level>DEBUG</level> <template>noteFailure, exception</template> </log> </log_entry> <log_entry> <log> <level>INFO</level> <template>Service {getName()} failed in state {getServiceState()}, exception</template> </log> </log_entry>"
  },
  "cd583080_2": {
    "exec_flow": "<context>org.apache.hadoop.service.AbstractService:init</context> <entry>Parent.ENTRY</entry> <flow>IF_FALSE: conf == null</flow> <flow>IF_FALSE: isInState(STATE.INITED)</flow> <sync>SYNC: stateChangeLock</sync> <flow>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</flow> <call>CALL: setConfig</call> <try>TRY</try> <call>CALL: serviceInit</call> <exception>EXCEPTION: serviceInit</exception> <catch>CATCH: Exception e</catch> <call>CALL: noteFailure</call> <call_chain> <entry>VIRTUAL_CALL</entry> <flow>Child.call</flow> </call_chain> <call>CALL: ServiceOperations.stopQuietly</call> <throw>THROW: ServiceStateException.convert(e)</throw> <exit>EXIT</exit>",
    "log": "<log_entry> <log> <level>DEBUG</level> <template>Service: {} entered state {}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>When stopping the service {service_name}</template> </log> </log_entry>"
  },
  "cd583080_3": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log_entry> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log> </log_entry>"
  },
  "493cd0c5_1": {
    "exec_flow": "ENTRY->IF_FALSE:request.anythingAllocatedOrReserved()->CALL:getApplicationAttemptId->CALL:getSchedulerApplicationAttempt->CALL:get->CALL:getContainersToRelease->LOG:[DEBUG]Try to commit allocation proposal={},request->IF_TRUE:attemptId!=null->IF_TRUE:app!=null&&attemptId.equals(app.getApplicationAttemptId())->IF_TRUE:app.accept(cluster,request,updatePending)&&app.apply(cluster,request,updatePending)->CALL:CapacitySchedulerMetrics.getMetrics().addCommitSuccess->LOG:[DEBUG]Allocation proposal accepted={},proposal={},isSuccess,request->IF_FALSE:updateUnconfirmedAllocatedResource->RETURN->EXIT",
    "log": "[DEBUG] Try to commit allocation proposal={} [DEBUG] Allocation proposal accepted={}"
  },
  "493cd0c5_2": {
    "exec_flow": "ENTRY->CALL:readLock.lock->TRY->IF_TRUE:user==null->LOG:[DEBUG]User {} has been removed!,userName->RETURN->EXIT",
    "log": "[DEBUG] User {} has been removed!, userName"
  },
  "493cd0c5_3": {
    "exec_flow": "ENTRY->IF_FALSE:lhs.equals(rhs)->IF_FALSE:isAllInvalidDivisor(clusterResource)->TRY->EXCEPTION:ArrayIndexOutOfBoundsException->CALL:org.slf4j.Logger:error(java.lang.String)->CALL:org.slf4j.Logger:error(java.lang.String)->THROW:YarnRuntimeException",
    "log": "[ERROR] A problem was encountered while calculating resource availability that should not occur under normal circumstances. Please report this error to the Hadoop community by opening a JIRA ticket at http://issues.apache.org/jira and including the following information: * Exception encountered: <stack_trace> * Cluster resources: <cluster_resources> * LHS resource: <lhs_resources> * RHS resource: <rhs_resources> [ERROR] The resource manager is in an inconsistent state. It is safe for the resource manager to be restarted as the error encountered should be transitive. If high availability is enabled, failing over to a standby resource manager is also safe."
  },
  "ec7dc547_1": {
    "exec_flow": "ENTRY→CALL:Validate.checkNotNull→IF_FALSE:data.stateEqualsOneOf(BufferData.State.PREFETCHING, BufferData.State.CACHING, BufferData.State.DONE)→SYNC:data→CALL:BufferData:throwIfStateIncorrect→CALL:CachingBlockManager:read→ENTRY→SYNC:data→TRY→IF_FALSE:data.stateEqualsOneOf(BufferData.State.DONE, BufferData.State.READY)→CALL:throwIfStateIncorrect→IF_FALSE:cache.containsBlock(blockNumber)→IF_TRUE:isPrefetch→CALL:prefetch→NEW:Operation→CALL:end→IF_TRUE:debugMode→LOG:LOG.INFO:op.getDebugInfo()→RETURN→EXIT→RETURN→EXIT→[VIRTUAL_CALL]→Validate.checkGreater ENTRY→Preconditions.checkArgument",
    "log": "<log>[DEBUG] Checking if blockNumber is negative</log> <log>[INFO] Adding new operation of type PREFETCH</log> <log>[INFO] op.getDebugInfo()</log> <log>[ERROR] error reading block {blockNumber}, {e}</log> <log>[DEBUG] waiting to acquire block: {}</log> <log>[DEBUG] state = {}</log> <log>[WARN] releasing 'ready' block: releaseTarget</log> <log id=\"L1\" class=\"org.apache.hadoop.util.Preconditions\" method=\"checkArgument\" level=\"DEBUG\">\"Error formatting message\"</log>"
  },
  "5a1334aa_1": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Getting the file status for {}, f.toString() → IF_FALSE: key.length() == 0 → TRY → CALL: retrieveMetadata → IF_TRUE: meta != null → IF_FALSE: meta.isDirectory() → LOG: LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: updateFileStatusPath → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Getting the file status for {}, f.toString()</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Found the path: {} as a file., f.toString()</template> </log_entry>"
  },
  "5a1334aa_2": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → TRY → CALL: retrieveMetadata → IF_TRUE: meta != null → IF_TRUE: meta.isDirectory() → LOG: LOG.DEBUG: Path {} is a folder., f.toString() → CALL: conditionalRedoFolderRename → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Path {} is a folder., f.toString()</template> </log_entry>"
  },
  "5a1334aa_3": {
    "exec_flow": "ENTRY → CALL: return new FileStatus(-1, false, 1, DEFAULT_BLOCK_SIZE, 0, makeQualified(path))",
    "log": ""
  },
  "dbaca173_1": {
    "exec_flow": "ENTRY → IF_TRUE:props != null → CALL:loadResources → IF_FALSE:overlay != null → CALL:createKey → CALL:generateKey → CALL:org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey → RETURN → EXIT",
    "log": "[DEBUG] Generating key material [INFO] Key creation with LoadBalancingKMSClientProvider successful"
  },
  "dbaca173_2": {
    "exec_flow": "ENTRY → IF_TRUE:props != null → CALL:loadResources → IF_FALSE:overlay != null → CALL:createKey → CALL:generateKey → CALL:org.apache.hadoop.crypto.key.KeyProviderExtension:createKey → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration [DEBUG] Generating key material [INFO] Key creation with KeyProviderExtension successful"
  },
  "dbaca173_3": {
    "exec_flow": "ENTRY → IF_FALSE:providers.length==0 → FOR_INIT → FOR_COND:TRUE → TRY → CALL:call → CATCH:IOException → CALL:warn → CALL:shouldRetry → IF_TRUE:RetryDecision.FAIL → FOR_EXIT → EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException:"
  },
  "dbaca173_4": {
    "exec_flow": "ENTRY → IF_FALSE:providers.length==0 → FOR_INIT → FOR_COND:TRUE → TRY → CALL:call → CATCH:IOException → CALL:warn → CALL:shouldRetry → IF_TRUE:numFailovers>=providers.length-1 → CALL:error → THROW:ioe → EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException: [ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable"
  },
  "1303fe14_1": {
    "exec_flow": "ENTRY→IF_TRUE: isCircular(jobsInProgress)→THROW: IllegalArgumentException→EXIT→CALL:failAllJobs→CALL:error→EXIT",
    "log": "[ERROR] Error while trying to run jobs. LOG.error(\"Error while tyring to clean up \" + j.getJobName(), e) LOG.error(\"Error while tyring to clean up \" + j.getJobName(), e)"
  },
  "3b428a1f_1": {
    "exec_flow": "ENTRY→CALL:getFileLinkStatus→IF_TRUE:srcStatus==null→THROW:new FileNotFoundException→EXIT",
    "log": "<log> <template>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log> <log> <template>LOG.DEBUG: \"Path {} is a folder.\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log>"
  },
  "3b428a1f_2": {
    "exec_flow": "ENTRY→CALL:getFileLinkStatus→IF_FALSE:srcStatus==null→CALL:getFileLinkStatus→IF_FALSE:dstStatus!=null→CALL:getFileStatus→THROW:new FileNotFoundException→EXIT",
    "log": "<log> <template>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log> <log> <template>LOG.DEBUG: \"Found the path: {} as a file.\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log>"
  },
  "3b428a1f_3": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE: meta != null → LOG: DEBUG: List COS key: [{}] to check the existence of the path. → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE: listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → IF_TRUE: LOG.isDebugEnabled() → LOG: DEBUG: Path: [{}] is a directory. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log> <template>DEBUG: List COS key: [{}] to check the existence of the path.</template> </log> <log> <template>DEBUG: Path: [{}] is a directory. COS key: [{}]</template> </log>"
  },
  "48025d2d_1": {
    "exec_flow": "ENTRY → CALL:genericForward → IF_TRUE:hsr!=null → CALL:getCallerUserGroupInformation → IF_TRUE:callerUGI==null → LOG:LOG.ERROR:Unable to obtain user name, user not authenticated → RETURN → EXIT",
    "log": "[ERROR] Unable to obtain user name, user not authenticated"
  },
  "48025d2d_2": {
    "exec_flow": "ENTRY → CALL:genericForward → IF_FALSE:hsr!=null → CALL:createRemoteUser → IF_TRUE:callerUGI==null → LOG:LOG.ERROR:Unable to obtain user name, user not authenticated → RETURN → EXIT",
    "log": "[ERROR] Unable to obtain user name, user not authenticated"
  },
  "48025d2d_3": {
    "exec_flow": "ENTRY → CALL:genericForward → TRY → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}] → CATCH:PrivilegedActionException → LOG:LOG.DEBUG:PrivilegedActionException as: {} → THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "9bc249ae_1": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:doEditTx() op={} txid={}→CALL:org.slf4j.Logger:debug→TRY→CALL:editLogStream.write→EXCEPTION:write→CATCH:IOException ex→CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT",
    "log": "[DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "9bc249ae_2": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:doEditTx() op={} txid={}→CALL:org.slf4j.Logger:debug→TRY→CALL:editLogStream.write→CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT",
    "log": "[DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "9bc249ae_3": {
    "exec_flow": "ENTRY→TRY→SYNC: this→TRY→CALL: printStatistics→WHILE: mytxid > synctxid && isSyncRunning→WHILE_COND: mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE: mytxid <= synctxid→CALL: getLastJournalledTxId→LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}→IF_FALSE: lastJournalledTxId <= synctxid→TRY→IF_TRUE: journalSet.isEmpty()→THROW: new IOException(\"No journals available to flush\")→CALL: org.apache.hadoop.util.ExitUtil:terminate→CALL: org.slf4j.Logger:error→CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger→EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()> [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "9bc249ae_4": {
    "exec_flow": "ENTRY→TRY→SYNC: this→TRY→CALL: printStatistics→WHILE: mytxid > synctxid && isSyncRunning→WHILE_COND: mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE: mytxid <= synctxid→CALL: getLastJournalledTxId→LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}→IF_FALSE: lastJournalledTxId <= synctxid→TRY→IF_TRUE: journalSet.isEmpty()→THROW: new IOException(\"No journals available to flush\")→CALL: org.apache.hadoop.util.ExitUtil:terminate→CALL: org.slf4j.Logger:error→CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger→ENTRY→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()> [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions} [DEBUG] Exception in closing {}"
  },
  "f15b5fd9_1": {
    "exec_flow": "<sequence> <step>VIRTUAL_CALL</step> <step>CALL:clone</step> <step>CALL:multiplyTo</step> <step>FOR_INIT</step> <step>CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes</step> <step>FOR_COND: i < maxLength</step> <step>TRY</step> <step>CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation</step> <step>CATCH:ResourceNotFoundException</step> <step>CALL:org.slf4j.Logger:warn</step> <step>FOR_ITERATION</step> <step>FOR_EXIT</step> <step>RETURN</step> <step>EXIT</step> </sequence>",
    "log": "<entry>[WARN] Resource is missing:...</entry>"
  },
  "2e47747d_1": {
    "exec_flow": "ENTRY→CALL:dfsClient.checkOpen→IF_FALSE:closed.get()→IF_FALSE:(position<0)||(position>=filelen)→CALL:getFileLength→IF_FALSE:(position+length)>filelen→VIRTUAL_CALL to getBlockRange paths→FOREACH:blockRange→CALL:fetchBlockByteRange→ENTRY→IF_FALSE→IF_TRUE→CALL:DFSClient.LOG.info→CALL:DFSClient.LOG.info→TRY→CALL:DFSClient.LOG.warn→CATCH→CALL:Thread.currentThread().interrupt→THROW:InterruptedIOException→FOREACH_EXIT→RETURN→EXIT",
    "log": "<template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>DEBUG</level> <content>Sanity checks completed</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>INFO</level> <content>Fetching block byte range</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>INFO</level> <content>Found corruption while reading {file}. Error repairing corrupt blocks. Bad blocks remain.</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>INFO</level> <content>Will fetch a new encryption key and retry, encryption key was invalid when connecting to [datanode]</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>WARN</level> <content>fetchBlockByteRange(). Got a checksum exception for [src] at [block]:[pos] from [datanode]</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>WARN</level> <content>Connection failure: Failed to connect to [datanode] for file [src] for block [block]</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>DEBUG</level> <content>The reading thread has been interrupted.</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>INFO</level> <content>No node available for {blockInfo}</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>INFO</level> <content>Could not obtain {block.getBlock()} from any node: {errMsg}. Will get new block locations from namenode and retry...</content> </log_statement> </template> <template> <class>DFSInputStream</class> <method>pread</method> <log_statement> <level>WARN</level> <content>DFS chooseDataNode: got #{failures + 1} IOException, will wait for {waitTime} msec.</content> </log_statement> </template>"
  },
  "fee95c40_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.webapp.WebServices:getApp→TRY→CALL:getUser→IF_FALSE: callerUGI == null→CALL:doAs→IF_TRUE: LOG.isDebugEnabled()→LOG: PrivilegedAction [as: {}][action: {}]→NEW: PrivilegedExceptionAction→CALL:newInstance→CALL:getApplicationReport→IF_TRUE: app == null→LOG: PrivilegedActionException as: {}→THROW: new NotFoundException→CALL:org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)→IF_TRUE: finished > 0 && started > 0→IF_FALSE: elapsed >= 0 → LOG: LOG.WARN: Finished time + finished + is ahead of started time + started → EXIT",
    "log": "[DEBUG] User information retrieved [DEBUG] PrivilegedAction [as: {}][action: {}] [INFO] Privileged action performed [DEBUG] PrivilegedActionException as: {} [WARN] Finished time + finished + is ahead of started time + started"
  },
  "fee95c40_2": {
    "exec_flow": "ENTRY→CALL:add→TRY→CALL:LogWebServiceUtils:getUser→IF_FALSE: callerUGI == null→CALL:setUserName→CALL:doAs→NEW:PrivilegedExceptionAction<TimelineEntity>→ENTRY→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG: PrivilegedAction [as: {}][action: {}]→CALL:getEntity→IF_TRUE:appEntity == null→RETURN→EXIT→CATCH:PrivilegedActionException→LOG:LOG.DEBUG: PrivilegedActionException as: {}",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "fee95c40_3": {
    "exec_flow": "ENTRY→CALL:add→TRY→CALL:LogWebServiceUtils:getUser→IF_FALSE: callerUGI == null→CALL:setUserName→CALL:doAs→NEW:PrivilegedExceptionAction<TimelineEntity>→ENTRY→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG: PrivilegedAction [as: {}][action: {}]→CALL:getEntity→IF_FALSE:appEntity == null→NEW:BasicAppInfo→RETURN→EXIT→CATCH:PrivilegedActionException→LOG:LOG.DEBUG: PrivilegedActionException as: {}",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "fee95c40_4": {
    "exec_flow": "<sequence> <step>Check if remoteUser is null</step> <step>Create RemoteUser with remoteUser if not null</step> <step>Return UserGroupInformation</step> </sequence>",
    "log": "[ERROR] Response from the timeline reader server is {msg}"
  },
  "fee95c40_5": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:org.slf4j.Logger:error(java.lang.String)→THROW:new IOException→EXIT",
    "log": "[ERROR] Response from the timeline reader server is {msg}"
  },
  "41c57ebe_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND:!Thread.interrupted()→IF_TRUE:elapsed >= getRenewalTime()→TRY→CALL:renew→CATCH:SocketTimeoutException→LOG:LOG.WARN:Failed to renew lease for + clientsString() + \" for \" + (elapsed / 1000) + \" seconds. Aborting ...→CALL:DFSClient.closeAllFilesBeingWritten(true)→ENTRY→FOR_INIT→FOR_COND: TRUE→SYNC: filesBeingWritten→IF_FALSE: filesBeingWritten.isEmpty()→CALL:org.apache.hadoop.hdfs.DFSOutputStream:abort→LOG:org.slf4j.Logger:error→RETURN→EXIT→BREAK→CALL:monotonicNow→SYNC:this→IF_TRUE:id != currentId || isRenewerExpired()→IF_TRUE:id != currentId→LOG:LOG.DEBUG:Lease renewer daemon for + clientsString() + \" with renew id \" + id + \" is not current→RETURN→EXIT",
    "log": "<log>[WARN] Failed to renew lease for [clientsString] for [elapsed/1000] seconds. Aborting ...</log> <log>[ERROR] Failed to abort file: ... with inode: ...</log> <log>[DEBUG] Lease renewer daemon for [clientsString] with renew id [id] is not current</log>"
  },
  "41c57ebe_2": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND:!Thread.interrupted()→IF_TRUE:elapsed >= getRenewalTime()→TRY→CALL:renew→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Lease renewer daemon for + clientsString() + \" with renew id \" + id + \" executed→CALL:monotonicNow→SYNC:this→IF_TRUE:id != currentId || isRenewerExpired()→IF_TRUE:LOG.isDebugEnabled()→IF_TRUE:id != currentId→LOG:LOG.DEBUG:Lease renewer daemon for + clientsString() + \" with renew id \" + id + \" is not current→RETURN→EXIT",
    "log": "<log>[DEBUG] Lease renewer daemon for [clientsString] with renew id [id] executed</log> <log>[DEBUG] Lease renewer daemon for [clientsString] with renew id [id] is not current</log>"
  },
  "72aac49a_1": {
    "exec_flow": "ENTRY → CALL:initForReadableEndpoints → CALL:getCallerUserGroupInformation → IF_TRUE:callerUGI != null → CALL:getUserName → TRY → CALL:getRMAppForAppId → LOG: [WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition) → CALL:RMAuditLogger:logFailure → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: [DEBUG] PrivilegedAction [as: {}][action: {}] → CATCH:PrivilegedActionException → LOG: [DEBUG] PrivilegedActionException as: {} → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <template>Configuring readable endpoints</template> </log> <log> <level>DEBUG</level> <template>PrivilegedAction [as: {}][action: {}]</template> </log> <log> <level>DEBUG</level> <template>PrivilegedActionException as: {}</template> </log> <log> <level>ERROR</level> <template>Trying to get queue of an absent application</template> </log> <log> <level>WARN</level> <template>createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)</template> </log>"
  },
  "c14cc7bf_1": {
    "exec_flow": "ENTRY → TRY → CALL:openConnection → CALL:setRequestMethod → IF_TRUE:setRequestMethod(v) → LOG:Handling deprecation for all properties in config... → CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:makeQualified → EXCEPTION:setRequestMethod → CATCH:Exception ex → THROW:new IOException(ex) → EXIT",
    "log": "<log_entry> <class_method>LOG</class_method> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <class_method>LOG</class_method> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <class_method>LOG</class_method> <level>DEBUG</level> <template>Listing status for {f}</template> </log_entry> <log_entry> <class_method>LOG</class_method> <level>ERROR</level> <template>Could not retrieve owner information for path -</template> </log_entry> <log_entry> <class_method>LOG</class_method> <level>DEBUG</level> <template>Retrieving metadata for {key}</template> </log_entry>"
  },
  "3444c2b3_1": {
    "exec_flow": "ENTRY → LOG: [DEBUG] List status for path: {path} → CALL: getFileStatus → IF_TRUE: fileStatus.isDirectory() → LOG: [DEBUG] listStatus: doing listObjects for directory {key} → WHILE: true → CALL: toArray → CALL: size → RETURN → EXIT",
    "log": "[DEBUG] List status for path: {path} [DEBUG] listStatus: doing listObjects for directory {key}"
  },
  "3444c2b3_2": {
    "exec_flow": "ENTRY → TRY → CALL: getFileStatus → CALL: innerDelete [EXCEPTION: FileNotFoundException] → CATCH → CALL: debug → RETURN → EXIT",
    "log": "LOG.debug(\"Couldn't delete {} - does not exist\", path)"
  },
  "4e982277_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → CALL:substituteVars → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → RETURN → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "4e982277_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.conf.Configuration:getTrimmedStrings → FOR_INIT → FOR_COND: i < bypassUsers.length → CALL:org.slf4j.Logger:info → FOR_EXIT → EXIT",
    "log": "<log>[INFO] Add user {tmp} to the list that will bypass external attribute provider.</log>"
  },
  "4e982277_3": {
    "exec_flow": "ENTRY → LOG.INFO: Starting up re-encrypt thread with interval={} millisecond. → WHILE: true → TRY → SYNC: this → CALL: wait → CALL: checkPauseForTesting → ENTRY → WHILE: shouldPauseForTesting → WHILE_COND: shouldPauseForTesting → LOG.INFO: Sleeping in the re-encrypt handler for unit test. → SYNC → CALL: Object.wait → LOG.INFO: Continuing re-encrypt handler after pausing. → WHILE_EXIT → EXIT → CALL: getFSNamesystem → CALL: readLock → CALL: getReencryptionStatus → CALL: getNextUnprocessedZone → CATCH: zoneId == null → LOG.INFO: Executing re-encrypt commands on zone {}. Current zones:{} → CALL: getReencryptionStatus → CALL: markZoneStarted → ENTRY → CALL: Preconditions.checkNotNull → LOG.INFO: Zone {} starts re-encryption processing → CALL: setState → EXIT → CALL: resetSubmissionTracker → CALL: dir.getFSNamesystem().readUnlock → CALL: reencryptEncryptionZone → CALL: throttleTimerAll.reset().start → CALL: throttleTimerLocked.reset → CALL: readLock → TRY → CALL: getInode → IF_TRUE: zoneNode==null → LOG.INFO: Directory with id {} removed during re-encrypt, skipping → RETURN → EXIT → IF_FALSE: zoneNode==null → IF_TRUE: !zoneNode.isDirectory() → LOG.INFO: Cannot re-encrypt directory with id {} because it's not a directory. → RETURN → EXIT → IF_FALSE: zoneNode==null → IF_FALSE: !zoneNode.isDirectory() → CALL: getReencryptionStatus → CALL: getZoneStatus → LOG.INFO: Re-encrypting zone {}(id={}) → IF_TRUE: zs.getLastCheckpointFile()==null → CALL: traverseDir → CALL: submitCurrentBatch → LOG.INFO: Submission completed of zone {} for re-encryption. → CALL: markZoneSubmissionDone → EXIT → CATCH: RetriableException | SafeModeException re → LOG.INFO: Re-encryption caught exception, will retry → CALL: getReencryptionStatus → CALL: markZoneForRetry → CATCH: IOException ioe → LOG.WARN: IOException caught when re-encrypting zone {} → CATCH: InterruptedException ie → LOG.INFO: Re-encrypt handler interrupted. Exiting → CALL: Thread.currentThread().interrupt → RETURN → EXIT",
    "log": "<log>[INFO] Starting up re-encrypt thread with interval={} millisecond.</log> <log>[INFO] Sleeping in the re-encrypt handler for unit test.</log> <log>[INFO] Continuing re-encrypt handler after pausing.</log> <log>[INFO] Executing re-encrypt commands on zone {}. Current zones:{}</log> <log>[INFO] Zone {} starts re-encryption processing</log> <log>[INFO] Re-encryption caught exception, will retry</log> <log>[INFO] Directory with id {} removed during re-encrypt, skipping</log> <log>[INFO] Cannot re-encrypt directory with id {} because it's not a directory.</log> <log>[INFO] Re-encrypting zone {}(id={})</log> <log>[INFO] Submission completed of zone {} for re-encryption.</log> <log>[WARN] IOException caught when re-encrypting zone {}</log> <log>[INFO] Re-encrypt handler interrupted. Exiting</log>"
  },
  "aafff753_1": {
    "exec_flow": "ENTRY → TRY → LOG: LOG.INFO: Reconfiguring {} to {}, property, newVal → IF_TRUE: property.equals(DFS_DATANODE_OUTLIERS_REPORT_INTERVAL_KEY) → CALL: checkNotNull → CALL: dnConf.setOutliersReportIntervalMs → FOREACH: blockPoolManager.getAllNamenodeThreads() → FOREACH_EXIT → LOG: LOG.INFO: RECONFIGURE* changed {} to {}, property, newVal → RETURN → EXIT",
    "log": "<log>[INFO] Reconfiguring {} to {}, property, newVal</log> <log>[INFO] RECONFIGURE* changed {} to {}, property, newVal</log>"
  },
  "aafff753_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.hdfs.server.common.Util:isDiskStatsEnabled → IF_TRUE: fileIOSamplingPercentage <= 0 → CALL: org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT",
    "log": "<log>[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Disabling file IO profiling</log>"
  },
  "aafff753_3": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.hdfs.server.common.Util:isDiskStatsEnabled → IF_FALSE: fileIOSamplingPercentage <= 0 → CALL: org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT",
    "log": "<log>[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Enabling file IO profiling</log>"
  },
  "aafff753_4": {
    "exec_flow": "ENTRY → CALL: slowDiskDetectionDaemon.interrupt → TRY → CALL: slowDiskDetectionDaemon.join → CATCH: InterruptedException e → LOG: LOG.ERROR: Disk Outlier Detection daemon did not shutdown, e → CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → EXIT",
    "log": "<log>[ERROR] Disk Outlier Detection daemon did not shutdown</log>"
  },
  "aafff753_5": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "aafff753_6": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "148c6a01_1": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_TRUE: meta.isDirectory()→THROW: new FileNotFoundException(f.toString() + \" is a directory not a file.\")→EXIT",
    "log": "[DEBUG] Opening file: {} for append"
  },
  "148c6a01_2": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: meta.isDirectory()→IF_TRUE: store.isPageBlobKey(key)→THROW: new IOException(\"Append not supported for Page Blobs\")→EXIT",
    "log": "[DEBUG] Opening file: {} for append"
  },
  "148c6a01_3": {
    "exec_flow": "ENTRY→IF_FALSE: !appendSupportEnabled→LOG: LOG.DEBUG: Opening file: {} for append, f→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: meta.isDirectory()→IF_FALSE: store.isPageBlobKey(key)→TRY→CALL: retrieveAppendStream→NEW: FSDataOutputStream→RETURN→EXIT",
    "log": "[DEBUG] Opening file: {} for append"
  },
  "148c6a01_4": {
    "exec_flow": "ENTRY → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → ENTRY → TRY → LOG:LOG.DEBUG:openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite → CALL:startTracking → CALL:client.getPathStatus → IF_FALSE:parseIsDirectory(resourceType) → CALL:perfInfo.registerSuccess → IF_TRUE:isAppendBlobKey → CALL:isAppendBlobKey → CALL:maybeCreateLease → NEW:AbfsOutputStream → CALL:populateAbfsOutputStreamContext → CALL:perfInfo.close → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize} [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite"
  },
  "148c6a01_5": {
    "exec_flow": "ENTRY → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified → ENTRY → TRY → LOG:LOG.DEBUG:openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite → CALL:startTracking → CALL:client.getPathStatus → IF_FALSE:parseIsDirectory(resourceType) → CALL:perfInfo.registerSuccess → IF_FALSE:isAppendBlobKey → NEW:AbfsOutputStream → CALL:populateAbfsOutputStreamContext → CALL:perfInfo.close → RETURN → EXIT → EXCEPTION:AzureBlobFileSystemException → CALL:checkException → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize} [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite"
  },
  "99c7cf95_1": {
    "exec_flow": "ENTRY→TRY→CALL:getRootDir→IF_TRUE: !exists(dataDirPath) →CALL:exists→LOG:info→CALL:mkdir→IF_TRUE: !mkdir(dataDirPath) →LOG:error→RETURN→EXIT",
    "log": "[INFO] {} data directory doesn't exist, creating it [ERROR] Cannot create data directory {}"
  },
  "99c7cf95_2": {
    "exec_flow": "ENTRY→TRY→CALL:getRootDir→IF_TRUE: !exists(dataDirPath) →CALL:exists→LOG:info→CALL:mkdir→IF_FALSE: mkdir(dataDirPath) →RETURN→EXIT",
    "log": "[INFO] {} data directory doesn't exist, creating it"
  },
  "88f88343_1": {
    "exec_flow": "ENTRY → CALL:getUser → IF_TRUE:userName != null → CALL:get → CALL:handleDeprecation → CALL:getProps → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → FOREACH:names → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "7b0fa469_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "7b0fa469_2": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:loginUser==null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser) → CALL:createLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → EXCEPTION:Read → CATCH:IOException → CALL:IOUtils.cleanupWithLogger → THROW:IOException → DO_COND:loginUser==null → DO_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] Cannot add token {}: {} [DEBUG] Loaded {} base64 tokens [DEBUG] UGI loginUser: {}"
  },
  "0a91963f_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→RETURN→EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "0a91963f_2": {
    "exec_flow": "ENTRY→CALL: LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[INFO] message"
  },
  "0a91963f_3": {
    "exec_flow": "ENTRY→NEW:JobConf→CALL:org.apache.hadoop.mapred.JobConf:<init>(org.apache.hadoop.conf.Configuration)→CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY→CALL:get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null→CALL:warn→LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→RETURN→EXIT",
    "log": "[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_TASK_ULIMIT [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "c1e5a066_1": {
    "exec_flow": "ENTRY→IF_FALSE: targetWorkPath == null→CALL:org.apache.hadoop.fs.FileSystem:globStatus→CALL:glob→NEW:Globber→CALL:addKVAnnotation→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→CALL:doGlob→CALL:org.apache.hadoop.util.DurationInfo:close()→RETURN→IF_TRUE: tempFiles != null && tempFiles.length > 0→FOREACH: tempFiles→CALL:org.slf4j.Logger:info→CALL:org.apache.hadoop.fs.FileSystem:delete→FOREACH_EXIT→EXIT",
    "log": "[INFO] glob path pattern [DEBUG] Filesystem glob {} [DEBUG] Pattern: {} [DEBUG] Component {}, patterned={} [WARN] File/directory {} not found: it may have been deleted. If this is an object store, this can be a sign of eventual consistency problems. [DEBUG] No matches found and there was no wildcard in the path {} [INFO] Cleaning up {file.getPath()} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration, se [DEBUG] Ready to delete path: [{}]. recursive: [{}] [DEBUG] Delete the file: {} [DEBUG] Create parent key: {parentKey} [DEBUG] Store a empty file in COS failed., e"
  },
  "c1e5a066_2": {
    "exec_flow": "ENTRY→LOG: AzureBlobFileSystem.delete path: {} recursive: {}→CALL: statIncrement→CALL: makeQualified→IF_FALSE: f.isRoot()→TRY→CALL: abfsStore.delete→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.delete path: {f.toString()} recursive: {recursive}"
  },
  "c1e5a066_3": {
    "exec_flow": "ENTRY→IF_FALSE: targetWorkPath == null→CALL:org.apache.hadoop.fs.FileSystem:globStatus→CALL:glob→NEW:Globber→CALL:addKVAnnotation→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→CALL:doGlob→CALL:org.apache.hadoop.util.DurationInfo:close()→RETURN→IF_FALSE: tempFiles != null && tempFiles.length > 0→EXIT",
    "log": "[INFO] glob path pattern [DEBUG] Filesystem glob {} [DEBUG] Pattern: {} [DEBUG] Component {}, patterned={}"
  },
  "57bdffb2_1": {
    "exec_flow": "ENTRY→CALL:setupUser→IF_TRUE:this.nextInterceptor != null→CALL:this.nextInterceptor.init→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→ENTRY→LOG:Handling deprecation for all properties in config...→IF_TRUE:props != null→IF_TRUE:loadDefaults && fullReload→FOREACH:defaultResources→CALL:loadResource→FOREACH_EXIT→FOR_INIT→FOR_COND:i < resources.size()→CALL:loadResource→FOR_EXIT→CALL:putAll→IF_TRUE:backup != null→FOREACH:overlay.entrySet()→FOREACH_EXIT→CALL:addTags→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG:[DEBUG] Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT→EXIT→IF_TRUE:loginUserRef == null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser == null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→EXCEPTION:readTokenStorageStream→CATCH:IOException ioe→CALL:IOUtils.cleanupWithLogger→THROW:new IOException(\"Exception reading \" + filename, ioe)→EXIT→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds(false)→DO_COND:loginUser == null→DO_EXIT→RETURN→EXIT",
    "log": "<log_entry>[INFO] Error while creating Router ClientRM Service for user:, user: + user</log_entry> <log_entry>[WARN] auth_to_local rule mechanism not set. Using default of DEFAULT_MECHANISM</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[INFO] message</log_entry> <log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Reading credentials from location {}</log_entry> <log_entry>[DEBUG] Loaded {} tokens from {}</log_entry> <log_entry>[INFO] Token file {} does not exist</log_entry> <log_entry>[INFO] Cleaning up resources</log_entry> <log_entry>[DEBUG] Failure to load login credentials</log_entry> <log_entry>[DEBUG] UGI loginUser: {}</log_entry>"
  },
  "f3c922c0_1": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <call>rpcServer.checkOperation</call> <call>rpcServer.getLocationsForPath</call> <virtual_call> <callee>getMountPoints</callee> <exec_flow> ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs ≤ 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[ERROR] Cannot get method {} with types {} from {}</log_entry> <log_entry>[DEBUG] User {} NN {} is using connection {}</log_entry> <log_entry>[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out</log_entry>"
  },
  "f3c922c0_2": {
    "exec_flow": "ENTRY→IF_TRUE: subclusterResolver instanceof MountTableResolver→TRY→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→RETURN→EXIT",
    "log": "<log_entry>[ERROR] Cannot get mount point</log_entry>"
  },
  "1c7a1312_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→CALL:checkNNStartup→CALL:namesystem.metaSave→CALL:println→CALL:blockManager.metaSave→CALL:readUnlock→CALL:this.fsLock.readUnlock→CALL:org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:readUnlock→CALL:coarseLock.readLock().unlock→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→EXIT",
    "log": "<!-- Merged log sequence --> [LOG] Total count of filesystem objects calculated and output [INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "4d8463b1_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CALL: Subject.doAs → RETURN → EXIT CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause → THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "<log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "4d8463b1_2": {
    "exec_flow": "ENTRY→CALL:incrementWriteOps→CALL:adlClient.createDirectory→CALL:toRelativeFilePath→CALL:Integer.toOctalString→CALL:applyUMask→RETURN→EXIT",
    "log": "<log>[DEBUG] Applying UMask</log>"
  },
  "124729fe_1": {
    "exec_flow": "<step> <description>Parent Entry</description> <condition>lock obtained</condition> </step> <step> <description>Check if counter is null and create is true</description> <condition>counter == null && create</condition> </step> <step> <description>Add counter implementation</description> <condition>add counter if null</condition> </step> <step>Constructor</step> <step>addCounterImpl</step> <step>[ConditionalBranch] addCounter</step>",
    "log": "<log_entry>Log1: Initialization complete</log_entry> <log_entry>Log2: Counter added successfully</log_entry>"
  },
  "124729fe_2": {
    "exec_flow": "ENTRY → CALL:findCounter → IF_TRUE → CALL:setValue → RETURN → EXIT",
    "log": "<log_entry>[WARN] New counter created</log_entry> <log_entry>[WARN] {name} is not a known counter.</log_entry>"
  },
  "124729fe_3": {
    "exec_flow": "ENTRY → CALL:findCounter → IF_FALSE → RETURN → EXIT",
    "log": "<log_entry>[WARN] Counter already exists</log_entry> <log_entry>[WARN] {name} is not a known counter.</log_entry>"
  },
  "eb0bb9ac_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_TRUE: configName != null → IF_FALSE: target == null → CALL: trim → CALL: createURI → IF_TRUE: port == -1 → IF_FALSE: (host == null) || (port < 0) || (!hasScheme && path != null && !path.isEmpty()) → CALL: createSocketAddrForHost → RETURN → EXIT → VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)",
    "log": "[DEBUG] Handling deprecation for all properties in config... [WARN] Unexpected SecurityException in Configuration [INFO] message"
  },
  "eb0bb9ac_2": {
    "exec_flow": "ENTRY → CALL:handleDeprecation → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → IF_TRUE:props != null → CALL:loadResources → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE:overlay != null → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "eb0bb9ac_3": {
    "exec_flow": "ENTRY → VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String) → LOG:Unexpected SecurityException in Configuration → EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "617aee27_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {scheme} → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {property} → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \\\"\"+scheme+\"\\\" \") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {uri}: {e.toString()} → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Bypassing cache to create filesystem {uri}</message> </log> <log> <level>DEBUG</level> <message>Looking for FS supporting {scheme}</message> </log> <log> <level>DEBUG</level> <message>looking for configuration option {property}</message> </log> <log> <level>DEBUG</level> <message>Looking in service filesystems for implementation class</message> </log> <log> <level>WARN</level> <message>Failed to initialize filesystem {uri}: {e.toString()}</message> </log> <log> <level>DEBUG</level> <message>Failed to initialize filesystem</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "617aee27_2": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {uri} created while awaiting semaphore\") → RETURN → EXIT → IF(fs != null) → SYNC → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {uri}; discarding {fs}\") → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Filesystem {uri} created while awaiting semaphore</message> </log> <log> <level>DEBUG</level> <message>Duplicate FS created for {uri}; discarding {fs}</message> </log>"
  },
  "617aee27_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG: Call the getFileStatus to obtain the metadata for the file: [{f}]. → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{f}] is a file. COS key: [{key}]. → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Call the getFileStatus to obtain the metadata for the file: [{f}].</message> </log> <log> <level>DEBUG</level> <message>Path: [{f}] is a file. COS key: [{key}].</message> </log>"
  },
  "c15aef58_1": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "590ffd77_1": {
    "exec_flow": "ENTRY→IF_TRUE:e instanceof RemoteException→CALL:org.apache.hadoop.io.retry.RetryPolicy:shouldRetry→ENTRY→TRY→CALL:RetryPolicies$RemoteExceptionDependentRetry:shouldRetry→RETURN→EXIT→RETURN→EXIT",
    "log": "<!-- No logs to merge since parent had logs and child did not add new -->"
  },
  "590ffd77_2": {
    "exec_flow": "ENTRY→TRY→CALL:RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry→IF_FALSE:failovers >= maxFailovers→IF_FALSE:retries - failovers > maxRetries→IF_FALSE:isSaslFailure(e)→IF_FALSE:e instanceof ConnectException || e instanceof EOFException || e instanceof NoRouteToHostException || e instanceof UnknownHostException || e instanceof StandbyException || e instanceof ConnectTimeoutException || shouldFailoverOnException(e)→IF_FALSE:e instanceof RetriableException || getWrappedRetriableException(e) != null→IF_FALSE:e instanceof InvalidToken→IF_FALSE:e instanceof AccessControlException || hasWrappedAccessControlException(e)→IF_FALSE:e instanceof SocketException || (e instanceof IOException && !(e instanceof RemoteException))→RETURN→EXIT",
    "log": "<!-- Empty since there are no logs -->"
  },
  "590ffd77_3": {
    "exec_flow": "ENTRY→CALL:shouldRetry→CALL:RetryPolicy:shouldRetry→RETURN→EXIT",
    "log": "<!-- No logs generated, paths are valid if conditions hold -->"
  },
  "590ffd77_4": {
    "exec_flow": "ENTRY→CALL:shouldRetry→RETURN→EXIT",
    "log": "<!-- No logs generated in path -->"
  },
  "590ffd77_5": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkArgument→IF_TRUE:exception instanceof AmazonClientException→CALL:translateException→ENTRY→LOG:LOG.INFO:Bulk delete operation failed to delete all objects; + failure count = {}, errors.size() →CALL:append →FOREACH:deleteException.getErrors() →CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) →FOREACH_EXIT →IF_TRUE:ACCESS_DENIED.equals(exitCode) →CALL:initCause →NEW:AccessDeniedException →CALL:toString →RETURN →EXIT→CALL:shouldRetry→RETURN→EXIT",
    "log": "[INFO] Bulk delete operation failed to delete all objects; failure count = {errors.size()} [INFO] {item}"
  },
  "590ffd77_6": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkArgument→IF_TRUE:exception instanceof AmazonClientException→CALL:translateException→ENTRY→LOG:LOG.INFO:Bulk delete operation failed to delete all objects; + failure count = {}, errors.size() →CALL:append→FOREACH:deleteException.getErrors() →CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) →FOREACH_EXIT →IF_FALSE:ACCESS_DENIED.equals(exitCode) →NEW:AWSS3IOException →CALL:toString →RETURN →EXIT→CALL:shouldRetry→RETURN→EXIT",
    "log": "[INFO] Bulk delete operation failed to delete all objects; failure count = {errors.size()} [INFO] {item}"
  },
  "590ffd77_7": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkArgument→IF_FALSE:exception instanceof AmazonClientException→CALL:shouldRetry→RETURN→EXIT",
    "log": "<!-- No logs generated in path -->"
  },
  "590ffd77_8": {
    "exec_flow": "ENTRY→IF_TRUE: e instanceof ServiceException→IF_TRUE: cause != null && cause instanceof Exception→IF_TRUE: e instanceof RetriableException || RetryPolicies.getWrappedRetriableException(e) != null→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL: shouldRetry→RETURN→EXIT",
    "log": "[DEBUG] RETRY {}) policy={}"
  },
  "590ffd77_9": {
    "exec_flow": "ENTRY→IF_TRUE: e instanceof ServiceException→IF_TRUE: cause != null && cause instanceof Exception→IF_FALSE: e instanceof RetriableException || RetryPolicies.getWrappedRetriableException(e) != null→IF_TRUE: e instanceof RemoteException→CALL: equals→CALL: getClassName→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL: shouldRetry→RETURN→EXIT",
    "log": "[DEBUG] RETRY {}) policy={}"
  },
  "55577bbe_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "188d94b5_1": {
    "exec_flow": "<seq>ENTRY→CALL:debugLogFileSystemClose→TRY→SYNC:deleteOnExit→FOR_INIT→FOR_COND:iter.hasNext()→CALL:exists→IF→CALL:delete→CATCH→CALL:info→REMOVE→FOR_CONTINUE→EXIT→CALL:CACHE.remove→EXIT</seq>",
    "log": "<log>[DEBUG] Closing FileSystem: Key: someKey; URI: someUri; Object Identity Hash: someHash</log> <log>[INFO] Ignoring failure to deleteOnExit for path {}</log>"
  },
  "188d94b5_2": {
    "exec_flow": "ENTRY→IF_TRUE: LOGGER.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→IF_TRUE: LOGGER.isTraceEnabled()→CALL: org.slf4j.Logger:isTraceEnabled()→LOG: LOGGER.DEBUG: FileSystem.{}() by method: {}); {}, methodName, throwable.getStackTrace()[2], additionalInfo→CALL: org.slf4j.Logger:trace(java.lang.String,java.lang.Object,java.lang.Object)→LOG: LOGGER.TRACE: FileSystem.{}() full stack trace:, methodName, throwable→EXIT",
    "log": "<log>[DEBUG] FileSystem.{}() by method: {}); {}</log> <log>[TRACE] FileSystem.{}() full stack trace:</log>"
  },
  "188d94b5_3": {
    "exec_flow": "ENTRY→IF_TRUE: LOGGER.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: LOGGER.DEBUG: FileSystem.{}() by method: {}); {}, methodName, throwable.getStackTrace()[2], additionalInfo→IF_FALSE: LOGGER.isTraceEnabled()→EXIT",
    "log": "<log>[DEBUG] FileSystem.{}() by method: {}); {}</log>"
  },
  "188d94b5_4": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: canRefreshDelegationToken && delegationToken != null → CALL: cancelDelegationToken → CATCH: IOException → CALL: debug → FINALLY → IF_TRUE: connectionFactory != null → CALL: connectionFactory.destroy → CALL: close → EXIT",
    "log": "<log>[DEBUG] Token cancel failed: {ioe}</log>"
  },
  "188d94b5_5": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: canRefreshDelegationToken && delegationToken != null → CALL: cancelDelegationToken → CATCH: IOException → CALL: debug → FINALLY → IF_FALSE: connectionFactory != null → CALL: close → EXIT",
    "log": "<log>[DEBUG] Token cancel failed: {ioe}</log>"
  },
  "144b0ecc_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:fileStatus.isDirectory()→IF_FALSE:!overwrite→LOG: LOG.DEBUG: Creating a new file: [{}] in COS.→NEW:FSDataOutputStream→NEW:CosNOutputStream→CALL:getConf→RETURN→EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS."
  },
  "144b0ecc_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Creating file: {}, f.toString()→IF_FALSE: containsColon(f)→CALL: performAuthCheck→CALL: createInternal→RETURN→EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "144b0ecc_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "82336eab_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "82336eab_2": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "82336eab_3": {
    "exec_flow": "ENTRY→CALL:trim→CALL:toLowerCase→IF_CONDITION1:null==vUnit→CALL:unitFor→IF_CONDITION2:vUnit.unit().convert(converted, returnUnit)<raw→CALL:logDeprecation→EXIT→RETURN→EXIT",
    "log": "<log>[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}</log>"
  },
  "1787ab34_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.conf.Configuration:get → IF_FALSE:null == newLocalDirs → CALL:org.apache.hadoop.fs.FileSystem:getLocal → IF_TRUE:!newLocalDirs.equals(ctx.savedLocalDirs) → CALL:org.apache.hadoop.fs.FileSystem:mkdirs → CALL:org.apache.hadoop.fs.FileSystem:exists → CALL:org.apache.hadoop.util.DiskValidator:checkStatus → CALL:org.slf4j.Logger:warn → CALL:org.slf4j.Logger:warn → RETURN → EXIT",
    "log": "<log>[WARN] Failed to create [dirStrings[i]]</log> <log>[WARN] [dirStrings[i]] is not writable</log>"
  },
  "1787ab34_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: abfsStore.getFileStatus → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "1787ab34_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: abfsStore.getFileStatus → CATCH: AzureBlobFileSystemException → CALL: checkException → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "38301d24_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG: DEBUG: Path: [{}] is a file. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log_entry> <class>unknown</class> <method>unknown</method> <level>DEBUG</level> <template>Path: [{}] is a file. COS key: [{}]</template> </log_entry>"
  },
  "38301d24_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isFile() → LOG: DEBUG: Path: [{}] is a dir. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>unknown</class> <method>unknown</method> <level>DEBUG</level> <template>Path: [{}] is a dir. COS key: [{}]</template> </log_entry>"
  },
  "38301d24_3": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG: DEBUG: List COS key: [{}] to check the existence of the path. → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0 → LOG: DEBUG: Path: [{}] is a directory. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>unknown</class> <method>unknown</method> <level>DEBUG</level> <template>List COS key: [{}] to check the existence of the path.</template> </log_entry> <log_entry> <class>unknown</class> <method>unknown</method> <level>DEBUG</level> <template>Path: [{}] is a directory. COS key: [{}]</template> </log_entry>"
  },
  "38301d24_4": {
    "exec_flow": "ENTRY → CALL: LOG.debug → CALL: ListObjectsRequest → CALL: callCOSClientWithRetry → TRY → CALL: ObjectListing.getObjectSummaries → CALL: COSObjectSummary.getKey → CALL: CosNFileSystem.PATH_DELIMITER → CALL: FileMetadata → CALL: ObjectListing.getCommonPrefixes → CALL: FileMetadata → RETURN → EXIT",
    "log": "<log_entry> <class>unknown</class> <method>unknown</method> <level>DEBUG</level> <template>List objects. prefix: [{}], delimiter: [{}], maxListLength: [{}], priorLastKey: [{}].</template> </log_entry>"
  },
  "9c4176ea_1": {
    "exec_flow": "ENTRY→CALL:resolveComponents→RETURN→EXIT",
    "log": "logAuditEvent(false, operationName, src); logAuditEvent(true, operationName, src);"
  },
  "9c4176ea_2": {
    "exec_flow": "ENTRY→CALL:coarseLock.readLock().unlock→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→EXIT",
    "log": "[INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "7fc7d90a_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Processing the event JOB_ABORT→WHILE: iterator.hasNext()→WHILE_COND: iterator.hasNext()→CALL: iterator.next→CALL: iterator.next.getType→CALL: counterMap.containsKey→CALL: counterMap.put→CALL: counterMap.get→CALL: counterMap.put→WHILE_EXIT→FOREACH: counterMap.entrySet()→CALL: Map.Entry.getValue→CALL: Map.Entry.getKey→CALL: org.slf4j.Logger:info→FOREACH_EXIT→EXIT",
    "log": "[INFO] Processing the event JOB_ABORT [INFO] Event type: {entry.getKey()}, Event record counter: {num}"
  },
  "7fc7d90a_2": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→METHOD: org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:cancelJobCommit→CONDITION: threadCommitting != null && threadCommitting.isAlive()→ACTION: LOG.info(\"Cancelling commit\")→ACTION: threadCommitting.interrupt()→EXIT",
    "log": "[INFO] Cancelling commit"
  },
  "7fc7d90a_3": {
    "exec_flow": "ENTRY→IF_TRUE: hasOutputPath→CALL: getJobAttemptPath→CALL: getFileSystem→CALL: mkdirs→IF_TRUE: !fs.mkdirs→CALL: LOG.ERROR→EXIT",
    "log": "[ERROR] Mkdirs failed to create ${jobAttemptPath}"
  },
  "7fc7d90a_4": {
    "exec_flow": "ENTRY→IF_FALSE: hasOutputPath→CALL: LOG.WARN→EXIT",
    "log": "[WARN] Output Path is null in setupJob()"
  },
  "193de598_1": {
    "exec_flow": "ENTRY → CALL:qualify → LOG:LOG.DEBUG:listFiles({}, {}), path, recursive → TRY → CALL:getListFilesAssumingDir → IF_TRUE:recursive → LOG:LOG.DEBUG:Recursive list of all entries under {}, key → CALL:createLocatedFileStatusIterator → CALL:createFileStatusListingIterator → CALL:createListObjectsRequest → CALL:createListObjectsRequest → CALL:createFileStatusListingIterator → CALL:createListObjectsRequest → CALL:createListObjectsRequest → RETURN → EXIT",
    "log": "[DEBUG] listFiles({}, {}) [DEBUG] Recursive list of all entries under {}"
  },
  "41010bbb_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for +(String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "41010bbb_2": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for +(String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "41010bbb_3": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta!=null→LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_FALSE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0→THROW:FileNotFoundException→EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path."
  },
  "41010bbb_4": {
    "exec_flow": "ENTRY→CALL:connect→IF_TRUE:client!=null→IF_FALSE:!client.isConnected()→CALL:logout→CALL:disconnect→IF_TRUE:!logoutSuccess→LOG:LOG.WARN:Logout failed while disconnecting, error code - +client.getReplyCode()→EXIT",
    "log": "[WARN] Logout failed while disconnecting, error code -"
  },
  "cd7875d9_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → CALL: setConfig → TRY → CALL: org.slf4j.Logger:debug(java.lang.String) → CALL: setConfig → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Service: {} entered state {}</message> </log> <log> <level>DEBUG</level> <message>Config has been overridden during init</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {}</message> </log>"
  },
  "cd7875d9_2": {
    "exec_flow": "CALL: ServiceOperations.stopQuietly → CALL: Logger.warn",
    "log": "<log> <level>WARN</level> <message>When stopping the service {serviceName}</message> </log>"
  },
  "cd7875d9_3": {
    "exec_flow": "CALL: ServiceOperations.stopQuietly → IF: (service != null) → CALL: ServiceOperations.stop → CALL: Logger.warn",
    "log": "<log> <level>WARN</level> <message>When stopping the service {serviceName}</message> </log>"
  },
  "da1da00b_1": {
    "exec_flow": "<optimized_flow> ENTRY → CALL: Preconditions.checkArgument → LOG: LOG.DEBUG: Propagating entries under {}, bucketPrefix → CALL: org.apache.hadoop.conf.Configuration:<init> → FOREACH: source → IF_TRUE: !key.startsWith(bucketPrefix) || bucketPrefix.equals(key) → CONTINUE → CALL: org.slf4j.Logger:debug → CALL: org.apache.hadoop.conf.Configuration:getPropertySources → IF_TRUE: stripped.startsWith(\"bucket.\") || \"impl\".equals(stripped) → LOG: LOG.DEBUG: Ignoring bucket option {} → CALL: org.apache.hadoop.conf.Configuration:set → FOREACH_EXIT → RETURN → EXIT </optimized_flow>",
    "log": "[DEBUG] Propagating entries under {} [DEBUG] Ignoring bucket option {}"
  },
  "da1da00b_2": {
    "exec_flow": "<optimized_flow> ENTRY → CALL: Preconditions.checkArgument → LOG: LOG.DEBUG: Propagating entries under {}, bucketPrefix → CALL: org.apache.hadoop.conf.Configuration:<init> → FOREACH: source → CALL: org.apache.hadoop.conf.Configuration:iterator → CALL: org.apache.hadoop.conf.Configuration:getPropertySources → FOREACH_EXIT → RETURN → EXIT </optimized_flow>",
    "log": "[DEBUG] Propagating entries under {}"
  },
  "da1da00b_3": {
    "exec_flow": "<optimized_flow> Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT </optimized_flow>",
    "log": "<!-- Inherit from child path as parent has no logs -->"
  },
  "da1da00b_4": {
    "exec_flow": "<optimized_flow> Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT </optimized_flow>",
    "log": "<!-- Inherit from child path as parent has no logs -->"
  },
  "73bd7615_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "5a6a61c5_1": {
    "exec_flow": "ENTRY → IF_FALSE: !block.isComplete() → IF_FALSE: storedBlock == null || storedBlock.isDeleted() → IF_FALSE: result == AddBlockResult.ADDED → IF_FALSE: result == AddBlockResult.REPLACED → CALL: corruptReplicas.removeFromCorruptReplicasMap → CALL: blockLog.debug → RETURN → EXIT → IF_FALSE: !isSafeModeTrackingBlocks() → SYNC: this → LOG: [DEBUG] Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal → CALL: setBlockTotal → CALL: checkSafeMode → IF_TRUE: bytesInFuture > 0 → IF_TRUE: force → LOG: [WARN] Leaving safe mode due to forceExit. This will cause a data loss of {} byte(s). → CALL: bytesInFutureBlocks.reset → CALL: bytesInFutureECBlockGroups.reset → IF_FALSE: blockManager.isPopulatingReplQueues() && blockManager.shouldPopulateReplQueues() → IF_FALSE: status != BMSafeModeStatus.OFF → CALL: NameNode.stateChangeLog.info → CALL: NameNode.getNameNodeMetrics().setSafeModeTime → CALL: NameNode.stateChangeLog.info → CALL: NameNode.stateChangeLog.info → CALL: org.apache.hadoop.hdfs.server.namenode.Namesystem:startSecretManagerIfNecessary() → IF_TRUE: !isSecurityEnabled() → IF_TRUE: shouldUseDelegationTokens() && !isInSafeMode() && getEditLog().isOpenForWrite() → IF_TRUE: dtSecretManager != null → TRY → CATCH: IOException IF_FALSE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE → CALL: StartupProgress.endStep → CALL: StartupProgress.endPhase → IF_TRUE: !isComplete() → CALL: monotonicNow → LOG: [DEBUG] End of the phase: {}, phase → CALL: org.slf4j.Logger:debug → RETURN → EXIT",
    "log": "[DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {} [DEBUG] Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal [WARN] Leaving safe mode due to forceExit. This will cause a data loss of {} byte(s). [INFO] STATE* Safe mode is OFF [INFO] STATE* Leaving safe mode after {} secs [INFO] STATE* Network topology has {} racks and {} datanodes [INFO] STATE* UnderReplicatedBlocks has {} blocks [DEBUG] End of the step. Phase: {}, Step: {} [DEBUG] End of the phase: {} [WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}"
  },
  "5a6a61c5_2": {
    "exec_flow": "ENTRY → IF_TRUE: addedNode == delNodeHint → FOREACH: blocksMap.getStorages(block) → IF_FALSE: storage.getState() != State.NORMAL → IF_TRUE: storage.areBlockContentsStale() → LOG: LOG.TRACE: BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information. → CALL: postponeBlock → RETURN → EXIT",
    "log": "LOG.TRACE: BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information."
  },
  "5a6a61c5_3": {
    "exec_flow": "ENTRY → IF_FALSE: addedNode == delNodeHint → FOREACH: blocksMap.getStorages(block) → IF_FALSE: storage.getState() != State.NORMAL → IF_TRUE: storage.areBlockContentsStale() → LOG: LOG.TRACE: BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information. → CALL: postponeBlock → RETURN → EXIT",
    "log": "LOG.TRACE: BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information."
  },
  "5a6a61c5_4": {
    "exec_flow": "ENTRY → IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) → CALL: NameNode.blockStateChangeLog.debug → CALL: decrementBlockStat → RETURN → EXIT",
    "log": "[DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {} from priority queue {}"
  },
  "26903f6b_1": {
    "exec_flow": "ENTRY→TRY→CALL:isAllowedDelegationTokenOp→IF_FALSE:!isAllowedDelegationTokenOp()→CALL:getCurrentUser→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→CALL:rmDTSecretManager.cancelToken→IF_TRUE:id.getUser() == null→THROW:new InvalidToken(\"Token with no owner \" + formatTokenId(id))→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Token cancellation requested for identifier: + formatTokenId(id)"
  },
  "26903f6b_2": {
    "exec_flow": "ENTRY→TRY→CALL:isAllowedDelegationTokenOp→IF_FALSE:!isAllowedDelegationTokenOp()→CALL:getCurrentUser→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→CALL:rmDTSecretManager.cancelToken→IF_FALSE:id.getUser() == null→CALL:getShortName→IF_TRUE:!canceller.equals(owner) && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName.equals(renewer.toString()))→THROW:new AccessControlException(canceller + \" is not authorized to cancel the token \" + formatTokenId(id))→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Token cancellation requested for identifier: + formatTokenId(id)"
  },
  "26903f6b_3": {
    "exec_flow": "ENTRY→TRY→CALL:isAllowedDelegationTokenOp→IF_FALSE:!isAllowedDelegationTokenOp()→CALL:getCurrentUser→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→CALL:rmDTSecretManager.cancelToken→IF_FALSE:id.getUser() == null→CALL:getShortName→IF_FALSE:!canceller.equals(owner) && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName.equals(renewer.toString()))→IF_TRUE:info == null→THROW:new InvalidToken(\"Token not found \" + formatTokenId(id))→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Token cancellation requested for identifier: + formatTokenId(id)"
  },
  "26903f6b_4": {
    "exec_flow": "ENTRY→TRY→CALL:isAllowedDelegationTokenOp→IF_FALSE:!isAllowedDelegationTokenOp()→CALL:getCurrentUser→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→CALL:rmDTSecretManager.cancelToken→IF_FALSE:id.getUser() == null→CALL:getShortName→IF_FALSE:!canceller.equals(owner) && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName.equals(renewer.toString()))→IF_FALSE:info == null→CALL:METRICS.trackRemoveToken→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Token cancellation requested for identifier: + formatTokenId(id)"
  },
  "26903f6b_5": {
    "exec_flow": "ENTRY→IF_FALSE:proxy==null→TRY→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→EXIT",
    "log": "[ERROR] Closing proxy or invocation handler caused exception"
  },
  "26903f6b_6": {
    "exec_flow": "ENTRY→IF_FALSE:proxy==null→TRY→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→EXIT",
    "log": "[ERROR] RPC.stopProxy called on non proxy: class="
  },
  "857bdea0_1": {
    "exec_flow": "ENTRY → TRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead. → TRY → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → LOG:LOGGER.DEBUG: Looking for FS supporting {} → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead. [DEBUG] Looking for FS supporting {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {}"
  },
  "857bdea0_2": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→ IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→ CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→ CALL:notifyListeners→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→ CALL:globalListeners.notifyListeners→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started"
  },
  "857bdea0_3": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→ IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→ IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→TRY→ CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→ CALL:globalListeners.notifyListeners→CATCH: Throwable e→ LOG:LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {}"
  },
  "6f5fd285_1": {
    "exec_flow": "ENTRY→FOREACH:regexMountPointList→CALL:regexMountPoint.resolve→FOREACH:interceptorList→FOREACH_EXIT→LOG:LOGGER.DEBUG→WHILE:srcMatcher.find()→CALL:replaceRegexCaptureGroupInPath→IF_FALSE:groupValue==null→FOREACH:groupRepresentationStrSetInDest→LOG:LOGGER.DEBUG→FOREACH_EXIT→WHILE_EXIT→IF_TRUE:URISyntaxException_occurred→CALL:org.slf4j.Logger:error→RETURN→EXIT",
    "log": "[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex() [DEBUG] parsedDestPath value is:${parsedDestPath} [ERROR] Got Exception while build resolve result. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null."
  },
  "5d53887d_1": {
    "exec_flow": "ENTRY→CALL:checkNotEmpty→CALL:createURL→TRY→CALL:getDoAsUser →CALL:getActualUgi→CALL:doAs→NEW:PrivilegedExceptionAction<HttpURLConnection> →CALL:createAuthenticatedURL→CALL:openConnection→CALL:getActualUgi →CALL:setUseCaches→CALL:setRequestMethod →IF_FALSE:method.equals(HTTP_POST)||method.equals(HTTP_PUT) →CALL:configureConnection→CALL:call→CALL:parseJSONKeyVersion→RETURN→EXIT",
    "log": "[WARN] Failed to connect to: [URL] [WARN] Failed to connect to {}: {} [DEBUG] Using loginUser when Kerberos is enabled but the actual user does not have either KMS Delegation Token or Kerberos Credentials [DEBUG] Logging user info [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "5d53887d_2": {
    "exec_flow": "ENTRY→TRY→IF_FALSE: jsonOutput != null→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN && (conn.getResponseMessage().equals(ANONYMOUS_REQUESTS_DISALLOWED) || conn.getResponseMessage().contains(INVALID_SIGNATURE))) || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED→ IF_TRUE: LOG.isDebugEnabled()→LOG: [DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW: DelegationTokenAuthenticatedURL.Token→ IF_TRUE: authRetryCount > 0→CALL: createConnection→CALL: org.apache.hadoop.crypto.key.kms.KMSClientProvider:call→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "66faa230_1": {
    "exec_flow": "ENTRY→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log>[DEBUG] Exception in closing {}</log>"
  },
  "66faa230_2": {
    "exec_flow": "ENTRY→IF_TRUE: state == State.UNINIT→CALL:init(verifyVersion)→CALL:Preconditions.checkState→TRY→CALL:getInputStream→NEW:BufferedInputStream→NEW:FSEditLogLoader.PositionTrackingInputStream→NEW:DataInputStream→TRY→CALL:readLogVersion→IF_TRUE:logVersion == -1→THROW:LogHeaderCorruptException(\"No header present in log (value is -1), probably due to disk space issues when it was created. The log has no transactions and will be sidelined.\")→CALL:IOUtils.cleanupWithLogger→RETURN→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log>[LOG] Reading log version</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "66faa230_3": {
    "exec_flow": "ENTRY→IF_TRUE: state == State.UNINIT→CALL:init(verifyVersion)→CALL:Preconditions.checkState→TRY→CALL:getInputStream→NEW:BufferedInputStream→NEW:FSEditLogLoader.PositionTrackingInputStream→NEW:DataInputStream→TRY→CALL:readLogVersion→IF_FALSE:logVersion != -1→CALL:NameNodeLayoutVersion.supports→IF_TRUE:supports AND logVersion < NameNodeLayoutVersion.CURRENT_LAYOUT_VERSION→TRY→CALL:LayoutFlags.read→IF_FALSE:EOFException→LOG:create→CALL:IOUtils.cleanupWithLogger→RETURN→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "<log>[LOG] Reading log version</log> <log>[LOG] Creating FSEditLogOp reader</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "66faa230_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:nextOp→RETURN→EXIT",
    "log": "<log>[INFO] Next operation retrieved from file input stream</log>"
  },
  "66faa230_5": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextOp→RETURN→EXIT",
    "log": "<log>[INFO] Next operation retrieved from redundant input stream</log>"
  },
  "66faa230_6": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:nextOp→RETURN→EXIT",
    "log": "<log>[INFO] Next operation retrieved from backup input stream</log>"
  },
  "90f30af4_1": {
    "exec_flow": "<segment>Parent.ENTRY</segment> <segment>[VIRTUAL_CALL]</segment> <segment>Child.PATH1: ENTRY→CALL:org.apache.hadoop.fs.RemoteIterator:hasNext()→EXIT</segment> <sequence>ENTRY→CALL:org.apache.hadoop.fs.RemoteIterator:next→RETURN→EXIT</sequence>",
    "log": "<log>[DEBUG] [{}], Requesting next {} uploads prefix {}, next key {}, next upload id {}</log> <log>[DEBUG] Listing found {} upload(s)</log> <log>[DEBUG] New listing state: {}</log> <log><![CDATA[[DEBUG] Next element retrieved from RemoteIterator]]></log> <log_entry>[DEBUG] Stripping trailing '/' from {q}</log_entry> <log_entry>[DEBUG] Getting path status for {} ({}); needEmptyDirectory={}</log_entry> <log_entry>[DEBUG] S3GetFileStatus {}</log_entry>"
  },
  "90f30af4_2": {
    "exec_flow": "<segment>ENTRY→IF_FALSE:currIterator.hasNext()→CALL:getNextIterator→ENTRY→CALL:fetchBatchesAsync→TRY→WHILE:listResult == null && (!isIterationComplete || !listResultQueue.isEmpty())→WHILE_EXIT→IF_FALSE:listResult == null→IF_TRUE:listResult.isFailedListing()→CATCH:InterruptedException→CALL:org.slf4j.Logger:error→THROW:IOException→RETURN→EXIT</segment>",
    "log": "<log><![CDATA[[ERROR] Thread got interrupted: {exception}]]></log>"
  },
  "56c9bcdb_1": {
    "exec_flow": "ENTRY→CALL:checkNotNull→TRY→CALL:rollNewVersionInternal→CALL:checkNotEmpty→IF_TRUE:material != null→CALL:jsonMaterial.put→CALL:createConnection→CALL:setRequestProperty→TRY→IF_TRUE: jsonOutput != null→CALL: getOutputStream→CALL: writeJson→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED)→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW: DelegationTokenAuthenticatedURL.Token→IF_TRUE: authRetryCount > 0→CALL: createConnection→CALL: call→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "56c9bcdb_2": {
    "exec_flow": "ENTRY→CALL:checkNotNull→TRY→CALL:rollNewVersionInternal→CALL:checkNotEmpty→IF_FALSE:material != null→CALL:createConnection→CALL:setRequestProperty→TRY→IF_FALSE: jsonOutput != null→IF_TRUE: (conn.getResponseCode() == HttpURLConnection.HTTP_FORBIDDEN || conn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED)→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()→NEW: DelegationTokenAuthenticatedURL.Token→IF_TRUE: authRetryCount > 0→CALL: createConnection→CALL: call→RETURN→EXIT",
    "log": "[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()"
  },
  "56c9bcdb_3": {
    "exec_flow": "ENTRY→TRY→CALL:getDoAsUser→CALL:getActualUgi→CALL:doAs→NEW:PrivilegedExceptionAction<HttpURLConnection>→CALL:createAuthenticatedURL→CALL:openConnection→CALL:getActualUgi→CALL:setUseCaches→CALL:setRequestMethod→IF_FALSE:method.equals(HTTP_POST)||method.equals(HTTP_PUT)→CALL:configureConnection→RETURN→EXIT",
    "log": "[WARN] Failed to connect to: [URL] [WARN] Failed to connect to {}: {}"
  },
  "6929b990_1": {
    "exec_flow": "ENTRY→CALL:StringUtils.startupShutdownMessage→CALL:GenericOptionsParser:<init>→TRY→CALL:parse→CALL:preProcessForWindows→CALL:preProcessForWindows→CALL:processGeneralOptions→IF_TRUE:line.hasOption(\"fs\")→CALL:FileSystem.setDefaultUri→IF_TRUE:line.hasOption(\"jt\")→IF_TRUE:optionValue.equalsIgnoreCase(\"local\")→CALL:set→CALL:set→IF_FALSE:line.hasOption(\"conf\")→IF_FALSE:line.hasOption('D')→IF_FALSE:line.hasOption(\"libjars\")→IF_FALSE:line.hasOption(\"files\")→IF_FALSE:line.hasOption(\"archives\")→CALL:setBoolean→IF_FALSE:line.hasOption(\"tokenCacheFile\")→EXCEPTION:processGeneralOptions→CATCH:ParseException e→CALL:printHelp→RETURN→CALL:launchDNSServer→EXIT",
    "log": "[INFO] Starting shutdown message [DEBUG] Generic options parsed [WARN] options parsing failed: + e.getMessage() [DEBUG] Launching DNS server"
  },
  "6929b990_2": {
    "exec_flow": "<entry>org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run()</entry> <virtual_call>[VIRTUAL_CALL]</virtual_call> <callees> <callee>org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service)</callee> </callees>",
    "log": "[DEBUG] noteFailure [INFO] Service {getName()} failed in state {failureState} [WARN] When stopping the service {service.getName()}"
  },
  "2d8e6ea4_1": {
    "exec_flow": "ENTRY→CALL:getPiggyBacksFromInput→CALL:encode→CALL:doEncode→CALL:org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode→CALL:PerformanceAdvisory.LOG.debug→CALL:doEncode→FOR_INIT→FOR_COND:i<encodingState.outputs.length→CALL:ByteBuffer.get→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs"
  },
  "ad29fefd_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> VIRTUAL_CALL → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getNameServiceIds → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG: WARN: Unexpected SecurityException in Configuration → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "879a2a4c_1": {
    "exec_flow": "ENTRY → CALL:checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "879a2a4c_2": {
    "exec_flow": "ENTRY → CALL:checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → CALL:getNamespaces → CALL:invokeConcurrent → CALL:updateDnMap → CALL:toArray → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "879a2a4c_3": {
    "exec_flow": "ENTRY → CALL:checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "879a2a4c_4": {
    "exec_flow": "ENTRY → CALL:checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → CALL:getNamespaces → CALL:invokeConcurrent → CALL:updateDnMap → CALL:toArray → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "60e9f71a_1": {
    "exec_flow": "ENTRY → IF_FALSE: clazz != null → CALL: org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:isAvailable() → IF_TRUE: ProcfsBasedProcessTree.isAvailable() → NEW: ProcfsBasedProcessTree → RETURN → EXIT",
    "log": "<log>[INFO] ProcfsBasedProcessTree currently is supported only on Linux.</log>"
  },
  "60e9f71a_2": {
    "exec_flow": "ENTRY→IF_TRUE: Shell.WINDOWS→IF_FALSE: !Shell.hasWinutilsPath()→TRY→CALL:shellExecutor.execute→EXCEPTION: execute→CATCH: IOException e→LOG: LOG.ERROR: StringUtils.stringifyException(e)→IF_TRUE: output != null && output.contains(\"Prints to stdout a list of processes in the task\")→RETURN→EXIT",
    "log": "<log>[ERROR] StringUtils.stringifyException(e)</log>"
  },
  "60e9f71a_3": {
    "exec_flow": "ENTRY→IF_TRUE: Shell.WINDOWS→IF_FALSE: !Shell.hasWinutilsPath()→TRY→CALL:shellExecutor.execute→EXCEPTION: execute→CATCH: IOException e→LOG: LOG.ERROR: StringUtils.stringifyException(e)→IF_FALSE: output != null && output.contains(\"Prints to stdout a list of processes in the task\")→RETURN→EXIT",
    "log": "<log>[ERROR] StringUtils.stringifyException(e)</log>"
  },
  "959ec9c5_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop()→FOR_EXIT→IF_TRUE: firstException != null→THROW: ServiceStateException.convert(firstException)→EXIT",
    "log": "[DEBUG] Stopping service #i: {service} [DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "959ec9c5_2": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop()→FOR_EXIT→IF_FALSE: firstException != null→EXIT",
    "log": "[DEBUG] Stopping service #i: {service} [DEBUG] Service: {} entered state {} [DEBUG] noteFailure, exception [INFO] Service {} failed in state {}"
  },
  "d99ca1c3_1": {
    "exec_flow": "<sequence> ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT </sequence> <sequence> ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT </sequence> <sequence> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT → CALL:getInt → NEW:ScheduledThreadPoolExecutor → LOG:LOG.DEBUG:Scheduler started → EXIT → LOG:LOG.INFO:Using FileSystemAccess JARs version [{}], VersionInfo.getVersion() → IF_TRUE:security.equals(\"kerberos\") → CALL:getServiceConfig → CALL:get → CALL:trim → CALL:getServiceConfig → CALL:get → CALL:trim → CALL:UserGroupInformation.setConfiguration → CALL:UserGroupInformation.loginUserFromKeytab → LOG:LOG.INFO:Using FileSystemAccess Kerberos authentication, principal [{}] keytab [{}] → CALL:loadHadoopConf → CALL:getNewFileSystemConfiguration → CALL:setRequiredServiceHadoopConf → CALL:getTrimmedStringCollection → CALL:iterator → LOG:LOG.DEBUG:FileSystemAccess FileSystem configuration: → EXIT </sequence>",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Scheduler started</template> </log_entry> <log_entry> <level>INFO</level> <template>Using FileSystemAccess JARs version {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Using FileSystemAccess Kerberos authentication, principal {} keytab {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>FileSystemAccess FileSystem configuration:</template> </log_entry>"
  },
  "d99ca1c3_2": {
    "exec_flow": "<sequence> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT → LOG:LOG.INFO:Using FileSystemAccess JARs version [{}], VersionInfo.getVersion() → IF_TRUE:security.equals(\"simple\") → CALL:getServer → CALL:getConfigDir → CALL:set → CALL:UserGroupInformation.setConfiguration → LOG:LOG.INFO:Using FileSystemAccess simple/pseudo authentication, principal [{}] → CALL:loadHadoopConf → CALL:getNewFileSystemConfiguration → CALL:setRequiredServiceHadoopConf → CALL:getTrimmedStringCollection → CALL:iterator → LOG:LOG.DEBUG:FileSystemAccess FileSystem configuration: → EXIT </sequence>",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>Using FileSystemAccess JARs version {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Using FileSystemAccess simple/pseudo authentication, principal {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>FileSystemAccess FileSystem configuration:</template> </log_entry>"
  },
  "d99ca1c3_3": {
    "exec_flow": "Initialize Security Groups with Configuration → [VIRTUAL_CALL] Initialize Security Groups with Configuration and Timer",
    "log": "<log>org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration) - Initializing with Configuration</log> <log>org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration, org.apache.hadoop.util.Timer) - Initializing with Configuration and Timer</log>"
  },
  "d1c371d4_1": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Processing {} of type {}, event.getNodeId(), event.getType() → CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→ LOG:LOG.ERROR:Can't handle this event at current state, e→LOG:LOG.ERROR:Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState→ IF_FALSE:oldState != getState()→CALL:writeLock.unlock→IF_FALSE: c != null → LOG:LOG.WARN:Event + event + sent to absent container + event.getContainerID()→EXIT",
    "log": "[DEBUG] Processing {} of type {}, event.getNodeId(), event.getType() [ERROR] Can't handle this event at current state, e [ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState [WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "9bf689d8_1": {
    "exec_flow": "ENTRY→CALL:resolvePathForStartFile→IF_TRUE: dir.resolvePath returns INodesInPath→IF_TRUE: dir.isPermissionEnabled()→CALL: dir.checkAncestorAccess→INode inode = iip.getLastINode()→IF_TRUE: inode.isDirectory() throws FileAlreadyExistsException→CALL: check INodes and permissions→CHECK: flag.contains(CreateFlag.OVERWRITE) and dir.isPermissionEnabled()→IF_TRUE: dir.checkPathAccess ENTRY→IF_TRUE→CALL:readLock→TRY→CALL:checkPermission→CALL:org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:checkPermission→THROW:new AccessControlException→EXIT→CALL: FSDirectory:resolvePath→ENTRY→CALL:resolveComponents→IF_TRUE: remainder > 0→CALL: copyOf→CALL: System.arraycopy→IF_TRUE: NameNode.LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→CALL: NameNode.LOG.debug→CALL: org.slf4j.Logger:debug→RETURN→EXIT",
    "log": "[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)]"
  },
  "25190db3_1": {
    "exec_flow": "ENTRY → IF_TRUE:isInSafeMode → THROW:SafeModeException → IF_TRUE:haEnabled && haContext != null && haContext.getState().getServiceState() == HAServiceState.ACTIVE && isInStartupSafeMode() → THROW:RetriableException → RETURN → EXIT",
    "log": "<!-- No additional log statements to merge, inheriting from parent logic -->"
  },
  "115c9582_1": {
    "exec_flow": "<step>ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>CALL:org.apache.hadoop.fs.RemoteIterator:next</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Next element retrieved from RemoteIterator</log>"
  },
  "115c9582_2": {
    "exec_flow": "<step>ENTRY</step> <step>FOR_INIT</step> <step>FOR_COND: isLink</step> <step>CALL: org.apache.hadoop.fs.FileContext:getFSofPath</step> <step>CALL: org.apache.hadoop.fs.FSLinkResolver:next</step> <step>CATCH: UnresolvedLinkException</step> <step>CALL: qualifySymlinkTarget</step> <step>TRY</step> <step>FOR_COND: isLink</step> <step>FOR_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[INFO] Resolving file system path</log> <log>[WARN] Unresolved link encountered</log> <log>[ERROR] Error while scanning directory {path}</log>"
  },
  "33421cfe_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: isSorted()→CALL:processPaths→CALL:getDirectoryContents→EXIT→IF_FALSE: groupSize == 0→WHILE: itemsIterator.hasNext()→CALL: hasNext→CALL: next→CALL: processPaths→WHILE_COND: itemsIterator.hasNext()→WHILE_EXIT→CALL: cleanupRemoteIterator→ENTRY→CALL:org.slf4j.Logger:isDebugEnabled→CALL:org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString→CALL:org.slf4j.Logger:debug→IF_TRUE:source instanceof Closeable→CALL:IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "<log>[INFO] Processing items with groupSize > 0</log> <log>[DEBUG] Next element retrieved from RemoteIterator</log> <log>org.apache.hadoop.fs.FileSystem$5:handleFileStat - BEGIN</log> <log>org.slf4j.Logger:debug - Handling file status</log> <log>org.slf4j.Logger:debug - Next element retrieved from RemoteIterator</log> <log>org.apache.hadoop.fs.FileSystem$5:handleFileStat - END</log> <log>[DEBUG] RemoteIterator Statistics: {}</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "33421cfe_2": {
    "exec_flow": "ENTRY→CALL:checkIfExists→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta==null→LOG:DEBUG: List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0→LOG.isDebugEnabled()→LOG:DEBUG: Path: [{}] is a directory. COS key: [{}]→CALL:org.apache.hadoop.fs.shell.PathData:<init>→FOR_END→CALL:Arrays.sort→RETURN→EXIT",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "33421cfe_3": {
    "exec_flow": "ENTRY→CALL:checkIfExists→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:DEBUG: Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.shell.PathData:<init>→FOR_END→CALL:Arrays.sort→RETURN→EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "33421cfe_4": {
    "exec_flow": "ENTRY→CALL:checkIfExists→FOR_INIT→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→IF_TRUE:(stats.length > 0)→CALL:Arrays.sort→RETURN→EXIT→CALL:listStatus→RETURN",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "8c9550b4_1": {
    "exec_flow": "ENTRY→CALL:validatePositionedReadArgs→IF_FALSE:length==0→TRY→READ_LOOP(length)→CALL:org.slf4j.Logger:debug→FINALLY→CLOSE→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "62727d2a_1": {
    "exec_flow": "ENTRY → WHILE: true → WHILE_COND: true → IF_FALSE: !keepRunning → TRY → LOG:DEBUG:Interrupted while waiting for queue → CALL:getProgress → CALL:onError → CONTINUE → WHILE_COND → TRY → CALL:take → IF_FALSE: object-instanceof-Throwable → CAST:AllocateResponse → IF_TRUE: response-getCollectorInfo → CALL:getRegisteredTimelineV2Client → IF_TRUE: timelineClient-not-null → CALL:setTimelineCollectorInfo → IF_TRUE: !updatedNodes-empty → CALL:onNodesUpdated → CALL:str.append(\"Nodes updated info: \").append → FOREACH:updatedNodes → CALL:updatedNodes.getNodeId().append → CALL:updatedNodes.getNodeState().append → CALL:updatedNodes.getHealthReport().append → FOREACH_EXIT → CALL:org.slf4j.Logger:warn → LOG:INFO:onNodesUpdated: + Joiner.on(,).join(updatedNodes) → CALL:org.slf4j.Logger:info → IF_TRUE: !completed-empty → CALL:onContainersCompleted → IF_TRUE: handler-instanceof-AMRMClientAsync$AbstractCallbackHandler → CALL:onContainersUpdated → IF_TRUE: !allocated-empty → CALL:onContainersAllocated → IF_TRUE: preemptionMessage-not-null → CALL:onPreemptionMessageReceived → IF_TRUE: !response-getContainersFromPreviousAttempts-empty → CALL:onContainersReceivedFromPreviousAttempts → IF_TRUE: !rejectedSchedulingRequests-empty → CALL:onRequestsRejected → CALL:getProgress → CATCH:Throwable → LOG:ERROR:Error in RMCallbackHandler: <exception details> → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → THROW:YarnRuntimeException → EXIT",
    "log": "[DEBUG] Interrupted while waiting for queue [ERROR] Error in RMCallbackHandler: <exception details> [WARN] Nodes updated info: (detailed node information) [INFO] onNodesUpdated: <concatenated node updates>"
  },
  "62727d2a_2": {
    "exec_flow": "ENTRY → IF_FALSE:collectorInfo==null → IF_TRUE:collectorInfo.getCollectorToken()!=null → CALL:setTimelineDelegationToken → IF_TRUE:collectorInfo.getCollectorAddr()!=null&&!collectorInfo.getCollectorAddr().isEmpty()&&!collectorInfo.getCollectorAddr().equals(timelineServiceAddress) → CALL:getCollectorAddr → LOG:INFO:Updated timeline service address to + timelineServiceAddress → EXIT",
    "log": "[INFO] Updated timeline service address to + timelineServiceAddress"
  },
  "62727d2a_3": {
    "exec_flow": "ENTRY → LOG:LOG.INFO:onContainersUpdated: + Joiner.on(,).join(containers) → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[INFO] onContainersUpdated: ..."
  },
  "1d8d0a40_1": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:forceCompleteBlock</entry> ENTRY→CALL:commitBlock→CALL:removeStaleReplicas→ENTRY <entry>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStaleReplicas</entry> ENTRY→FOREACH: staleReplicas→CALL:removeStoredBlock <entry>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock</entry> EXIT <log_sequence> <log>BLOCK* removeStoredBlock: {} from {}</log> <log>BLOCK* removeStoredBlock: {} has already been removed from node {}</log> <log>BLOCK* ExcessRedundancyMap.remove({}, {})</log> <log>[DEBUG] BLOCK* Removing stale replica {} of {}</log> </log_sequence> →FOREACH_EXIT→EXIT →CALL:completeBlock→ENTRY <entry>org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock</entry> ENTRY→IF_FALSE:!isSafeModeTrackingBlocks()→SYNC:this→LOG:LOG.DEBUG:Adjusting block totals from {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal→CALL:setBlockTotal→CALL:checkSafeMode→EXIT",
    "log": "<log>BLOCK* removeStoredBlock: {} from {}</log> <log>BLOCK* removeStoredBlock: {} has already been removed from node {}</log> <log>BLOCK* ExcessRedundancyMap.remove({}, {})</log> <log>[DEBUG] BLOCK* Removing stale replica {} of {}</log>"
  },
  "1d8d0a40_2": {
    "exec_flow": "ENTRY→IF_FALSE: queue == null→CALL:processQueuedMessages→EXIT",
    "log": "<log>BLOCK* removeStoredBlock: {} from {}</log> <log>BLOCK* removeStoredBlock: {} has already been removed from node {}</log> <log>BLOCK* ExcessRedundancyMap.remove({}, {})</log> <log>[DEBUG] BLOCK* Removing stale replica {} of {}</log> <log>[DEBUG] Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal</log>"
  },
  "1d8d0a40_3": {
    "exec_flow": "ENTRY→IF_FALSE: uc == null→IF_TRUE: uc.getUnderConstructionFeature() != null→CALL: DatanodeStorageInfo.decrementBlocksScheduled→CALL: fsd.getBlockManager().removeBlockFromMap→IF_TRUE: NameNode.stateChangeLog.isDebugEnabled()→CALL: NameNode.stateChangeLog.debug→CALL: updateCount→RETURN→EXIT",
    "log": "<log>[DEBUG] Processing previously queued message</log> <log>[DEBUG] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}</log>"
  },
  "1d8d0a40_4": {
    "exec_flow": "ENTRY→IF_FALSE: uc == null→IF_FALSE: uc.getUnderConstructionFeature() != null→CALL: fsd.getBlockManager().removeBlockFromMap→IF_TRUE: NameNode.stateChangeLog.isDebugEnabled()→CALL: NameNode.stateChangeLog.debug→CALL: updateCount→RETURN→EXIT",
    "log": "<log>[DEBUG] DIR* FSDirectory.removeBlock: with block is removed from the file system</log>"
  },
  "23c21dd5_1": {
    "exec_flow": "ENTRY→CALL:readLock.lock→TRY→FOREACH:appSchedulingInfo.getSchedulerKeys()→CALL:AppPlacementAllocator:getPendingAsk(java.lang.String)→CALL:Resources:createResource(int)→CALL:Resources:multiply(org.apache.hadoop.yarn.api.records.Resource,double)→CALL:Resources:addTo(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)→ENTRY→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes()→FOR_COND: i < maxLength →TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int)→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int)→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue(int,long)→FOR_BODY→FOR_UPDATE→FOR_COND: i < maxLength→EXCEPTION:ResourceNotFoundException→LOG.warn→FOR_UPDATE→FOR_COND: i < maxLength→FOR_EXIT→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:"
  },
  "973b5d87_1": {
    "exec_flow": "<sequence> ENTRY → LOG:INFO:Starting AMRMProxyService → CALL:YarnRPC:create → CALL:UserGroupInformation:setConfiguration → [VIRTUAL_CALL] → LOG:DEBUG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:DEBUG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT → CALL:getSocketAddr → CALL:Configuration:getInt → CALL:Configuration:get → CALL:serverConf:set → CALL:AMRMProxyTokenSecretManager:start → CALL:YarnRPC:getServer → IF_TRUE:conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,false) → CALL:Server:refreshServiceAcl → IF_TRUE:services != null → FOREACH:services → CALL:org.apache.hadoop.conf.Configuration:get → LOG:DEBUG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:DEBUG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:org.apache.hadoop.security.authorize.AccessControlList:<init> → NEW:HashSet<String> → FOREACH:userGroupStrings → FOREACH_EXIT → IF_TRUE:!allAllowed → IF_TRUE:userGroupStrings.length >= 1 && userGroupStrings[0] != null → CALL:getTrimmedStringCollection → IF_TRUE:userGroupStrings.length == 2 && userGroupStrings[1] != null → CALL:getTrimmedStringCollection → CALL:groupsMapping.cacheGroupsAdd → ENTRY → TRY → CALL:impl.cacheGroupsAdd → EXCEPTION:IOException → CATCH:IOException → LOG:WARN:Error caching groups, e → CALL:org.slf4j.Logger:warn → EXIT → FOREACH_EXIT → EXIT → CALL:Server:start → LOG:INFO:AMRMProxyService listening on address: + this.server.getListenerAddress() → CALL:CompositeService:serviceStart → EXIT </sequence>",
    "log": "<log>[INFO] Starting AMRMProxyService</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Error caching groups, e</log> <log>[INFO] AMRMProxyService listening on address: + this.server.getListenerAddress()</log>"
  },
  "973b5d87_2": {
    "exec_flow": "<sequence> ENTRY → LOG:INFO:Starting AMRMProxyService → CALL:YarnRPC:create → CALL:UserGroupInformation:setConfiguration → [VIRTUAL_CALL] → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → CALL:getSocketAddr → CALL:Configuration:getInt → CALL:Configuration:get → CALL:serverConf:set → CALL:AMRMProxyTokenSecretManager:start → CALL:YarnRPC:getServer → IF_FALSE:conf.getBoolean(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,false) → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:DEBUG:Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT → CALL:Server:start → LOG:INFO:AMRMProxyService listening on address: + this.server.getListenerAddress() → CALL:CompositeService:serviceStart → EXIT </sequence>",
    "log": "<log>[INFO] Starting AMRMProxyService</log> <log>[DEBUG] Creating new Groups object</log> <log>[INFO] AMRMProxyService listening on address: + this.server.getListenerAddress()</log>"
  },
  "973b5d87_3": {
    "exec_flow": "ENTRY → IF_TRUE:this.currentMasterKey == null → CALL:createNewMasterKey → IF_TRUE:this.nmStateStore != null → TRY → CALL:this.nmStateStore.storeAMRMProxyCurrentMasterKey → EXCEPTION:storeAMRMProxyCurrentMasterKey → CATCH:IOException e → LOG:ERROR:Unable to update current master key in state store, e → CALL:org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager$MasterKeyRoller:<init> → CALL:scheduleAtFixedRate → EXIT",
    "log": "<log>[ERROR] Unable to update current master key in state store, e</log>"
  },
  "973b5d87_4": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Thread.currentThread().getName() + : starting → CALL:SERVER.set → WHILE:running → WHILE_COND:running → CALL:callQueue.take → CALL:call.getClientStateId → CALL:alignmentContext.getLastSeenStateId → CALL:requeueCall → LOG:LOG.DEBUG:Thread.currentThread().getName() + : call for RpcKind + call.rpcKind → CALL:CurCall.set → CALL:call.getRemoteUser → ENTRY-CHILD:UserGroupInformation:doAs → TRY → LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}] → CATCH:PrivilegedActionException → LOG:LOG.DEBUG:PrivilegedActionException as: {} → CALL:call.run → CATCH:InterruptedException → LOG:LOG.INFO:Thread.currentThread().getName() + unexpectedly interrupted → CALL:traceScope.addTimelineAnnotation → CATCH:Exception → LOG:LOG.INFO:Thread.currentThread().getName() caught an exception → CALL:IOUtils.cleanupWithLogger → CALL:updateMetrics → LOG:ProcessingDetails.LOG.DEBUG:Served → WHILE_EXIT → LOG:LOG.DEBUG:Thread.currentThread().getName() + : exiting → ENTRY → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "<log>[DEBUG] Thread.currentThread().getName(): starting</log> <log>[DEBUG] Thread.currentThread().getName(): call for RpcKind + call.rpcKind</log> <log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log> <log>[INFO] Thread.currentThread().getName() unexpectedly interrupted</log> <log>[INFO] Thread.currentThread().getName() caught an exception</log> <log>[DEBUG] Served: [{}]</log> <log>[DEBUG] Thread.currentThread().getName(): exiting</log>"
  },
  "973b5d87_5": {
    "exec_flow": "ENTRY → CALL:getServer → [VIRTUAL_CALL] → LOG:debug → CALL:getServerFactory → CALL:getServer → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating a HadoopYarnProtoRpc server for protocol {} with {} handlers</log>"
  },
  "154e3572_1": {
    "exec_flow": "<!-- Merged and optimized execution flow structure for P1-C3 -->",
    "log": "[SERVICE]: Error in handling event type {0} [ERROR] [SERVICE]: Invalid event {1} at {2}. [INFO] [SERVICE] Transitioned from {} to {} on {} event."
  },
  "154e3572_2": {
    "exec_flow": "ENTRY→IF_TRUE:taskAttempt.getLaunchTime()==0→CALL:sendJHStartEventForAssignedFailTask→CALL:taskAttempt.setFinishTime→IF_TRUE:event instanceof TaskAttemptKillEvent→CALL:taskAttempt.addDiagnosticInfo→CALL:taskAttempt.eventHandler.handle→IF_TRUE:withdrawsContainerRequest→CALL:taskAttempt.eventHandler.handle→SWITCH:finalState→CASE:[]→LOG:LOG.ERROR:Task final state is not FAILED or KILLED: + finalState→IF_TRUE:finalState==TaskAttemptStateInternal.FAILED→CALL:taskAttempt.eventHandler.handle→CALL:taskAttempt.eventHandler.handle→EXIT",
    "log": "[ERROR] Task final state is not FAILED or KILLED: + finalState"
  },
  "154e3572_3": {
    "exec_flow": "ENTRY→IF_TRUE:taskAttempt.getLaunchTime()==0→CALL:sendJHStartEventForAssignedFailTask→CALL:taskAttempt.setFinishTime→IF_TRUE:event instanceof TaskAttemptKillEvent→CALL:taskAttempt.addDiagnosticInfo→CALL:taskAttempt.eventHandler.handle→IF_TRUE:withdrawsContainerRequest→CALL:taskAttempt.eventHandler.handle→SWITCH:finalState→CASE:[]→LOG:LOG.ERROR:Task final state is not FAILED or KILLED: + finalState→IF_FALSE:finalState==TaskAttemptStateInternal.FAILED→IF_TRUE:finalState==TaskAttemptStateInternal.KILLED→CALL:taskAttempt.eventHandler.handle→CALL:taskAttempt.eventHandler.handle→EXIT",
    "log": "[ERROR] Task final state is not FAILED or KILLED: + finalState"
  },
  "45534e73_1": {
    "exec_flow": "ENTRY→VIRTUAL_CALL→ENTRY→IF:hsr!=null→CALL:getCallerUserGroupInformation→IF:callerUGI==null→LOG:LOG.ERROR:Unable to obtain user name, user not authenticated→RETURN→EXIT ELSE:CALL:createRemoteUser→IF:callerUGI==null→LOG:LOG.ERROR:Unable to obtain user name, user not authenticated→RETURN→EXIT ELSE:LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}]→CALL:doAs",
    "log": "[ERROR] Unable to obtain user name, user not authenticated [DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "78042a3c_1": {
    "exec_flow": "ENTRY -> IF_FALSE: conf == null -> IF_FALSE: isInState(STATE.INITED) -> SYNC: stateChangeLock -> IF_TRUE: enterState(STATE.INITED) != STATE.INITED -> CALL: setConfig -> TRY -> CALL: org.apache.hadoop.yarn.client.api.TimelineClient:init(org.apache.hadoop.conf.Configuration) -> TRY -> CALL: serviceInit -> IF_TRUE: isInState(STATE.INITED) -> CALL: notifyListeners -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> EXIT",
    "log": "[DEBUG] Service: {} entered state {}"
  },
  "78042a3c_2": {
    "exec_flow": "ENTRY -> IF_FALSE: conf == null -> IF_FALSE: isInState(STATE.INITED) -> SYNC: stateChangeLock -> IF_TRUE: enterState(STATE.INITED) != STATE.INITED -> CALL: setConfig -> TRY -> ENTRY(serviceInit) -> IF_TRUE(conf != config) -> CALL: org.slf4j.Logger:debug(java.lang.String) -> CALL: setConfig -> EXIT(serviceInit) -> IF_TRUE: isInState(STATE.INITED) -> CALL: notifyListeners -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> EXIT",
    "log": "[DEBUG] Config has been overridden during init [DEBUG] Service: {} entered state {}"
  },
  "78042a3c_3": {
    "exec_flow": "ENTRY -> IF_FALSE: conf == null -> IF_FALSE: isInState(STATE.INITED) -> SYNC: stateChangeLock -> IF_TRUE: enterState(STATE.INITED) != STATE.INITED -> CALL: setConfig -> TRY -> CALL: serviceInit -> IF_TRUE: isInState(STATE.INITED) -> TRY -> CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners -> CALL: globalListeners.notifyListeners -> CATCH: Throwable e -> LOG: LOG.WARN: Exception while notifying listeners of {}, this, e -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> EXIT",
    "log": "[WARN] Exception while notifying listeners of {} [DEBUG] Service: {} entered state {}"
  },
  "78042a3c_4": {
    "exec_flow": "ENTRY → FOR_INIT → FOR_COND: i >= 0 → CALL: org.apache.hadoop.service.AbstractService:stop() → FOR_EXIT → IF_TRUE: firstException != null → THROW: ServiceStateException.convert(firstException) → EXIT",
    "log": "[DEBUG] Stopping service #i: {service} [DEBUG] Service: {} entered state {} [DEBUG] noteFailure [WARN] Exception while notifying listeners of {} [DEBUG] Ignoring re-entrant call to stop() [INFO] Service {} failed in state {}"
  },
  "78042a3c_5": {
    "exec_flow": "ENTRY → FOR_INIT → FOR_COND: i >= 0 → CALL: org.apache.hadoop.service.AbstractService:stop() → FOR_EXIT → IF_FALSE: firstException != null → EXIT",
    "log": "[DEBUG] Stopping service #i: {service} [DEBUG] Service: {} entered state {} [DEBUG] noteFailure [WARN] Exception while notifying listeners of {} [DEBUG] Ignoring re-entrant call to stop() [INFO] Service {} failed in state {}"
  },
  "cf68b944_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:substituteVars → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → RETURN → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "cf68b944_2": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "cf68b944_3": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "434988ff_1": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "434988ff_2": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Cannot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "defa5749_1": {
    "exec_flow": "ENTRY → CALL: trashPolicy.createCheckpoint → CALL: trashPolicy.deleteCheckpointsImmediately → LOG: LOG.INFO: Created trash checkpoint: {checkpoint.toUri().getPath()} → CALL: listStatus → FOR_INIT → FOR_COND: i < dirs.length → FOR_BODY → TRY → CALL: getTimeFromCheckpoint → IF_COND: (now - deletionInterval) > time || deleteImmediately → IF_BODY_TRUE → CALL: fs.delete → LOG: LOG.INFO: Deleted trash checkpoint: + dir → FOR_UPDATE → FOR_COND: i < dirs.length → FOR_EXIT → EXIT",
    "log": "[INFO] Created trash checkpoint: {checkpoint.toUri().getPath()} [INFO] Deleted trash checkpoint: + dir"
  },
  "defa5749_2": {
    "exec_flow": "ENTRY→CALL:exists→IF_FALSE: !fs.exists(new Path(trashRoot, CURRENT))→SYNC: CHECKPOINT→NEW: Path→CALL:format→WHILE: true→WHILE_COND: true→TRY→CALL: rename→LOG: LOG.INFO: Created trash checkpoint: {checkpoint.toUri().getPath()} →EXCEPTION: FileAlreadyExistsException→CATCH: FileAlreadyExistsException e→IF_TRUE: ++attempt > 1000→THROW: IOException→EXIT",
    "log": "[INFO] Created trash checkpoint: {checkpoint.toUri().getPath()}"
  },
  "defa5749_3": {
    "exec_flow": "ENTRY → CALL: trashPolicy.createCheckpoint → CALL: trashPolicy.deleteCheckpointsImmediately → LOG: Created trash checkpoint: {checkpoint.toUri().getPath()} → CALL: org.apache.hadoop.fs.FileSystem:listStatus → FOR_INIT → FOR_COND → IF: (name.equals(CURRENT.getName())) → IF_Else → CALL: getTimeFromCheckpoint → EXCEPTION: ParseException → CALL: org.slf4j.Logger:warn → FOR_END → EXIT",
    "log": "[INFO] Created trash checkpoint: {checkpoint.toUri().getPath()} [WARN] Unexpected item in trash: ... Ignoring."
  },
  "2235d661_1": {
    "exec_flow": "<step>ENTRY</step> <step> <condition>IF_TRUE: isInState(STATE.STOPPED)</condition> <steps> <step>RETURN</step> </steps> </step> <step> <condition>IF_FALSE: isInState(STATE.STOPPED)</condition> <steps> <step> <sync>stateChangeLock</sync> <step> <condition>IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED</condition> <steps> <step>TRY</step> <step>CALL: serviceStop</step> <step> <sync> <condition>EXCEPTION: serviceStop</condition> <steps> <step>CATCH: Exception e</step> <step>CALL: noteFailure</step> <step>THROW: ServiceStateException.convert(e)</step> </steps> </sync> </step> </steps> </step> <step> <condition>FINALLY</condition> <steps> <step>terminationNotification.set(true)</step> <step> <sync> <condition /> <steps> <step>terminationNotification.notifyAll()</step> <step>CALL: notifyListeners</step> <step>ENTRY</step> <step>TRY</step> <step>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>EXIT</step> <step>CATCH: Throwable e</step> <step>LOG: LOG.WARN: Exception while notifying listeners of {getName()}</step> </steps> </sync> </step> </steps> </step> <step> <condition>IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED</condition> <steps> <step>LOG: LOG.DEBUG: Ignoring re-entrant call to stop()</step> <step>CALL: notifyListeners</step> <step>ENTRY</step> <step>TRY</step> <step>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>EXIT</step> <step>CATCH: Throwable e</step> <step>LOG: LOG.WARN: Exception while notifying listeners of {getName()}</step> </steps> </step> </step> </steps> </step> <step>EXIT</step>",
    "log": "<log> <level>DEBUG</level> <message>Ignoring re-entrant call to stop()</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {getName()}</message> </log>"
  },
  "5b78f110_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG: LOG.DEBUG: Creating new Groups object → NEW: Groups → CALL:<init> → RETURN → EXIT ENTRY→CALL:org.slf4j.Logger:info→LOG:LOG.INFO:clearing userToGroupsMap cache→TRY→CALL:cacheGroupsRefresh→CALL:invalidateAll→IF_TRUE:isNegativeCacheEnabled()→CALL:negativeCache.clear→EXIT ENTRY→[VIRTUAL_CALL]→ENTRY→CALL:formatLock.readLock().lock→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore:retrieve()→ENTRY→TRY→CALL:getZkData→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→RETURN→EXIT→CALL:formatLock.readLock().unlock→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [WARN] Unexpected SecurityException in Configuration [DEBUG] Creating new Groups object [INFO] clearing userToGroupsMap cache [ERROR] Failed to retrieve configuration from zookeeper store"
  },
  "5b78f110_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "5b78f110_3": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Refreshing Reservation system→CALL: writeLock.lock→TRY→FOREACH: planQueueNames→IF_TRUE: !plans.containsKey→CALL:initializePlan→CALL:plans.put→FOREACH_EXIT→IF_TRUE: planFollower != null→CALL: planFollower.setPlans→CALL: writeLock.unlock→EXIT",
    "log": "[INFO] Refreshing Reservation system"
  },
  "5b78f110_4": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Refreshing Reservation system→CALL: writeLock.lock→TRY→FOREACH: planQueueNames→IF_FALSE: !plans.containsKey→LOG: LOG.WARN: Plan based on reservation queue {} already exists.→FOREACH_EXIT→IF_TRUE: planFollower != null→CALL: planFollower.setPlans→CALL: writeLock.unlock→EXIT",
    "log": "[INFO] Refreshing Reservation system [WARN] Plan based on reservation queue {} already exists."
  },
  "5b78f110_5": {
    "exec_flow": "ENTRY→FOREACH:confFileNames→CALL:org.apache.hadoop.yarn.conf.ConfigurationProvider:getConfigurationInputStream→ENTRY→IF_FALSE: name == null || name.isEmpty()→IF_TRUE: YarnConfiguration.RM_CONFIGURATION_FILES.contains(name) || YarnConfiguration.NM_CONFIGURATION_FILES.contains(name)→TRY→IF_TRUE: url == null→CALL: org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT→IF_FALSE: url == null→CALL: org.slf4j.Logger:info(java.lang.String)→CALL: openStream→RETURN→EXIT→CALL:org.apache.hadoop.conf.Configuration:addResource→FOREACH_EXIT→RETURN→EXIT",
    "log": "[INFO] name not found [INFO] found resource name at url [INFO] {filePath} not found"
  },
  "c57b39e6_1": {
    "exec_flow": "<step>LOG.debug(\"noteFailure\", exception);</step> <step>if (exception == null) { return; }</step> <step> <sync> <condition>failureCause == null</condition> <steps> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> </steps> </sync> </step>",
    "log": "<log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log>"
  },
  "98c7ec43_1": {
    "exec_flow": "ENTRY → IF_TRUE: request == null || request.getApplicationId() == null → CALL: routerMetrics.incrAppsFailedKilled → CALL: RouterServerUtil.logAndThrowException → TRY → CALL: getApplicationHomeSubCluster → CALL: getApplicationId → CALL: getApplicationId → TRY → LOG: LOG.INFO: forceKillApplication applicationId on SubCluster subClusterId → CALL: getClientRMProxyForSubCluster → CALL: forceKillApplication → IF_FALSE: response == null → CALL: routerMetrics.succeededAppsKilled → RETURN → EXIT",
    "log": "<log>[INFO] forceKillApplication applicationId on SubCluster subClusterId</log>"
  },
  "98c7ec43_2": {
    "exec_flow": "ENTRY → IF_FALSE: request == null || request.getApplicationId() == null → TRY → CALL: getApplicationHomeSubCluster → CALL: getApplicationId → CALL: getApplicationId → TRY → LOG: LOG.INFO: forceKillApplication applicationId on SubCluster subClusterId → EXCEPTION: info → CATCH: Exception e → CALL: routerMetrics.incrAppsFailedKilled → LOG: LOG.ERROR: Unable to kill the application report for applicationId to SubCluster subClusterIdId, e → THROW:e → EXIT",
    "log": "<log>[ERROR] Unable to kill the application report for applicationId to SubCluster subClusterIdId</log>"
  },
  "98c7ec43_3": {
    "exec_flow": "ENTRY → IF_FALSE: request == null || request.getApplicationId() == null → TRY → CALL: getApplicationHomeSubCluster → CALL: getApplicationId → CALL: getApplicationId → TRY → LOG: LOG.INFO: forceKillApplication applicationId on SubCluster subClusterId → CALL: getClientRMProxyForSubCluster → CALL: forceKillApplication → IF_TRUE: response == null → LOG: LOG.ERROR: No response when attempting to kill the application applicationId to SubCluster subClusterIdId → CALL: routerMetrics.succeededAppsKilled → RETURN → EXIT",
    "log": "<log>[ERROR] No response when attempting to kill the application applicationId to SubCluster subClusterIdId</log>"
  },
  "98c7ec43_4": {
    "exec_flow": "ENTRY → IF_FALSE: request == null || request.getApplicationId() == null → TRY → CALL: getApplicationHomeSubCluster → CALL: getApplicationId → CALL: getApplicationId → TRY → LOG: LOG.INFO: forceKillApplication applicationId on SubCluster subClusterId → CALL: getClientRMProxyForSubCluster → CALL: forceKillApplication → IF_FALSE: response == null → CALL: routerMetrics.succeededAppsKilled → RETURN → EXIT",
    "log": "<log>[INFO] forceKillApplication applicationId on SubCluster subClusterId</log>"
  },
  "98c7ec43_5": {
    "exec_flow": "ENTRY→SYNC: this.userPipelineMap→IF_TRUE: this.userPipelineMap.containsKey(user)→LOG: LOG.INFO: Request to start an already existing user: {} was received, so ignoring., user→CALL: get→RETURN→EXIT",
    "log": "<log>[INFO] Request to start an already existing user: {} was received, so ignoring.</log>"
  },
  "98c7ec43_6": {
    "exec_flow": "ENTRY→SYNC: this.userPipelineMap→IF_FALSE: this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: LOG.INFO: Initializing request processing pipeline for application for the user: {}, user→CALL: interceptorChain.init→CALL: chainWrapper.init→CALL: this.userPipelineMap.put→RETURN→EXIT",
    "log": "<log>[INFO] Initializing request processing pipeline for application for the user: {}</log>"
  },
  "98c7ec43_7": {
    "exec_flow": "ENTRY→SYNC: this.userPipelineMap→IF_FALSE: this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: LOG.INFO: Initializing request processing pipeline for application for the user: {}, user→EXCEPTION: info →CATCH: Exception e→LOG: LOG.ERROR: Init ClientRequestInterceptor error for user: + user, e→THROW: e→EXIT",
    "log": "<log>[INFO] Initializing request processing pipeline for application for the user: {}</log> <log>[ERROR] Init ClientRequestInterceptor error for user: {}</log>"
  },
  "e1991747_1": {
    "exec_flow": "<seq> ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT </seq>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "e1991747_2": {
    "exec_flow": "<seq> ENTRY → IF_FALSE: numOpts > 3 || args.length != 2 → CALL:resolveTarget → CALL:resolveTarget → CALL: fromNode.setTransitionTargetHAStatus → CALL: toNode.setTransitionTargetHAStatus → CALL: Preconditions.checkState → IF_TRUE: fromNode.isAutoFailoverEnabled() → IF_TRUE: forceFence || forceActive → CALL: errOut.println → RETURN → EXIT </seq>",
    "log": "<log>[ERROR] FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.</log>"
  },
  "e1991747_3": {
    "exec_flow": "<seq> ENTRY → IF_FALSE: numOpts > 3 || args.length != 2 → CALL:resolveTarget → CALL:resolveTarget → CALL: fromNode.setTransitionTargetHAStatus → CALL: toNode.setTransitionTargetHAStatus → CALL: Preconditions.checkState → IF_FALSE: fromNode.isAutoFailoverEnabled() → CALL:new FailoverController → TRY → CALL: failover → CALL: println → RETURN → EXIT </seq>",
    "log": "<log>[INFO] Failover from <node1> to <node2> successful</log>"
  },
  "e1991747_4": {
    "exec_flow": "ENTRY → CALL:getProxy → CALL:getServiceStatus → IF_TRUE:!toSvcStatus.isReadyToBecomeActive() → IF_FALSE:!forceActive → LOG:[WARN] Service is not ready to become active, but forcing: {notReadyReason} → CALL:HAServiceProtocolHelper.monitorHealth → EXIT",
    "log": "<log>[WARN] Service is not ready to become active, but forcing: {notReadyReason}</log>"
  },
  "e1991747_5": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>CALL:transitionToActive</step> <step>CALL:org.apache.hadoop.yarn.server.resourcemanager.AdminService:transitionToActive</step> <step>EXIT</step> </sequence>",
    "log": "<log>[INFO] Requested transition to active state.</log> <log>[ERROR] Transition failed due to ServiceFailedException.</log>"
  },
  "65d5ce48_1": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → EXIT → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "65d5ce48_2": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: LOGGER.DEBUG: Exception in closing {} → FOREACH_EXIT → THROW:e → EXIT",
    "log": "<log_entry>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log_entry> <log_entry>[DEBUG] Failed to initialize filesystem</log_entry> <log_entry>[DEBUG] Exception in closing {}</log_entry>"
  },
  "65d5ce48_3": {
    "exec_flow": "ENTRY → checkPath(path) → IF_TRUE: name.equals(\"local\") → LOGGER.warn(\"local\" is a deprecated filesystem name. Use \"file:///\" instead.) → SET: name = \"file:///\" → ELSE_IF: name.indexOf('/') == -1 → LOGGER.warn(\"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.) → SET: name = \"hdfs://\" + name → RETURN: name → EXIT → return path.makeQualified(this.getUri(), this.getWorkingDirectory())",
    "log": "<log_entry>[WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead.</log_entry> <log_entry>[WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.</log_entry>"
  },
  "1ea0cbc0_1": {
    "exec_flow": "ENTRY→CALL: stop()→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners→TRY→CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL: globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}→EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "ac81065f_1": {
    "exec_flow": "ENTRY → CALL:getRMStateStoreEventHandler → CALL:handle → FOREACH:listofHandlers → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Processing [TaskAttemptID] of type [eventType] → CALL: writeLock.lock → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getInternalState() → IF_TRUE: getInternalState() == TaskAttemptStateInternal.FAILED → LOG: LOG.INFO: [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId] → CALL: writeLock.unlock → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Processing [TaskAttemptID] of type [eventType] [INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]"
  },
  "ac81065f_2": {
    "exec_flow": "ENTRY → CALL:getRMStateStoreEventHandler → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Processing [TaskAttemptID] of type [eventType] → CALL: writeLock.lock → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getInternalState() → IF_FALSE: getInternalState() == TaskAttemptStateInternal.FAILED → LOG: LOG.INFO: [attemptId] TaskAttempt Transitioned from [oldState] to [newState] → CALL: writeLock.unlock → EXIT",
    "log": "[DEBUG] Processing [TaskAttemptID] of type [eventType] [INFO] [attemptId] TaskAttempt Transitioned from [oldState] to [newState]"
  },
  "ac81065f_3": {
    "exec_flow": "ENTRY → CALL:getRMStateStoreEventHandler → IF_FALSE: c != null → LOG: LOG.WARN: Event {event} sent to absent container {event.getContainerID()} → EXIT",
    "log": "[WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "ac81065f_4": {
    "exec_flow": "ENTRY → CALL:getRMStateStoreEventHandler → IF_FALSE: app != null → LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "[WARN] Event + event + sent to absent application + event.getApplicationID()"
  },
  "cd2a30db_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.fs.azure.SASKeyGeneratorInterface:getRelativeBlobSASUri → CALL:blob.getServiceClient().setDefaultRequestOptions → EXCEPTION:setDefaultRequestOptions → CATCH:SASKeyGenerationException sasEx → LOG:LOG.ERROR:errorMsg → THROW:new StorageException(SAS_ERROR_CODE, errorMsg, sasEx) → EXIT",
    "log": "LOG.ERROR: errorMsg"
  },
  "010dfa00_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → CALL: newInstance → CALL: authHandler.init → IF_FALSE: principal == null || principal.trim().length() == 0 → CALL: getProperty → IF_FALSE: keytab == null || keytab.trim().length() == 0 → IF_FALSE: !keytabFile.exists() → IF_FALSE: principal.equals(\"*\") → CALL: serverSubject.getPrivateCredentials().add → CALL: serverSubject.getPrincipals().add → CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) → EXCEPTION: add → CATCH: Exception ex/on ClassNotFoundException | InstantiationException | IllegalAccessException ex → THROW: new ServletException(ex) → EXIT",
    "log": "<!-- Merged log sequence, incorporating child's log statements --> [INFO] Using keytab {}, for principal {}"
  },
  "8183935b_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:apply→EXCEPTION:Exception→RETURN→EXIT",
    "log": "[INFO] upload part request"
  },
  "8183935b_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:apply→CALL:invokeTrackingDuration→CALL:drainOrAbortHttpStream→RETURN→EXIT",
    "log": "[INFO] upload part request"
  },
  "8183935b_3": {
    "exec_flow": "ENTRY→LOG:INFO:Bulk delete operation failed to delete all objects; failure count = {errors.size()} → CALL:append → FOREACH:deleteException.getErrors() → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) → FOREACH_EXIT → IF_TRUE:ACCESS_DENIED.equals(exitCode) → CALL:initCause → NEW:AccessDeniedException → CALL:toString → RETURN → EXIT",
    "log": "[INFO] Bulk delete operation failed to delete all objects; failure count = {errors.size()} [INFO] {item}"
  },
  "160c951e_1": {
    "exec_flow": "ENTRY→CALL:FederationMembershipStateStoreInputValidator.validate→TRY→CALL:getConnection→CALL:prepareCall→CALL:setString→CALL:registerOutParameter→CALL:execute→CALL:getString→IF:amRMAddress == null || clientRMAddress == null→LOG:warn→CALL:FederationStateStoreUtils.returnToPool→RETURN→EXIT",
    "log": "[WARN] The queried SubCluster: {} does not exist."
  },
  "160c951e_2": {
    "exec_flow": "ENTRY→CALL:FederationMembershipStateStoreInputValidator.validate→TRY→CALL:getConnection→CALL:prepareCall→CALL:setString→CALL:registerOutParameter→CALL:execute→CALL:getString→TRY→CATCH:FederationStateStoreInvalidInputException→CALL:logAndThrowStoreException→CALL:FederationStateStoreUtils.returnToPool→RETURN→EXIT",
    "log": "[ERROR] SubCluster {} does not exist"
  },
  "160c951e_3": {
    "exec_flow": "ENTRY→CALL:FederationMembershipStateStoreInputValidator.validate→TRY→CALL:getConnection→CALL:prepareCall→CALL:setString→CALL:registerOutParameter→EXCEPTION:SQLException→CATCH:SQLException e→CALL:FederationStateStoreClientMetrics.failedStateStoreCall→CALL:logAndThrowRetriableException→IF_TRUE: t != null → CALL: log.ERROR: errMsg, t → THROW: new FederationStateStoreRetriableException(errMsg, t) → CALL:FederationStateStoreUtils.returnToPool→RETURN→EXIT",
    "log": "[ERROR] Unable to obtain the SubCluster information for {}, {t}"
  },
  "160c951e_4": {
    "exec_flow": "ENTRY→IF_TRUE: methodMetric == null→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Object)→LOG: LOG.ERROR: UNKOWN_FAIL_ERROR_MSG, methodName→RETURN→EXIT",
    "log": "[ERROR] UNKOWN_FAIL_ERROR_MSG, methodName"
  },
  "1e5e7da1_1": {
    "exec_flow": "<segment>ENTRY→LOG: selected by alias={} token={}, service, token</segment> <segment>IF_TRUE: token != null && TOKEN_KIND.equals(token.getKind())→RETURN→EXIT</segment>",
    "log": "<log>[DEBUG] selected by alias={} token={}, service, token</log>"
  },
  "9654821a_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE: props != null → CALL:loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "9654821a_2": {
    "exec_flow": "ENTRY → CALL:getFileStatus → IF_FALSE:fs.isDirectory() → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN:gFSDataInputStream → NEW:BufferedFSInputStream → NEW:CosNInputStream → RETURN → EXIT",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "9654821a_3": {
    "exec_flow": "ENTRY → CALL:getFileStatus → IF_FALSE:fs.isDirectory() → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a dir. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN:gFSDataInputStream → NEW:BufferedFSInputStream → NEW:CosNInputStream → RETURN → EXIT",
    "log": "[DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "f2af70ce_1": {
    "exec_flow": "ENTRY→IF_FALSE: fp == null→IF_FALSE: doubleBuf.isFlushed()→CALL: preallocate→ WHILE: need > 0→WHILE_EXIT→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→ CALL:org.slf4j.Logger:debug(java.lang.String)→LOG: LOG.DEBUG: Preallocated + total + bytes at the end of + the edit log (offset + oldSize + )→CALL: doubleBuf.flushTo→IF_TRUE: durable && !shouldSkipFsyncForTests && !shouldSyncWritesAndSkipFsync→CALL: force→EXIT",
    "log": "[DEBUG] Preallocated + total + bytes at the end of + the edit log (offset + oldSize + )"
  },
  "f2af70ce_2": {
    "exec_flow": "ENTRY→IF_TRUE: doubleBuf.isFlushed()→LOG: LOG.INFO: Nothing to flush→RETURN→EXIT",
    "log": "[INFO] Nothing to flush"
  },
  "ecd09be8_1": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration →CALL:getRaw→LOG:Handling deprecation for all properties in config... →CALL:getProps→IF_TRUE:props!=null→CALL:loadResources →CALL:addAll→FOREACH:keys →LOG:Handling deprecation for (String)item →CALL:handleDeprecation→FOREACH_EXIT →CALL:LOG_DEPRECATION.info →FOREACH:names→CALL:getProps→IF_TRUE:props!=null→CALL:loadResources →FOREACH_EXIT →RETURN→EXIT",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "8663970a_1": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating file: {}, f.toString() → IF_TRUE: containsColon(f) → THROW: new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\") → EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "8663970a_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE: containsColon(f) → CALL: performAuthCheck → CALL: createInternal → RETURN → EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "6fd58739_1": {
    "exec_flow": "ENTRY→CALL:getNodeIdsByResourceName→CALL:Preconditions.checkArgument→IF_FALSE:ResourceRequest.ANY.equals(resourceName)→IF_FALSE:nodeNameToNodeMap.containsKey(resourceName)→IF_FALSE:nodesPerRack.containsKey(resourceName)→LOG:LOG.INFO: Could not find a node matching given resourceName + resourceName→RETURN→RETURN→EXIT",
    "log": "[INFO] Could not find a node matching given resourceName RESOURCE_NAME"
  },
  "6fd58739_2": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→EXIT",
    "log": "<!-- Merged log sequence -->"
  },
  "d0c9a87a_1": {
    "exec_flow": "ENTRY→CALL:addCounter(org.apache.hadoop.mapreduce.Counter)→CALL:findCounter→IF_TRUE: counters[i] == null→CALL: newCounter→CALL: org.slf4j.Logger:warn→IF_TRUE: counter != null→CALL:setValue→RETURN→EXIT",
    "log": "[WARN] New counter created"
  },
  "d0c9a87a_2": {
    "exec_flow": "ENTRY→CALL:addCounter(org.apache.hadoop.mapreduce.Counter)→CALL:findCounter→IF_FALSE: counters[i] == null→IF_TRUE: counter != null→CALL:setValue→RETURN→EXIT",
    "log": "[WARN] Counter already exists"
  },
  "d0c9a87a_3": {
    "exec_flow": "ENTRY→CALL:addCounter(org.apache.hadoop.mapreduce.Counter)→CALL:findCounter→IF_FALSE: counters[i] == null→IF_FALSE: counter != null→CALL: org.slf4j.Logger:warn(name + \"is not a known counter.\")→RETURN→EXIT",
    "log": "[WARN] Counter already exists [WARN] name + \"is not a known counter.\""
  },
  "6fbf13f8_1": {
    "exec_flow": "ENTRY→TRY→CALL:isAllowedDelegationTokenOp→IF_FALSE:!isAllowedDelegationTokenOp()→CALL:getCurrentUser→CALL:rmDTSecretManager.cancelToken→ENTRY→CALL:createIdentifier→CALL:readFields→LOG:INFO:Token cancellation requested for identifier: + formatTokenId(id)→CALL:getUser→IF_FALSE:id.getUser() == null→CALL:getShortName→IF_FALSE:!canceller.equals(owner) && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName.equals(renewer.toString()))→IF_TRUE:info == null→THROW:new InvalidToken(\"Token not found \" + formatTokenId(id))→CALL:trackRemoveToken→EXIT",
    "log": "[INFO] Token cancellation requested for identifier: + formatTokenId(id)"
  },
  "6fbf13f8_2": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:subject==null || subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→TRY→NEW: DataInputStream→NEW: BufferedInputStream→CALL: newInputStream→CALL: toPath→CALL: toPath→CALL: readTokenStorageStream→CALL:addCredentials→CALL:debug→EXCEPTION: readTokenStorageStream→CATCH: IOException ioe→CALL: IOUtils.cleanupWithLogger→THROW: new IOException(\"Exception reading \" + filename, ioe)→EXIT→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds(false)→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object [LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [INFO] Cleaning up resources [DEBUG] Failure to load login credentials"
  },
  "9872ce33_1": {
    "exec_flow": "ENTRY → IF_FALSE:isCircular(jobsInProgress) → TRY → CALL:failAllJobs → CALL:error → EXIT",
    "log": "<log_statement> LOG.error(\"Error while trying to run jobs.\") </log_statement>"
  },
  "9872ce33_2": {
    "exec_flow": "<steps> <step>Parent Entry</step> <step>[VIRTUAL_CALL]</step> <step>Child Path Execution</step> <step>ENTRY</step> <step>IF_TRUE: props != null</step> <step>CALL: loadResources</step> <step>IF_TRUE: loadDefaults && fullReload</step> <step>FOREACH: defaultResources</step> <step>CALL: loadResource</step> <step>FOREACH_EXIT</step> <step>FOR_INIT</step> <step>FOR_COND: i < resources.size()</step> <step>CALL: loadResource</step> <step>FOR_EXIT</step> <step>CALL: addTags</step> <step>IF_TRUE: overlay != null</step> <step>CALL: putAll</step> <step>IF_TRUE: backup != null</step> <step>FOREACH: overlay.entrySet()</step> <step>FOREACH_EXIT</step> <step>CALL: checkAndWarnDeprecation</step> <step>EXIT</step> </steps>",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] Unexpected SecurityException in Configuration [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "9872ce33_3": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.util.Preconditions:checkArgument → CALL: org.apache.hadoop.util.Preconditions:checkArgument → CALL: trim → IF_TRUE: deprecations.getDeprecatedKeyMap().isEmpty() → CALL: getProps → CALL: getOverlay().setProperty → CALL: getProps().setProperty → IF_FALSE: !isDeprecated(name) → CALL: handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String) → FOREACH: names → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [INFO] message"
  },
  "6c3ca0e5_1": {
    "exec_flow": "ENTRY→CALL:ResourceLocalizationService:heartbeat→SYNC:privLocalizers→IF_TRUE:null==localizer→LOG:LOG.INFO:Unknown localizer with localizerId + locId + is sending heartbeat. Ordering it to DIE→CALL:setLocalizerAction→RETURN→EXIT",
    "log": "<log> <level>INFO</level> <template>Heartbeat called</template> </log> <log> <level>INFO</level> <template>Unknown localizer with localizerId + locId + is sending heartbeat. Ordering it to DIE</template> </log>"
  },
  "6c3ca0e5_2": {
    "exec_flow": "ENTRY→FOREACH:remoteResourceStatuses→TRY→NEW:LocalResourceRequest→IF_TRUE:assoc==null→LOG:LOG.ERROR:Unknown resource reported: {req}→CONTINUE→FOREACH_EXIT→IF_TRUE:fetchFailed || killContainerLocalizer.get()→CALL:setLocalizerAction→RETURN→EXIT",
    "log": "<log> <level>ERROR</level> <template>Unknown resource reported: {req}</template> </log>"
  },
  "b9b1a48d_1": {
    "exec_flow": "ENTRY→CALL:setIsPaused→CALL:container.dispatcher.getEventHandler().handle→CALL:addDiagnostics→FOREACH: diags→FOREACH_EXIT→IF_TRUE: diagnostics.length() > diagnosticsMaxSize→CALL: delete→TRY→CALL: stateStore.storeContainerDiagnostics→EXCEPTION: storeContainerDiagnostics→CATCH: IOException e→CALL: org.slf4j.Logger:warn→EXIT",
    "log": "<log>[WARN] Unable to update diagnostics in state store for [containerId], e</log>"
  },
  "b9b1a48d_2": {
    "exec_flow": "ENTRY→CALL:setIsPaused→CALL:container.dispatcher.getEventHandler().handle→CALL:access$20→CALL:addDiagnostics→FOREACH: diags→FOREACH_EXIT→IF_FALSE: diagnostics.length() > diagnosticsMaxSize→TRY→CALL: stateStore.storeContainerDiagnostics→EXCEPTION: storeContainerDiagnostics→CATCH: IOException e→CALL: org.slf4j.Logger:warn→EXIT",
    "log": "<log>[WARN] Unable to update diagnostics in state store for [containerId], e</log>"
  },
  "b9b1a48d_3": {
    "exec_flow": "ENTRY → CALL: getTrimmed → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "b9b1a48d_4": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: qSize != 0 && qSize % 1000 == 0 → IF_TRUE: remCapacity < 1000 → CALL: org.slf4j.Logger:info → LOG: Very low remaining capacity on {getName()} event queue: {remCapacity} → CALL: this.eventQueue.put → EXCEPTION: put → CATCH: InterruptedException e → CALL: org.slf4j.Logger:info → LOG: Interrupted. Trying to exit gracefully. → EXIT",
    "log": "<log>[INFO] Size of {getName()} event-queue is {qSize}</log> <log>[INFO] Very low remaining capacity on {getName()} event queue: {remCapacity}</log> <log>[INFO] Interrupted. Trying to exit gracefully.</log>"
  },
  "b9b1a48d_5": {
    "exec_flow": "ENTRY → IF_TRUE: app != null → SWITCH: event.getType() → CASE: [APPLICATION_RESOURCES_CLEANEDUP] → LOG: LOG.INFO: Application stop event received for stopping AppId: {ApplicationID} → CALL: AMRMProxyService.this.stopApplication → CALL: Preconditions.checkArgument → IF_FALSE: pipeline == null → CALL: this.secretManager.applicationMasterFinished → LOG: LOG.INFO: Stopping the request processing pipeline for application: {applicationId} → IF_TRUE: this.nmContext.getNMStateStore() != null → TRY → CALL: this.nmContext.getNMStateStore().removeAMRMProxyAppContext → EXIT",
    "log": "<log>[INFO] Application stop event received for stopping AppId: {ApplicationID}</log> <log>[INFO] Stopping the request processing pipeline for application: {applicationId}</log>"
  },
  "b9b1a48d_6": {
    "exec_flow": "ENTRY→CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→CALL:org.slf4j.Logger:error→IF_TRUE:oldState != getState()→CALL:org.slf4j.Logger:info→CALL:writeLock.unlock→EXIT",
    "log": "<log>[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState</log> <log>[INFO] [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType()</log>"
  },
  "b9b1a48d_7": {
    "exec_flow": "ENTRY→IF_FALSE: c != null→LOG:LOG.WARN: Event + event + sent to absent container + event.getContainerID()→EXIT",
    "log": "<log>[WARN] Event {event} sent to absent container {event.getContainerID()}</log>"
  },
  "b59ee669_1": {
    "exec_flow": "ENTRY→IF_FALSE:block.getBlockUCState() == BlockUCState.COMMITTED→IF_FALSE:block.getGenerationStamp() != commitBlock.getGenerationStamp()→CALL:removeStaleReplicas→FOREACH:staleReplicas→CALL:removeStoredBlock→CALL:blockLog.debug→IF_TRUE:storedBlock == null || !blocksMap.removeNode(storedBlock, node)→CALL:blockLog.debug→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] BLOCK* Removing stale replica {} of {} [DEBUG] BLOCK* removeStoredBlock: {} from {} [DEBUG] BLOCK* removeStoredBlock: {} has already been removed from node {}"
  },
  "b59ee669_2": {
    "exec_flow": "ENTRY→IF_FALSE:!isSafeModeTrackingBlocks()→SYNC:this→LOG:LOG.DEBUG:Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal→CALL:setBlockTotal→CALL:checkSafeMode→IF_FALSE:namesystem.inTransitionToActive()→SWITCH:status→CASE:[PENDING_THRESHOLD]→IF_TRUE:areThresholdsMet()→IF_TRUE:blockTotal > 0 && extension > 0→CALL:reachedTime.set→CALL:start→CALL:initializeReplQueuesIfNecessary→CALL:reportStatus→BREAK→EXIT",
    "log": "[DEBUG] Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal [DEBUG] STATE* Safe mode extension entered."
  },
  "b59ee669_3": {
    "exec_flow": "ENTRY → IF_FALSE: status == BMSafeModeStatus.OFF → IF_TRUE: storageNum == safeNumberOfNodes → IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE → IF_TRUE: this.awaitingReportedBlocksCounter == null → CALL: getCounter → CALL: this.awaitingReportedBlocksCounter.increment → CALL: checkSafeMode → ENTRY→IF_FALSE:namesystem.inTransitionToActive()→SWITCH:status→CASE:[PENDING_THRESHOLD]→IF_TRUE:areThresholdsMet()→IF_TRUE:blockTotal > 0 && extension > 0→CALL:reachedTime.set→CALL:start→CALL:initializeReplQueuesIfNecessary→CALL:reportStatus→BREAK→EXIT",
    "log": "[DEBUG] STATE* Safe mode extension entered."
  },
  "a6529864_1": {
    "exec_flow": "ENTRY → CALL: getTrimmed → CALL: handleDeprecation → FOREACH: names → CALL: getProps → CALL: substituteVars → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry> <log_entry>[DEBUG] Loading filesystems</log_entry> <log_entry>[WARN] Cannot load filesystem:</log_entry> <log_entry>[DEBUG] Stack Trace</log_entry>"
  },
  "a6529864_2": {
    "exec_flow": "ENTRY → CALL: new AuditContextUpdater(context).updateCurrentAuditContext → TRY → IF_FALSE: !jobSetup && getUUIDSource() == JobUUIDSource.GeneratedLocally → CALL: getTaskAttemptPath → CALL: getFileSystem → CALL: delete → CALL: mkdirs → CALL: finished → IF_FALSE: logAtInfo → IF_TRUE: log.isDebugEnabled() → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → LOG:[DEBUG] {} → EXIT",
    "log": "<log_entry>[INFO] Setup Task %s</log_entry> <log_entry>[DEBUG] {}</log_entry>"
  },
  "a6529864_3": {
    "exec_flow": "ENTRY → IF_FALSE: jobId != null → CALL: currentAuditContext().remove → IF_TRUE: taskAttemptId != null → CALL: auditCtx.put → IF_TRUE: LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → CALL: org.slf4j.Logger:trace → LOG: LOG.TRACE: Remove context entry {}, key → CALL: evaluatedEntries.remove → EXIT",
    "log": "<log_entry>[TRACE] Remove context entry {}</log_entry>"
  },
  "a6529864_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → Child paths execution",
    "log": "<log>org.slf4j.Logger:debug - [LOG] Execution started with parameters</log>"
  },
  "182a5585_1": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→TRY→SYNC:this→CALL:toArray→CALL:size→FOREACH:callbacks→CALL:stateChanged→FOREACH_EXIT→CALL:globalListeners.notifyListeners→CATCH:Throwable e→LOG:[WARN]Exception while notifying listeners of {}, this, e→CALL:ServiceOperations.stopQuietly→CALL:warn→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {} [WARN] When stopping the service {service.getName()} [DEBUG] noteFailure [INFO] Service {getName()} failed in state {failureState}"
  },
  "6b3ff660_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "6b3ff660_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "3b861db8_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → RETURN → EXIT → CALL: org.apache.hadoop.ipc.metrics.RpcMetrics:<init> → IF_TRUE:StringUtils.isNotEmpty(timeunit) → CALL:org.apache.hadoop.conf.Configuration:get → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → LOG:Handling deprecation for (String)item → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → TRY → CALL:TimeUnit.valueOf → CALL:org.slf4j.Logger:info → RETURN → EXIT → CALL: DefaultMetricsSystem.instance → CALL: register → RETURN → EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Config key {} 's value {} does not correspond to enum values of java.util.concurrent.TimeUnit. Hence default unit {} will be used"
  },
  "0802c236_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "0802c236_2": {
    "exec_flow": "<step>getPasswordFromCredentialProviders</step> <step>getPasswordFromConfig</step> <step>resolveConfIndirection</step> <step>parseAuth</step> <!-- Exception handling step -->",
    "log": "<log> <level>ERROR</level> <template>Couldn't read Auth based on {configKey}</template> </log>"
  },
  "0802c236_3": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "9be92d2e_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CALL:org.apache.hadoop.security.UserGroupInformation.addCredentials → CALL:org.apache.hadoop.security.Credentials.addAll → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Token file {} does not exist</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Failure to load login credentials</template> </log_entry> <log_entry> <level>DEBUG</level> <template>UGI loginUser: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>WARN</level> <template>Null token ignored for {alias}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "2aa67cae_1": {
    "exec_flow": "ENTRY → CALL:checkAcls → CALL:checkRMStatus → TRY → CALL:writeLock.lock → TRY → IF_FALSE:!nodeLabelsEnabled → IF_FALSE:null == labels || labels.isEmpty() → CALL:normalizeNodeLabels → CALL:checkExclusivityMatch → FOREACH:labels → CALL:NodeLabelUtil.checkAndThrowLabelName → FOREACH_EXIT → FOREACH:labels → CALL:org.apache.hadoop.yarn.nodelabels.RMNodeLabel:<init>(org.apache.hadoop.yarn.api.records.NodeLabel) → FOREACH_EXIT → IF_TRUE:null != dispatcher && !newLabels.isEmpty() → CALL:dispatcher.getEventHandler().handle → CALL:org.apache.hadoop.yarn.event.EventHandler:handle → LOG:LOG.INFO:Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ] → EXIT → CALL:writeLock.unlock → EXIT → CALL:rm.getRMContext().getNodeLabelManager().addToCluserNodeLabels → IF_TRUE:LOG.isInfoEnabled() → CALL:org.slf4j.Logger:isInfoEnabled() → CALL:org.slf4j.Logger:info(java.lang.String) → CALL:createSuccessLog(user, operation, target, null, null, null, null) → RETURN → EXIT",
    "log": "[INFO] Add labels: [ + StringUtils.join(labels.iterator(), \",\") + ] [INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "2aa67cae_2": {
    "exec_flow": "ENTRY → CALL:getCurrentUser → IF_FALSE:chain != null && chain.getRootInterceptor() != null → SYNC:this.userPipelineMap → IF_FALSE:this.userPipelineMap.containsKey(user) → TRY → LOG:Initializing request processing pipeline for user: {} → CALL:org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → EXCEPTION:createRequestInterceptorChain → CATCH:Exception e → LOG:Init RMAdminRequestInterceptor error for user: {} → THROW:e → EXIT",
    "log": "[INFO] Initializing request processing pipeline for user: {} [ERROR] Init RMAdminRequestInterceptor error for user: {}"
  },
  "2aa67cae_3": {
    "exec_flow": "ENTRY → CALL:getCurrentUser → IF_FALSE:chain != null && chain.getRootInterceptor() != null → SYNC:this.userPipelineMap → IF_FALSE:this.userPipelineMap.containsKey(user) → TRY → LOG:Initializing request processing pipeline for user: {} → CALL:org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → CALL:interceptorChain.init → CALL:chainWrapper.init → EXCEPTION:init → CATCH:Exception e → LOG:Init RMAdminRequestInterceptor error for user: {} → THROW:e → EXIT",
    "log": "[INFO] Initializing request processing pipeline for user: {} [ERROR] Init RMAdminRequestInterceptor error for user: {}"
  },
  "602c01ef_1": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:addToQueue</step> <step>CALL:org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask:<init></step> <step>LOG: LOG.INFO: Total number of queued jobs: + (queueDepth - sem.availablePermits())</step> <step>TRY</step> <step>TRY</step> <step>CALL:buildSplits</step> <step>LOG: LOG.INFO: [JobSubmitter] Time taken to build splits for job + job.getJob().getJobID() + : + (end - start) + ms.</step> <step>EXCEPTION:info</step> <step>CATCH:IOException e</step> <step>LOG: LOG.WARN: Failed to submit + job.getJob().getJobName() + as + job.getUgi(), e</step> <step>CALL:submissionFailed</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[INFO] Generating distributed cache data of size + conf.getLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1) [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [INFO] Total number of queued jobs: [INFO] Time taken to build splits for job: [JobID] ms. [WARN] Failed to submit [JobName] as [Ugi]"
  },
  "602c01ef_2": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:handleDeprecation</step> <step>FOREACH:names</step> <step>CALL:getProps</step> <step>CALL:substituteVars</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step> <step>CALL:getTrimmed(java.lang.String)</step> <step>RETURN</step> <step>ENTRY</step> <step>TRY</step> <step>LOG:Unexpected SecurityException in Configuration</step> <step>CATCH:SecurityException</step> <step>EXIT</step>",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] Unexpected SecurityException in Configuration"
  },
  "ef0caf55_1": {
    "exec_flow": "ENTRY IF_FALSE: hasOutputPath() LOG: WARN: Output Path is null in cleanupJob() SYNC: this CALL: getFileSystem IF_FALSE: scheme == null && authority == null IF_TRUE: scheme != null && authority == null IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null IF_TRUE: conf.getBoolean(disableCacheName, false) LOG: DEBUG: Bypassing cache to create filesystem {uri} SYNC: this CALL: get IF_FALSE: fs != null TRY CALL: creatorPermits.acquireUninterruptibly CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) SYNC: this CALL: get IF_FALSE: fs != null CALL: createFileSystem SYNC: this IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() CALL: ShutdownHookManager.get().addShutdownHook LOG: DEBUG: Duplicate FS created for {}; discarding {}, uri, fs CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) RETURN EXIT",
    "log": "<log> <template>Output Path is null in cleanupJob()</template> <level>warn</level> </log> <log> <template>Bypassing cache to create filesystem {uri}</template> <level>debug</level> </log> <log> <template>Duplicate FS created for {}; discarding {}</template> <level>debug</level> </log>"
  },
  "ef0caf55_2": {
    "exec_flow": "ENTRY LOG: DEBUG: Couldn't delete {} - does not exist: {} CALL: checkNotClosed TRY CALL: qualify LOG: DEBUG: Getting path status for {} ({}) ; needEmptyDirectory={} CALL: createSpan IF_TRUE: outcome TRY CALL: maybeCreateFakeParentDirectory IF_TRUE: parent != null && !parent.isRoot() && !isUnderMagicCommitPath(parent) CALL: createFakeDirectoryIfNecessary EXIT EXCEPTION: maybeCreateFakeParentDirectory CATCH: AccessDeniedException LOG: WARN: Cannot create directory marker at {}: {}, e.toString LOG: DEBUG: Failed to create fake dir above {}, path RETURN EXIT",
    "log": "<log> <template>Couldn't delete {} - does not exist: {}</template> <level>debug</level> </log> <log> <template>Getting path status for {} ({}) ; needEmptyDirectory={}</template> <level>debug</level> </log> <log> <template>Cannot create directory marker at {}: {}</template> <level>warn</level> </log> <log> <template>Failed to create fake dir above {}</template> <level>debug</level> </log>"
  },
  "ef0caf55_3": {
    "exec_flow": "ENTRY IF_TRUE: client != null IF_FALSE: !client.isConnected() CALL: logout CALL: disconnect IF_TRUE: !logoutSuccess LOG: WARN: Logout failed while disconnecting, error code - + client.getReplyCode() EXIT",
    "log": "<log> <template>Logout failed while disconnecting, error code - </template> <level>warn</level> </log>"
  },
  "ecb94d3d_1": {
    "exec_flow": "ENTRY → LOG: LOGGER.debug(\"Handling deprecation for all properties in config...\") → LOG: LOGGER.debug(\"Handling deprecation for (String)item\") → [VIRTUAL_CALL] → fileSplitIsValid → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "83d7f0e4_1": {
    "exec_flow": "ENTRY→CALL:copyReplicaToVolume→CALL:FsDatasetImpl.copyBlockFiles→CALL:ReplicaBuilder.build→CALL:newReplicaInfo.setNumBytes→CALL:finalizeNewReplica→CALL:removeOldReplica→RETURN→EXIT",
    "log": "[DEBUG] Copied {srcReplica.getBlockURI()} to {dstFile}"
  },
  "83d7f0e4_2": {
    "exec_flow": "ENTRY→CALL:copyReplicaToVolume→CALL:FsDatasetImpl.copyBlockFiles→CALL:ReplicaBuilder.build→CALL:newReplicaInfo.setNumBytes→CALL:finalizeNewReplica→CALL:removeOldReplica→RETURN→EXIT",
    "log": "[DEBUG] Copied {srcReplica.getMetadataURI()} meta to {dstMeta} and calculated checksum"
  },
  "83d7f0e4_3": {
    "exec_flow": "ENTRY→IF_FALSE:bpos != null→LOG:LOG.ERROR:Cannot find BPOfferService for reporting block received + for bpid={}→EXIT",
    "log": "[ERROR] Cannot find BPOfferService for reporting block received + for bpid={}"
  },
  "83d7f0e4_4": {
    "exec_flow": "ENTRY→IF_TRUE:FsDatasetImpl.LOG.isTraceEnabled()→CALL:printReferenceTraceInfo→IF_TRUE:FsDatasetImpl.LOG.isDebugEnabled()→IF_TRUE:reference.getReferenceCount()<=0→CALL:FsDatasetImpl.LOG.debug→CALL:checkReference→CALL:unreference→EXIT",
    "log": "[DEBUG] Decrease reference count <= 0 on ..."
  },
  "afdde0d1_1": {
    "exec_flow": "<seq> ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:getRaw→ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addTags→IF_TRUE: overlay != null→CALL:putAll→IF_TRUE: backup != null→FOREACH: overlay.entrySet()→FOREACH_EXIT→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT </seq>",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "62de1773_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>ENTRY</step> <step>[VIRTUAL_CALL] → AliyunOSSFileSystem:getFileStatus</step> <step>TRY</step> <step>CALL:setLogEnabled</step> <step>CALL:getObjectMetadata(request)</step> <step>EXCEPTION:getObjectMetadata</step> <step>CATCH:OSSException osse</step> <step>CALL:LOG.debug</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<!-- Merged log sequence --> <log_entry> <log_level>DEBUG</log_level> <template>Exception thrown when get object meta: + key + , exception: + osse</template> </log_entry>"
  },
  "62de1773_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>ENTRY</step> <step>CALL:checkPathIsSlash</step> <step>NEW:FileStatus</step> <step>CALL:getShortUserName</step> <step>CALL:getPrimaryGroupName</step> <step>ENTRY</step> <step>CALL:getGroups</step> <step>IF_TRUE:groups.isEmpty()</step> <step>LOG: Failed to get groups for user {}</step> <step>THROW:new IOException(\"There is no primary group for UGI \" + this)</step> <step>EXIT</step>",
    "log": "<!-- Merged log sequence --> <log_entry> <log_level>DEBUG</log_level> <template>Failed to get groups for user {}</template> </log_entry>"
  },
  "62de1773_3": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>ENTRY</step> <step>IF_FALSE:!useDeprecatedFileStatus</step> <step>IF_TRUE:dereference</step> <step>CALL:deprecatedGetFileStatus</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<!-- Merged log sequence --> <log_entry> <log_level>ERROR</log_level> <template>File \" + f + \" does not exist</template> </log_entry>"
  },
  "58ac3b95_1": {
    "exec_flow": "ENTRY → CALL:create → CALL:getUMask → RETURN → EXIT",
    "log": "[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask."
  },
  "58ac3b95_2": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.fs.FileSystem:create → RETURN → EXIT",
    "log": "[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}"
  },
  "58ac3b95_3": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_FALSE:!overwrite → LOG:[DEBUG] Creating a new file: [{}] in COS. → NEW:FSDataOutputStream → NEW:CosNOutputStream → CALL:getConf → RETURN → EXIT",
    "log": "[DEBUG] Creating a new file: [{}] in COS."
  },
  "58ac3b95_4": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:status.isDirectory() → IF_FALSE:!overwrite → LOG:[DEBUG] Overwriting file {} → CALL:getMultipartSizeProperty → NEW:FSDataOutputStream → NEW:AliyunOSSBlockOutputStream → CALL:getConf → NEW:SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "[DEBUG] Overwriting file {}"
  },
  "58ac3b95_5": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → CALL:statIncrement → CALL:trailingPeriodCheck → CALL:makeQualified → TRY → CALL:createFile → CALL:statIncrement → NEW:FSDataOutputStream → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}"
  },
  "58ac3b95_6": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → CALL:statIncrement → CALL:trailingPeriodCheck → CALL:makeQualified → TRY → EXCEPTION:createFile → CATCH:AzureBlobFileSystemException → CALL:checkException → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}"
  },
  "6ad82489_1": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> CALL:getLocationsForPath -> CALL:invokeSequential -> TRY -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser -> CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod -> FOREACH: locations -> TRY -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice -> IF_FALSE: namenodes == null || namenodes.isEmpty() -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod -> CATCH -> CALL:org.slf4j.Logger:error -> FOREACH_EXIT -> IF_TRUE: !thrownExceptions.isEmpty() -> FOR_INIT -> FOR_COND: i < thrownExceptions.size() -> IF_TRUE: isUnavailableException(ioe) -> THROW: ioe -> EXIT",
    "log": "[DEBUG] Proxying operation: {} <log>Operation not allowed because the path is a mount point or there are sub-mount points</log> <log>Cannot find locations for path</log> <log>Check path quota</log> <log>Filter disabled subclusters</log> [ERROR] Unexpected exception {} proxying {} to {}"
  },
  "6ad82489_2": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> CALL:getLocationsForPath -> CALL:invokeSequential -> TRY -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser -> CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod -> FOREACH: locations -> TRY -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice -> IF_FALSE: namenodes == null || namenodes.isEmpty() -> CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod -> CATCH -> CALL:org.slf4j.Logger:error -> FOREACH_EXIT -> IF_TRUE: !thrownExceptions.isEmpty() -> FOR_INIT -> FOR_COND: i < thrownExceptions.size() -> IF_TRUE: isUnavailableException(ioe) -> THROW: ioe -> EXIT",
    "log": "[DEBUG] Proxying operation: {} <log>Operation not allowed because the path is a mount point or there are sub-mount points</log> <log>Cannot find locations for path</log> <log>Check path quota</log> <log>Filter disabled subclusters</log> [ERROR] Unexpected exception {} proxying {} to {}"
  },
  "c3dffc8d_1": {
    "exec_flow": "ENTRY→CALL:skip(long)→CALL:org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:skip(long) →IF_TRUE: (dataBuf != null) && dataBuf.hasRemaining()→CALL:min→CALL:remaining→CALL:remaining →CALL:dataBuf.position→CALL:org.slf4j.Logger:trace →LOG:LOG.TRACE:skip(n={}, block={}, filename={}): discarded {} bytes from dataBuf and advanced dataPos by {}, n, block, filename, discardedFromBuf, remaining→RETURN→EXIT",
    "log": "[TRACE] skip(n={}, block={}, filename={}): discarded {} bytes from dataBuf and advanced dataPos by {}"
  },
  "c3dffc8d_2": {
    "exec_flow": "ENTRY→CALL:skip(long)→CALL:org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:skip(long) →IF_FALSE: (dataBuf != null) && dataBuf.hasRemaining() →CALL:org.slf4j.Logger:trace →LOG:LOG.TRACE:skip(n={}, block={}, filename={}): discarded {} bytes from dataBuf and advanced dataPos by {}, n, block, filename, discardedFromBuf, remaining→RETURN→EXIT",
    "log": "[TRACE] skip(n={}, block={}, filename={}): discarded {} bytes from dataBuf and advanced dataPos by {}"
  },
  "c3dffc8d_3": {
    "exec_flow": "ENTRY → WHILE:skipped<n → CALL:readNextPacket → IF_TRUE: skipped<n → CALL:packetReceiver.receiveNextPacket → CALL:getDataSlice → CALL:getHeader → LOG: LOG.TRACE: DFSClient readNextPacket got header {}, curHeader → IF_FALSE: !curHeader.sanityCheck(lastSeqNo) → IF_TRUE: curHeader.getDataLen()>0 → CALL:getSeqno → IF_TRUE: verifyChecksum && curDataSlice.remaining()>0 → CALL:verifyChunkedSums → CALL:getDataLen → IF_TRUE: curHeader.getOffsetInBlock()<startOffset → CALL:curDataSlice.position → IF_TRUE: bytesNeededToFinish<=0 → CALL:readTrailingEmptyPacket → LOG: LOG.TRACE: Reading empty packet at end of read → CALL:packetReceiver.receiveNextPacket → CALL:packetReceiver.getHeader → IF_TRUE: !trailer.isLastPacketInBlock() || trailer.getDataLen() != 0 → THROW: new IOException(\"Expected empty end-of-read packet! Header: \" + trailer) → RETURN → EXIT",
    "log": "[TRACE] DFSClient readNextPacket got header {} [TRACE] Reading empty packet at end of read"
  },
  "c3dffc8d_4": {
    "exec_flow": "ENTRY → TRY → CALL:writeReadResult → EXCEPTION:writeReadResult → CATCH:IOException e → CALL:org.slf4j.Logger:info → EXIT",
    "log": "[INFO] Could not send read status ( + statusCode + ) to datanode + peer.getRemoteAddressString() + : + e.getMessage()"
  },
  "e0d51915_1": {
    "exec_flow": "ENTRY → TRY → CALL:toBoolean → CATCH:TrileanConversionException → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext) → CATCH:AbfsRestOperationException → CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode() → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close() → ENTRY → VIRTUAL_CALL → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → LOG:LOG.DEBUG: Creating file: {}, f.toString() → IF_TRUE: containsColon(f) → THROW: new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\") → EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}</log> <log>[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call</log> <log>[DEBUG] Get root ACL status</log> <log>[DEBUG] IOStatisticsBinding track duration started</log> <log>[TRACE] Fetch SAS token for {operation} on {path}</log> <log>[TRACE] SAS token fetch complete for {operation} on {path}</log> <log>[DEBUG] First execution of REST operation - {}</log> <log>[DEBUG] Retrying REST operation {}. RetryCount = {}</log> <log>[DEBUG] Unexpected error.</log> <log>[TRACE] {} REST operation complete</log> <log>[DEBUG] IOStatisticsBinding track duration completed</log> <log>[DEBUG] Attempting to acquire lease on {}, retry {}</log> <log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log>"
  },
  "e0d51915_2": {
    "exec_flow": "ENTRY → IF_TRUE:client != null → IF_FALSE:!client.isConnected() → CALL:logout → CALL:disconnect → IF_TRUE:!logoutSuccess → LOG:LOG.WARN:Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "ceeb8fcb_1": {
    "exec_flow": "ENTRY -> [VIRTUAL_CALL] -> IF_TRUE: oldState != newState -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> RETURN -> EXIT",
    "log": "[INFO] Service {} failed in state {}"
  },
  "a9dac6ba_1": {
    "exec_flow": "ENTRY→IF_TRUE: !initializedResources→SYNC: ResourceUtils.class→IF_TRUE: !initializedResources→virtual_call: ResourceUtils.getNumberOfKnownResourceTypes()→CALL: Resources.multiply→CALL: clone→CALL: multiplyTo→CALL: multiplyAndRound→RETURN→EXIT",
    "log": "<log>[DEBUG] Request for appInfo of unknown attempt {}</log> <log>ResourceUtils:startResourceTypeCheck</log> <log>ResourceUtils:startResourceTypeCheck</log>"
  },
  "64524644_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "64524644_2": {
    "exec_flow": "ENTRY → TRY → CALL:get → IF_FALSE:journalsUri==null → IF_FALSE:!journalsUri.startsWith(\"qjournal://\") → CALL:getAddressesList → FOREACH:socketAddresses → FOREACH_EXIT → CATCH:UnknownHostException → CALL:error → THROW:UnknownHostException → EXIT → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[ERROR] The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "64524644_3": {
    "exec_flow": "ENTRY → TRY → CALL:get → IF_FALSE:journalsUri==null → IF_FALSE:!journalsUri.startsWith(\"qjournal://\") → CALL:getAddressesList → FOREACH:socketAddresses → FOREACH_EXIT → CATCH:URISyntaxException → CALL:error → THROW:URISyntaxException → EXIT → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[ERROR] The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "fc3bcce4_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→TRY→CALL:deleteTaskAttemptPathQuietly→CALL:deleteTaskWorkingPathQuietly→CALL:wrappedCommitter.abortTask→EXCEPTION:abortTask→CATCH IOException e→LOG:LOG.ERROR(\"{}: exception when aborting task {}\", getRole(), context.getTaskAttemptID(), e)→THROW:e→CALL:org.apache.hadoop.util.DurationInfo:close→EXIT",
    "log": "[ERROR] {}: exception when aborting task {}"
  },
  "fef385aa_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → [VIRTUAL_CALL] → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → SYNC: this → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.debug(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {} [DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "fef385aa_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → [VIRTUAL_CALL] → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {}"
  },
  "8de9f923_1": {
    "exec_flow": "ENTRY→CALL:IOUtils.cleanupWithLogger→CALL:IOUtils.cleanupWithLogger→CALL:shutdownActor→IF_TRUE:!ibrExecutorService.isShutdown()→CALL:ibrExecutorService.shutdownNow→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→CALL:offerServices.remove→CALL:BPOfferService.hasBlockPoolId→IF_TRUE→CALL:BPOfferService.getBlockPoolId→CALL:bpByBlockPoolId.remove→FOR_INIT→FOR_COND:it.hasNext() && !removed→CALL:Logger.info→EXIT",
    "log": "[DEBUG] Exception in closing {} [INFO] Removed BPOfferService"
  },
  "a7239d92_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:absOrFqPath.checkNotSchemeWithRelative→CALL:absOrFqPath.checkNotRelative→TRY→CALL:defaultFS.checkPath→CALL:getAbstractFileSystem→ENTRY→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}]→CATCH:PrivilegedActionException→LOG:LOG.DEBUG:PrivilegedActionException as: {}→THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException→RETURN→EXIT→CALL:org.apache.hadoop.fs.FSLinkResolver:next→ENTRY→CALL:org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus→RETURN→EXIT→TRY→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {} [INFO] Resolving file system path [DEBUG] Attempting symlink resolution [INFO] Executing next method [INFO] ChRootedFs listStatus call invoked"
  },
  "971ee097_1": {
    "exec_flow": "<step>ENTRY→CALL:getTrimmed→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→IF_TRUE:LOG.isDebugEnabled()→LOG:DEBUG:List status for path: {path}→CALL:getFileStatus→IF_FALSE:fileStatus.isDirectory()→IF_TRUE:LOG.isDebugEnabled()→LOG:DEBUG:Adding: rd (not a dir): {path}→CALL:add→CALL:toArray→CALL:size→CALL:size→RETURN→EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] List status for path: {path}</log> <log>[DEBUG] Adding: rd (not a dir): {path}</log>"
  },
  "971ee097_2": {
    "exec_flow": "<step>ENTRY→LOG:DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}→IF_TRUE:continuation == null || continuation.isEmpty()→IF_TRUE:startFrom != null && !startFrom.isEmpty()→CALL:getIsNamespaceEnabled→CALL:generateContinuationTokenForXns→DO_WHILE→TRY→CALL:startTracking→CALL:client.listPath→CALL:getResult→CALL:op.getResult→IF_TRUE:retrievedSchema == null→THROW:new AbfsRestOperationException→EXIT</step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "971ee097_3": {
    "exec_flow": "<step>ENTRY→LOG:DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}→IF_TRUE:continuation == null || continuation.isEmpty()→IF_FALSE:startFrom != null && !startFrom.isEmpty()→DO_WHILE→TRY→CALL:startTracking→CALL:client.listPath→CALL:getResult→CALL:op.getResult→IF_TRUE:retrievedSchema == null→THROW:new AbfsRestOperationException→EXIT</step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "971ee097_4": {
    "exec_flow": "<step>ENTRY→LOG:DEBUG:listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}→IF_FALSE:continuation == null || continuation.isEmpty()→DO_WHILE→TRY→CALL:startTracking→CALL:client.listPath→CALL:getResult→CALL:op.getResult→IF_TRUE:retrievedSchema == null→THROW:new AbfsRestOperationException→EXIT</step>",
    "log": "<log>[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}</log>"
  },
  "ee9d4ab6_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: [DEBUG] Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT → CALL:rpcServer.getLocationsForPath → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getLocationsForPath → STEP: Check for mount points under the path → STEP: Determine destination path location → STEP: Check for read-only status → ENTRY_POINT: RouterRpcServer:isPathReadOnly → CALL: isPathReadOnly → STEP: Verify quota usage if required → CATCH → CALL:org.slf4j.Logger:error → LOG: [ERROR] Cannot get mount point → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → FOREACH: locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod → CATCH → CALL:org.slf4j.Logger:error → LOG: [ERROR] Unexpected exception {} proxying {} to {} → FOREACH_EXIT → IF_TRUE: !thrownExceptions.isEmpty() → FOR_INIT → FOR_COND: i < thrownExceptions.size() → IF_TRUE: isUnavailableException(ioe) → THROW: ioe → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Cannot get mount point [ERROR] Unexpected exception {} proxying {} to {}"
  },
  "eaaa29d3_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → CALL:initialize → ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE:props != null → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → CALL:addTags → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:LOG.DEBUG: Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT → EXIT → IF_TRUE:loginUserRef == null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser) → CALL:createLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser == null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → EXCEPTION:readTokenStorageStream → CATCH:IOException ioe → CALL:IOUtils.cleanupWithLogger → THROW:new IOException(\"Exception reading \" + filename, ioe) → EXIT → CALL:debug → CALL:loginUser.spawnAutoRenewalThreadForUserCreds(false) → DO_COND:loginUser == null → DO_EXIT → RETURN → EXIT",
    "log": "<!-- Merged log sequence --> <log_entry>[INFO] message</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for {item}</log_entry> <log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Reading credentials from location {}</log_entry> <log_entry>[DEBUG] Loaded {} tokens from {}</log_entry> <log_entry>[INFO] Token file {} does not exist</log_entry> <log_entry>[INFO] Cleaning up resources</log_entry> <log_entry>[DEBUG] Failure to load login credentials</log_entry> <log_entry>[WARN] auth_to_local rule mechanism not set. Using default of DEFAULT_MECHANISM</log_entry> <log_entry>[DEBUG] UGI loginUser: {}</log_entry>"
  },
  "bd40beb0_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info → LOG:Handling deprecation for all properties in config... → LOG:Handling deprecation for (String)item → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "bd40beb0_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "bd40beb0_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ebd01f17_1": {
    "exec_flow": "ENTRY→CALL:AllocateBlockIdOp.getInstance→CALL:setBlockId→CALL:doEditTx() op={} txid={}→LOG:DEBUG: Recording a newly allocated block ID in the edit log→INFO: AllocateBlockIdOp instance created and block ID set→CALL:org.slf4j.Logger:debug→TRY→CALL:editLogStream.write→EXCEPTION:write→CATCH:IOException ex→CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT",
    "log": "[DEBUG] Recording a newly allocated block ID in the edit log [INFO] AllocateBlockIdOp instance created and block ID set [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "5318c9d4_1": {
    "exec_flow": "ENTRY → CALL: getSplitHostsAndCachedHosts → RETURN → EXIT",
    "log": "<!-- No logs in this path -->"
  },
  "5318c9d4_2": {
    "exec_flow": "ENTRY → IF_FALSE: node == null → CALL: netlock.writeLock().lock → TRY → IF_FALSE: node instanceof InnerNode → IF_TRUE: (depthOfAllLeaves != -1) && (depthOfAllLeaves != newDepth) → LOG: LOG.ERROR: Error: can’t add leaf node {} at depth {} to topology:{}\\n, NodeBase.getPath(node), newDepth, this → THROW: new InvalidTopologyException → CALL: netlock.writeLock().unlock → EXIT",
    "log": "[ERROR] Error: can’t add leaf node {} at depth {} to topology:{}\\n"
  },
  "5318c9d4_3": {
    "exec_flow": "ENTRY → IF_FALSE: node == null → CALL: netlock.writeLock().lock → TRY → IF_FALSE: node instanceof InnerNode → IF_FALSE: (depthOfAllLeaves != -1) && (depthOfAllLeaves != newDepth) → IF_FALSE: rack != null && !(rack instanceof InnerNode) → IF_TRUE: clusterMap.add(node) → LOG: LOG.INFO: Adding a new node: + NodeBase.getPath(node) → IF_FALSE: rack == null → CALL: interAddNodeWithEmptyRack → IF_FALSE: depthOfAllLeaves == -1 → LOG: LOG.DEBUG: NetworkTopology became:\\n{}, this → CALL: netlock.writeLock().unlock → EXIT",
    "log": "[INFO] Adding a new node: [DEBUG] NetworkTopology became:\\n{}"
  },
  "155a22d1_1": {
    "exec_flow": "ENTRY→LOG:LOG.INFO→IF_TRUE→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG→CALL:getJobState→CATCH→LOG:LOG.WARN→IF_TRUE→TRY→LOG:LOG.INFO→CALL:killJob→LOG:LOG.INFO→EXIT",
    "log": "[INFO] Attempting to clean up remaining running applications. [DEBUG] PrivilegedAction [as: {}][action: {}] [INFO] Attempting to kill workload app: {} [INFO] Killed workload app"
  },
  "155a22d1_2": {
    "exec_flow": "ENTRY→LOG:LOG.INFO→IF_TRUE→TRY→CALL:getJobState→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG: PrivilegedActionException as: {}→CATCH→LOG:LOG.WARN→EXIT",
    "log": "[INFO] Attempting to clean up remaining running applications. [WARN] Unable to fetch completion status of workload job. Will proceed to attempt to kill it. [DEBUG] PrivilegedActionException as: {}"
  },
  "155a22d1_3": {
    "exec_flow": "ENTRY→LOG:LOG.INFO→IF_TRUE→TRY→CALL:getJobState→IF_TRUE→TRY→LOG:LOG.INFO→LOG:LOG.DEBUG: PrivilegedAction [as: {}][action: {}]→CALL:killJob→CATCH→LOG:LOG.ERROR→EXIT",
    "log": "[INFO] Attempting to clean up remaining running applications. [DEBUG] PrivilegedAction [as: {}][action: {}] [INFO] Attempting to kill workload app: {} [ERROR] Unable to kill workload app ({})"
  },
  "155a22d1_4": {
    "exec_flow": "ENTRY→LOG:LOG.INFO→IF_FALSE→IF_TRUE→TRY→LOG:LOG.INFO→CALL:forceKillApplication→LOG:LOG.INFO→EXIT",
    "log": "[INFO] Attempting to clean up remaining running applications. [INFO] Attempting to kill infrastructure app: {} [INFO] Killed infrastructure app"
  },
  "155a22d1_5": {
    "exec_flow": "ENTRY→LOG:LOG.INFO→IF_FALSE→IF_TRUE→TRY→LOG:LOG.INFO→CALL:forceKillApplication→CATCH→LOG:LOG.ERROR→EXIT",
    "log": "[INFO] Attempting to clean up remaining running applications. [INFO] Attempting to kill infrastructure app: {} [ERROR] Unable to kill infrastructure app ({})"
  },
  "155a22d1_6": {
    "exec_flow": "ENTRY→CALL:setApplicationId→IF_TRUE:diagnostics!=null→CALL:setDiagnostics→TRY→WHILE:true→WHILE_COND:true→CALL:rmClient.forceKillApplication→IF_FALSE:response.getIsKillCompleted()→CALL:LOG.info→CALL:Thread.sleep→WHILE→EXIT",
    "log": "[INFO] Waiting for application"
  },
  "155a22d1_7": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapred.ClientCache:getClient(org.apache.hadoop.mapreduce.JobID)→CALL:org.apache.hadoop.mapred.ClientServiceDelegate:getJobStatus(org.apache.hadoop.mapreduce.JobID)→IF_FALSE:status==null→IF_FALSE:status.getState()!=JobStatus.State.RUNNING→CALL:org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)→TRY→CALL:org.apache.hadoop.mapred.ClientServiceDelegate:killJob(org.apache.hadoop.mapreduce.JobID)→EXCEPTION:IOException io→CATCH:IOException io→LOG:LOG.DEBUG:Error when checking for application status, io→IF_TRUE:status!=null&&!isJobInTerminalState(status)→CALL:org.apache.hadoop.mapred.YARNRunner:killApplication→EXIT",
    "log": "[DEBUG] Error when checking for application status"
  },
  "92280b10_1": {
    "exec_flow": "ENTRY -> IF_FALSE: routerId == null -> IF_TRUE: isStoreAvailable() -> TRY -> CALL: org.apache.hadoop.hdfs.server.federation.store.records.RouterState:newInstance -> CALL: org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:getStateStoreVersion -> CALL: org.apache.hadoop.hdfs.server.federation.store.records.StateStoreVersion:newInstance -> CALL: setStateStoreVersion -> CALL: org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getHostPortString -> CALL: setAdminAddress -> CALL: org.apache.hadoop.hdfs.server.federation.store.protocol.RouterHeartbeatRequest:newInstance -> CALL: org.apache.hadoop.hdfs.server.federation.store.RouterStore:routerHeartbeat -> IF_FALSE: !response.getStatus() -> LOG: LOG.DEBUG: Router heartbeat for router {routerId} -> EXIT",
    "log": "<log level=\"DEBUG\">Router heartbeat for router {routerId}</log>"
  },
  "92280b10_2": {
    "exec_flow": "ENTRY -> IF_FALSE: routerId == null -> IF_TRUE: isStoreAvailable() -> TRY -> CALL: org.apache.hadoop.hdfs.server.federation.store.records.RouterState:newInstance -> CALL: org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:getStateStoreVersion -> CALL: org.apache.hadoop.hdfs.server.federation.store.records.StateStoreVersion:newInstance -> CALL: setStateStoreVersion -> CALL: org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getHostPortString -> CALL: setAdminAddress -> CALL: org.apache.hadoop.hdfs.server.federation.store.protocol.RouterHeartbeatRequest:newInstance -> EXCEPTION: IOException -> CATCH: IOException e -> LOG: LOG.ERROR: Cannot heartbeat router {routerId}, routerId, e -> EXIT",
    "log": "<log level=\"ERROR\">Cannot heartbeat router {routerId}</log>"
  },
  "92280b10_3": {
    "exec_flow": "ENTRY -> IF_FALSE: routerId == null -> IF_FALSE: isStoreAvailable() -> LOG: LOG.WARN: Cannot heartbeat router {}: State Store unavailable, routerId -> EXIT",
    "log": "<log level=\"WARN\">Cannot heartbeat router {}: State Store unavailable</log>"
  },
  "bbc6452e_1": {
    "exec_flow": "ENTRY→IF_TRUE: (newRecords == null || currentDriverTime <= 0)→LOG: [ERROR] Cannot check overrides for record→RETURN→EXIT",
    "log": "<log>[ERROR] Cannot check overrides for record</log>"
  },
  "bbc6452e_2": {
    "exec_flow": "ENTRY→IF_FALSE: (newRecords == null || currentDriverTime <= 0)→FOREACH: newRecords→IF_TRUE: record.shouldBeDeleted(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: getDriver().remove→IF_TRUE: (remove==success)→CALL: Logger.info→FOREACH_EXIT→IF_TRUE: (commitRecords.size() > 0)→CALL: putAll→IF_TRUE: (deleteRecords.size() > 0)→CALL: newRecords.removeAll→EXIT",
    "log": "<log>[INFO] Deleted State Store record {}: {}</log>"
  },
  "bbc6452e_3": {
    "exec_flow": "ENTRY→IF_FALSE: (newRecords == null || currentDriverTime <= 0)→FOREACH: newRecords→IF_TRUE: record.shouldBeDeleted(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: getDriver().remove→IF_FALSE: (remove!=success)→CALL: Logger.warn→FOREACH_EXIT→IF_TRUE: (commitRecords.size() > 0)→CALL: putAll→IF_FALSE: (deleteRecords.size() > 0)→EXIT",
    "log": "<log>[WARN] Couldn’t delete State Store record {}: {}</log>"
  },
  "bbc6452e_4": {
    "exec_flow": "ENTRY→IF_FALSE: (newRecords == null || currentDriverTime <= 0)→FOREACH: newRecords→IF_FALSE: record.shouldBeDeleted(currentDriverTime)→IF_TRUE: record.checkExpired(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: Logger.info→FOREACH_EXIT→IF_TRUE: (commitRecords.size() > 0)→CALL: putAll→IF_FALSE: (deleteRecords.size() > 0)→EXIT",
    "log": "<log>[INFO] Override State Store record {}: {}</log>"
  },
  "d2c77e5e_1": {
    "exec_flow": "ENTRY→CALL:subtract→[VIRTUAL_CALL]→ENTRY→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "cf2f58b2_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:LOG.INFO:DefaultSpeculator.addSpeculativeAttempt -- we are speculating +taskID</step> <step>CALL:eventHandler.handle</step> <step>LOG:LOG.ERROR:Failed to store label modification to storage</step> <step>LOG:LOG.WARN:Event + event + sent to absent application + event.getApplicationID()</step> <step>LOG:LOG.ERROR:Unknown event type on UpdateContainer: {event.getType()}</step> <step>LOG:LOG.ERROR:Unknown event arrived at ContainerScheduler: {event.toString()}</step> <step>CALL:mayHaveSpeculated.add</step> <step>EXIT</step>",
    "log": "[INFO] DefaultSpeculator.addSpeculativeAttempt -- we are speculating + taskID [ERROR] Failed to store label modification to storage [WARN] Event + event + sent to absent application + event.getApplicationID() [ERROR] Unknown event type on UpdateContainer: {event.getType()} [ERROR] Unknown event arrived at ContainerScheduler: {event.toString()}"
  },
  "cf2f58b2_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE: rmApp != null</step> <step>TRY</step> <step>CALL: rmApp.handle</step> <step>EXCEPTION: handle</step> <step>CATCH: Throwable t</step> <step>LOG: LOG.ERROR: Error in handling event type + event.getType() + for application + appID, t</step> <step>EXIT</step>",
    "log": "[ERROR] Error in handling event type + event.getType() + for application + appID, t"
  },
  "d6f18950_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getCallerUgi→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:verifyUserAccessForRMApp→IF_TRUE:applicationTimeouts.isEmpty()→IF_TRUE:LOG.isWarnEnabled()→CALL:org.slf4j.Logger:isWarnEnabled()→CALL:org.slf4j.Logger:warn(java.lang.String)→THROW:RPCUtil:getRemoteException→EXIT",
    "log": "[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)"
  },
  "d6f18950_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getCallerUgi→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:verifyUserAccessForRMApp→IF_FALSE:applicationTimeouts.isEmpty()→IF_TRUE:!EnumSet.of(RMAppState.SUBMITTED, RMAppState.ACCEPTED, RMAppState.RUNNING).contains(state)→IF_TRUE:application.isAppInCompletedStates()→CALL:RMAuditLogger:logSuccess→CALL:setApplicationTimeouts→RETURN→EXIT",
    "log": "[SUCCESS] <dynamic_message_based_on_execution>"
  },
  "d6f18950_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getCallerUgi→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:verifyUserAccessForRMApp→IF_FALSE:applicationTimeouts.isEmpty()→IF_TRUE:!EnumSet.of(RMAppState.SUBMITTED, RMAppState.ACCEPTED, RMAppState.RUNNING).contains(state)→IF_FALSE:application.isAppInCompletedStates()→CALL:RMAuditLogger:logFailure→THROW:RPCUtil:getRemoteException→EXIT",
    "log": "[FAILURE] <dynamic_message_based_on_execution>"
  },
  "d6f18950_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getCallerUgi→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:verifyUserAccessForRMApp→IF_FALSE:applicationTimeouts.isEmpty()→IF_FALSE:!EnumSet.of(RMAppState.SUBMITTED, RMAppState.ACCEPTED, RMAppState.RUNNING).contains(state)→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:updateApplicationTimeout→CALL:RMAuditLogger:logSuccess→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:setApplicationTimeouts→RETURN→EXIT",
    "log": "[SUCCESS] <dynamic_message_based_on_execution>"
  },
  "f000267d_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.slf4j.Logger:debug→RETURN→TRY→CALL:org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils:getContainerLogDirs→CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:readAggregatedLogsMeta→TRY→CALL:org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils:getContainerLogFile→CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory:getFileControllerForRead→ENTRY→IF_TRUE:LogAggregationUtils.isOlderPathEnabled(conf)→FOREACH:controllers→TRY→CALL:getOlderRemoteAppLogDir→IF_TRUE:defaultFsUri.getScheme() != null && !defaultFsUri.getScheme().trim().isEmpty()→CALL:LogAggregationUtils.getNodeFiles→IF_TRUE:RemoteIterator.hasNext→[VIRTUAL_CALL]→CALL:org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:hasNext()→CALL:org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation$1:next→LOG:LOG.DEBUG:Next element retrieved from RemoteIterator→RETURN→EXIT",
    "log": "[DEBUG] {} [WARN] Failed to find pathStr at dir [WARN] Failed to find log file [DEBUG] Next element retrieved from RemoteIterator"
  },
  "3f3ac7b3_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:org.apache.hadoop.hdfs.server.datanode.DataStorage:clearTrash(java.lang.String)→IF_TRUE:trashEnabledBpids.contains(bpid)→CALL:getBPStorage(bpid).clearTrash→ENTRY→FOREACH: getStorageDirs()→CALL:getTrashRootDir→CALL:exists→CALL:exists→CALL:error→FOREACH_EXIT→CALL:stopTrashCleaner→CALL:Daemon→CALL:newInstance→CALL:fullyDelete→CALL:info→CALL:trashCleaner.start→EXIT→CALL:trashEnabledBpids.remove→LOG:LOG.INFO:Cleared trash for bpid {}, bpid→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)→EXIT",
    "log": "[ERROR] Trash and PreviousDir shouldn't both exist for storage directory {} [INFO] Cleared trash for storage directory {} [INFO] Cleared trash for bpid {}"
  },
  "ecaf21d5_1": {
    "exec_flow": "ENTRY → IF_TRUE: excludeDatanodes.getValue() != null → CALL: System.arraycopy → CALL: toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]) → ENTRY → CALL: makeQualified → CALL: getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL: getDelegationToken → IF_TRUE: token != null → CALL: authParams.add → CALL: encodeToUrlString → CALL: toArray → RETURN → EXIT → CALL: getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Delegation token encoded [INFO] Returning authentication parameters [TRACE] url={}"
  },
  "ecaf21d5_2": {
    "exec_flow": "ENTRY → IF_TRUE: excludeDatanodes.getValue() != null → CALL: System.arraycopy → CALL: toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]) → ENTRY → CALL: makeQualified → CALL: getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL: getDelegationToken → IF_FALSE: token != null → IF_TRUE: realUgi != null → CALL: authParams.add → IF_TRUE: isInsecureCluster → CALL: authParams.add → CALL: toArray → RETURN → EXIT → CALL: getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Auth parameters added for proxy user [INFO] Insecure cluster detected [TRACE] url={}"
  },
  "ecaf21d5_3": {
    "exec_flow": "ENTRY → IF_TRUE: excludeDatanodes.getValue() != null → CALL: System.arraycopy → CALL: toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]) → ENTRY → CALL: makeQualified → CALL: getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL: getDelegationToken → IF_FALSE: token != null → IF_TRUE: realUgi != null → CALL: authParams.add → IF_FALSE: isInsecureCluster → CALL: toArray → RETURN → EXIT → CALL: getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Auth parameters added for proxy user [INFO] Secure cluster detected [TRACE] url={}"
  },
  "ecaf21d5_4": {
    "exec_flow": "ENTRY → IF_TRUE: excludeDatanodes.getValue() != null → CALL: System.arraycopy → CALL: toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]) → ENTRY → CALL: makeQualified → CALL: getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL: getDelegationToken → IF_FALSE: token != null → IF_FALSE: realUgi != null → IF_TRUE: isInsecureCluster → CALL: authParams.add → CALL: toArray → RETURN → EXIT → CALL: getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT → RETURN → EXIT",
    "log": "[INFO] Insecure cluster detected [TRACE] url={}"
  },
  "ecaf21d5_5": {
    "exec_flow": "ENTRY → IF_TRUE: excludeDatanodes.getValue() != null → CALL: System.arraycopy → CALL: toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]) → ENTRY → CALL: makeQualified → CALL: getAuthParameters → ENTRY → IF_TRUE: !op.getRequireAuth() → CALL: getDelegationToken → IF_FALSE: token != null → IF_FALSE: realUgi != null → IF_FALSE: isInsecureCluster → CALL: toArray → RETURN → EXIT → CALL: getNamenodeURL → LOG[TRACE] url={} → RETURN → EXIT → RETURN → EXIT",
    "log": "[INFO] Secure cluster detected [TRACE] url={}"
  },
  "54432dc2_1": {
    "exec_flow": "ENTRY→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<AbstractFileSystem>→EXCEPTION:InterruptedException→CALL:error→CALL:ensureInitialized→IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→RETURN→EXIT",
    "log": "[ERROR] ex.toString() [LOG] getLoginUser"
  },
  "e72367d0_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → FOREACH_EXIT → CALL: noteFailure → CALL: ServiceOperations.stopQuietly → TRY → LOG: LOG.WARN: When stopping the service {} → CATCH: Exception e → EXIT",
    "log": "<log>[DEBUG] Service: {} entered state {}</log> <log>[DEBUG] Config has been overridden during init</log> <log>[DEBUG] noteFailure</log> <log>[INFO] Service {} failed in state {}</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>[WARN] When stopping the service {}</log>"
  },
  "52aec135_1": {
    "exec_flow": "ENTRY → IF_TRUE:!isEnabled() → LOG:INFO:Not starting CacheReplicationMonitor as name-node caching is disabled. → RETURN → EXIT",
    "log": "<log>[INFO] Not starting CacheReplicationMonitor as name-node caching is disabled.</log>"
  },
  "52aec135_2": {
    "exec_flow": "ENTRY → IF_FALSE:!isEnabled() → CALL:crmLock.lock → TRY → IF_TRUE:this.monitor==null → NEW:CacheReplicationMonitor → CALL:org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:start → ENTRY→CALL:Thread.currentThread().setName→LOG:INFO:Starting CacheReplicationMonitor with interval {intervalMs} milliseconds→TRY→WHILE:true→WHILE_COND:false→CALL:rescan→CALL:Time.monotonicNow→CALL:lock→TRY→CALL:scanFinished.signalAll→LOG:DEBUG:Scanned {scannedDirectives} directive(s) and {scannedBlocks} block(s) in {curTimeMs - startTimeMs} millisecond(s).→WHILE_EXIT→EXIT",
    "log": "<log>[INFO] CacheReplicationMonitor started</log> <log>[INFO] Starting CacheReplicationMonitor with interval {intervalMs} milliseconds</log> <log>[DEBUG] Scanned {scannedDirectives} directive(s) and {scannedBlocks} block(s) in {curTimeMs - startTimeMs} millisecond(s).</log>"
  },
  "52aec135_3": {
    "exec_flow": "ENTRY → IF_FALSE:!isEnabled() → CALL:crmLock.lock → TRY → IF_FALSE:this.monitor==null → CALL:crmLock.unlock → EXIT",
    "log": "<log>[INFO] CacheReplicationMonitor already running</log>"
  },
  "52aec135_4": {
    "exec_flow": "ENTRY→IF_FALSE: bytesInFuture > 0→IF_FALSE: force→IF_TRUE: blockManager.isPopulatingReplQueues() && blockManager.shouldPopulateReplQueues()→CALL: blockManager.initializeReplQueues→IF_FALSE: status != BMSafeModeStatus.OFF→CALL: NameNode.stateChangeLog.info→CALL: NameNode.getNameNodeMetrics().setSafeModeTime→CALL: NameNode.stateChangeLog.info→RETURN→EXIT",
    "log": "<log>[INFO] STATE* Safe mode is OFF</log> <log>[INFO] STATE* Leaving safe mode after {} secs</log> <log>[INFO] STATE* Network topology has {} racks and {} datanodes</log> <log>[INFO] STATE* UnderReplicatedBlocks has {} blocks</log>"
  },
  "2dbd1814_1": {
    "exec_flow": "ENTRY → IF_FALSE: shouldCloseConnection.get() → CALL: touch → TRY → CALL: ipcStreams.readResponse → CALL: RpcWritable.Buffer.wrap → CALL: packet.getValue → CALL: checkResponse → IF_TRUE: LOG.isDebugEnabled → CALL: getName → CALL: getCallId → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → CALL: substituteVars → FOREACH_EXIT → IF_TRUE: instance instanceof Configurable → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Handling deprecation for all properties in config...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for (String)item</message> </log>"
  },
  "49a530e1_1": {
    "exec_flow": "ENTRY → CALL:errorState.resetInternalError → CALL:lastException.clear → CALL:getExcludedNodes → VIRTUAL_CALL → ENTRY → CALL:addBlock → VIRTUAL_CALL → ENTRY → WHILE:true → WHILE_COND:true → TRY → CALL:org.apache.hadoop.hdfs.protocol.ClientProtocol:addBlock → CATCH:RemoteException → IF:NotReplicatedYetException → CALL:org.slf4j.Logger:info → CALL:org.slf4j.Logger:warn → RETURN → EXIT → CALL:block.setCurrentBlock → CALL:errorState.getBadNodeIndex → CALL:LOG.warn → WHILE → IF → RETURN",
    "log": "[INFO] Exception while adding a block [INFO] Waiting for replication for {elapsed/1000} seconds [WARN] NotReplicatedYetException sleeping {src} retries left {retries} [WARN] Caught exception [WARN] Abandoning block: + block [WARN] Excluding datanode: + badNode"
  },
  "21530fb9_1": {
    "exec_flow": "ENTRY → FOREACH:args → TRY → CALL:expandArgument → PathData.expandAsGlob → [Match Type Checks] → if(items.length == 0) → CATCH:IOException → CALL:displayError → ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→EXIT FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "1e109504_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Updating lastPromisedEpoch from + lastPromisedEpoch.get() + to + newEpoch + for client + Server.getRemoteIp() + ; journal id: + journalId→CALL: lastPromisedEpoch.get→CALL: lastPromisedEpoch.set→EXIT",
    "log": "[INFO] Updating lastPromisedEpoch from <lastPromisedEpoch value> to <newEpoch> for client <RemoteIp> ; journal id: <journalId>"
  },
  "1e109504_2": {
    "exec_flow": "ENTRY→CALL:checkRequest→CALL:PersistentLongFile:get→IF_FALSE:reqInfo.getEpoch()!=lastWriterEpoch.get()→EXIT",
    "log": ""
  },
  "cb69764d_1": {
    "exec_flow": "ENTRY → LOG:INFO:Done acknowledgment from + taskAttemptID.toString() → CALL:taskHeartbeatHandler.progressing → CALL:context.getEventHandler().handle → EXIT",
    "log": "<log> [INFO] Done acknowledgment from + taskAttemptID.toString() </log>"
  },
  "cb69764d_2": {
    "exec_flow": "ENTRY → LOG: LOG.ERROR: Received + event → IF_TRUE: HAUtil.isHAEnabled(getConfig()) → LOG: LOG.WARN: Transitioning the resource manager to standby. → CALL: handleTransitionToStandByInNewThread → EXIT",
    "log": "<log> [ERROR] Received event </log> <log> [WARN] Transitioning the resource manager to standby. </log>"
  },
  "cb69764d_3": {
    "exec_flow": "ENTRY → LOG: LOG.ERROR: Received + event → IF_FALSE: HAUtil.isHAEnabled(getConfig()) → SWITCH: event.getType() → CASE: [STATE_STORE_FENCED] → LOG: LOG.ERROR: FATAL, State store fenced even though the resource manager is not configured for high availability. Shutting down this resource manager to protect the integrity of the state store. → CALL: ExitUtil.terminate → EXIT",
    "log": "<log> [ERROR] Received event </log> <log> [ERROR] FATAL, State store fenced even though the resource manager is not configured for high availability. Shutting down this resource manager to protect the integrity of the state store. </log>"
  },
  "cb69764d_4": {
    "exec_flow": "ENTRY → IF_TRUE: component == null → LOG: [ERROR] No component exists for {event.getName()} → RETURN → EXIT",
    "log": "<log> [ERROR] No component exists for {event.getName()} </log>"
  },
  "cb69764d_5": {
    "exec_flow": "ENTRY → IF_TRUE: rmApp != null → TRY → CALL: rmApp.handle → EXCEPTION: handle → CATCH: Throwable t → LOG: LOG.ERROR: Error in handling event type + event.getType() + for application + appID, t → EXIT",
    "log": "<log> [ERROR] Error in handling event type + event.getType() + for application + appID, t </log>"
  },
  "cb69764d_6": {
    "exec_flow": "ENTRY → IF_FALSE: component == null → TRY → CALL: component.handle → EXCEPTION: component.handle → CATCH: Throwable t → LOG: [ERROR] [COMPONENT {component.getName()}]: Error in handling event type {event.getType()} → EXIT",
    "log": "<log> [ERROR] [COMPONENT {component.getName()}]: Error in handling event type {event.getType()} </log>"
  },
  "cb69764d_7": {
    "exec_flow": "ENTRY → IF_FALSE: app != null → LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "<log> [WARN] Event + event + sent to absent application + event.getApplicationID() </log>"
  },
  "cb69764d_8": {
    "exec_flow": "ENTRY → IF_FALSE: c != null → LOG: LOG.WARN: Event + event + sent to absent container + event.getContainerID() → EXIT",
    "log": "<log> [WARN] Event {event} sent to absent container {event.getContainerID()} </log>"
  },
  "ee775cd3_1": {
    "exec_flow": "ENTRY→CALL:checkNNStartup→CALL:checkOperation→IF_FALSE:skipRetryCache()→CALL:waitForCompletion→CALL:newEntry→CALL:org.slf4j.Logger:isTraceEnabled()→CALL:org.slf4j.Logger:trace→RETURN→IF_TRUE:cacheEntry != null && cacheEntry.isSuccess()→RETURN→EXIT",
    "log": "[TRACE] Execution trace"
  },
  "ee775cd3_2": {
    "exec_flow": "ENTRY->CALL:checkOperation->TRY->CALL:checkSuperuserPrivilege->CALL:writeLock->CALL:checkOperation->CALL:checkNameNodeSafeMode->CALL:FSNDNCacheOp:modifyCachePool->CALL:writeUnlock->CALL:logSync->LOG:logAuditEvent->EXIT",
    "log": "[INFO] logAuditEvent - success"
  },
  "ee775cd3_3": {
    "exec_flow": "ENTRY->CALL:checkOperation->TRY->CALL:checkSuperuserPrivilege->EXCEPTION:checkSuperuserPrivilege->CATCH:AccessControlException ace->LOG:logAuditEvent->THROW:ace->EXIT",
    "log": "[WARNING] logAuditEvent - failed"
  },
  "ee775cd3_4": {
    "exec_flow": "ENTRY→CALL:cacheManager.modifyCachePool→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_TRUE:info.getOwnerName()!=null→CALL:setOwnerName→CALL:append→LOG:LOG.INFO→CALL:fsn.getEditLog().logModifyCachePool→ENTRY→LOG:logRpcIds→CALL:ModifyCachePoolOp.getInstance→CALL:beginTransaction→TRY→CALL:editLogStream.writeRaw→CALL:endTransaction→EXIT→CALL:logSync()→EXIT",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set owner to {info.getOwnerName()} [INFO] logRpcIds invoked [INFO] logEdit invoked"
  },
  "ee775cd3_5": {
    "exec_flow": "ENTRY→CALL:cacheManager.modifyCachePool→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_TRUE:info.getGroupName()!=null→CALL:setGroupName→CALL:append→LOG:LOG.INFO→CALL:fsn.getEditLog().logModifyCachePool→ENTRY→LOG:logRpcIds→CALL:ModifyCachePoolOp.getInstance→CALL:beginTransaction→TRY→CALL:editLogStream.writeRaw→CALL:endTransaction→EXIT→CALL:logSync()→EXIT",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set group to {info.getGroupName()} [INFO] logRpcIds invoked [INFO] logEdit invoked"
  },
  "d7fcd312_1": {
    "exec_flow": "ENTRY→CALL:getBoolean→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config... →FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→IF_TRUE:props != null →CALL:loadResources→IF_TRUE:loadDefaults && fullReload→FOREACH:defaultResources→CALL:loadResource →FOREACH_EXIT→FOR_INIT→FOR_COND:i < resources.size()→CALL:loadResource→FOR_EXIT→CALL:addTags→IF_TRUE:overlay != null →CALL:putAll→IF_TRUE:backup != null→FOREACH:overlay.entrySet()→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration [INFO] message"
  },
  "d7fcd312_2": {
    "exec_flow": "ENTRY→CALL:borrow→IF_TRUE:decompressor==null→CALL:createDecompressor→CALL:getDefaultExtension→LOG:[INFO] Got brand-new decompressor [<default extension>]→IF_TRUE:decompressor!=null&&!decompressor.getClass().isAnnotationPresent(DoNotPool.class)→CALL:updateLeaseCount→RETURN→EXIT",
    "log": "[INFO] Got brand-new decompressor [<default extension>]"
  },
  "d7fcd312_3": {
    "exec_flow": "ENTRY→CALL:borrow→IF_FALSE:decompressor==null→CALL:isDebugEnabled→IF_TRUE→LOG:[DEBUG] Got recycled decompressor→IF_TRUE:decompressor!=null&&!decompressor.getClass().isAnnotationPresent(DoNotPool.class)→CALL:updateLeaseCount→RETURN→EXIT",
    "log": "[DEBUG] Got recycled decompressor"
  },
  "81b5944e_1": {
    "exec_flow": "PARENT_ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: path.length <= 1 → IF_FALSE: root.isLink() → CALL: Preconditions.checkState → CALL: tryResolveInRegexMountpoint → IF_TRUE: resolveResult != null → LOG: LOGGER.DEBUG → WHILE: srcMatcher.find() → CALL: replaceRegexCaptureGroupInPath → WHILE_EXIT → IF_FALSE: 0 == mappedCount → FOREACH: interceptorList → FOREACH_EXIT → CALL: buildResolveResultForRegexMountPoint → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()</log_entry>"
  },
  "ffc4aec9_1": {
    "exec_flow": "ENTRY→IF_TRUE: !timelineServiceV15Enabled→THROW: new YarnException(\"This API is not supported under current Timeline Service Version:\")→EXIT TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}], this, action, new Exception()→CALL:Subject.doAs→RETURN→EXIT CATCH:PrivilegedActionException→LOG:LOG.DEBUG:PrivilegedActionException as: {}, this, cause→THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "ffc4aec9_2": {
    "exec_flow": "ENTRY→IF_FALSE: !timelineServiceV15Enabled→CALL: timelineWriter.putDomain→EXIT TRY→CALL:doAs→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→IF_TRUE:resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()→LOG:LOG.ERROR:msg→CALL:getStatus→LOG:LOG.DEBUG:HTTP error code: {} Server response : \\n{}, resp.getStatus(), resp.getEntity(String.class)→THROW:new YarnException(msg)→EXIT",
    "log": "[ERROR] Failed to get the response from the timeline server. [DEBUG] HTTP error code: {some_error_code} Server response : {some_response}"
  },
  "461ed426_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.net.DNS:getDefaultIP→EXCEPTION:UnknownHostException→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "[WARN] Could not find ip address of \"default\" inteface."
  },
  "74cdcc7f_1": {
    "exec_flow": "ENTRY→IF_FALSE:eA == null || eA.getValue() == null→IF_FALSE:eB == null || eB.getValue() == null→SWITCH:op→CASE:[subtractTestNonNegative]→IF_FALSE:!Resources.fitsIn(b, a)→CALL:subtract→[VIRTUAL_CALL]→ENTRY→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND:i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "74cdcc7f_2": {
    "exec_flow": "ENTRY→CALL:clone→CALL:multiplyTo→CALL:multiplyAndRound→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_COND:i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_ITERATION→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:..."
  },
  "3615a664_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → IF_TRUE:loginUser==null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser) → CALL:createLoginUser → CALL:loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND:loginUser==null → DO_EXIT → RETURN → EXIT",
    "log": "<log>[LOG] getLoginUser</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log>"
  },
  "3615a664_2": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "3615a664_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3615a664_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3615a664_5": {
    "exec_flow": "ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG: LOG.DEBUG: Creating new Groups object → NEW: Groups → CALL:<init> → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating new Groups object</log>"
  },
  "3615a664_6": {
    "exec_flow": "ENTRY → IF_TRUE:sources != null && (!Arrays.asList(sources).contains(\"core-default.xml\") || sources.length > 1) → TRY → CALL:getFileContext → VIRTUAL_CALL → handleDeprecation → CHECK → FOREACH:names → CALL:getProps → VIRTUAL_CALL → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[INFO] Default file system [ + fc.getDefaultFileSystem().getUri() + ]</log> <log>[ERROR] Unable to create default file context [ + defaultConf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ], e</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3615a664_7": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "3615a664_8": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3615a664_9": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "0b8b3fae_1": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:setConfig→EXIT",
    "log": "[DEBUG] Config has been overridden during init"
  },
  "0b8b3fae_2": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "11639aa4_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:org.apache.hadoop.fs.azure.NativeAzureFileSystem:open(org.apache.hadoop.fs.Path,int) → LOG: Opening file: {}, f.toString() → CALL: performAuthCheck → TRY → CALL: retrieveMetadata → IF_FALSE: retrieveMetadata successful and meta != null → IF_FALSE: meta.isDirectory() → TRY → CALL: retrieve → NEW: FSDataInputStream → NEW: BufferedFSInputStream → NEW: NativeAzureFsInputStream → CALL: getLen → RETURN → EXIT",
    "log": "[DEBUG] Opening file: {f.toString()}"
  },
  "11639aa4_2": {
    "exec_flow": "ENTRY → CALL:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open → CALL:org.slf4j.Logger:debug → NEW:FSDataInputStream → RETURN → EXIT",
    "log": "[DEBUG] Incrementing stat CALL_OPEN [INFO] File qualified [INFO] File opened for read"
  },
  "11639aa4_3": {
    "exec_flow": "ENTRY → CALL:getFileStatus → IF_FALSE:fs.isDirectory() → CALL:NativeFileSystemStore:getFileLength(key) → LOG: Open the file: [{f}] for reading. → NEW: FSDataInputStream → CALL:getConf → NEW: BufferedFSInputStream → NEW: CosNInputStream → RETURN → EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "11639aa4_4": {
    "exec_flow": "ENTRY → CALL:getFileStatus → IF_FALSE:fs.isDirectory() → CALL:NativeFileSystemStore:getFileLength(key) → LOG: Open the file: [{f}] for reading. → NEW: FSDataInputStream → CALL:getConf → NEW: BufferedFSInputStream → NEW: CosNInputStream → RETURN → EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "eab98959_1": {
    "exec_flow": "ENTRY→CALL:remove→CALL:serviceRecordMap.remove→CALL:serviceMetaData.remove→IF_TRUE:s!=null→LOG:LOG.INFO:Removing aux service + sName→CALL:stopAuxService→ENTRY→IF_TRUE:service.getServiceState()==Service.STATE.STARTED→SYNC:stateChangeLock→IF_FALSE:isInState(STATE.STOPPED)→IF_TRUE:enterState(STATE.STOPPED)!=STATE.STOPPED→TRY→CALL:serviceStop→EXCEPTION:serviceStop→CATCH:Exception e→CALL:noteFailure→THROW:ServiceStateException.convert(e)→CALL:notifyListeners→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH:Throwable e→LOG:LOG.WARN:Exception while notifying listeners of {}, this, e",
    "log": "[INFO] Removing aux service [DEBUG] Service: {} entered state {} [WARN] Exception while notifying listeners of {} [DEBUG] noteFailure [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {}"
  },
  "2c6ffb85_1": {
    "exec_flow": "ENTRY → CALL:FederationApplicationHomeSubClusterStoreInputValidator:validate → IF_TRUE:request==null → LOG:WARN:Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information. → THROW:new FederationStateStoreInvalidInputException(Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.) → IF_FALSE:request==null → TRY → CALL:getConnection → CALL:Connection:prepareCall → CALL:CallableStatement:setString → CALL:CallableStatement:registerOutParameter → CALL:CallableStatement:execute → CALL:CallableStatement:getString → IF_TRUE:homeSubCluster==null → CALL:org.slf4j.Logger:error → THROW:new FederationStateStoreException → TRY_EXIT → FINALLY → CALL:FederationStateStoreUtils:returnToPool → EXIT",
    "log": "[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information. [DEBUG] Got the information about the specified application {}. The AM is running in {}"
  },
  "2c6ffb85_2": {
    "exec_flow": "ENTRY → IF_TRUE:request==null → LOG:LOG.WARN:Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information. → THROW:new FederationStateStoreInvalidInputException(Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.) → EXIT",
    "log": "[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information."
  },
  "2c6ffb85_3": {
    "exec_flow": "ENTRY → IF_TRUE:request!=null → CALL:checkApplicationId(request.getApplicationId()) → IF_TRUE:appId==null → LOG:LOG.WARN:Missing Application Id. Please try again by specifying an Application Id. → THROW:new FederationStateStoreInvalidInputException(Missing Application Id. Please try again by specifying an Application Id.) → EXIT",
    "log": "[WARN] Missing Application Id. Please try again by specifying an Application Id."
  },
  "2c6ffb85_4": {
    "exec_flow": "ENTRY → CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate → IF_TRUE:!applications.containsKey(appId) → CALL:org.slf4j.Logger:error → THROW:new FederationStateStoreException → EXIT",
    "log": "[ERROR] Application {appId} does not exist"
  },
  "639061b1_1": {
    "exec_flow": "ENTRY→IF_TRUE: finished > 0 && started > 0→IF_FALSE: elapsed >= 0→CALL:org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Finished time + finished + is ahead of started time + started"
  },
  "639061b1_2": {
    "exec_flow": "ENTRY→IF_FALSE: finished > 0 && started > 0→IF_TRUE: isRunning→IF_FALSE: elapsed >= 0→CALL:org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Current time + current + is ahead of started time + started"
  },
  "639061b1_3": {
    "exec_flow": "ENTRY→IF_FALSE: hasAccess(job, request)→THROW: WebApplicationException(Status.UNAUTHORIZED)→EXIT",
    "log": "<!-- As there are no logs in parent or child nodes, this remains empty -->"
  },
  "bb53c12b_1": {
    "exec_flow": "ENTRY -> CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate -> IF_FALSE: request == null -> CALL:checkApplicationId -> CALL:checkApplicationHomeSubCluster -> ENTRY -> IF_FALSE: applicationHomeSubCluster == null -> CALL:FederationMembershipStateStoreInputValidator.checkSubClusterId -> ENTRY → IF_TRUE:subClusterId==null→CALL:org.slf4j.Logger:warn→THROW:new FederationStateStoreInvalidInputException() → EXIT",
    "log": "[WARN] Missing SubCluster Id information. Please try again by specifying Subcluster Id information."
  },
  "bb53c12b_2": {
    "exec_flow": "ENTRY -> CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate -> IF_FALSE: request == null -> CALL:checkApplicationId -> CALL:checkApplicationHomeSubCluster -> ENTRY -> IF_FALSE: applicationHomeSubCluster == null -> CALL:FederationMembershipStateStoreInputValidator.checkSubClusterId -> ENTRY → IF_FALSE:subClusterId==null → IF_TRUE:subClusterId.getId().isEmpty() → CALL:org.slf4j.Logger:warn → THROW:new FederationStateStoreInvalidInputException() → EXIT",
    "log": "[WARN] Invalid SubCluster Id information. Please try again by specifying valid Subcluster Id."
  },
  "bb53c12b_3": {
    "exec_flow": "ENTRY -> CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate -> IF_TRUE: request == null -> LOG: LOG.WARN: Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information. -> THROW: new FederationStateStoreInvalidInputException() -> EXIT",
    "log": "[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information."
  },
  "bb53c12b_4": {
    "exec_flow": "ENTRY -> CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate -> IF_FALSE: request == null -> CALL:checkApplicationId -> CALL:checkApplicationHomeSubCluster -> ENTRY -> IF_TRUE: applicationHomeSubCluster == null -> LOG: LOG.WARN: Missing ApplicationHomeSubCluster Info. Please try again by specifying an ApplicationHomeSubCluster information. -> THROW: new FederationStateStoreInvalidInputException() -> EXIT",
    "log": "[WARN] Missing ApplicationHomeSubCluster Info. Please try again by specifying an ApplicationHomeSubCluster information."
  },
  "bb53c12b_5": {
    "exec_flow": "ENTRY → CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate → TRY → CALL:putApp → EXCEPTION:putApp → CATCH:Exception e → CALL:FederationStateStoreUtils.logAndThrowStoreException → ENTRY → CALL:org.slf4j.Logger:error → THROW:new FederationStateStoreException → EXIT → TRY → CALL:getApp → CALL:newInstance → RETURN → EXIT",
    "log": "[ERROR] Cannot add application home subcluster for appId [ERROR] Cannot check app home subcluster for appId"
  },
  "f7b9592b_1": {
    "exec_flow": "ENTRY→CALL:getQueueInfo→TRY→CALL:getCallerUgi→CALL:recordFactory.newRecordInstance→CALL:request.getQueueName→CALL:request.getIncludeApplications→CALL:request.getIncludeChildQueues→CALL:request.getRecursive→CALL:scheduler.getQueueInfo→IF_TRUE:request.getIncludeApplications→CALL:scheduler.getAppsInQueue→NEW:ArrayList<ApplicationReport>→CALL:apps.size→FOREACH:apps→CALL:rmContext.getRMApps→CALL:app.getApplicationId→IF_TRUE:rmApp != null→CALL:rmApp.getUser→CALL:checkAccess→CALL:rmApp.createAndGetApplicationReport→CALL:appReports.add→FOREACH_EXIT→CALL:queueInfo.setApplications→CALL:response.setQueueInfo→CALL:RMAuditLogger.logSuccess→RETURN→EXIT",
    "log": "[INFO] RMAuditLogger logSuccess"
  },
  "d2e715e7_1": {
    "exec_flow": "ENTRY→FOREACH:queues.getQueues→CALL:clearPriorityACLs→CALL:addPrioirityACLs→IF_TRUE: null == priorityACL→NEW: ArrayList<PriorityACL>→CALL: allAcls.put→CALL: Collections.sort→FOREACH: priorityACLGroups→CALL: org.slf4j.Logger:isDebugEnabled()→CALL: org.slf4j.Logger:debug(java.lang.String)→FOREACH_EXIT→FOREACH_EXIT→CALL:setPermission→CALL:getCurrentUser→ENTRY→IF_TRUE: GROUPS == null→IF_TRUE: LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG: [DEBUG] Creating new Groups object→NEW: Groups→CALL:<init>→RETURN→IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty()→VIRTUAL_CALL→ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser == null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser)→CALL:createLoginUser→LOG:[WARN] Unexpected SecurityException in Configuration→CALL:doSubjectLogin→IF_TRUE:proxyUser == null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→LOG:[DEBUG] Reading credentials from location {file_path}→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→TRY→LOG:[DEBUG] Loaded {} tokens from {file_path}→LOG:[INFO] Token file {file_path} does not exist→CALL:readTokenStorageStream→LOG:[DEBUG] Loaded {} base64 tokens→LOG:[DEBUG] Failure to load login credentials→LOG:[DEBUG] UGI loginUser: {loginUser}→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser == null→DO_EXIT→EXIT→CALL: getAuthenticationMethod→IF_TRUE: overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()→TRY→CALL: HadoopKerberosName.setConfiguration→TRY→CALL: getLong→CALL: getBoolean→IF_TRUE: !(groups instanceof TestingGroups)→CALL: getUserToGroupsMappingService→IF_FALSE: metrics.getGroupsQuantiles == null→EXIT→VIRTUAL_CALL→ENTRY→IF_TRUE: GROUPS == null→IF_TRUE: LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG: [DEBUG] Creating new Groups object→NEW: Groups→CALL:<init>→RETURN→EXIT→LOG: [DEBUG] Handling deprecation for all properties in config...→LOG: [DEBUG] Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→LOG:[ERROR] Cannot add token {}: {}→LOG:[INFO] Cleaning up resources→RETURN→CALL: IOUtils.cleanupWithLogger→EXIT→LOG:[WARN] Null token ignored for {}",
    "log": "<log_entry>[DEBUG] Priority ACL group added: max-priority - {priorityACLGroup.getMaxPriority()} default-priority - {priorityACLGroup.getDefaultPriority()}</log_entry> <log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[DEBUG] Reading credentials from location {file_path}</log_entry> <log_entry>[DEBUG] Loaded {} tokens from {file_path}</log_entry> <log_entry>[INFO] Token file {file_path} does not exist</log_entry> <log_entry>[DEBUG] Loaded {} base64 tokens</log_entry> <log_entry>[DEBUG] Failure to load login credentials</log_entry> <log_entry>[DEBUG] UGI loginUser: {loginUser}</log_entry> <log_entry>[ERROR] Cannot add token {}: {}</log_entry> <log_entry>[INFO] Cleaning up resources</log_entry> <log_entry>[WARN] Null token ignored for {}"
  },
  "83bedd43_1": {
    "exec_flow": "ENTRY→IF_TRUE: numRec_ == nextStatusRec_→CALL:getStatus→CALL:setStatus→EXIT",
    "log": "[INFO] status"
  },
  "d22cc506_1": {
    "exec_flow": "Entry → [VIRTUAL_CALL] → Entry → CALL: ensureInitialized → IF_TRUE: !isInitialized() → SYNC: UserGroupInformation.class → IF_TRUE: !isInitialized() → CALL: initialize → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → ENTRY → CALL:getLoginUser → IF_TRUE: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null,newLoginUser) → CALL: createLoginUser → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log>"
  },
  "d22cc506_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {tokenFile.getCanonicalPath()}</log> <log>[DEBUG] Loaded {cred.numberOfTokens()} tokens from {tokenFile.getCanonicalPath()}</log> <log>[INFO] Token file {tokenFile.getCanonicalPath()} does not exist</log> <log>[ERROR] Cannot add token {tokenBase64}: {ioe.getMessage()}</log> <log>[DEBUG] Loaded {numTokenBase64} base64 tokens</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {loginUser}</log>"
  },
  "d22cc506_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {tokenFile.getCanonicalPath()}</log> <log>[DEBUG] Loaded {cred.numberOfTokens()} tokens from {tokenFile.getCanonicalPath()}</log> <log>[INFO] Token file {tokenFile.getCanonicalPath()} does not exist</log> <log>[ERROR] Cannot add token {tokenBase64}: {ioe.getMessage()}</log> <log>[DEBUG] Loaded {numTokenBase64} base64 tokens</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {loginUser}</log>"
  },
  "d22cc506_4": {
    "exec_flow": "ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: LOG.DEBUG: Creating new Groups object → NEW: Groups → CALL:<init> → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating new Groups object</log>"
  },
  "d22cc506_5": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "d22cc506_6": {
    "exec_flow": "Parent.ENTRY → LOG: Reading credentials from location → IF: tokenFile.exists() && tokenFile.isFile() → LOG: Loaded tokens from → CALL: addCredentials → ELSE → LOG: Token file does not exist → TRY → LOG: Failure to load login credentials → EXCEPTION: IOException",
    "log": "<log>[DEBUG] Reading credentials from location {file_path}</log> <log>[DEBUG] Loaded {number_of_tokens} tokens from {file_path}</log> <log>[INFO] Token file {file_path} does not exist</log> <log>[DEBUG] Failure to load login credentials</log>"
  },
  "7ee9fef9_1": {
    "exec_flow": "ENTRY → CALL: transition → LOG: container.addDiagnostics(\"Killed by external signal\\n\") → ENTRY → FOREACH: diags → FOREACH_EXIT → IF_TRUE: diagnostics.length() > diagnosticsMaxSize → CALL: delete → TRY → ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: LOG.DEBUG: storeContainerDiagnostics: containerId={}, diagnostics=, containerId, diagnostics → TRY → CALL: put → EXCEPTION: put → CATCH: DBException e → CALL: markStoreUnHealthy → THROW: new IOException(e) → EXIT",
    "log": "<log> container.addDiagnostics(\"Killed by external signal\\n\") </log> <log> [WARN] Unable to update diagnostics in state store for [containerId], e </log> <log> [DEBUG] storeContainerDiagnostics: containerId={}, diagnostics= </log>"
  },
  "7ee9fef9_2": {
    "exec_flow": "ENTRY → CALL: transition → LOG: container.addDiagnostics(\"Killed by external signal\\n\") → ENTRY → FOREACH: diags → FOREACH_EXIT → IF_FALSE: diagnostics.length() > diagnosticsMaxSize → TRY → ENTRY → [VIRTUAL_CALL] → ENTRY → LOG: LOG.DEBUG: storeContainerDiagnostics: containerId={}, diagnostics=, containerId, diagnostics → TRY → CALL: put → EXCEPTION: put → CATCH: DBException e → CALL: markStoreUnHealthy → THROW: new IOException(e) → EXIT",
    "log": "<log> container.addDiagnostics(\"Killed by external signal\\n\") </log> <log> [WARN] Unable to update diagnostics in state store for [containerId], e </log> <log> [DEBUG] storeContainerDiagnostics: containerId={}, diagnostics= </log>"
  },
  "7ee9fef9_3": {
    "exec_flow": "ENTRY → FOREACH: listofHandlers → CALL: org.apache.hadoop.yarn.event.EventHandler:handle → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Processing [TaskAttemptID] of type [eventType] → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getInternalState() → IF_TRUE: getInternalState() == TaskAttemptStateInternal.FAILED → LOG: LOG.INFO: [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId] → CALL: writeLock.unlock → IF_FALSE: app != null → LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "<log> [DEBUG] Processing [TaskAttemptID] of type [eventType] </log> <log> [INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId] </log> <log> [WARN] Event + event + sent to absent application + event.getApplicationID() </log>"
  },
  "7ee9fef9_4": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Processing [TaskAttemptID] of type [eventType] → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getInternalState() → IF_FALSE: getInternalState() == TaskAttemptStateInternal.FAILED → LOG: LOG.INFO: [attemptId] TaskAttempt Transitioned from [oldState] to [newState] → CALL: writeLock.unlock → EXIT",
    "log": "<log> [DEBUG] Processing [TaskAttemptID] of type [eventType] </log> <log> [INFO] [attemptId] TaskAttempt Transitioned from [oldState] to [newState] </log>"
  },
  "3c8fbb5c_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG:[INFO] message → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3c8fbb5c_2": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "3c8fbb5c_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "3c8fbb5c_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "d09419e5_1": {
    "exec_flow": "ENTRY→TRY→CALL:readFully→CALL:org.apache.hadoop.fs.FSInputStream:readFully→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→RETURN→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "c59e05fb_1": {
    "exec_flow": "ENTRY → IF_TRUE: fs != null → CALL:debugLogFileSystemClose → SYNC:deleteOnExit → FOR_INIT → FOR_COND:iter.hasNext() → CALL:exists → IF (getFileStatus result != null) → CALL:delete → CATCH (IOException) → CALL:info → REMOVE → FOR_CONTINUE → CALL:CACHE.remove → EXIT",
    "log": "[DEBUG] FileSystem.close() by method: java.lang.String); Key: someKey; URI: someUri; Object Identity Hash: someHash [TRACE] FileSystem.close() full stack trace: [DEBUG] Closing FileSystem: Key: someKey; URI: someUri; Object Identity Hash: someHash [INFO] Ignoring failure to deleteOnExit for path {}"
  },
  "e6359fd2_1": {
    "exec_flow": "ENTRY→IF_TRUE: GROUPS == null→IF_TRUE: LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG: LOG.DEBUG: Creating new Groups object→NEW: Groups→CALL:<init>→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "e6359fd2_2": {
    "exec_flow": "ENTRY→NEW: HashSet<String>→NEW: HashSet<String>→FOREACH: userGroupStrings→FOREACH_EXIT→IF_TRUE: !allAllowed→IF_TRUE: userGroupStrings.length >= 1 && userGroupStrings[0] != null→CALL: getTrimmedStringCollection→IF_TRUE: userGroupStrings.length == 2 && userGroupStrings[1] != null→CALL: getTrimmedStringCollection→TRY→CALL: impl.cacheGroupsAdd→EXCEPTION: IOException→CATCH: IOException→LOG: LOG.WARN: Error caching groups, e→EXIT",
    "log": "[WARN] Error caching groups"
  },
  "172314a0_1": {
    "exec_flow": "ENTRY → IF_TRUE: rsRawDecoder == null → CALL: getConf → CALL: getOptions → CALL: org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder → CALL:Preconditions.checkNotNull → CALL:Preconditions.checkNotNull → CALL:createRawDecoderWithFallback → CALL:getRawCoderNames → FOREACH: coders → CALL:isDebugEnabled → TRY → IF_FALSE: rawCoderName != null → FOREACH_NEXT → TRY → CALL:isDebugEnabled → CALL:debug → FOREACH_EXIT → THROW: new IllegalArgumentException(\"Fail to create raw erasure \" + \"encoder with given codec: \" + codecName) → RETURN → EXIT",
    "log": "[DEBUG] Failed to create raw erasure decoder, fallback to next codec if possible [DEBUG] Failed to create raw erasure encoder {rawCoderName}, fallback to next codec if possible"
  },
  "e3ccfaad_1": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>IF_FALSE: otherArgs.length == 0</step> <step>LOG: LOG.DEBUG: Executing command {subCommand}</step> <step>IF_FALSE: UNSUPPORTED_COMMANDS.contains(subCommand)</step> <step>SWITCH: subCommand</step> <step>CASE: [BucketInfo.NAME]</step> <step>NEW: BucketInfo</step> <step>CALL: ToolRunner.run</step> <step>FINALLY</step> <step>CALL: IOUtils.cleanupWithLogger</step> <step>EXIT</step> <step>CALL: createStoreContext</step> <step>CALL: println</step> <step>IF_TRUE: policy == DirectoryPolicy.MarkerPolicy.Authoritative</step> <step>CALL: println</step> <step>TRY</step> <step>CALL: getFilesystem().getFileStatus</step> <step>EXCEPTION: getFileStatus</step> <step>CATCH: UnknownStoreException ex</step> <step>THROW: new ExitUtil.ExitException(EXIT_NOT_FOUND, ex.toString(), ex)</step> <step>EXIT</step> </sequence>",
    "log": "<log>[DEBUG] Executing command {BucketInfo.NAME}</log> <log>[INFO] The directory marker policy of ... is \"...\"</log> <log>[INFO] Authoritative path list is \"...\"</log> <log>[INFO] Swapping -min (...) and -max (...) values</log>"
  },
  "e3ccfaad_2": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>CALL: createStoreContext</step> <step>CALL: println</step> <step>IF_TRUE: policy == DirectoryPolicy.MarkerPolicy.Authoritative</step> <step>CALL: println</step> <step>TRY</step> <step>CALL: getFilesystem().getFileStatus</step> <step>EXCEPTION: getFileStatus</step> <step>CATCH: UnknownStoreException ex</step> <step>THROW: new ExitUtil.ExitException(EXIT_NOT_FOUND, ex.toString(), ex)</step> <step>EXIT</step> </sequence>",
    "log": "<log>[INFO] The directory marker policy of ... is \"...\"</log> <log>[INFO] Authoritative path list is \"...\"</log> <log>[INFO] Swapping -min (...) and -max (...) values</log>"
  },
  "002bc7df_1": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT ENTRY → LOG: Handling deprecation for all properties in config... → FOREACH: keys → LOG: Handling deprecation for (String)item → LOG: [INFO] message ENTRY → LOG:LOG.DEBUG → CALL:statIncrement → CALL:trailingPeriodCheck → CALL:makeQualified → TRY → CALL:createFile → CALL:statIncrement → NEW:FSDataOutputStream → RETURN → EXIT",
    "log": "[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask. [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {} [DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}"
  },
  "002bc7df_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → CALL:statIncrement → CALL:trailingPeriodCheck → CALL:makeQualified → TRY → EXCEPTION:createFile → CATCH:AzureBlobFileSystemException → CALL:checkException → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {} [DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call [DEBUG] Creating default headers [DEBUG] Creating default URI query builder [INFO] Adding query param: GET_ACCESS_CONTROL [INFO] Adding query param: UPN [INFO] Appending SAS token to query [DEBUG] Creating request URL [INFO] Executing REST operation [DEBUG] Get root ACL status"
  },
  "002bc7df_3": {
    "exec_flow": "ENTRY → TRY → FOREACH:regexMountPointList → CALL:regexMountPoint.resolve → FOREACH:interceptorList → FOREACH_EXIT → LOG:LOGGER.DEBUG → WHILE:srcMatcher.find() → CALL:replaceRegexCaptureGroupInPath → WHILE_EXIT → IF_FALSE:0==mappedCount → FOREACH:interceptorList → FOREACH_EXIT → CALL:buildResolveResultForRegexMountPoint → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()"
  },
  "002bc7df_4": {
    "exec_flow": "ENTRY → TRY → FOREACH:regexMountPointList → CALL:regexMountPoint.resolve → FOREACH:interceptorList → FOREACH_EXIT → LOG:LOGGER.DEBUG → WHILE:srcMatcher.find() → CALL:replaceRegexCaptureGroupInPath → WHILE_EXIT → IF_TRUE:0==mappedCount → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()"
  },
  "002bc7df_5": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→LOG:debug→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "[DEBUG] Overwriting file {}"
  },
  "002bc7df_6": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG: Creating file: {}, f.toString() → IF_TRUE:containsColon(f) → THROW:new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\") → EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "002bc7df_7": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE:containsColon(f) → CALL:performAuthCheck → CALL:createInternal → RETURN → EXIT",
    "log": "[DEBUG] Creating file: {}"
  },
  "002bc7df_8": {
    "exec_flow": "ENTRY→TRY→CALL:setLogEnabled→CALL:getObjectMetadata(request)→EXCEPTION:getObjectMetadata→CATCH:OSSException osse→CALL:LOG.debug→RETURN→EXIT",
    "log": "[DEBUG] Exception thrown when get object meta: + key + , exception: + osse"
  },
  "002bc7df_9": {
    "exec_flow": "ENTRY → CALL: Preconditions.checkNotNull → IF_FALSE: InodeTree.SlashPath.equals(f) → IF_TRUE: this.fsState.getRootFallbackLink() != null → IF_FALSE: theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL: org.apache.hadoop.fs.FileSystem:create → RETURN → EXIT",
    "log": "[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}"
  },
  "595e31d4_1": {
    "exec_flow": "<seq> ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "c8b25940_1": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this + : + caller + starting sendCallback for fd + fd → CALL: Preconditions.checkNotNull → IF_FALSE: entry.getHandler().handle(sock) → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this: NotificationHandler: doing a read on sock.fd → IF_TRUE: sock.getInputStream().read() == -1 → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this: NotificationHandler: got EOF on sock.fd → THROW: new EOFException() → TRY → IF_TRUE: LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → LOG: LOG.TRACE: this + : + caller + : sendCallback not + closing fd + fd → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] this + : + caller + starting sendCallback for fd + fd [TRACE] this: NotificationHandler: doing a read on sock.fd [TRACE] this: NotificationHandler: got EOF on sock.fd [DEBUG] this + : + caller + : sendCallback not + closing fd + fd [DEBUG] Exception in closing {}"
  },
  "c8b25940_2": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this + : + caller + starting sendCallback for fd + fd → CALL: Preconditions.checkNotNull → IF_TRUE: entry.getHandler().handle(sock) → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this + : + caller + : closing fd + fd + at the request of the handler → IF_TRUE: toRemove.remove(fd) ≠ null → IF_FALSE: LOG.isTraceEnabled() → TRY → CALL: sock.refCount.unreferenceCheckClosed → CALL: IOUtils.cleanupWithLogger → CALL: fdSet.remove → RETURN → EXIT",
    "log": "[DEBUG] this + : + caller + starting sendCallback for fd + fd [TRACE] this: NotificationHandler: doing a read on sock.fd [DEBUG] this + : + caller + : closing fd + fd + at the request of the handler. [DEBUG] Exception in closing {}"
  },
  "c8b25940_3": {
    "exec_flow": "ENTRY → CALL: put → TRY → CALL: notificationSockets[1].refCount.reference → CALL: fdSet.add → IF_TRUE: LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → LOG: LOG.TRACE: this + : adding notificationSocket + notificationSockets[1].fd + , connected to + notificationSockets[0].fd → CALL: org.slf4j.Logger:trace(java.lang.String) → EXIT",
    "log": "[TRACE] this: adding notificationSocket [fd_value], connected to [fd_value]"
  },
  "e4135c1c_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Updating lastPromisedEpoch from <lastPromisedEpoch> to <newEpoch> for client <RemoteIp> ; journal id: <journalId>→CALL: lastPromisedEpoch.get→CALL: lastPromisedEpoch.set→EXIT",
    "log": "[INFO] Updating lastPromisedEpoch from <lastPromisedEpoch value> to <newEpoch> for client <RemoteIp> ; journal id: <journalId>"
  },
  "e4135c1c_2": {
    "exec_flow": "ENTRY→IF_TRUE: value != newVal || !loaded→CALL:writeFile→ENTRY→TRY→CALL:write→CALL:write→CALL:flush→CALL:((FileOutputStream) out).getChannel().force→CALL:close→IF_FALSE: success→IF_TRUE: !triedToClose→CALL:IOUtils.closeStream→IF_TRUE: !tmpFile.delete()→LOG:LOG.WARN: Unable to delete tmp file + tmpFile→EXIT",
    "log": "LOG.WARN: Unable to delete tmp file + tmpFile"
  },
  "e4135c1c_3": {
    "exec_flow": "ENTRY→IF_TRUE: value != newVal || !loaded→CALL:writeFile→ENTRY→TRY→CALL:write→CALL:write→CALL:flush→CALL:((FileOutputStream) out).getChannel().force→CALL:close→IF_FALSE: success→IF_FALSE: !triedToClose→IF_TRUE: !tmpFile.delete()→LOG:LOG.WARN: Unable to delete tmp file + tmpFile→EXIT",
    "log": "LOG.WARN: Unable to delete tmp file + tmpFile"
  },
  "e4135c1c_4": {
    "exec_flow": "ENTRY→IF_TRUE: value != newVal || !loaded→CALL:writeFile→ENTRY→TRY→CALL:close→EXCEPTION:close→CATCH:IOException ioe→LOG:LOG.WARN:Unable to abort file + tmpFile, ioe→CALL:org.slf4j.Logger:warn→IF_TRUE:!tmpFile.delete()→LOG:LOG.WARN:Unable to delete tmp file during abort + tmpFile→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "LOG.WARN: Unable to abort file + tmpFile, ioe LOG.WARN: Unable to delete tmp file during abort + tmpFile"
  },
  "e4135c1c_5": {
    "exec_flow": "ENTRY→IF_TRUE: value != newVal || !loaded→CALL:writeFile→ENTRY→TRY→CALL:close→EXCEPTION:close→CATCH:IOException ioe→LOG:LOG.WARN:Unable to abort file + tmpFile, ioe→CALL:org.slf4j.Logger:warn→IF_FALSE:!tmpFile.delete()→EXIT",
    "log": "LOG.WARN: Unable to abort file + tmpFile, ioe"
  },
  "e4135c1c_6": {
    "exec_flow": "ENTRY→IF_TRUE: value != newVal || !loaded→CALL:writeFile→ENTRY→TRY→CALL:close→IF_TRUE:!tmpFile.delete()→LOG:LOG.WARN:Unable to delete tmp file during abort + tmpFile→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "LOG.WARN: Unable to delete tmp file during abort + tmpFile"
  },
  "80a8432c_1": {
    "exec_flow": "ENTRY→CALL:loadAWSProviderClasses→IF_FALSE:awsClasses.isEmpty()→FOREACH:awsClasses→IF_TRUE:!Arrays.asList(...)→CALL:V2Migration.v1ProviderReferenced→CALL:createAWSCredentialProvider→IF_FALSE:!AWSCredentialsProvider.class.isAssignableFrom(credClass)→IF_FALSE:Modifier.isAbstract(credClass.getModifiers())→TRY→IF_FALSE:cons != null→CALL:getConstructor→IF_FALSE:cons != null→IF_TRUE:factory != null→CALL:invoke→RETURN→FOREACH_EXIT→RETURN→EXIT <log_sequence> <!-- Merged log sequence from parent and child nodes --> [DEBUG] Validating AWS credential provider classes [DEBUG] Credential provider class is {} [WARN] Directly referencing AWS SDK V1 credential provider {}. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2 [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log_sequence>",
    "log": "<!-- Merged log sequence from parent and child nodes --> [DEBUG] Validating AWS credential provider classes [DEBUG] Credential provider class is {} [WARN] Directly referencing AWS SDK V1 credential provider {}. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2 [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "07c67890_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.conf.Configuration:get(java.lang.String) → IF_TRUE: dirPath != null → NEW: Path → LOG: LOG.INFO: System Service Directory is configured to {}, systemServiceDir → CALL: org.apache.hadoop.fs.Path:getFileSystem → ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → IF_TRUE:props!=null → CALL:loadResources → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → IF_TRUE:props!=null → CALL:loadResources → FOREACH_EXIT → RETURN → EXIT → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean) → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → RETURN → EXIT → CALL: org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled → CALL: org.apache.hadoop.security.UserGroupInformation:getLoginUser → LOG: LOG.INFO: UserGroupInformation initialized to {}, loginUGI → EXIT",
    "log": "[INFO] System Service Directory is configured to {} [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Duplicate FS created for {}; discarding {} [INFO] UserGroupInformation initialized to {}"
  },
  "d998cebe_1": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Requesting buffer of size {}, limit→CALL:org.slf4j.Logger:debug→CALL:buffersOutstanding.incrementAndGet→CALL:getBuffer→RETURN→EXIT",
    "log": "[DEBUG] Requesting buffer of size {}"
  },
  "a81771f8_1": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → CALL: get → IF_TRUE: dnsInterface != null → CALL: getDefaultHost → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "a81771f8_2": {
    "exec_flow": "ENTRY→CALL:getHosts→CALL:getHosts→CALL:warn→RETURN→EXIT",
    "log": "<log> <level>WARN</level> <template>I/O error finding interface {}: {}</template> </log>"
  },
  "41540c51_1": {
    "exec_flow": "ENTRY → LOG: Processing {} of type {}, event.getNodeId(), event.getType() → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: Can't handle this event at current state, e → LOG: Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState → IF_TRUE: oldState != getState() → LOG: nodeId + Node Transitioned from + oldState + to + getState() → CALL: writeLock.unlock → EXIT",
    "log": "<log>[DEBUG] Processing {} of type {}</log> <log>[ERROR] Can't handle this event at current state</log> <log>[ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState</log> <log>[INFO] nodeId + Node Transitioned from + oldState + to + getState()</log>"
  },
  "d5e72896_1": {
    "exec_flow": "ENTRY → CALL:getBoolean → ENTRY→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→RETURN→EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "d5e72896_2": {
    "exec_flow": "ENTRY→CALL: LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[INFO] message"
  },
  "d5e72896_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "d5e72896_4": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_FALSE: backup != null → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "d5e72896_5": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_FALSE: overlay != null → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "d5e72896_6": {
    "exec_flow": "ENTRY→CALL:handle→IF_TRUE: action != null → SWITCH: action → CASE: [WARNING] → IF_TRUE: message != null → CALL:conversionOptions.handleWarning→CALL:log.WARN:message→BREAK→EXIT",
    "log": "[LOG] conversionOptions.handleWarning [WARN] message"
  },
  "d5e72896_7": {
    "exec_flow": "ENTRY→CALL:handle→IF_TRUE: action != null → SWITCH: action → CASE: [WARNING] → IF_FALSE: message != null → CALL:format→CALL:conversionOptions.handleWarning →CALL:log.WARN:format(\"Setting %s is not supported, ignoring conversion\", fsSetting)→BREAK→EXIT",
    "log": "[LOG] conversionOptions.handleWarning [WARN] format(\"Setting %s is not supported, ignoring conversion\", fsSetting)"
  },
  "f802c76c_1": {
    "exec_flow": "ENTRY → CALL:substituteVars → LOG:Handling deprecation for all properties in config... → CALL:YarnRPC:create → LOG:Creating a HadoopYarnProtoRpc server for protocol {protocol} with {numHandlers} handlers → CALL:InetSocketAddress → TRY → CALL:org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL:org.slf4j.Logger:isTraceEnabled() → IF_TRUE: elapsedMs >= slowLookupThresholdMs → CALL:org.slf4j.Logger:warn → IF_TRUE: staticHost != null → CALL:getByAddress → CALL:getAddress → CALL:getAddress → NEW: InetSocketAddress → RETURN → EXIT → CALL:Configuration:getBoolean → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:refreshServiceAcls → CALL:Server:start → CALL:NetUtils:createSocketAddrForHost → LOG:Handling deprecation for (String)item → TRY → CALL:Configuration:getBoolean → CALL:WebApps$Builder:start → LOG:Web app + name + started at + httpServer.getConnectorAddress(0).getPort() → CATCH → LOG:Webapps failed to start. Ignoring for now: [dynamic exception] → CALL:MRClientService:super.serviceStart → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Creating a HadoopYarnProtoRpc server for protocol {protocol} with {numHandlers} handlers</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Web app + name + started at + httpServer.getConnectorAddress(0).getPort()</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[INFO] Instantiated MRClientService at [dynamic address]</log> <log>[ERROR] Webapps failed to start. Ignoring for now: [dynamic exception]</log> <log>[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms.</log>"
  },
  "caee4a0d_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.conf.Configuration:get→IF_TRUE→NEW:ApplicationHistoryManagerOnTimelineStore→RETURN→EXIT",
    "log": "<!-- No logs in this path -->"
  },
  "caee4a0d_2": {
    "exec_flow": "ENTRY→LOG:org.slf4j.Logger:warn→NEW:ApplicationHistoryManagerImpl→CALL:org.apache.hadoop.conf.Configuration:get→IF_FALSE→RETURN→EXIT",
    "log": "[WARN] The filesystem based application history store is deprecated. [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "bf133243_1": {
    "exec_flow": "ENTRY→CALL:getJobConf→IF_TRUE: jobConf == null→CALL:sanitizeValue → IF_TRUE: oldVal == -1→LOG:warn→RETURN→EXIT ENTRY→CALL:getNumberMaps→CALL:sanitizeValue→IF_TRUE: oldVal == -1→LOG:warn→RETURN→CALL:getJobID→RETURN→EXIT ENTRY→CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL:warn→LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY→CALL:get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL:warn→LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null→CALL:warn→LOG: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null→CALL:warn→LOG: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→EXIT",
    "log": "[WARN] NumberReduces not defined for jobID [WARN] name + not defined for + id [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "bf133243_2": {
    "exec_flow": "ENTRY→CALL:setNumMapTasks→CALL:setInt→EXIT",
    "log": "<!-- No logs to merge, maintain original identity -->"
  },
  "0ddaeb80_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: Save namespace ...→CALL: attemptRestoreRemovedStorage→IF_TRUE: !restoreFailedStorage || removedStorageDirs.size() == 0→RETURN→EXIT→IF_FALSE: !restoreFailedStorage || removedStorageDirs.size() == 0→SYNC: this.restorationLock→CALL: org.slf4j.Logger:info→CALL: org.slf4j.Logger:info→EXIT→CALL: editLogWasOpen→CALL: editLog.endCurrentLogSegment→CALL: Preconditions.checkState→IF_TRUE: writeEndTxn→TRY→CALL: saveFSImageInAllDirs→IF_TRUE: canceler.isCancelled→CALL: deleteCancelledCheckpoint→EXIT→CALL: FSImageSaver:<init>→FOR_COND: it.hasNext()→CALL: Thread:start→FOR_EXIT→CALL: waitForThreads→CALL: saveThreads.clear→CALL: reportErrorsOnDirectories→IF_FALSE: storage.getNumStorageDirs→IF_FALSE: canceler.isCancelled()→CALL: renameCheckpoint→IF_FALSE: exitAfterSave.get()→CALL: purgeOldStorage→CALL: archivalManager.purgeCheckpoints→CALL: ctx.markComplete→CALL: endPhase→EXIT→LOG: LOG.INFO: Ending log segment + curSegmentTxId + , + getLastWrittenTxId()→LOG: LOG.INFO: logSyncAll toSyncToTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid→LOG: LOG.INFO: Done logSyncAll lastWrittenTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid→LOG: LOG.INFO: Number of transactions: + numTransactions + Total time for transactions(ms): + totalTimeTransactions + Number of transactions batched in Syncs: + numTransactionsBatchedInSync.longValue() + Number of syncs: + editLogStream.getNumSync() + SyncTimes(ms): + journalSet.getSyncTimes()→LOG: LOG.INFO: Stream closed→LOG: LOG.INFO: Log segment finalized",
    "log": "<log>[INFO] Save namespace ...</log> <log>[INFO] NNStorage.attemptRestoreRemovedStorage: check removed(failed) storage. removedStorages size = {}</log> <log>[INFO] currently disabled dir {}; type={} ;canwrite={}</log> <log>[INFO] restoring dir {}</log> <log>[ERROR] Checkpoint failed</log> <log>[WARN] writeTransactionIdToStorage failed on {}</log> <log>[INFO] Ending log segment</log> <log>[INFO] logSyncAll toSyncToTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid</log> <log>[INFO] Done logSyncAll lastWrittenTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid</log> <log>[INFO] Number of transactions: + numTransactions + Total time for transactions(ms): + totalTimeTransactions + Number of transactions batched in Syncs: + numTransactionsBatchedInSync.longValue() + Number of syncs: + editLogStream.getNumSync() + SyncTimes(ms): + journalSet.getSyncTimes()</log> <log>[INFO] Stream closed</log> <log>[INFO] Log segment finalized</log>"
  },
  "4a9bc4b5_1": {
    "exec_flow": "ENTRY→TRY→CALL:getActiveNamenodeRegistrations→FOREACH: states→CALL:applyAsLong→FOREACH_EXIT→RETURN→EXIT ENTRY→TRY→CALL:MembershipNamenodeResolver:getNamenodesForNameserviceId→IF_FALSE: ret != null→TRY→CALL:MembershipState.newInstance→CALL: setNameserviceId→EXCEPTION: setNameserviceId→CATCH: StateStoreUnavailableException e→CALL:Logger.error→RETURN→EXIT ENTRY→TRY→CALL:MembershipNamenodeResolver:getNamenodesForNameserviceId→IF_FALSE: ret != null→TRY→CALL:MembershipState.newInstance→CALL: setNameserviceId→CALL:GetNamenodeRegistrationsRequest.newInstance→CALL: getRecentRegistrationForQuery→IF_TRUE: result == null || result.isEmpty()→CALL:Logger.error→RETURN→EXIT ENTRY→TRY→CALL:MembershipNamenodeResolver:getNamenodesForNameserviceId→IF_FALSE: ret != null→TRY→CALL:MembershipState.newInstance→CALL: setNameserviceId→CALL:GetNamenodeRegistrationsRequest.newInstance→CALL: getRecentRegistrationForQuery→IF_FALSE: result == null || result.isEmpty()→TRY→IF_TRUE: disabled == null→CALL:Logger.error→CALL: unmodifiableList→CALL: cacheNS.put→RETURN→EXIT",
    "log": "[ERROR] \"Unable to extract metrics: {}\" [ERROR] Cannot get active NN for {}, State Store unavailable [ERROR] Cannot locate eligible NNs for {} [ERROR] Cannot get disabled name services"
  },
  "956f8546_1": {
    "exec_flow": "ENTRY→IF_FALSE: !timelineServiceV15Enabled→CALL: putEntities→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→IF_TRUE:resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()→LOG:LOG.ERROR:Failed to get the response from the timeline server.→LOG:LOG.DEBUG:HTTP error code: {resp.getStatus()} Server response : {resp.getEntity(String.class)}→THROW:new YarnException(\"API call failed\")→EXIT",
    "log": "[ERROR] Failed to get the response from the timeline server. [DEBUG] HTTP error code: {some_error_code} Server response : {some_response}"
  },
  "956f8546_2": {
    "exec_flow": "ENTRY→IF_FALSE: !timelineServiceV15Enabled→CALL: putEntities→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→NEW:PrivilegedExceptionAction<ClientResponse>→CALL:doPostingObject→IF_TRUE:resp == null || resp.getStatusInfo().getStatusCode() != ClientResponse.Status.OK.getStatusCode()→LOG:LOG.ERROR:Failed to get the response from the timeline server.→THROW:new YarnException(\"API call failed\")→EXIT",
    "log": "[ERROR] Failed to get the response from the timeline server."
  },
  "0372d053_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "0372d053_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "0372d053_3": {
    "exec_flow": "ENTRY → FOR_LOOP: newNames → GET: deprecatedKey → IF_TRUE: deprecatedKey != null && !getProps().containsKey(newName) → IF_TRUE: deprecatedValue != null → SET_PROPERTY: newName → EXIT",
    "log": "[INFO] message"
  },
  "7f590967_1": {
    "exec_flow": "ENTRY→IF_FALSE:serviceTimelinePublisher.isStopped()→FOREACH:record.tags()→FOREACH_EXIT→IF_TRUE:isServiceMetrics && appId != null→LOG:log.DEBUG:Publishing service metrics. {}, record→CALL:serviceTimelinePublisher.publishMetrics→FOREACH:metrics→CALL:setId→CALL:addValue→FOREACH_EXIT→CALL:setMetrics→CALL:putEntity→TRY→IF_TRUE:log.isDebugEnabled()→LOG:log.DEBUG:Publishing the entity + entity + , JSON-style content: + TimelineUtils.dumpTimelineRecordtoJSON(entity)→IF_TRUE:timelineClient != null→CALL:timelineClient.putEntitiesAsync→EXIT",
    "log": "[DEBUG] Publishing service metrics. {} [DEBUG] Publishing the entity + entity + , JSON-style content: + TimelineUtils.dumpTimelineRecordtoJSON(entity)"
  },
  "7f590967_2": {
    "exec_flow": "ENTRY → CALL: metricsPathPrefix.append(metricsPrefix).append(\".\").append(record.context()).append(\".\").append → FOREACH: record.tags() → FOREACH_EXIT → FOREACH: record.metrics() → FOREACH_EXIT → TRY → CALL: write → CATCH: Exception e → LOG: LOG.WARN: Error sending metrics to Graphite, e → TRY → CALL: close → CATCH: Exception e1 → THROW: new MetricsException(\"Error closing connection to Graphite\", e1) → EXIT",
    "log": "[WARN] Error sending metrics to Graphite"
  },
  "7f590967_3": {
    "exec_flow": "ENTRY → IF_TRUE: !isConnected() → IF_FALSE: isConnected() → IF_FALSE: tooManyConnectionFailures() → TRY → NEW: Socket → CATCH: Exception → INCREMENT: connectionFailures → IF_TRUE: tooManyConnectionFailures() → CALL: org.slf4j.Logger:error(java.lang.String) → THROW: new MetricsException → EXIT",
    "log": "[ERROR] Too many connection failures, would not try to connect again."
  },
  "7f590967_4": {
    "exec_flow": "<seq>ENTRY→FOREACH:record.tags()→FOREACH_EXIT→IF_TRUE:!skipHostname&&hn!=null→IF_TRUE:idx==-1→CALL:append→CALL:buf.append(sn).append(PERIOD).append(ctx).append(PERIOD).append(record.name().replaceAll(\"\\\\\\\\.\", \"-\")).append→FOREACH:record.metrics()→CALL:writeMetric→FOREACH_EXIT→EXIT</seq> <step>Check tag value and assign if exists</step> <step>Append hostname, service name, context, and record name to buffer</step> <step>Process metric types and values</step>",
    "log": "<log>[WARN] Error sending metrics to StatsD, e</log> <log>[DEBUG] Sending metric: {msg}</log>"
  },
  "21b90962_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry> <template>LOG.debug(\"Handling deprecation for all properties in config...\")</template> <level>DEBUG</level> </log_entry> <log_entry> <template>LOG.debug(\"Handling deprecation for (String)item\")</template> <level>DEBUG</level> </log_entry>"
  },
  "21b90962_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry> <template>LOG.debug(\"Handling deprecation for all properties in config...\")</template> <level>DEBUG</level> </log_entry> <log_entry> <template>LOG.debug(\"Handling deprecation for (String)item\")</template> <level>DEBUG</level> </log_entry> <log_entry> <template>LOG.info(\"message\")</template> <level>INFO</level> </log_entry>"
  },
  "21b90962_3": {
    "exec_flow": "ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "21b90962_4": {
    "exec_flow": "ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "21b90962_5": {
    "exec_flow": "ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "64de6051_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → [RETURN_FROM_CALL] → Parent.EXIT → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG:[INFO] message → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[INFO] message</log>"
  },
  "64de6051_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "64de6051_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child nodes -->"
  },
  "64de6051_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child nodes -->"
  },
  "6b39c574_1": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Active scan starting→TRY→CALL: scanActiveLogs→LOG: [DEBUG] Scanned {} active applications→IF_TRUE: (iter.hasNext() == true) AND (appId != null OR stat.isDirectory() == true)→LOG: [DEBUG] scan logs for {} in {}→CATCH: Exception e→IF_TRUE: t instanceof InterruptedException→LOG: [INFO] File scanner interrupted→LOG: [DEBUG] Active scan complete→EXIT",
    "log": "<log>[DEBUG] Active scan starting</log> <log>[DEBUG] Scanned {} active applications</log> <log>[DEBUG] scan logs for {} in {}</log> <log>[INFO] File scanner interrupted</log> <log>[DEBUG] Active scan complete</log>"
  },
  "6b39c574_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY→LOG: [DEBUG] Creating directory: {}, f.toString()→IF_FALSE: containsColon(f)→IF_FALSE: absolutePath.equals(ancestor)→CALL: performAuthCheck→IF_FALSE: noUmask→CALL: createPermissionStatus→CALL: applyUMask→CALL: createPermissionStatus→CALL: applyUMask→FOR_INIT→FOR_COND: parent != null→IF_TRUE: currentMetadata != null && !currentMetadata.isDirectory()→THROW: new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\")→EXIT",
    "log": "<log>[DEBUG] Creating directory: {}</log>"
  },
  "6b39c574_3": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→SYNC:this→CALL:toArray→CALL:size→FOREACH:callbacks→CALL:stateChanged→FOREACH_EXIT→EXIT→CALL:globalListeners.notifyListeners→CATCH:Throwable e→LOG:LOG.WARN:Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "6b39c574_4": {
    "exec_flow": "ENTRY→IF:isInState(STATE.STARTED)→RETURN→SYNC:stateChangeLock→TRY→CALL:stateModel.enterState(STATE.STARTED)→CALL:System.currentTimeMillis→CALL:serviceStart→IF:isInState(STATE.STARTED)→LOG:LOG.DEBUG:Service {} is started, getName()→CALL:notifyListeners→CATCH:Exception e→CALL:noteFailure→CALL:ServiceOperations.stopQuietly→THROW:ServiceStateException.convert→EXIT",
    "log": "[DEBUG] Service {} is started"
  },
  "44f6cea8_1": {
    "exec_flow": "<step> <method>org.apache.hadoop.yarn.webapp.Dispatcher$1:run()</method> <condition>Evaluated conditions specific to this path</condition> </step> <step> <method>org.slf4j.Logger:info(java.lang.String,java.lang.Object)</method> <condition>Processed specific conditions and ensured log sequence compliance</condition> </step>",
    "log": "<log>Class::Method [LEVEL] TemplatePlaceholder1</log> <log>Class::Method [LEVEL] TemplatePlaceholder2</log>"
  },
  "e3cdcaf3_1": {
    "exec_flow": "ENTRY→IF_FALSE:!createParent→CALL:getFileStatus→IF_TRUE:stat==null→THROW:new FileNotFoundException(\"Missing parent:\" + f)→EXIT →IF_FALSE:stat==null→IF_TRUE:!stat.isDirectory()→THROW:new ParentNotDirectoryException(\"parent is not a dir\")→EXIT →IF_FALSE:!stat.isDirectory()→CALL:mkdirs→IF_TRUE:!this.mkdirs(f, absolutePermission)→THROW:new IOException(\"mkdir of \" + f + \" failed\")→EXIT →IF_FALSE:!this.mkdirs(f, absolutePermission)→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] List COS key: [{}] to check the existence of the path.</log_entry> <log_entry>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log_entry>"
  },
  "e3cdcaf3_2": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→CATCH:FileNotFoundException→CALL:validatePath→CALL:mkDirRecursively→ENTRY→DO_WHILE→CALL:add→CALL:getParent→DO_COND:absolutePath != null→DO_EXIT→FOREACH:paths→IF_FALSE:path.equals(new Path(CosNFileSystem.PATH_DELIMITER))→TRY→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus→IF_FALSE:fileStatus.isFile()→IF_FALSE:fileStatus.isDirectory()→CATCH:FileNotFoundException→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:storeEmptyFile→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Making dir: [{}] in COS</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] List COS key: [{}] to check the existence of the path.</log_entry> <log_entry>[DEBUG] The Path: [{}] does not exist.</log_entry>"
  },
  "df267272_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.conf.Configuration:getBoolean → IF_FALSE: enabled → LOG:[INFO] CORS filter not enabled. Please set key to 'true' to enable it → EXIT",
    "log": "[INFO] CORS filter not enabled. Please set key to 'true' to enable it"
  },
  "df267272_2": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String) item WARN: Unexpected SecurityException in Configuration"
  },
  "df267272_3": {
    "exec_flow": "ENTRY → CALL: addGlobalFilter → CALL: getFilterHolder → CALL: getFilterMapping → CALL: defineFilter → FOREACH: defaultContexts.keySet() → CALL: defineFilter → FOREACH_EXIT → CALL: org.slf4j.Logger:info → EXIT",
    "log": "[INFO] Added global filter ' + name + ' (class= + classname + )"
  },
  "df267272_4": {
    "exec_flow": "ENTRY → CALL:setAuthFilterConfig → IF_TRUE:authType.equals(PseudoAuthenticationHandler.TYPE) → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → CALL: addFilter → LOG: [INFO] Added filter (name) (class=classname) to context webAppContext.getDisplayName() → LOG: [INFO] Added filter (name) (class=classname) to context ctx.getDisplayName() → LOG: [INFO] Added global filter ' + name + ' (class= + classname + ) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String) item WARN: Unexpected SecurityException in Configuration [INFO] Added filter (name) (class=classname) to context webAppContext.getDisplayName() [INFO] Added filter (name) (class=classname) to context ctx.getDisplayName() [INFO] Added global filter ' + name + ' (class= + classname + )"
  },
  "8f5b383f_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL:readFields → NEW: HashSet<String> → FOREACH: userGroupStrings → IF_TRUE: !allAllowed → LOG:LOG.WARN:No access for all, e → CALL:org.slf4j.Logger:warn → EXIT",
    "log": "<log_entry>[WARN] No access for all</log_entry>"
  },
  "8f5b383f_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL:readFields → NEW: HashSet<String> → NEW: HashSet<String> → FOREACH: userGroupStrings → FOREACH_EXIT → IF_TRUE: !allAllowed → IF_TRUE: userGroupStrings.length >= 1 && userGroupStrings[0] != null → CALL: getTrimmedStringCollection → IF_TRUE: userGroupStrings.length == 2 && userGroupStrings[1] != null → CALL: getTrimmedStringCollection → CALL: groupsMapping.cacheGroupsAdd → TRY → CALL:impl.cacheGroupsAdd → EXCEPTION:IOException → CATCH:IOException → LOG:LOG.WARN:Error caching groups, e → CALL:org.slf4j.Logger:warn → EXIT",
    "log": "<log_entry>[WARN] Error caching groups</log_entry>"
  },
  "8f5b383f_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → IF_TRUE: conf != null → IF_TRUE: theObject instanceof Configurable → CALL: ((Configurable) theObject).setConf → ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource → CALL: setJobConf → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>FOREACH: [DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "0c862463_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL: getInternal → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → ENTRY → TRY → CALL: org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration) → CATCH:IOException|RuntimeException → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean) → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "[DEBUG] Duplicate FS created for {}; discarding {} [WARN] Failed to initialize filesystem {}: {} [DEBUG] Failed to initialize filesystem [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "0c862463_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL: getInternal → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC: this → CALL: get → IF_TRUE: fs != null → LOG: LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "0c862463_3": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "0c862463_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "0c862463_5": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG:looking for configuration option {}, property → CALL: getClass → IF_FALSE: clazz != null → LOG: LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE: clazz == null → LOG: LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → TRY → CALL: initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG: LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG:Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT → THROW: e → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem, e"
  },
  "3142fc5f_1": {
    "exec_flow": "ENTRY→IF_TRUE:rmApp != null→IF_TRUE:rmAppAttempt != null→TRY→CALL:rmAppAttempt.handle→CATCH:Throwable t→LOG:LOG.ERROR:Error in handling event type for applicationAttempt→EXIT",
    "log": "[ERROR] Error in handling event type for applicationAttempt"
  },
  "3142fc5f_2": {
    "exec_flow": "ENTRY→IF_TRUE:rmApp != null→IF_FALSE:rmAppAttempt != null→IF_TRUE:rmApp.getApplicationSubmissionContext() != null AND rmApp.getApplicationSubmissionContext().getKeepContainersAcrossApplicationAttempts() AND event.getType() == RMAppAttemptEventType.CONTAINER_FINISHED→IF_TRUE:previousFailedAttempt != null→TRY→LOG:LOG.DEBUG:Event handled by→CALL:previousFailedAttempt.handle→EXIT",
    "log": "[DEBUG] Event handled by"
  },
  "3142fc5f_3": {
    "exec_flow": "ENTRY→IF_TRUE:rmApp != null→IF_FALSE:rmAppAttempt != null→IF_TRUE:rmApp.getApplicationSubmissionContext() != null AND rmApp.getApplicationSubmissionContext().getKeepContainersAcrossApplicationAttempts() AND event.getType() == RMAppAttemptEventType.CONTAINER_FINISHED→IF_TRUE:previousFailedAttempt != null→TRY→LOG:LOG.DEBUG:Event handled by→CALL:previousFailedAttempt.handle→EXCEPTION:handle→CATCH:Throwable t→LOG:LOG.ERROR:Error in handling event type for applicationAttempt with previousFailedAttempt→EXIT",
    "log": "[DEBUG] Event handled by [ERROR] Error in handling event type for applicationAttempt with previousFailedAttempt"
  },
  "3142fc5f_4": {
    "exec_flow": "ENTRY→TRY→CALL:serviceManager.handle→ENTRY→IF_FALSE:blockNewEvents→IF_TRUE:qSize!=0 && qSize % 1000==0 && lastEventQueueSizeLogged!=qSize→LOG:INFO:Size of event-queue is + qSize→IF_TRUE:qSize!=0 && qSize % detailsInterval==0 && lastEventDetailsQueueSizeLogged!=qSize→CALL:printEventQueueDetails→WHILE:iterator.hasNext()→CALL:iterator.next→CALL:iterator.next.getType→CALL:counterMap.containsKey→CALL:counterMap.put→CALL:counterMap.get→WHILE_EXIT→CALL:counterMap.put→FOREACH:counterMap.entrySet()→CALL:Map.Entry.getValue→CALL:Map.Entry.getKey→CALL:org.slf4j.Logger.info→FOREACH_EXIT→CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→IF_TRUE:oldState!=getState()→LOG:LOG.INFO:[SERVICE] Transitioned from {} to {} on {} event., oldState, getState(), event.getType()→CALL:writeLock.unlock→EXIT→EXIT→IF_TRUE:remCapacity<1000→LOG:WARN:Very low remaining capacity in event-queue:+remCapacity→TRY→CALL:eventQueue.put→EXIT",
    "log": "[INFO] [SERVICE] Transitioned from {} to {} on {} event. [INFO] Size of event-queue is + qSize [INFO] Event type: {entry.getKey()}, Event record counter: {num} [WARN] Very low remaining capacity in event-queue: + remCapacity"
  },
  "1c21662e_1": {
    "exec_flow": "ENTRY→CALL:FederationApplicationHomeSubClusterStoreInputValidator.validate→TRY→CALL:getConnection→CALL:prepareCall→CALL:setString→CALL:setString→CALL:registerOutParameter→CALL:registerOutParameter→CALL:getTime→CALL:executeUpdate→CALL:getTime→CALL:getString→CALL:SubClusterId.newInstance→CALL:FederationStateStoreUtils.logAndThrowStoreException→CALL:FederationStateStoreUtils.returnToPool→CALL:SubClusterId.newInstance→RETURN→EXIT IF_TRUE:request==null → LOG:org.slf4j.Logger:warn → THROW:new FederationStateStoreInvalidInputException IF_FALSE:request==null → CALL:checkApplicationId → IF_FALSE:appId==null",
    "log": "[ERROR] The application {} does exist but was overwritten [INFO] Insert into the StateStore the application: {} in SubCluster: {} [WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information. [WARN] Invalid SubCluster Id information. Please try again by specifying valid Subcluster Id. [WARN] Missing Application Id. Please try again by specifying an Application Id."
  },
  "1c21662e_2": {
    "exec_flow": "ENTRY→IF_TRUE: methodMetric == null→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Object)→LOG: LOG.ERROR: UNKOWN_FAIL_ERROR_MSG, methodName→RETURN→EXIT",
    "log": "[ERROR] UNKOWN_FAIL_ERROR_MSG, methodName"
  },
  "1c21662e_3": {
    "exec_flow": "ENTRY→IF_TRUE: methodMetric == null || methodQuantileMetric == null →CALL: org.slf4j.Logger:error →RETURN →EXIT",
    "log": "[ERROR] UNKNOWN_SUCCESS_ERROR_MSG, methodName"
  },
  "ee7528bc_1": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null,newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_EXIT → RETURN → EXIT → EXIT → LOG: [WARN] Unexpected SecurityException in Configuration → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → LOG: [DEBUG] Handling deprecation for all properties in config... → EXIT → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → LOG: [DEBUG] Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: [DEBUG] Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → EXIT IF_FALSE: !isInitialized() → VIRTUAL_CALL ENTRY → IF_TRUE: types != null → CALL: getDeclaredMethod → CALL: org.slf4j.Logger:error(java.lang.String, java.lang.Object[]) → THROW: IOException → EXIT → IF_FALSE: namenodes == null || namenodes.isEmpty() → CALL: addClientInfoToCallerContext → IF_FALSE: rpcMonitor != null → FOREACH: namenodes → TRY → CALL: getConnection → LOG: [DEBUG] User {} NN {} is using connection {},ugi.getUserName(),rpcAddress,connection → CALL: invoke → IF_FALSE: failover → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[ERROR] Cannot get method {} with types {} from {}</log_entry> <log_entry>[DEBUG] User {} NN {} is using connection {}</log_entry>"
  },
  "7cfda2e9_1": {
    "exec_flow": "ENTRY→TRY→CALL:doFence→EXCEPTION:doFence→CATCH:Throwable→CALL:recordActiveAttempt→THROW:t→EXIT",
    "log": "[INFO] Unable to fence old active: <exception_message>"
  },
  "14c495c9_1": {
    "exec_flow": "ENTRY → CALL: checkFormatted → CALL: checkRequest → IF_TRUE: startTxId==curSegmentTxId → IF_FALSE: curSegment!=null → CALL: checkSync → IF_FALSE: elf==null → CALL: getLogFile → IF_TRUE: elf.isInProgress() → IF_TRUE: needsValidation → LOG: LOG.INFO:\"Validating log segment {elf.getFile()} about to be finalized ; journal id: {journalId}\" → CALL: scanLog → CALL: checkSync → CALL: finalizeLogSegment → ENTRY → LOG: LOG.INFO:\"Finalizing edits file {inprogressFile} -> {dstFile}\" → CALL: Preconditions.checkState → TRY → CALL: NativeIO.renameTo → IF_TRUE: inprogressFile.equals(currentInProgress) → EXIT",
    "log": "[INFO] \"Validating log segment {elf.getFile()} about to be finalized ; journal id: {journalId}\" [INFO] \"Finalizing edits file {inprogressFile} -> {dstFile}\""
  },
  "14c495c9_2": {
    "exec_flow": "ENTRY → IF_TRUE: isOutOfSync() → CALL: heartbeatIfNecessary → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → EXIT",
    "log": "[DEBUG] Handling deprecation for (String)item"
  },
  "baa81574_1": {
    "exec_flow": "<operation> <name>resolvePath</name> <params>pc, link, DirOp.WRITE_LINK</params> </operation> <operation> <name>verifyParentDir</name> <params>iip</params> </operation> <operation> <name>isValidToCreate</name> <params>link, iip</params> </operation> <operation> <name>checkAncestorAccess</name> <params>pc, iip, FsAction.WRITE</params> </operation> <operation> <name>checkFsObjectLimit</name> </operation> <operation> <name>addSymlink</name> <params>fsd, link, iip, target, dirPerms, createParent, logRetryCache</params> </operation> <operation> <name>createAncestorDirectories</name> <params>fsd, iip, dirPerms</params> </operation> <operation> <name>unprotectedAddSymlink</name> <params>fsd, parent, iip.getLastLocalName(), id, target, mtime, mtime, perm</params> </operation> <operation> <name>fsd.getEditLog().logSymlink</name> <params>path, target, mtime, mtime, newNode, logRetryCache</params> </operation>",
    "log": "[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)] [DEBUG] DIR* NameSystem.createSymlink: target= + target + link= + link [DEBUG] addSymlink: [path] is added [DEBUG] doEditTx() op={} txid={}"
  },
  "baa81574_2": {
    "exec_flow": "ENTRY→NEW:FSPermissionChecker→CALL:getUserFilteredAttributeProvider→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [INFO] Cleaning up resources [DEBUG] Failure to load login credentials [INFO] message"
  },
  "b17966dd_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "b17966dd_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "b17966dd_3": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource ]]>",
    "log": "<!-- Integrated log sequence adhering to parent's log sequence if any -->"
  },
  "b17966dd_4": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource ]]>",
    "log": "<!-- Log sequence accounting for possible logs under 'loadProps' -->"
  },
  "b4aec734_1": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE:excludeDatanodes.getValue()!=null</step> <step>CALL:System.arraycopy</step> <step>CALL:toUrl</step> <step>ENTRY</step> <step>CALL:makeQualified</step> <step>CALL:getAuthParameters</step> <step>CALL:getDelegationToken</step> <step>IF_TRUE: token != null</step> <step>CALL:authParams.add</step> <step>CALL:encodeToUrlString</step> <step>CALL:toArray</step> <step>RETURN</step> <step>CALL:getNamenodeURL</step> <step>LOG:[TRACE] url={}</step> <step>RETURN</step> <step>EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[TRACE] url={}</log> <log>[DEBUG] Delegation token encoded</log> <log>[INFO] Returning authentication parameters</log>"
  },
  "b4aec734_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE:excludeDatanodes.getValue()!=null</step> <step>CALL:toUrl</step> <step>ENTRY</step> <step>CALL:makeQualified</step> <step>CALL:getAuthParameters</step> <step>CALL:getDelegationToken</step> <step>IF_FALSE: token != null</step> <step>IF_TRUE: realUgi != null</step> <step>CALL:authParams.add</step> <step>CALL:toArray</step> <step>RETURN</step> <step>CALL:getNamenodeURL</step> <step>LOG:[TRACE] url={}</step> <step>RETURN</step> <step>EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[TRACE] url={}</log> <log>[DEBUG] Auth parameters added for proxy user</log> <log>[INFO] Secure cluster detected</log>"
  },
  "8450f977_1": {
    "exec_flow": "ENTRY → CALL:cacheManager.modifyCachePool → CALL:fsn.getEditLog().logModifyCachePool → ENTRY → LOG:logRpcIds → CALL:ModifyCachePoolOp.getInstance → CALL:logEdit → LOG: LOG.DEBUG: doEditTx() op={} txid={} → TRY → CALL: editLogStream.write → CALL: reset → CALL: endTransaction → CALL: shouldForceSync → RETURN → EXIT → EXIT",
    "log": "[INFO] logRpcIds invoked [DEBUG] doEditTx() op={} txid={}"
  },
  "8450f977_2": {
    "exec_flow": "ENTRY → CALL:cacheManager.modifyCachePool → CALL:fsn.getEditLog().logModifyCachePool → ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}"
  },
  "8450f977_3": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→EXCEPTION:validate→CATCH:IOException e→CALL:LOG.INFO→THROW:e→EXIT",
    "log": "[DEBUG] Exception occurred while modifying cache pool"
  },
  "8450f977_4": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_TRUE:info.getOwnerName()!=null→CALL:setOwnerName→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set owner to {info.getOwnerName()}"
  },
  "8450f977_5": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_TRUE:info.getGroupName()!=null→CALL:setGroupName→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set group to {info.getGroupName()}"
  },
  "8450f977_6": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_FALSE:info.getGroupName()!=null→IF_TRUE:info.getMode()!=null→CALL:setMode→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set mode to {info.getMode()}"
  },
  "8450f977_7": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_FALSE:info.getGroupName()!=null→IF_FALSE:info.getMode()!=null→IF_TRUE:info.getLimit()!=null→CALL:setLimit→CALL:append→CALL:setNeedsRescan→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set limit to {info.getLimit()}"
  },
  "8450f977_8": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_FALSE:info.getGroupName()!=null→IF_FALSE:info.getMode()!=null→IF_FALSE:info.getLimit()!=null→IF_TRUE:info.getDefaultReplication()!=null→CALL:setDefaultReplication→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set default replication to {info.getDefaultReplication()}"
  },
  "8450f977_9": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_FALSE:info.getGroupName()!=null→IF_FALSE:info.getMode()!=null→IF_FALSE:info.getLimit()!=null→IF_FALSE:info.getDefaultReplication()!=null→IF_TRUE:info.getMaxRelativeExpiryMs()!=null→CALL:setMaxRelativeExpiryMs→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; set maxRelativeExpiry to {info.getMaxRelativeExpiryMs()}"
  },
  "8450f977_10": {
    "exec_flow": "ENTRY→TRY→CALL:CachePoolInfo.validate→IF_FALSE:pool==null→IF_FALSE:info.getOwnerName()!=null→IF_FALSE:info.getGroupName()!=null→IF_FALSE:info.getMode()!=null→IF_FALSE:info.getLimit()!=null→IF_FALSE:info.getDefaultReplication()!=null→IF_FALSE:info.getMaxRelativeExpiryMs()!=null→IF_TRUE:prefix.isEmpty()→CALL:append→LOG:LOG.INFO",
    "log": "[INFO] modifyCachePool of {info.getPoolName()} successful; no changes."
  },
  "816ff054_1": {
    "exec_flow": "ENTRY → IF_TRUE:!createParent → CALL:getFileStatus → IF_TRUE:stat == null → THROW:new FileNotFoundException(\"Missing parent:\" + f) → EXIT",
    "log": "<log> <template>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log>"
  },
  "816ff054_2": {
    "exec_flow": "ENTRY → IF_TRUE:!createParent → CALL:getFileStatus → IF_FALSE:stat == null → IF_FALSE:!stat.isDirectory() → CALL:mkdirs → IF_FALSE:!this.mkdirs(f, absolutePermission) → EXIT",
    "log": "<log> <template>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log>"
  },
  "816ff054_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating directory: {}, f.toString() → IF_FALSE: containsColon(f) → IF_FALSE: absolutePath.equals(ancestor) → CALL: performAuthCheck → IF_FALSE: noUmask → CALL: createPermissionStatus → CALL: applyUMask → CALL: applyUMask → FOR_INIT → FOR_COND: parent != null → IF_TRUE: currentMetadata != null && !currentMetadata.isDirectory() → THROW: new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\") → EXIT",
    "log": "<log>[DEBUG] Creating directory: {}</log>"
  },
  "6bd19e5e_1": {
    "exec_flow": "ENTRY→CALL:writeLock→TRY→IF_TRUE:bpServiceToActive==actor→CALL:bpServices.remove→IF_TRUE:bpServices.isEmpty()→CALL:shutdownBlockPool→CALL:org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:shutdownBlockPool→RETURN→CALL:writeUnlock→EXIT",
    "log": "<!-- No log entries available for this path -->"
  },
  "6bd19e5e_2": {
    "exec_flow": "<sequence> ENTRY → LOG: LOG.INFO: this + starting to offer service → TRY → WHILE: true → BREAK → RUNNING_STATE: RUNNING → CALL: IBRTaskHandler<init> → CALL: ibrExecutorService.submit; WHEN_SUBMIT=EXCEPTION: submit → CATCH: Throwable ex → LOG: LOG.WARN: Unexpected exception in block pool this, ex → LOG: LOG.WARN: Ending block pool service for: this → CALL: cleanUp → ENTRY → CALL:IOUtils.cleanupWithLogger → CALL:IOUtils.cleanupWithLogger → CALL:shutdownActor → IF_TRUE:!ibrExecutorService.isShutdown() → CALL:ibrExecutorService.shutdownNow → FOREACH: bpServices → IF_TRUE: actor.getNNSocketAddress().equals(removedNN) → CALL: stop → CALL: shutdownActor → BREAK → EXIT </sequence>",
    "log": "[ERROR]BPServiceActor$LifelineSender: Uncaught Exception in thread [INFO] this starting to offer service [WARN] Unexpected exception in block pool this [WARN] Ending block pool service for: this"
  },
  "6bd19e5e_3": {
    "exec_flow": "ENTRY→CALL:offerServices.remove→CALL:BPOfferService.hasBlockPoolId→IF_TRUE→CALL:BPOfferService.getBlockPoolId→CALL:bpByBlockPoolId.remove→FOR_INIT→FOR_COND:it.hasNext() && !removed→CALL:Logger.info→EXIT",
    "log": "[INFO] Removed BPOfferService"
  },
  "6bd19e5e_4": {
    "exec_flow": "ENTRY→CALL:offerServices.remove→CALL:BPOfferService.hasBlockPoolId→FOR_INIT→FOR_COND:it.hasNext() && !removed→CALL:Logger.warn→EXIT",
    "log": "[WARN] Couldn't remove BPOS BPOfferService from bpByNameserviceId map"
  },
  "da18b5a3_1": {
    "exec_flow": "ENTRY → VIRTUAL_CALL:org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:getLogAggPolicyParameters(org.apache.hadoop.conf.Configuration) → VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String) → CALL:handleDeprecation → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration [INFO] message"
  },
  "991f058d_1": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Proxying operation: {}"
  },
  "991f058d_2": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Proxying operation: {}"
  },
  "991f058d_3": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Proxying operation: {}"
  },
  "991f058d_4": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Proxying operation: {}"
  },
  "ba57da39_1": {
    "exec_flow": "ENTRY➝LOG:debug➝IF_FALSE:src.isRoot()➝IF_FALSE:src.equals(dst)➝FOR➝IF_FALSE:commonParent➝TRY➝CALL:getFileStatus➝IF_FALSE:dstFileStatus.isFile()➝CALL:listStatus➝IF_FALSE:fileExists➝IF_FALSE:srcFileStatus.isDirectory()➝CALL:copyFile➝RETURN➝EXIT",
    "log": "<log>[DEBUG] Rename source path: [{}] to dest path: [{}]</log> <log>[DEBUG] Cannot rename the root directory of a filesystem.</log> <log>[DEBUG] Source path and dest path refer to the same file or directory: [{}].</log> <log>[DEBUG] Recursively find the common parent directory of the source and destination paths. The currently found parent path: {}</log> <log>[DEBUG] It is not allowed to rename a parent directory:[{}] to its subdirectory:[{}].</log> <log>[DEBUG] Cannot rename source file: [{}] to dest file: [{}], because the file already exists.</log>"
  },
  "ba57da39_2": {
    "exec_flow": "ENTRY→LOG:Moving {} to {}, src, dst→IF_FALSE:containsColon(dst)→IF_FALSE:srcParentFolder==null→IF_FALSE:srcKey.length()==0→CALL:performAuthCheck→IF_FALSE:this.azureAuthorization→TRY→CALL:retrieveMetadata→IF_FALSE:dstMetadata!=null && dstMetadata.isDirectory()→IF_TRUE:dstMetadata!=null→LOG:Destination {} is already existing file, failing the rename.→RETURN→CALL:updateParentFolderLastModifiedTime→EXIT→TRY→CALL:qualify→CALL:qualify→LOG:Rename path {} to {}, src, dst→CALL:initiateRename→CALL:RenameOperation<init>→CALL:trackDurationAndSpan→EXCEPTION:RenameFailedException→CATCH:RenameFailedException e→CALL:getExitCode→RETURN→EXIT",
    "log": "<log_entry> <log_type>DEBUG</log_type> <description>Moving {} to {}, src, dst</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Destination {} is already existing file, failing the rename.</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Rename path {} to {}</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>rename failure</description> </log_entry>"
  },
  "ba57da39_3": {
    "exec_flow": "ENTRY→CALL:retrieveMetadata→IF_TRUE:srcMetadata==null→LOG:Source {} doesn't exist. Failing rename.→THROW:FileNotFoundException→EXIT",
    "log": "<log_entry> <log_type>DEBUG</log_type> <description>Source {} doesn't exist. Failing rename.</description> </log_entry>"
  },
  "ba57da39_4": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "ba57da39_5": {
    "exec_flow": "ENTRY → IF_TRUE: client != null → IF_FALSE: !client.isConnected() → CALL: logout → CALL: disconnect → IF_TRUE: !logoutSuccess → LOG: LOG.WARN: Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code -</log>"
  },
  "ba57da39_6": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.conf.Configuration:get → IF_FALSE: mode==null → CALL: org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode → IF_FALSE: upper.equals(\"STREAM_TRANSFER_MODE\") → IF_FALSE: upper.equals(\"COMPRESSED_TRANSFER_MODE\") → IF_TRUE: !upper.equals(\"BLOCK_TRANSFER_MODE\") → CALL: org.slf4j.Logger:warn → RETURN → EXIT",
    "log": "<log>[WARN] Cannot parse the value for FS_FTP_TRANSFER_MODE: mode. Using default.</log>"
  },
  "1a70af0e_1": {
    "exec_flow": "<step>Entry: ViewFileSystem:open(Path, int)</step> <step>Call: getUriPath(Path)</step> <step>Process: checkPath(Path)</step> <step>Return: makeAbsolute(p).toUri().getPath()</step> <step>Process: Resolved target file system</step> <step>Call: res.targetFileSystem.open(res.remainingPath, bufferSize)</step> <step>Entry: NativeAzureFileSystem:open(Path, int)</step> <step>LOG: Opening file: {f.toString()}</step> <step>Call: performAuthCheck</step> <step>Try: Call: retrieveMetadata</step> <step>If False: retrieveMetadata successful and meta != null</step> <step>If False: meta.isDirectory()</step> <step>Try: Call: retrieve</step> <step>New: FSDataInputStream</step> <step>New: BufferedFSInputStream</step> <step>New: NativeAzureFsInputStream</step> <step>Call: getLen</step> <step>Return</step> <step>Exit</step>",
    "log": "<log_entry> <log_type>INFO</log_type> <description>Start processing URI path</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Path checked for validity</description> </log_entry> <log_entry> <log_type>INFO</log_type> <description>Completed URI path transformation</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Opening file: {f.toString()}</description> </log_entry>"
  },
  "1a70af0e_2": {
    "exec_flow": "<step>Entry</step> <step>Call: getFileStatus</step> <step>If False: fs.isDirectory()</step> <step>Call: NativeFileSystemStore:getFileLength(key)</step> <step>LOG: Open the file: [{f}] for reading.</step> <step>New: FSDataInputStream</step> <step>Call: getConf</step> <step>New: BufferedFSInputStream</step> <step>New: CosNInputStream</step> <step>Return</step> <step>Exit</step>",
    "log": "<log_entry> <log_type>INFO</log_type> <description>Open the file: [{f}] for reading.</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Path: [{}] is a dir. COS key: [{}]</description> </log_entry>"
  },
  "1a70af0e_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: key.length()==0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE: meta!=null</step> <step>LOG: [DEBUG] List COS key: [{}] to check the existence of the path.</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list</step> <step>IF_TRUE: listing.getFiles().length>0||listing.getCommonPrefixes().length>0</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>LOG: [DEBUG] Path: [{}] is a directory. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log_entry> <log_type>DEBUG</log_type> <description>List COS key: [{}] to check the existence of the path.</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Path: [{}] is a directory. COS key: [{}]</description> </log_entry>"
  },
  "f03df521_1": {
    "exec_flow": "<![CDATA[ ENTRY→CALL: FSImageSerialization:writeString→CALL: System.arraycopy→CALL: org.slf4j.Logger:warn→CALL:set→CALL:write→EXIT ]]>",
    "log": "<![CDATA[ WARN: org.slf4j.Logger:warn ]]>"
  },
  "9f1c593f_1": {
    "exec_flow": "<sequence> ENTRY → CALL:getHttpAddress → IF_TRUE: configName != null → IF_FALSE: target == null → CALL: trim → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL: createURI → IF_TRUE: port == -1 → IF_FALSE: (host == null) || (port < 0) || (!hasScheme && path != null && !path.isEmpty()) → CALL: createSocketAddrForHost → RETURN → EXIT → [VIRTUAL_CALL] → CALL:httpServerTemplateForNNAndJN → CALL:getBoolean → CALL:build → CALL:infoServer.setAttribute → CALL:infoServer.setAttribute → CALL:infoServer.setAttribute → CALL:addInternalServlet → CALL:infoServer.start → LOG:INFO Web server init done → CALL:getHttpPolicy → [VIRTUAL_CALL] → CALL:openListeners → CALL:webServer.start → IF_TRUE: prometheusSupport → CALL:DefaultMetricsSystem.instance().register → FOREACH: hs → IF_TRUE: handler.isFailed() → THROW:new IOException(\"Problem in starting http server. Server handlers failed\") → EXIT </sequence>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Web server init done</log>"
  },
  "9f1c593f_2": {
    "exec_flow": "ENTRY → CALL:getHttpPolicy → FOREACH:parts → CALL:trim → IF_TRUE → CONTINUE → FOREACH_EXIT → CALL:join → CALL:set → LOG:Filter initializers set : {initializers} → IF_TRUE:UserGroupInformation.isSecurityEnabled → LOG:Starting web server as: {serverPrincipal} → IF_TRUE:policy.isHttpEnabled → IF_FALSE:httpAddr.getPort() == 0 → CALL:addEndpoint → LOG:Starting Web-server for {name} at: {uri} → IF_FALSE:policy.isHttpsEnabled && httpsAddr != null → RETURN → EXIT → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT → LOG:Setting password to null since IOException is caught when getting password → CALL:org.apache.hadoop.conf.Configuration:getPassword → IF_TRUE: passchars != null → NEW: String → RETURN → EXIT",
    "log": "<log>[INFO] Filter initializers set : {initializers}</log> <log>[INFO] Starting web server as: {serverPrincipal}</log> <log>[INFO] Starting Web-server for {name} at: {uri}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Setting password to null since IOException is caught when getting password</log>"
  },
  "9f1c593f_3": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.conf.Configuration:get → RETURN → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "b0bf8971_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {scheme} → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {property} → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \" \"+\" \"\\\"\" + scheme + \"\\\" \") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {uri}: {e.toString()} → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Bypassing cache to create filesystem {uri}</message> </log> <log> <level>DEBUG</level> <message>Looking for FS supporting {scheme}</message> </log> <log> <level>DEBUG</level> <message>looking for configuration option {property}</message> </log> <log> <level>DEBUG</level> <message>Looking in service filesystems for implementation class</message> </log> <log> <level>WARN</level> <message>Failed to initialize filesystem {uri}: {e.toString()}</message> </log> <log> <level>DEBUG</level> <message>Failed to initialize filesystem</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "b0bf8971_2": {
    "exec_flow": "ENTRY → SYNC:this → CALL:get → IF_FALSE:fs != null → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → LOG:LOGGER.DEBUG(\"Filesystem {uri} created while awaiting semaphore\") → RETURN → EXIT → IF(fs != null) → SYNC → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → CALL:org.apache.hadoop.conf.Configuration:getBoolean → LOG:LOGGER.DEBUG(\"Duplicate FS created for {uri}; discarding {fs}\") → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Filesystem {uri} created while awaiting semaphore</message> </log> <log> <level>DEBUG</level> <message>Duplicate FS created for {uri}; discarding {fs}</message> </log>"
  },
  "b0bf8971_3": {
    "exec_flow": "ENTRY → LOG: \"local\" is a deprecated filesystem name. Use \"file:///\" instead. → CALL:fixName → [VIRTUAL_CALL] → (CALL:handleDeprecation → CALL:getProps → FOREACH:names → CALL:substituteVars → FOREACH_EXIT → RETURN) → EXIT",
    "log": "<log> <level>WARN</level> <message>\"local\" is a deprecated filesystem name. Use \"file:///\" instead.</message> </log> <log> <level>WARN</level> <message>\"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.</message> </log>"
  },
  "b0bf8971_4": {
    "exec_flow": "ENTRY → TRY → NEW: DataInputStream → NEW: BufferedInputStream → CALL: newInputStream → CALL: toPath → CALL: toPath → CALL: readTokenStorageStream → RETURN → CALL: IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → EXIT",
    "log": "<log> <level>INFO</level> <message>Cleaning up resources</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "3808b445_1": {
    "exec_flow": "ENTRY→TRY→CALL:run→EXCEPTION:ExitUtil.ExitException→LOG:LOG.DEBUG:Exception raised, e→CALL:exit→[VIRTUAL_CALL]→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[DEBUG] Exception raised [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "3808b445_2": {
    "exec_flow": "ENTRY→TRY→CALL:run→EXCEPTION:FileNotFoundException→CALL:errorln→LOG:LOG.DEBUG:Not found:, e→CALL:exit→[VIRTUAL_CALL]→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[DEBUG] Not found: [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "f85ec63a_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_FALSE: overlay != null → EXIT → FOREACH:names → RETURN → EXIT",
    "log": "[INFO] DefaultRuleAssigned [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [INFO] org.apache.hadoop.security.User:<init> - Initializing user with authentication method... [DEBUG] org.apache.hadoop.security.HadoopKerberosName:getShortName - Acquiring short name..."
  },
  "f85ec63a_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] DefaultRuleAssigned [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [INFO] org.apache.hadoop.security.User:<init> - Initializing user with authentication method... [DEBUG] org.apache.hadoop.security.HadoopKerberosName:getShortName - Acquiring short name..."
  },
  "f85ec63a_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_FALSE: backup != null → EXIT",
    "log": "[INFO] DefaultRuleAssigned [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [INFO] org.apache.hadoop.security.User:<init> - Initializing user with authentication method... [DEBUG] org.apache.hadoop.security.HadoopKerberosName:getShortName - Acquiring short name..."
  },
  "f85ec63a_4": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_FALSE: overlay != null → EXIT",
    "log": "[INFO] DefaultRuleAssigned [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating new Groups object [INFO] org.apache.hadoop.security.User:<init> - Initializing user with authentication method... [DEBUG] org.apache.hadoop.security.HadoopKerberosName:getShortName - Acquiring short name..."
  },
  "3f9eec60_1": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {} -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> GetNamespaceInfoRequest.newInstance() -> getMembershipStore().getNamespaceInfo(request) -> Filter disabled namespaces -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "3f9eec60_2": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {} -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> GetNamespaceInfoRequest.newInstance() -> getMembershipStore().getNamespaceInfo(request) -> Filter disabled namespaces -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "3f9eec60_3": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {} -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> GetNamespaceInfoRequest.newInstance() -> getMembershipStore().getNamespaceInfo(request) -> Filter disabled namespaces -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "3f9eec60_4": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {} -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> GetNamespaceInfoRequest.newInstance() -> getMembershipStore().getNamespaceInfo(request) -> Filter disabled namespaces -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "a87d1d79_1": {
    "exec_flow": "ENTRY→CALL:incrGetDomainOps→TRY→CALL:RollingLevelDBTimelineStore.getDomain→IF_TRUE:domain != null→CALL:timelineACLsManager.checkAccess→IF_TRUE:checkAccess==true→RETURN:domain→FINALLY→CALL:addGetDomainTime→EXIT",
    "log": "<log> <class>org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore</class> <method>getDomain</method> <level>INFO</level> <message>Attempting to retrieve domain with ID: {domainId}</message> </log> <log> <class>org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore</class> <method>getDomain</method> <level>INFO</level> <message>Service stopped, return null for the storage</message> </log> <log> <level>DEBUG</level> <message>Verifying the access of [user]</message> </log>"
  },
  "684d205f_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [INFO] message"
  },
  "684d205f_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config..."
  },
  "b4886bd0_1": {
    "exec_flow": "ENTRY→IF_TRUE: conf != null→CALL: org.apache.hadoop.conf.Configuration:get→TRY→IF_TRUE: confUmask != null→CALL: getUMask→NEW: UmaskParser→NEW: FsPermission→RETURN→EXIT→ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→EXCEPTION:debug→CATCH:FileNotFoundException→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask. [DEBUG] Overwriting file {}"
  },
  "b4886bd0_2": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkNotNull→IF_FALSE:InodeTree.SlashPath.equals(f)→IF_TRUE:this.fsState.getRootFallbackLink() != null→IF_FALSE:theInternalDir.getChildren().containsKey(f.getName())→TRY→CALL:org.apache.hadoop.fs.FileSystem:create→RETURN→EXIT",
    "log": "[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}"
  },
  "eb9b0cbe_1": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "eb9b0cbe_2": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "eb9b0cbe_3": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "eb9b0cbe_4": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "eb9b0cbe_5": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "eb9b0cbe_6": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "eb9b0cbe_7": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "eb9b0cbe_8": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "6b71ff02_1": {
    "exec_flow": "ENTRY -> LOG: LOG.DEBUG: Getting the file status for {}, f.toString() -> IF_FALSE: key.length() == 0 -> TRY -> CALL: retrieveMetadata -> IF_TRUE: meta != null -> IF_FALSE: meta.isDirectory() -> LOG: LOG.DEBUG: Found the path: {} as a file., f.toString() -> CALL: updateFileStatusPath -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Getting the file status for {}, f.toString()</log> <log>[DEBUG] Found the path: {} as a file., f.toString()</log>"
  },
  "6b71ff02_2": {
    "exec_flow": "ENTRY -> IF_FALSE: key.length() == 0 -> TRY -> CALL: retrieveMetadata -> IF_TRUE: meta != null -> IF_TRUE: meta.isDirectory() -> LOG: LOG.DEBUG: Path {} is a folder., f.toString() -> CALL: conditionalRedoFolderRename -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Path {} is a folder., f.toString()</log>"
  },
  "aaba6d9d_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. → LOG: org.apache.hadoop.fs.FileSystem:fixName [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead."
  },
  "aaba6d9d_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {}, uri → CALL: createFileSystem → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "aaba6d9d_3": {
    "exec_flow": "ENTRY → CALL:finished → IF_TRUE:logAtInfo → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) → LOG:[INFO] {} → EXIT",
    "log": "[INFO] {}"
  },
  "aaba6d9d_4": {
    "exec_flow": "ENTRY → CALL:finished → IF_FALSE:logAtInfo → IF_TRUE:log.isDebugEnabled() → CALL:org.slf4j.Logger:isDebugEnabled() → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → LOG:[DEBUG] {} → EXIT",
    "log": "[DEBUG] {}"
  },
  "97a69c42_1": {
    "exec_flow": "ENTRY→TRY→CALL:getHistoryFileReader→WHILE: (!readStartData || !readFinishData) && hfReader.hasNext()→CALL:next→WHILE_COND: (!readStartData || !readFinishData) && hfReader.hasNext()→WHILE_EXIT→IF_FALSE: !readStartData && !readFinishData→IF_TRUE: !readStartData→LOG: LOG.WARN: Start information is missing for application attempt + appAttemptId→IF_TRUE: !readFinishData→LOG: LOG.WARN: Finish information is missing for application attempt + appAttemptId→LOG: LOG.INFO: Completed reading history information of application attempt + appAttemptId→RETURN→EXIT",
    "log": "[WARN] Start information is missing for application attempt [WARN] Finish information is missing for application attempt [INFO] Completed reading history information of application attempt"
  },
  "97a69c42_2": {
    "exec_flow": "ENTRY→TRY→CALL:getHistoryFileReader→WHILE: (!readStartData || !readFinishData) && hfReader.hasNext()→CALL:next→WHILE_COND: (!readStartData || !readFinishData) && hfReader.hasNext()→WHILE_EXIT→IF_FALSE: !readStartData && !readFinishData→IF_TRUE: !readStartData→LOG: LOG.WARN: Start information is missing for application attempt + appAttemptId→IF_FALSE: !readFinishData→LOG: LOG.INFO: Completed reading history information of application attempt + appAttemptId→RETURN→EXIT",
    "log": "[WARN] Start information is missing for application attempt [INFO] Completed reading history information of application attempt"
  },
  "97a69c42_3": {
    "exec_flow": "ENTRY→TRY→CALL:getHistoryFileReader→WHILE: (!readStartData || !readFinishData) && hfReader.hasNext()→CALL:next→WHILE_COND: (!readStartData || !readFinishData) && hfReader.hasNext()→WHILE_EXIT→IF_FALSE: !readStartData && !readFinishData→IF_FALSE: !readStartData→IF_TRUE: !readFinishData→LOG: LOG.WARN: Finish information is missing for application attempt + appAttemptId→LOG: LOG.INFO: Completed reading history information of application attempt + appAttemptId→RETURN→EXIT",
    "log": "[WARN] Finish information is missing for application attempt [INFO] Completed reading history information of application attempt"
  },
  "97a69c42_4": {
    "exec_flow": "ENTRY→TRY→CALL:getHistoryFileReader→WHILE: (!readStartData || !readFinishData) && hfReader.hasNext()→CALL:next→WHILE_COND: (!readStartData || !readFinishData) && hfReader.hasNext()→WHILE_EXIT→IF_FALSE: !readStartData && !readFinishData→IF_FALSE: !readStartData→IF_FALSE: !readFinishData→LOG: LOG.INFO: Completed reading history information of application attempt + appAttemptId→RETURN→EXIT",
    "log": "[INFO] Completed reading history information of application attempt"
  },
  "97a69c42_5": {
    "exec_flow": "ENTRY→TRY→CALL:getHistoryFileReader→WHILE: (!readStartData || !readFinishData) && hfReader.hasNext()→CALL:next→WHILE_COND: (!readStartData || !readFinishData) && hfReader.hasNext()→EXCEPTION:IOException→CATCH:IOException→LOG:ERROR→THROW:IOException→EXIT",
    "log": "[ERROR] Error when reading history file of container"
  },
  "39a5a244_1": {
    "exec_flow": "ENTRY→CALL:append→FOR_INIT→FOR_COND:idx < args.length→FOR_BODY→CALL:append→FOR_COND:idx < args.length→FOR_EXIT→IF_FALSE:null == dir→TRY→CALL:getResolvedPath→CALL:getCurrentNamenodeAddress→IF_FALSE:namenodeAddress != null→CALL:append→CALL:append→TRY→CALL:URLConnectionFactory:openConnection→ENTRY→IF_TRUE: isSpnego→CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser→CALL:org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab→NEW: AuthenticatedURL→NEW: KerberosUgiAuthenticator→CALL:org.apache.hadoop.security.authentication.client.AuthenticatedURL:openConnection→IF_FALSE: url == null→IF_FALSE: !url.getProtocol().equalsIgnoreCase(\"http\") && !url.getProtocol().equalsIgnoreCase(\"https\")→IF_FALSE: token == null→CALL: authenticate→CALL: openConnection→LOG: LOG.DEBUG: open AuthenticatedURL connection {}, url→RETURN→EXIT→TRY→CALL:input.readLine→WHILE_COND→WHILE_BODY→CALL:out.println→WHILE_END→FINALLY→CALL:input.close→RETURN→EXIT",
    "log": "[ERROR] Connecting to namenode via [url] [DEBUG] open AuthenticatedURL connection {}"
  },
  "cd60a171_1": {
    "exec_flow": "ENTRY → SYNC: userPipelineMap → IF_FALSE: userPipelineMap.containsKey(user) → LOG: [INFO] Initializing request processing pipeline for user: {} → CALL: createRequestInterceptorChain() → EXCEPTION: init → CATCH: Exception e → LOG: [ERROR] Init RMAdminRequestInterceptor error for user: {}, e → THROW: e → EXIT → CALL:org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:removeFromClusterNodeLabels → EXCEPTION:YarnException → EXIT",
    "log": "<log_entry>[INFO] Initializing request processing pipeline for user: {}</log_entry> <log_entry>[ERROR] Init RMAdminRequestInterceptor error for user: {}</log_entry>"
  },
  "cd60a171_2": {
    "exec_flow": "ENTRY → CALL: checkAcls → CALL: checkRMStatus → TRY → CALL: rm.getRMContext().getNodeLabelManager().removeFromClusterNodeLabels → EXCEPTION: IOException → CATCH: IOException ioe → CALL: logAndWrapException(ioe, user.getShortUserName(), operation, msg) → THROW: logAndWrapException(ioe, user.getShortUserName(), operation, msg) → EXIT",
    "log": "<log> <level>ERROR</level> <template>IOException occurred during removeFromClusterNodeLabels</template> </log>"
  },
  "cd60a171_3": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → IF_TRUE: !isInitNodeLabelStoreInProgress() → CALL: checkRemoveFromClusterNodeLabelsOfQueue → CALL: cloneNodeMap → CALL: removeFromClusterNodeLabels → FOREACH: nodeCollections.entrySet() → FOREACH_EXIT → FOREACH: labelsToRemove → FOREACH_EXIT → IF_TRUE: null != dispatcher → CALL: dispatcher.getEventHandler().handle → CALL: org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event) → LOG: LOG.INFO: Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ] → EXIT → CALL: updateResourceMappings → CALL: writeLock.unlock → EXIT",
    "log": "[INFO] Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ]"
  },
  "cd60a171_4": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → IF_TRUE: !isInitNodeLabelStoreInProgress() → CALL: checkRemoveFromClusterNodeLabelsOfQueue → CALL: cloneNodeMap → CALL: removeFromClusterNodeLabels → FOREACH: nodeCollections.entrySet() → FOREACH_EXIT → FOREACH: labelsToRemove → FOREACH_EXIT → IF_FALSE: null != dispatcher → LOG: LOG.INFO: Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ] → EXIT → CALL: updateResourceMappings → CALL: writeLock.unlock → EXIT",
    "log": "[INFO] Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ]"
  },
  "cd60a171_5": {
    "exec_flow": "ENTRY → SYNC: userPipelineMap → IF_TRUE: userPipelineMap.containsKey(user) → LOG: [INFO] Request to start an already existing user: {} was received, so ignoring. → CALL: get → RETURN → EXIT",
    "log": "<log_entry>[INFO] Request to start an already existing user: {} was received, so ignoring.</log_entry>"
  },
  "cd60a171_6": {
    "exec_flow": "ENTRY→IF_TRUE: LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→EXIT",
    "log": "<log> <level>INFO</level> <template>createSuccessLog(user, operation, target, null, null, null, null)</template> </log>"
  },
  "40879704_1": {
    "exec_flow": "ENTRY→CALL:SetOwnerOp.getInstance→CALL:SetOwnerOp.setSource→CALL:SetOwnerOp.setUser→CALL:SetOwnerOp.setGroup→CALL:logEdit→ENTRY→LOG: LOG.DEBUG: doEditTx() op={} txid={}→CALL: org.slf4j.Logger:debug→TRY→CALL: editLogStream.write→CALL: reset→CALL: endTransaction→CALL: shouldForceSync→RETURN→EXIT",
    "log": "[DEBUG] Set owner operation created [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "9771bad6_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileStatus→RETURN→EXIT→ENTRY→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Getting the file status for {}</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry>"
  },
  "60a3d582_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→IF_TRUE:logAtInfo→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)→LOG:[INFO] Preflight Load of pending files→CALL:TaskPool.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(commitContext.getOuterSubmitter()).run→ENTRY→CALL:getCurrentIOStatisticsContext→TRY→WHILE:iterator.hasNext()→WHILE_COND:iterator.hasNext()→CALL:next→CALL:submit→CALL:lambda$0→CALL:debug→CALL:lambda$1→WHILE_COND:iterator.hasNext()→WHILE_EXIT→CALL:waitFor→CALL:clear→IF_FALSE:taskFailed.get() && revertTask != null→IF_TRUE:!suppressExceptions && !exceptions.isEmpty()→CALL:TaskPool.throwOne→IF_FALSE:iteratorIOE != null→RETURN→EXIT→CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:lambda$precommitCheckPendingFiles$3→CALL:org.apache.hadoop.util.DurationInfo:close()→EXIT",
    "log": "[INFO] Preflight Load of pending files [DEBUG] Executing task [DEBUG] Task succeeded"
  },
  "60a3d582_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→IF_FALSE:logAtInfo→IF_TRUE:log.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→LOG:[DEBUG] Preflight Load of pending files→CALL:TaskPool.foreach(pending.getSourceFiles()).stopOnFailure().suppressExceptions(false).executeWith(commitContext.getOuterSubmitter()).run→ENTRY→CALL:getCurrentIOStatisticsContext→TRY→WHILE:iterator.hasNext()→WHILE_COND:iterator.hasNext()→CALL:next→CALL:submit→CALL:lambda$0→CALL:debug→CALL:lambda$1→WHILE_COND:iterator.hasNext()→WHILE_EXIT→CALL:waitFor→CALL:clear→IF_FALSE:taskFailed.get() && revertTask != null→IF_TRUE:!suppressExceptions && !exceptions.isEmpty()→CALL:TaskPool.throwOne→IF_FALSE:iteratorIOE != null→RETURN→EXIT→CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:lambda$precommitCheckPendingFiles$3→CALL:org.apache.hadoop.util.DurationInfo:close()→EXIT",
    "log": "[DEBUG] Preflight Load of pending files [DEBUG] Executing task [DEBUG] Task succeeded"
  },
  "60a3d582_3": {
    "exec_flow": "<sequence> <entry>AbstractS3ACommitter:lambda$3</entry> <virtual_call>PersistentCommitData:load</virtual_call> <branch>Reading commit data from file {path}</branch> <exception_handling>IOException, ValidationFailure</exception_handling> <exit>Validate commit data</exit> </sequence>",
    "log": "<log>Reading commit data from file {path}</log>"
  },
  "ab810c8f_1": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Processing {event.getJobId()} of type {event.getType()}→TRY→CALL:writeLock.lock→TRY→CALL:getStateMachine().doTransition→IF_TRUE:oldState != getInternalState()→LOG:LOG.INFO:jobId {jobId} Transitioned from {oldState} to {getInternalState()}→CALL:rememberLastNonFinalState→CALL:writeLock.unlock→RMAppManager:handle→finishApplication→IF_FALSE:applicationId == null→IF_TRUE:UserGroupInformation.isSecurityEnabled()→CALL:org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:applicationFinished→CALL:completedApps.add→CALL:org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:writeAuditLog→EXIT",
    "log": "[DEBUG] Processing {event.getJobId()} of type {event.getType()} [INFO] Job {jobId} Transitioned from {oldState} to {getInternalState()} [DEBUG] RMAppManager processing event for ApplicationId of type APP_COMPLETED [INFO] writeAuditLog"
  },
  "ab810c8f_2": {
    "exec_flow": "<step>Check TaskType: MAP/REDUCE</step> <step>Counter update based on TaskType</step> <step>Check if task is already completed</step> <step>Update Millis Counters if !taskAlreadyCompleted</step>",
    "log": "<log>[INFO] JobCounterUpdateEvent created for TaskAttempt</log>"
  },
  "ab810c8f_3": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Processing {event.getJobId()} of type {event.getType()}→TRY→CALL:writeLock.lock→TRY→CALL:getStateMachine().doTransition→IF_FALSE:oldState != getInternalState()→CALL:writeLock.unlock→EXIT",
    "log": "[DEBUG] Processing {event.getJobId()} of type {event.getType()} [DEBUG] Start Opportunistic Containers"
  },
  "ab810c8f_4": {
    "exec_flow": "ENTRY→IF_TRUE:app != null→CALL:handle→IF_TRUE:nmMetricsPublisher != null→SWITCH:event.getType()→CASE:[]→LOG:LOG.DEBUG:{} is not a desired ApplicationEvent which needs to be published by NMTimelinePublisher, event.getType()→EXIT→EXIT",
    "log": "[DEBUG] {} is not a desired ApplicationEvent which needs to be published by NMTimelinePublisher, event.getType()"
  },
  "ab810c8f_5": {
    "exec_flow": "ENTRY→CALL:handleStoreEvent→CALL:org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler:handle→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "[INFO] Handling NodeAttributesStoreEvent [WARN] Unsupported operation"
  },
  "ab810c8f_6": {
    "exec_flow": "ENTRY→CALL:handleStoreEvent→CALL:org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler:handle→CALL:org.slf4j.Logger:error→THROW:YarnRuntimeException→EXIT",
    "log": "[INFO] Handling NodeAttributesStoreEvent [ERROR] Failed to store attribute modification to storage"
  },
  "ab810c8f_7": {
    "exec_flow": "ENTRY→IF_TRUE: rmApp != null→TRY→CALL: rmApp.handle→EXCEPTION: handle→CATCH: Throwable t→LOG: LOG.ERROR: Error in handling event type + event.getType() + for application + appID, t→EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for application + appID, t"
  },
  "ab810c8f_8": {
    "exec_flow": "ENTRY→IF_TRUE: rmApp != null→TRY→CALL: rmApp.handle→EXCEPTION: handle→CATCH: Throwable t→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for application + appID, t"
  },
  "7205c12a_1": {
    "exec_flow": "ENTRY→CALL:writeInt→CALL:saveAllKeys→CALL:writeInt→CALL:saveCurrentTokens→ENTRY→CALL:beginStep→CALL:setTotal→CALL:writeInt→WHILE:iter.hasNext()→WHILE_COND:iter.hasNext()→CALL:write→CALL:getRenewDate→CALL:increment→WHILE_EXIT→CALL:endStep→EXIT",
    "log": "[INFO] Step DELEGATION_TOKENS started [INFO] End step DELEGATION_TOKENS [DEBUG] End of the step. Phase: {}, Step: {}"
  },
  "7205c12a_2": {
    "exec_flow": "ENTRY→CALL:writeInt→CALL:saveAllKeys→CALL:writeInt→CALL:saveCurrentTokens→ENTRY→IF_TRUE:!isComplete(phase)→CALL:lazyInitStep(phase, step)→CALL:monotonicNow→LOG:LOG.DEBUG:Beginning of the step. Phase: {}, Step: {}→CALL:beginStep→CALL:setTotal→CALL:writeInt→WHILE:iter.hasNext()→WHILE_COND:iter.hasNext()→CALL:write→CALL:getRenewDate→CALL:increment→WHILE_EXIT→CALL:endStep→EXIT",
    "log": "[DEBUG] Beginning of the step. Phase: {}, Step: {} [INFO] Step DELEGATION_TOKENS started [INFO] End step DELEGATION_TOKENS [DEBUG] End of the step. Phase: {}, Step: {}"
  },
  "7205c12a_3": {
    "exec_flow": "ENTRY→CALL:writeInt→CALL:saveAllKeys→CALL:writeInt→CALL:saveCurrentTokens→ENTRY→IF_FALSE:!isComplete(phase)→LOG:LOG.DEBUG:Beginning of the step. Phase: {}, Step: {}→CALL:beginStep→CALL:setTotal→CALL:writeInt→WHILE:iter.hasNext()→WHILE_COND:iter.hasNext()→CALL:write→CALL:getRenewDate→CALL:increment→WHILE_EXIT→CALL:endStep→EXIT",
    "log": "[DEBUG] Beginning of the step. Phase: {}, Step: {} [INFO] Step DELEGATION_TOKENS started [INFO] End step DELEGATION_TOKENS [DEBUG] End of the step. Phase: {}, Step: {}"
  },
  "b74c4392_1": {
    "exec_flow": "ENTRY→CALL:moveToNext→CALL:getInputBytes→CALL:fileInputByteCounter.increment→CALL:reporter.setProgress→IF_TRUE: Float.isNaN(progress)→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Illegal progress value found, progress is Float.NaN. Progress will be changed to 0"
  },
  "b74c4392_2": {
    "exec_flow": "ENTRY→CALL:moveToNext→CALL:getInputBytes→CALL:fileInputByteCounter.increment→CALL:reporter.setProgress→IF_FALSE: Float.isNaN(progress)→IF_TRUE: progress == Float.NEGATIVE_INFINITY→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Illegal progress value found, progress is Float.NEGATIVE_INFINITY. Progress will be changed to 0"
  },
  "b74c4392_3": {
    "exec_flow": "ENTRY→CALL:moveToNext→CALL:getInputBytes→CALL:fileInputByteCounter.increment→CALL:reporter.setProgress→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_TRUE: progress < 0→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Illegal progress value found, progress is less than 0. Progress will be changed to 0"
  },
  "b74c4392_4": {
    "exec_flow": "ENTRY→CALL:moveToNext→CALL:getInputBytes→CALL:fileInputByteCounter.increment→CALL:reporter.setProgress→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_FALSE: progress < 0→IF_TRUE: progress > 1→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Illegal progress value found, progress is larger than 1. Progress will be changed to 1"
  },
  "b74c4392_5": {
    "exec_flow": "ENTRY→CALL:moveToNext→CALL:getInputBytes→CALL:fileInputByteCounter.increment→CALL:reporter.setProgress→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_FALSE: progress < 0→IF_FALSE: progress > 1→IF_TRUE: progress == Float.POSITIVE_INFINITY→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Illegal progress value found, progress is Float.POSITIVE_INFINITY. Progress will be changed to 1"
  },
  "5506792f_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "5506792f_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "5506792f_3": {
    "exec_flow": "ENTRY → FOR_LOOP: newNames → GET: deprecatedKey → IF_TRUE: deprecatedKey != null && !getProps().containsKey(newName) → IF_TRUE: deprecatedValue != null → SET_PROPERTY: newName → EXIT",
    "log": "[INFO] message"
  },
  "af5a5ab3_1": {
    "exec_flow": "<step>ENTRY</step> <step> <condition>IF_TRUE: isInState(STATE.STOPPED)</condition> <steps> <step>RETURN</step> </steps> </step> <step> <condition>IF_FALSE: isInState(STATE.STOPPED)</condition> <steps> <step> <sync>stateChangeLock</sync> <step> <condition>IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED</condition> <steps> <step>TRY</step> <step>CALL: serviceStop</step> <step> <sync> <condition>EXCEPTION: serviceStop</condition> <steps> <step>CATCH: Exception e</step> <step>CALL: noteFailure</step> <step>THROW: ServiceStateException.convert(e)</step> </steps> </sync> </step> </steps> </step> <step> <condition>FINALLY</condition> <steps> <step>terminationNotification.set(true)</step> <step> <sync> <condition /> <steps> <step>terminationNotification.notifyAll()</step> <step>CALL: notifyListeners</step> <step>ENTRY</step> <step>TRY</step> <step>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>EXIT</step> <step>CATCH: Throwable e</step> <step>LOG: LOG.WARN: Exception while notifying listeners of {getName()}</step> </steps> </sync> </step> </steps> </step> <step> <condition>IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED</condition> <steps> <step>LOG: LOG.DEBUG: Ignoring re-entrant call to stop()</step> <step>CALL: notifyListeners</step> <step>ENTRY</step> <step>TRY</step> <step>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>EXIT</step> <step>CATCH: Throwable e</step> <step>LOG: LOG.WARN: Exception while notifying listeners of {getName()}</step> </steps> </step> </step> </steps> </step> <step>EXIT</step>",
    "log": "<log> <level>DEBUG</level> <message>Ignoring re-entrant call to stop()</message> </log> <log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {getName()}</message> </log>"
  },
  "41959348_1": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>TRY</step> <step>CALL: getFileStatus</step> <step>IF_FALSE: fsSupportsChmod</step> <step>RETURN</step> <step>EXIT</step> </sequence>",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "6eb66793_1": {
    "exec_flow": "ENTRY→LOG:LOG.INFO:placing the following ReservationRequest: + contract→TRY→CALL:planner.createReservation→IF_FALSE: alg.createReservation(reservationId, user, plan, contract)→IF_TRUE: res→LOG:LOG.INFO:OUTCOME: SUCCESS, Reservation ID: + reservationId.toString() + , Contract: + contract.toString()→RETURN→EXIT",
    "log": "[INFO] placing the following ReservationRequest: + contract [INFO] OUTCOME: SUCCESS, Reservation ID: + reservationId.toString() + , Contract: + contract.toString()"
  },
  "6eb66793_2": {
    "exec_flow": "ENTRY→CALL:checkReservationSystem→CALL:ReservationInputValidator:validateReservationSubmissionRequest→IF_FALSE:allocation != null→CALL:checkReservationACLs→TRY→CALL:ReservationAgent:createReservation→IF_FALSE:result→CALL:RMAuditLogger:logFailure→THROW:RPCUtil.getRemoteException→EXIT",
    "log": "[ERROR] Unable to create the reservation"
  },
  "6eb66793_3": {
    "exec_flow": "ENTRY→IF_TRUE: LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→EXIT",
    "log": "[INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "6eb66793_4": {
    "exec_flow": "ENTRY→IF_FALSE: reservationId == null→CALL: getPlanFromQueue→IF_TRUE: queue == null || queue.isEmpty()→CALL: RMAuditLogger.logFailure→THROW: RPCUtil.getRemoteException(nullQueueErrorMessage)→EXIT",
    "log": "[ERROR] validate reservation input"
  },
  "6eb66793_5": {
    "exec_flow": "ENTRY→IF_FALSE: reservationId == null→CALL: getPlanFromQueue→IF_FALSE: queue == null || queue.isEmpty()→IF_TRUE: plan == null→CALL: validateReservationDefinition→CALL: RMAuditLogger.logFailure→THROW: RPCUtil.getRemoteException(nullPlanErrorMessage)→EXIT",
    "log": "[ERROR] validate reservation input"
  },
  "6eb66793_6": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation→RETURN→EXIT",
    "log": "[INFO] Submitting reservation to ResourceManager"
  },
  "6eb66793_7": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:submitReservation→RETURN→EXIT",
    "log": "[INFO] Submitting reservation through RouterClient"
  },
  "6eb66793_8": {
    "exec_flow": "ENTRY→SYNC: this.userPipelineMap→IF_FALSE: this.userPipelineMap.containsKey(user)→TRY→CALL: createRequestInterceptorChain→LOG: [INFO] Initializing request processing pipeline for application for the user: {}→EXCEPTION: info→CATCH: Exception e→LOG: [ERROR] Init ClientRequestInterceptor error for user: {}→THROW: e→EXIT",
    "log": "[INFO] Initializing request processing pipeline for application for the user: {} [ERROR] Init ClientRequestInterceptor error for user: {}"
  },
  "24e70498_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "24e70498_2": {
    "exec_flow": "ENTRY→SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → TRY→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "24e70498_3": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [INFO] Service {} failed in state {}"
  },
  "24e70498_4": {
    "exec_flow": "ENTRY→IF_TRUE:service != null→CALL:stop→TRY→CATCH(Exception)→CALL:log.warn→EXIT",
    "log": "[WARN] When stopping the service {service_name}"
  },
  "97618e7b_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:component.getRestartPolicyHandler().isLongLived()</step> <step>IF_TRUE:isIncrement</step> <step>IF_TRUE:!component.upgradeStatus.areContainersUpgrading() && !component.cancelUpgradeStatus.areContainersUpgrading() && component.componentMetrics.containersReady.value() == component.componentMetrics.containersDesired.value()</step> <step>CALL:setComponentState</step> <step>ComponentState check and set</step> <step>CALL:component.context.getServiceManager().checkAndUpdateServiceState</step> <step>CALL:component.dispatcher.getEventHandler().handle</step> <step>IF_FALSE: app != null</step> <step>LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID()</step> <step>EXIT</step>",
    "log": "<log>[COMPONENT {}] spec state changed from {} -> {}</log> <log>[WARN] Event + event + sent to absent application + event.getApplicationID()</log>"
  },
  "7cc724d8_1": {
    "exec_flow": "ENTRY→CALL:getCompressor→CALL:org.slf4j.Logger:isDebugEnabled()→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.io.compress.CompressionCodec:createCompressor→CALL:org.apache.hadoop.io.compress.Compressor:reinit→CALL:org.apache.hadoop.io.compress.CompressionCodec:getDefaultExtension→CALL:org.slf4j.Logger:info→RETURN→EXIT",
    "log": "[DEBUG] Compression codec created [INFO] Compressor initialized"
  },
  "0edabc2d_1": {
    "exec_flow": "ENTRY→IF_TRUE: createParent→CALL: FSDirMkdirOp.createAncestorDirectories→IF_FALSE: parent == null→CALL: unprotectedAddSymlink→IF_FALSE: newNode == null→CALL: fsd.getEditLog().logSymlink→IF_TRUE: NameNode.stateChangeLog.isDebugEnabled()→CALL: NameNode.stateChangeLog.debug→RETURN→EXIT",
    "log": "[DEBUG] mkdirs: created directory {cur} [DEBUG] logEdit called [DEBUG] addSymlink: [path] is added [ERROR] FSDirectory.verifyMaxDirItems: Too many children. [ERROR] BUG: unexpected exception {e} [ERROR] ERROR in FSDirectory.verifyINodeName {e} [DEBUG] child: {}, posixAclInheritanceEnabled: {}, modes: {} [DEBUG] {}: no parent default ACL to inherit"
  },
  "07111e52_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "07111e52_2": {
    "exec_flow": "ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "07111e52_3": {
    "exec_flow": "ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "07111e52_4": {
    "exec_flow": "ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "07111e52_5": {
    "exec_flow": "ENTRY→CALL:getInt→IF_TRUE:this.maxConcurrentTrackedNodes < 0→CALL:Configuration:getInt→LOG:LOG.ERROR {} is set to an invalid value, it must be zero or greater. Defaulting to {}→CALL:processConf→EXIT→UID and GID obtained and validated→User and group are determined correctly→SingleUGIResolver.setConf.",
    "log": "[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT SET uid for user Exception handling user determination Group set successfully TextFileRegionAliasMap: read path {tmpfile}"
  },
  "07111e52_6": {
    "exec_flow": "ImageWriter$Options.setConf: Configuration set for ImageWriter options Outdir, StartBlock, StartInode, MaxDirCache resolved UGISClass, AliasMap, BlockIdsClass assigned ClusterID, BlockPoolID initialized",
    "log": "[WARN] Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead"
  },
  "07111e52_7": {
    "exec_flow": "ENTRY→CALL:getStrings→IF_FALSE→CALL:Iterators.cycle→CALL:next→CALL:getBoolean→IF_TRUE→CALL:loadSslConf→CALL:initializeBindUsers→CALL:getTrimmed→LOG:debug→CALL:getTrimmed→LOG:debug",
    "log": "[DEBUG] Usersearch baseDN: {userbaseDN} [DEBUG] Groupsearch baseDN: {groupbaseDN}"
  },
  "07111e52_8": {
    "exec_flow": "ENTRY→CALL:getStrings→IF_FALSE→CALL:Iterators.cycle→CALL:next→CALL:getBoolean→IF_FALSE→CALL:initializeBindUsers→CALL:getTrimmed→LOG:debug→CALL:getTrimmed→LOG:debug",
    "log": "[DEBUG] Usersearch baseDN: {userbaseDN} [DEBUG] Groupsearch baseDN: {groupbaseDN}"
  },
  "745b30cb_1": {
    "exec_flow": "ENTRY→CALL:readFully→CALL:validatePositionedReadArgs→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→RETURN→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "638511f4_1": {
    "exec_flow": "ENTRY→CALL:getApplicationAttempts→TRY→CALL:getHistoryFileReader→WHILE:hfReader.hasNext()→WHILE_COND:hfReader.hasNext()→CALL:next→IF→IF→CALL:mergeApplicationAttemptHistoryData→IF→CALL:mergeApplicationAttemptHistoryData→WHILE_EXIT→LOG:LOG.INFO:Completed reading history information of all application attempts of application appId→CALL:hfReader.close→CALL:IOUtils.cleanupWithLogger→RETURN→EXIT→FOREACH:attempts.keySet()→FOREACH_EXIT→CALL:get→RETURN→EXIT",
    "log": "[INFO] Completed reading history information of all application attempts of application appId"
  },
  "638511f4_2": {
    "exec_flow": "ENTRY→CALL:getApplicationAttempts→TRY→CALL:getHistoryFileReader→WHILE:hfReader.hasNext()→WHILE_COND:hfReader.hasNext()→CALL:next→IF→IF→CALL:mergeApplicationAttemptHistoryData→IF→CALL:mergeApplicationAttemptHistoryData→WHILE_EXIT→EXCEPTION:IOException→CATCH:IOException→LOG:LOG.INFO:Error when reading history information of some application attempts of application appId→CALL:hfReader.close→CALL:IOUtils.cleanupWithLogger→RETURN→EXIT→FOREACH:attempts.keySet()→FOREACH_EXIT→CALL:get→RETURN→EXIT",
    "log": "[INFO] Error when reading history information of some application attempts of application appId"
  },
  "1ff913c7_1": {
    "exec_flow": "ENTRY → IF_TRUE: out != null → TRY → CALL: finish → CALL: outShadow.close → CALL: IOUtils.closeStream → IF_TRUE: stream != null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT → EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "abaf147f_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→CALL:org.apache.hadoop.service.ServiceOperations:stopQuietly(LOG, this)→CATCH_EXCEPTION→LOG:When stopping the service {service.getName()}→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {} [WARN] When stopping the service {service.getName()} [DEBUG] noteFailure [INFO] Service {} failed in state {}"
  },
  "def3ccb1_1": {
    "exec_flow": "ENTRY → IF_TRUE: connectUgi == null → IF_TRUE: op.getRequireAuth() → CALL: connectUgi.checkTGTAndReloginFromKeytab → TRY → CALL: doAs → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}] → NEW: PrivilegedExceptionAction<T> → CALL: runWithRetry → CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {} → THROW: IOException | Error | RuntimeException | InterruptedException | UndeclaredThrowableException → RETURN → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "24beba00_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT → [VIRTUAL_CALL] → LOG:DEBUG: AzureBlobFileSystem.delete path: {}, recursive: {} → DO_WHILE → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:deletePath → CALL:perfInfo.registerResult → CALL:getResponseHeader → CALL:getResult → CALL:perfInfo.registerSuccess → CALL:isEmpty → IF_FALSE:!shouldContinue → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close → DO_COND:shouldContinue → DO_EXIT → EXIT → IF_FALSE:this.azureAuthorization → CALL:deleteWithoutAuth → ENTRY → LOG:DEBUG:Deleting file: {}, f → TRY → CALL:retrieveMetadata → IF_FALSE:null == metaFile → IF_TRUE:!metaFile.isDirectory() → IF_TRUE:parentPath.getParent() != null → TRY → CALL:retrieveMetadata → IF_FALSE:parentMetadata == null → IF_FALSE:!parentMetadata.isDirectory() → IF_FALSE:parentMetadata.getBlobMaterialization() == BlobMaterialization.Implicit → IF_FALSE:!skipParentFolderLastModifiedTimeUpdate → TRY → IF_TRUE:store.delete(key) → CALL:fileDeleted → LOG:DEBUG:Delete Successful for: {}, f → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] AzureBlobFileSystem.delete path: {}, recursive: {} [DEBUG] Deleting file: {} [DEBUG] Delete Successful for: {}"
  },
  "24beba00_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "730ff2ee_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Failed to initialize filesystem, e [DEBUG] Exception in closing {}"
  },
  "730ff2ee_2": {
    "exec_flow": "ENTRY → CALL:open → CALL:getUriPath → CALL:resolve → CALL:open → LOG:[INFO] Open the file: [{f}] for reading. → RETURN → EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "5a7b6b6f_1": {
    "exec_flow": "ENTRY→TRY→CALL:toBoolean→CATCH:TrileanConversionException→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable)→TRY→CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)→CATCH:AbfsRestOperationException→CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode()→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close()→ENTRY→IF_FALSE:!enabled→IF_TRUE:isValidInstant(perfInfo.getAggregateStart()) && perfInfo.getAggregateCount()>0→CALL:recordClientLatency→ENTRY→CALL:isValidInstant→CALL:isValidInstant→CALL:Duration.between→CALL:isValidInstant→CALL:isValidInstant→CALL:Duration.between→CALL:String.format→CALL:offerToQueue→IF_TRUE:LOG.isDebugEnabled()→CALL:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Queued latency info [{} ms]: {}, elapsed, latencyDetails→RETURN→EXIT→EXIT",
    "log": "[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call [DEBUG] Get root ACL status [DEBUG] Queued latency info [{} ms]: {}"
  },
  "37bd5adc_1": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → IF_TRUE: configName != null → IF_FALSE: target == null → CALL: trim → CALL: createURI → IF_TRUE: port == -1 → IF_FALSE: (host == null) || (port < 0) || (!hasScheme && path != null && !path.isEmpty()) → CALL: createSocketAddrForHost → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms.</log_entry>"
  },
  "ed4c0452_1": {
    "exec_flow": "ENTRY→CALL:listStatus→NEW:DirectoryEntries→RETURN→EXIT→ LOG:[DEBUG]Listing status for{f}→CALL:performAuthCheck→TRY→CALL:retrieveMetadata→ IF_TRUE:meta==null→LOG:[DEBUG]Did not find any metadata for path:{key}→THROW:new FileNotFoundException(f+\"isnotfound\")→EXIT",
    "log": "[DEBUG] Listing status for {f} [DEBUG] Did not find any metadata for path: {key}"
  },
  "ed4c0452_2": {
    "exec_flow": "ENTRY→CALL:listStatus→NEW:DirectoryEntries→RETURN→EXIT→ LOG:[DEBUG]Listing status for{f}→CALL:performAuthCheck→TRY→CALL:retrieveMetadata→ IF_FALSE:meta==null→IF_TRUE:!meta.isDirectory()→LOG:[DEBUG]Found path as a file→ CALL:updateFileStatusPath→RETURN→EXIT",
    "log": "[DEBUG] Listing status for {f} [DEBUG] Found path as a file"
  },
  "ed4c0452_3": {
    "exec_flow": "ENTRY→CALL:listStatus→NEW:DirectoryEntries→RETURN→EXIT→ LOG:[DEBUG]Listing status for{f}→CALL:performAuthCheck→TRY→CALL:retrieveMetadata→ IF_FALSE:meta==null→IF_FALSE:!meta.isDirectory()→CALL:listWithErrorHandling→ IF_FALSE:renamed→CALL:conditionalRedoFolderRenames→IF_TRUE:key.equals(\"/\")→ FOREACH:listing→IF_TRUE:fileMetadata.isDirectory()→ IF_TRUE:fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)→CONTINUE→EXIT",
    "log": "[DEBUG] Listing status for {f}"
  },
  "6187ecf6_1": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:Processing + event.getTaskAttemptID() + of type + event.getType()→CALL:handleNMTimelineEvent→LOG:LOG.INFO:attemptId + transitioned from state + oldState + to + getInternalState() + , event type is + event.getType() + and nodeId= + nodeId→IF_FALSE:HAUtil.isHAEnabled(getConfig())→SWITCH:event.getType()→CASE:[STATE_STORE_OP_FAILED]→IF_TRUE:YarnConfiguration.shouldRMFailFast(getConfig())→LOG:LOG.ERROR:FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.→CALL:ExitUtil.terminate→EXIT",
    "log": "[DEBUG] Processing + event.getTaskAttemptID() + of type + event.getType() [INFO] attemptId + transitioned from state + oldState + to + getInternalState() + , event type is + event.getType() + and nodeId= + nodeId [ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties."
  },
  "6187ecf6_2": {
    "exec_flow": "ENTRY→IF_FALSE:c != null→LOG:LOG.WARN:Event + event + sent to absent container + event.getContainerID()→EXIT",
    "log": "[WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "6187ecf6_3": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Processing + event.getTaskAttemptID() + of type + event.getType()→CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→LOG:LOG.ERROR:Can't handle this event at current state for + this.attemptId, e→CALL:eventHandler.handle→CALL:eventHandler.handle→CALL:writeLock.unlock→EXIT",
    "log": "[DEBUG] Processing + event.getTaskAttemptID() + of type + event.getType() [ERROR] Can't handle this event at current state for + this.attemptId, e"
  },
  "6187ecf6_4": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[]→LOG:LOG.ERROR:Unknown event arrived at + OpportunisticContainerAllocatorAMService: {}, event→EXIT",
    "log": "[ERROR] Unknown event arrived at OpportunisticContainerAllocatorAMService: {event}"
  },
  "6187ecf6_5": {
    "exec_flow": "ENTRY→IF_TRUE:component == null→LOG:LOG.ERROR:No component exists for + event.getName()→RETURN→EXIT",
    "log": "[ERROR] No component exists for + event.getName()"
  },
  "6187ecf6_6": {
    "exec_flow": "ENTRY→IF_FALSE:component == null→TRY→CALL:component.handle→EXCEPTION:component.handle→CATCH:Throwable t→LOG:LOG.ERROR:MessageFormat.format([COMPONENT {0}]: Error in handling event type {1}, component.getName(), event.getType()), t→EXIT",
    "log": "[ERROR] MessageFormat.format([COMPONENT {0}]: Error in handling event type {1}, component.getName(), event.getType()), t"
  },
  "9c4e81b8_1": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→CALL: noteFailure→THROW: ServiceStateException.convert(e)→CALL: notifyListeners",
    "log": "[DEBUG] Service: {} entered state {} [WARN] Exception while notifying listeners of {} [DEBUG] noteFailure [INFO] Service {} failed in state {}"
  },
  "9c4e81b8_2": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners",
    "log": "[DEBUG] Ignoring re-entrant call to stop()"
  },
  "30a0fc73_1": {
    "exec_flow": "ENTRY → CALL: trackDurationOfInvocation → IF_TRUE: pending.isEmpty() → LOG: org.slf4j.Logger:warn → CALL: org.apache.hadoop.util.DurationInfo:<init> → TRY → CALL: TaskPool.foreach → CALL: run → CALL: finished → IF_TRUE: logAtInfo → CALL: org.slf4j.Logger:info → EXIT",
    "log": "[WARN] {}: No pending uploads to commit [INFO] {}"
  },
  "30a0fc73_2": {
    "exec_flow": "ENTRY → CALL: trackDurationOfInvocation → IF_TRUE: pending.isEmpty() → LOG: org.slf4j.Logger:warn → CALL: org.apache.hadoop.util.DurationInfo:<init> → TRY → CALL: TaskPool.foreach → CALL: run → CALL: finished → IF_FALSE: logAtInfo → IF_TRUE: log.isDebugEnabled() → CALL: org.slf4j.Logger:debug → EXIT",
    "log": "[WARN] {}: No pending uploads to commit [DEBUG] {}"
  },
  "30a0fc73_3": {
    "exec_flow": "ENTRY → CALL: trackDurationOfInvocation → IF_TRUE: pending.isEmpty() → LOG: org.slf4j.Logger:warn → CALL: org.apache.hadoop.util.DurationInfo:<init> → TRY → CALL: TaskPool.foreach → CALL: run → CALL: finished → IF_FALSE: logAtInfo → IF_FALSE: log.isDebugEnabled() → EXIT",
    "log": "[WARN] {}: No pending uploads to commit"
  },
  "0d03a459_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND:TRUE→CALL:validatePositionedReadArgs→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→RETURN→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "405c4935_1": {
    "exec_flow": "ENTRY→IF_TRUE:totalInScopeNodes < availableNodes→LOG:LOG.WARN:Total Nodes in scope : {} are less than Available Nodes : {}, totalInScopeNodes, availableNodes→RETURN→EXIT",
    "log": "[WARN] Total Nodes in scope : {totalInScopeNodes} are less than Available Nodes : {availableNodes}"
  },
  "405c4935_2": {
    "exec_flow": "ENTRY→IF_FALSE:totalInScopeNodes < availableNodes→IF_FALSE:excludedNodes == null || excludedNodes.isEmpty()→LOG:LOG.DEBUG:nthValidToReturn is {}, nthValidToReturn→IF_TRUE:!excludedNodes.contains(ret)→LOG:LOG.DEBUG:Chosen node {} from first random, ret→RETURN→EXIT",
    "log": "[DEBUG] nthValidToReturn is {nthValidToReturn} [DEBUG] Chosen node {ret} from first random"
  },
  "405c4935_3": {
    "exec_flow": "ENTRY→IF_FALSE:totalInScopeNodes < availableNodes→IF_FALSE:excludedNodes == null || excludedNodes.isEmpty()→LOG:LOG.DEBUG:nthValidToReturn is {}, nthValidToReturn→IF_FALSE:!excludedNodes.contains(ret)→FOR_INIT→FOR_COND:i < totalInScopeNodes→FOR_EXIT→IF_TRUE:ret == null && lastValidNode != null→LOG:LOG.ERROR:BUG:Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}→RETURN→EXIT",
    "log": "[DEBUG] nthValidToReturn is {nthValidToReturn} [ERROR] BUG: Found lastValidNode {lastValidNode} but not nth valid node. parentNode={parentNode}, excludedScopeNode={excludedScopeNode}, excludedNodes={excludedNodes}, totalInScopeNodes={totalInScopeNodes}, availableNodes={availableNodes}, nthValidToReturn={nthValidToReturn}"
  },
  "405c4935_4": {
    "exec_flow": "ENTRY→IF_FALSE:totalInScopeNodes < availableNodes→IF_FALSE:excludedNodes == null || excludedNodes.isEmpty()→LOG:LOG.DEBUG:nthValidToReturn is {}, nthValidToReturn→IF_FALSE:!excludedNodes.contains(ret)→FOR_INIT→FOR_COND:i < totalInScopeNodes→FOR_EXIT→IF_FALSE:ret == null && lastValidNode != null→RETURN→EXIT",
    "log": "[DEBUG] nthValidToReturn is {nthValidToReturn}"
  },
  "73057e3c_1": {
    "exec_flow": "ENTRY→IF_TRUE:service != null→CALL:stop→TRY→CATCH(Exception)→CALL:log.warn→EXIT ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→RETURN→EXIT",
    "log": "[WARN] When stopping the service {service_name} [DEBUG] Service {} is started [INFO] Service {} failed in state {}"
  },
  "73057e3c_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "73057e3c_3": {
    "exec_flow": "ENTRY→SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → TRY→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "f6e085d4_1": {
    "exec_flow": "ENTRY→CALL:checkOperation→TRY→CALL:checkToken→IF_TRUE:info==null→CALL:getRealUser→LOG:LOG.WARN:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→EXIT",
    "log": "[WARN] {}, Token={}"
  },
  "f6e085d4_2": {
    "exec_flow": "ENTRY→CALL:checkOperation→TRY→CALL:checkToken→IF_FALSE:info==null→IF_TRUE:info.getRenewDate()<now→CALL:getRealUser→CALL:formatTime→CALL:formatTime→CALL:getRenewDate→CALL:getRenewDate→LOG:LOG.INFO:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→EXIT",
    "log": "[INFO] {}, Token={}"
  },
  "4caa2764_1": {
    "exec_flow": "ENTRY→NEW: InMemoryMapOutput<K, V>→CALL:borrow→IF_TRUE:decompressor==null→CALL:createDecompressor→CALL:getDefaultExtension→LOG:LOG.INFO→IF_TRUE:decompressor!=null&&!decompressor.getClass().isAnnotationPresent(DoNotPool.class)→CALL:updateLeaseCount→RETURN→EXIT",
    "log": "[INFO] Got brand-new decompressor [<default extension>]"
  },
  "4caa2764_2": {
    "exec_flow": "ENTRY→NEW: InMemoryMapOutput<K, V>→CALL:borrow→IF_FALSE:decompressor==null→CALL:isDebugEnabled→IF_TRUE→LOG:LOG.DEBUG→IF_TRUE:decompressor!=null&&!decompressor.getClass().isAnnotationPresent(DoNotPool.class)→CALL:updateLeaseCount→RETURN→EXIT",
    "log": "[DEBUG] Got recycled decompressor"
  },
  "9cd7e51d_1": {
    "exec_flow": "ENTRY→IF_TRUE: cacheCleaner == null →NEW: CacheCleaner→CALL:ShortCircuitCache.this.lock.lock→TRY→IF_FALSE:ShortCircuitCache.this.closed→LOG:LOG.DEBUG:{}: cache cleaner running at {}, this, curMs→WHILE:!evictable.isEmpty()→CALL:isTraceEnabled→IF_TRUE:isTraceEnabled→CALL:trace→CALL:access$5→LOG:LOG.TRACE:CacheCleaner: purging replica→WHILE_EXIT→LOG:LOG.DEBUG:{}: finishing cache cleaner run started at {}. Demoted {} mmapped replicas; purged {} replicas., this, curMs, numDemoted, numPurged→CALL:cacheCleaner.getRateInMs→CALL:cleanerExecutor.scheduleAtFixedRate→CALL:cacheCleaner.setFuture→LOG:LOG.DEBUG:{}: starting cache cleaner thread which will run every {} ms, this, rateMs→CALL:ShortCircuitCache.this.lock.unlock→EXIT",
    "log": "[DEBUG] {}: starting cache cleaner thread which will run every {} ms, this, rateMs [DEBUG] {}: cache cleaner running at {} [TRACE] CacheCleaner: purging replica [DEBUG] {}: finishing cache cleaner run started at {}. Demoted {} mmapped replicas; purged {} replicas."
  },
  "83d89f43_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "83d89f43_2": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "83d89f43_3": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "83d89f43_4": {
    "exec_flow": "ENTRY → IF_FALSE: locations.isEmpty() → IF_FALSE: locations.size() == 1 && timeOutMs <= 0 → FOREACH: locations → FOREACH_EXIT → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.proxyOp → TRY → IF_TRUE: timeOutMs > 0 → CALL: invokeAll → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → RETURN → EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "83d89f43_5": {
    "exec_flow": "ENTRY → IF_FALSE: locations.isEmpty() → IF_FALSE: locations.size() == 1 && timeOutMs <= 0 → FOREACH: locations → FOREACH_EXIT → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.proxyOp → TRY → IF_FALSE: timeOutMs > 0 → CALL: invokeAll → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "8110ef2e_1": {
    "exec_flow": "ENTRY→IF_TRUE: getServiceStopped()→LOG: LOG.INFO: Service stopped, return null for the storage→RETURN→EXIT",
    "log": "[INFO] Service stopped, return null for the storage"
  },
  "8110ef2e_2": {
    "exec_flow": "ENTRY → TRY → CALL:checkAccess → IF_TRUE:CALL:isDebugEnabled → CALL:debug → CALL:isAdmin → RETURN → EXIT → CATCH:YarnException → CALL:info → RETURN → EXIT",
    "log": "[DEBUG] Verifying the access of user on the timeline domain [INFO] Error when verifying access for user ... on the events of the timeline entity ..."
  },
  "83a5f263_1": {
    "exec_flow": "ENTRY→CALL:ProcfsBasedProcessTree:updateProcessTree→IF_TRUE→CALL:processTree.clear→FOREACH→CALL:constructProcessInfo→FOREACH_EXIT→IF_FALSE→FOREACH→FOREACH_EXIT→WHILE→WHILE_EXIT→FOREACH→CALL:updateJiffy→FOREACH_EXIT→LOG→CALL:debug→IF_TRUE→FOREACH→CALL:constructProcessSMAPInfo→FOREACH_EXIT→EXIT→CALL:CGroupsResourceCalculator:updateProcessTree→EXIT",
    "log": "[DEBUG] MemInfo : key : Value : value [WARN] Sum of stime () and utime () is greater than [WARN] Failed to parse + pid, e [DEBUG] Swap cgroups monitoring is not compiled into the kernel {}"
  },
  "83a5f263_2": {
    "exec_flow": "ENTRY→IF_TRUE: taskProcessId != null→CALL:org.apache.hadoop.yarn.util.WindowsBasedProcessTree:getAllProcessInfoFromShell()→TRY→CALL:shellExecutor.execute→EXCEPTION:execute→CATCH:IOException e→LOG:LOG.ERROR:StringUtils.stringifyException(e)→RETURN→EXIT→IF_TRUE: processesInfoStr != null && processesInfoStr.length() > 0→CALL:org.apache.hadoop.yarn.util.WindowsBasedProcessTree:createProcessInfo(java.lang.String)→FOREACH: processesStr→IF: processStr != null→IF: procInfo.length == procInfoSplitCount→TRY→CALL:ProcessInfo constructor→CALL:parseLong→CALL:parseLong→CALL:parseLong→CALL:allProcs.put→CATCH:NumberFormatException→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable)→TRY_EXIT→IF_EXIT→IF→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→IF_EXIT→FOREACH_EXIT→RETURN→EXIT→FOREACH: allProcessInfo.entrySet()→FOREACH_EXIT→CALL:processTree.clear→EXIT",
    "log": "LOG.error(StringUtils.stringifyException(e)) [DEBUG] Error parsing procInfo. [DEBUG] Expected split length of proc info to be {}. Got {}"
  },
  "6a7d0cf0_1": {
    "exec_flow": "ENTRY → SWITCH: event.getType() → CASE: [NODE_DECOMMISSIONING] → LOG: LOG.DEBUG: {} reported decommissioning, eventNode → CALL: sendRMAppNodeUpdateEventToNonFinalizedApps → BREAK → IF_TRUE: resolver instanceof CachedResolver → CALL: ((CachedResolver) resolver).removeFromCache → EXIT",
    "log": "[DEBUG] {} reported decommissioning, eventNode"
  },
  "6a7d0cf0_2": {
    "exec_flow": "ENTRY → IF_TRUE: node != null → TRY → CALL: ((EventHandler<RMNodeEvent>)node).handle → EXCEPTION: handle → CATCH: Throwable t → LOG: LOG.ERROR: Error in handling event type + event.getType() + for node + nodeId, t → EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for node + nodeId, t"
  },
  "6a7d0cf0_3": {
    "exec_flow": "ENTRY → IF_FALSE: app != null → LOG: LOG.WARN: Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "[WARN] Event + event + sent to absent application + event.getApplicationID()"
  },
  "b4d260b9_1": {
    "exec_flow": "ENTRY → LOG: [DEBUG] Deleting root content → TRY → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified(org.apache.hadoop.fs.Path) → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path) → FOREACH: ls → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$1:<init>(org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem,org.apache.hadoop.fs.FileStatus) → FOREACH_EXIT → CALL: executorService.shutdownNow → RETURN → EXIT",
    "log": "[DEBUG] Deleting root content"
  },
  "b4d260b9_2": {
    "exec_flow": "LOG: [DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()} → TRY → CALL: org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:listStatus",
    "log": "[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}"
  },
  "b4d260b9_3": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → FOREACH: names → CALL: fixName → STEP: Check if name equals \"local\" → LOG: warning \"local is a deprecated filesystem name. Use file:/// instead.\" → STEP: Update name to \"file:///\" → STEP: Check if name does not contain '/' → LOG: warning \"unqualified name is a deprecated filesystem name. Use hdfs://[name]/ instead.\" → STEP: Update name to \"hdfs://[name]\" → STEP: Return updated name → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. [WARN] unqualified name is a deprecated filesystem name. Use hdfs://[name]/ instead."
  },
  "b4d260b9_4": {
    "exec_flow": "ENTRY → LOG: listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom} → IF_FALSE: continuation == null || continuation.isEmpty() → DO_WHILE → TRY → CALL: startTracking → CALL: listPath → CALL: registerResult → CALL: getResponseHeader → CALL: getResult → CALL: parseLastModifiedTime → CALL: close → IF_TRUE: retrievedSchema == null → THROW: AbfsRestOperationException → EXIT",
    "log": "[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}"
  },
  "318cd542_1": {
    "exec_flow": "<entry>org.apache.hadoop.fs.FilterFileSystem:getFileStatus</entry> <call>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal</call> <sequence> <log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <if condition=\"key.length() != 0\"> <try> <call>retrieveMetadata</call> <if condition=\"meta != null\"> <if condition=\"meta.isDirectory()\"> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <call>conditionalRedoFolderRename</call> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </if> <else> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log> <call>updateFileStatusPath</call> <return>EXIT</return> </else> </if> </try> </if> <else> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </else> </sequence>",
    "log": "<log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log>"
  },
  "318cd542_2": {
    "exec_flow": "ENTRY → LOG: log.INFO: Looking for Hadoop tarball for version: + version → IF_TRUE: destinationFile.exists() → LOG: log.INFO: Found tarball at: + destinationFile.getAbsolutePath() → RETURN → EXIT",
    "log": "[INFO] Looking for Hadoop tarball for version: {version} [INFO] Found tarball at: {destinationFile.getAbsolutePath()}"
  },
  "318cd542_3": {
    "exec_flow": "ENTRY → LOG: log.INFO: Looking for Hadoop tarball for version: + version → IF_FALSE: destinationFile.exists() → CALL: org.apache.hadoop.conf.Configuration:get → IF_TRUE: apacheMirror == null → CALL: System.getProperty → IF_FALSE: !destinationDir.exists() → LOG: log.INFO: Downloading tarball from: {downloadURL} to {destinationFile.getAbsolutePath()} → CALL: FileUtils.copyURLToFile → LOG: log.INFO: Completed downloading of Hadoop tarball → RETURN → EXIT",
    "log": "[INFO] Looking for Hadoop tarball for version: {version} [INFO] Downloading tarball from: {downloadURL} to {destinationFile.getAbsolutePath()} [INFO] Completed downloading of Hadoop tarball"
  },
  "318cd542_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → [VIRTUAL_CALL] → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {}"
  },
  "318cd542_5": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → TRY → CALL: notifyListeners → CATCH: Throwable e → LOG: WARN: Exception while notifying listeners of {}, this, e → CALL: recordLifecycleEvent → EXIT → CALL: noteFailure → IF_TRUE: exception != null → LOG: DEBUG: noteFailure, exception → SYNC: failureCause == null → SET failureCause: exception → SET failureState: getServiceState() → LOG: INFO: Service {} failed in state {}, getName(), failureState, exception → CALL: ServiceOperations.stopQuietly",
    "log": "[WARN] Exception while notifying listeners of {} [DEBUG] Config has been overridden during init [DEBUG] noteFailure, exception [INFO] Service {} failed in state {}, getName(), failureState, exception [WARN] When stopping the service {service.getName()}"
  },
  "fff4b2e8_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CATCH: Throwable e → LOG.warn(\"Exception while notifying listeners of {}\", this, e) → EXCEPTION: serviceInit → CATCH: Exception e → CALL: noteFailure → LOG.debug(\"noteFailure\", exception) → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → CALL: ServiceOperations.stopQuietly → EXIT",
    "log": "<log>[DEBUG] Service: {getName()} entered state {getServiceState()}</log> <log>[DEBUG] Config has been overridden during init</log> <log>LOG.debug(\"noteFailure\", exception);</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>org.apache.hadoop.service.ServiceOperations:stopQuietly - WARN When stopping the service {service_name}</log>"
  },
  "c9554217_1": {
    "exec_flow": "<step>ENTRY</step> <step>FOR_INIT</step> <step>FOR_COND: i >= 0</step> <step>CALL:org.apache.hadoop.service.AbstractService:stop()</step> <step>FOR_EXIT</step> <step>IF_TRUE: firstException != null</step> <step>THROW: ServiceStateException.convert(firstException)</step> <step>EXIT</step> <step>LOG: DEBUG Stopping service #i: {service}</step> <step>LOG: DEBUG Ignoring re-entrant call to stop()</step> <step>LOG: WARN Exception while notifying listeners of {}, this, e</step> <step>LOG: DEBUG noteFailure, exception</step> <step>LOG: INFO Service {} failed in state {}, getName(), failureState, exception</step>",
    "log": "<log>[DEBUG] Stopping service #i: {service}</log> <log>[DEBUG] Ignoring re-entrant call to stop()</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>LOG.debug(\"noteFailure\", exception);</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</log>"
  },
  "ddd25ca5_1": {
    "exec_flow": "ENTRY→CALL:readLock.lock→TRY→IF_FALSE:null==rules||rules.isEmpty()→FOREACH:rules→CALL:getPlacementForApp→IF_TRUE:placement!=null→BREAK→EXIT",
    "log": "<!-- No logs in parent or log-merge applicable from child -->"
  },
  "ddd25ca5_2": {
    "exec_flow": "ENTRY→TRY→CALL:getGroups→IF_FALSE:groupList.isEmpty()→CALL:cleanName→IF_TRUE:getParentRule()!=null→LOG:[DEBUG] PrimaryGroup rule: parent rule found: {parentRule.getName()}→CALL:getPlacementForApp→IF_FALSE:parent==null||getQueueManager().getQueue(parent.getQueue())instanceof FSLeafQueue→LOG:[DEBUG] PrimaryGroup rule: parent rule result: {parent.getQueue()}→CALL:org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementForApp→CALL:org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:getPlacementForApp→IF_TRUE:mappings != null && mappings.size() > 0 →TRY→CALL:getAppPlacementContext→IF_TRUE:mappedQueue != null→IF_TRUE:queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME) || queueName.equals(mappedQueue.getQueue()) || overrideWithQueueMappings→LOG: Application {applicationName} mapping [{queueName}] to [{mappedQueue.getQueue()}] override {overrideWithQueueMappings}→RETURN→EXIT",
    "log": "[DEBUG] SecondaryGroupExisting rule: parent rule found: {parentRule.getName()} [DEBUG] SecondaryGroupExisting rule: parent rule result: {parentQueue} [INFO] FSPlacementRule applied [INFO] UserGroupMappingPlacementRule applied [INFO] AppNameMappingPlacementRule applied [INFO] Application {applicationName} mapping [{queueName}] to [{mappedQueue.getQueue()}] override {overrideWithQueueMappings}"
  },
  "ddd25ca5_3": {
    "exec_flow": "ENTRY→IF_TRUE:!isValidQueueName(queueName)→LOG:[ERROR] Specified queue name not valid: '{}'→THROW:new YarnException(\"Application submitted by user \" + user + \"with illegal queue name '\" + queueName + \"'.\")→EXIT",
    "log": "[ERROR] Specified queue name not valid: '{}'"
  },
  "ddd25ca5_4": {
    "exec_flow": "ENTRY→FOREACH:mappings→IF_TRUE:mapping.getType().equals(MappingType.USER)→IF_TRUE:mapping.getSource().equals(CURRENT_USER_MAPPING)→IF_TRUE:mapping.getParentQueue() != null && mapping.getParentQueue().equals(PRIMARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)→IF_TRUE:LOG.isDebugEnabled()→LOG:[DEBUG] Creating placement context for user {} using primary group current user mapping→CALL:getContextForGroupParent→CALL:getPrimaryGroup→RETURN→EXIT",
    "log": "[DEBUG] Creating placement context for user {} using primary group current user mapping"
  },
  "ddd25ca5_5": {
    "exec_flow": "ENTRY→FOREACH:mappings→IF_TRUE:mapping.getType().equals(MappingType.USER)→IF_TRUE:mapping.getSource().equals(CURRENT_USER_MAPPING)→IF_FALSE:mapping.getParentQueue() != null && mapping.getParentQueue().equals(PRIMARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)→IF_TRUE:mapping.getParentQueue() != null && mapping.getParentQueue().equals(SECONDARY_GROUP_MAPPING) && mapping.getQueue().equals(CURRENT_USER_MAPPING)→IF_TRUE:LOG.isDebugEnabled()→LOG:[DEBUG] Creating placement context for user {} using secondary group current user mapping→CALL:getContextForGroupParent→CALL:getSecondaryGroup→RETURN→EXIT",
    "log": "[DEBUG] Creating placement context for user {} using secondary group current user mapping"
  },
  "ddd25ca5_6": {
    "exec_flow": "ENTRY→DEBUG: Application tag based placement is enabled, checking for 'userid'...→IF:userid found→DEBUG: Found 'userid' '{}' in application tag→CHECK:placement rules→IF:rule exists→WARN: No rule was found for user '{}'→EXIT",
    "log": "[DEBUG] Application tag based placement is enabled, checking for 'userid' among the application tags [DEBUG] Found 'userid' '{}' in application tag [WARN] No rule was found for user '{}'"
  },
  "ddd25ca5_7": {
    "exec_flow": "ENTRY→DEBUG: Application tag based placement is enabled, checking for 'userid'...→DEBUG: Found 'userid' '{}' in application tag→CHECK: Proxy user's access→IF:access denied→WARN: Proxy user '{}' does not have access to queue '{}'. Placement done for '{}'",
    "log": "[DEBUG] Application tag based placement is enabled, checking for 'userid' among the application tags [DEBUG] Found 'userid' '{}' in application tag [WARN] Proxy user '{}' from application tag does not have access to queue '{}'. The placement is done for user '{}'"
  },
  "ddd25ca5_8": {
    "exec_flow": "ENTRY→DEBUG: Application tag based placement is enabled, checking for 'userid'...→IF:userid not found→WARN: 'userid' was not found in application tags→EXIT",
    "log": "[DEBUG] Application tag based placement is enabled, checking for 'userid' among the application tags [WARN] 'userid' was not found in application tags"
  },
  "8dff4acb_1": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → IF_TRUE: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → RETURN → EXIT → VIRTUAL_CALL → ENTRY → CALL: ensureInitialized → IF_FALSE: !isInitialized() → EXIT → VIRTUAL_CALL → ENTRY → CALL: getAuthenticationMethod → IF_TRUE: overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → TRY → CALL: HadoopKerberosName.setConfiguration → TRY → CALL: getLong → CALL: getBoolean → IF_TRUE: !(groups instanceof TestingGroups) → CALL: getUserToGroupsMappingService → IF_FALSE: metrics.getGroupsQuantiles == null → EXIT → VIRTUAL_CALL → ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → EXIT → VIRTUAL_CALL → ENTRY → CALL: getProps → CALL: addAll → FOREACH: keys → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → CALL: substituteVars → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → RETURN",
    "log": "<log>[DEBUG] Creating new Groups object</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log>"
  },
  "cc0b428d_1": {
    "exec_flow": "ENTRY→CALL:DeleteSnapshotOp.getInstance→CALL:logRpcIds→CALL:logEdit→ENTRY→LOG:LOG.DEBUG:doEditTx() op={} txid={}→CALL:org.slf4j.Logger:debug→TRY→CALL:editLogStream.write→EXCEPTION:write→CATCH:IOException ex→CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT→EXIT",
    "log": "[DEBUG] Logging RPC IDs [INFO] Edit logged [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "cc0b428d_2": {
    "exec_flow": "ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "cc0b428d_3": {
    "exec_flow": "ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_COND: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid <= synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} → IF_FALSE: lastJournalledTxId <= synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}"
  },
  "22cb6301_1": {
    "exec_flow": "ENTRY→CALL:mkdir→CALL:getUMask→CALL:resolve→ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→TRY→FOR_EXIT→RETURN→EXIT→EXIT",
    "log": "[INFO] Resolving file system path [DEBUG] Attempting symlink resolution"
  },
  "22cb6301_2": {
    "exec_flow": "ENTRY→CALL:mkdir→CALL:getUMask→CALL:resolve→ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→CATCH:UnresolvedLinkException→CALL:qualifySymlinkTarget→TRY→FOR_COND: isLink→FOR_EXIT→RETURN→EXIT→EXIT",
    "log": "[INFO] Resolving file system path [WARN] Unresolved link encountered"
  },
  "22cb6301_3": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT",
    "log": "[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask. [DEBUG] Handling deprecation for all properties in config... FOREACH: [DEBUG] Handling deprecation for (String)item"
  },
  "12146644_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "051b19fa_1": {
    "exec_flow": "ENTRY → TRY → CALL:getBlocks(datanode, size, minBlockSize) → IF_TRUE: getBlocksRateLimiter != null → CALL: getBlocksRateLimiter.acquire → IF_TRUE: requestToStandby && nsId != null && HAUtil.isHAEnabled(config, nsId) → FOREACH: namenodes → IF_TRUE: proxy.getHAServiceState().equals(HAServiceProtocol.HAServiceState.STANDBY) → ASSIGN: standbyAddress, nnproxy, isRequestStandby → BREAK → IF_FALSE: nnproxy == null → LOG: [WARN] Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way → ASSIGN: nnproxy → RETURN: nnproxy.getBlocks(datanode, size, minBlockSize) → FINALLY → IF_TRUE: isRequestStandby → LOG: [INFO] Request #getBlocks to Standby NameNode success, remoteAddress: {} → EXIT",
    "log": "<log>[WARN] Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way.</log> <log>[INFO] Request #getBlocks to Standby NameNode success, remoteAddress: {}</log>"
  },
  "a5a3ca95_1": {
    "exec_flow": "<entry>org.apache.hadoop.util.Preconditions:checkArgument</entry> <call>org.slf4j.Logger:warn</call> <call>org.apache.hadoop.fs.adl.AdlFileSystem:getHomeDirectory</call> <call>org.apache.hadoop.conf.Configuration:getBoolean</call> <call>org.apache.hadoop.fs.adl.AdlFileSystem:getNonEmptyVal</call> <call>org.apache.hadoop.fs.adl.AdlFileSystem:getAccessTokenProvider</call> <entry>org.apache.hadoop.conf.Configuration:get</entry> <log>Handling deprecation for all properties in config...</log> <call>getProps</call> <call>addAll</call> <foreach>keys</foreach> <log>Handling deprecation for (String)item</log> <call>handleDeprecation</call> <foreach_exit></foreach_exit> <call>LOG_DEPRECATION.info</call> <call>org.slf4j.Logger:info</call> <log>org.apache.hadoop.fs.FileSystem:fixName</log> <exit></exit>",
    "log": "<log_entry> <template>LOG.warn(\"Got exception when getting Hadoop user name. Set the user name to 'hadoop'.\")</template> <level>WARN</level> </log_entry> <log_entry> <template>LOG.info(\"No valid ADL SDK timeout configured: using SDK default.\")</template> <level>INFO</level> </log_entry> <log_entry> <template>[DEBUG] Handling deprecation for all properties in config...</template> <level>DEBUG</level> </log_entry> <log_entry> <template>[DEBUG] Handling deprecation for (String)item</template> <level>DEBUG</level> </log_entry> <log_entry> <template>[WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead.</template> <level>WARN</level> </log_entry> <log_entry> <template>[WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.</template> <level>WARN</level> </log_entry> <log_entry> <template>[INFO] No value for {key} found in conf file.</template> <level>INFO</level> </log_entry>"
  },
  "2b88cbc9_1": {
    "exec_flow": "<entry>org.apache.hadoop.service.AbstractService:start()</entry> <conditional> <condition>!isInState(STATE.STARTED) && stateModel.enterState(STATE.STARTED) != STATE.STARTED && serviceStart succeeded && isInState(STATE.STARTED)</condition> <sequence> <synchronized>stateChangeLock</synchronized> <try> <call>currentTimeMillis</call> <call>serviceStart</call> <conditional> <condition>isInState(STATE.STARTED)</condition> <call>debug</call> <call>notifyListeners</call> </conditional> </try> <catch> <exception>Exception</exception> <call>noteFailure</call> <call>ServiceOperations.stopQuietly</call> <throw>ServiceStateException.convert(e)</throw> </catch> </sequence> </conditional>",
    "log": "<log> <level>DEBUG</level> <template>Service {} is started</template> </log> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "2b88cbc9_2": {
    "exec_flow": "<step>LOG.debug(\"noteFailure\", exception);</step> <step>if (exception == null) return;</step> <step>synchronized (this) {</step> <step>if (failureCause == null) {</step> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> <step>}</step> <step>}</step>",
    "log": "<log> <level>DEBUG</level> <template>noteFailure</template> </log> <log> <level>INFO</level> <template>Service {} failed in state {}</template> </log>"
  },
  "4d6dc702_1": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:getRaw→ENTRY→LOG:Handling deprecation for all properties in config... →CALL:getProps→CALL:addTags→IF_TRUE: overlay != null→CALL:putAll→IF_TRUE: backup != null→FOREACH: overlay.entrySet()→FOREACH_EXIT→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT →ENTRY→CALL: namedCallbacks.put→CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder→CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource→CALL:checkNotNull→CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>→CALL:put→CALL:start→IF_TRUE:startMBeans→CALL:startMBeans→LOG:LOG.DEBUG:Registered source + name→EXIT",
    "log": "<log> [ERROR] Error in handling event type [INFO] Exiting, bbye.. [ERROR] Returning, interrupted : [Exception Detail] [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Registering the metrics source [DEBUG] Registered source + name </log>"
  },
  "f9b07ff1_1": {
    "exec_flow": "<call_chain> <call site=\"org.apache.hadoop.io.DefaultStringifier:<init>\" method=\"ENTRY\"/> <call site=\"org.apache.hadoop.io.serializer.SerializationFactory:add\" method=\"TRY\"/> <call site=\"org.apache.hadoop.io.serializer.SerializationFactory:add\" method=\"CALL:org.apache.hadoop.util.ReflectionUtils:newInstance\"/> <call site=\"org.apache.hadoop.util.ReflectionUtils:newInstance\" method=\"CALL:org.apache.hadoop.util.ReflectionUtils:setConf\"/> <call site=\"org.apache.hadoop.io.serializer.SerializationFactory:add\" method=\"EXIT\"/> <call site=\"org.apache.hadoop.io.serializer.SerializationFactory:add\" method=\"ENTRY\"/> <call site=\"org.apache.hadoop.io.serializer.SerializationFactory:add\" method=\"TRY\"/> <seq> ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT </seq> </call_chain>",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "f9b07ff1_2": {
    "exec_flow": "<seq> ENTRY → LOG:Error formatting message → EXIT </seq>",
    "log": "<log>[DEBUG] Error formatting message</log>"
  },
  "f7bf8632_1": {
    "exec_flow": "ENTRY→IF_TRUE: attempt == null→LOG: DEBUG: Request for appInfo of unknown attempt {}, appAttemptId→RETURN→EXIT",
    "log": "<log>[DEBUG] Request for appInfo of unknown attempt {}</log>"
  },
  "f7bf8632_2": {
    "exec_flow": "ENTRY→CALL:rememberTargetTransitions→CALL:getState→IF_TRUE:conf.getBoolean(YarnConfiguration.APPLICATION_HISTORY_ENABLED, YarnConfiguration.DEFAULT_APPLICATION_HISTORY_ENABLED)→CALL:setTrackingUrlToAHSPage→SWITCH:event.getType()→CASE:[LAUNCH_FAILED]→CALL:append→CALL:ApplicationAttemptStateData.newInstance→CALL:RMStateStore:updateApplicationAttemptState→LOG:INFO: Updating application attempt with final state→BREAK→EXIT",
    "log": "<log>[INFO] Updating application attempt with final state</log>"
  },
  "f7bf8632_3": {
    "exec_flow": "ENTRY→CALL:rememberTargetTransitions→CALL:getState→IF_FALSE:conf.getBoolean(YarnConfiguration.APPLICATION_HISTORY_ENABLED, YarnConfiguration.DEFAULT_APPLICATION_HISTORY_ENABLED)→CALL:setTrackingUrlToRMAppPage→SWITCH:event.getType()→CASE:[CONTAINER_FINISHED]→CALL:append→CALL:ApplicationAttemptStateData.newInstance→CALL:RMStateStore:updateApplicationAttemptState→LOG:INFO: Updating application attempt with final state→BREAK→EXIT",
    "log": "<log>[INFO] Updating application attempt with final state</log>"
  },
  "f7bf8632_4": {
    "exec_flow": "ENTRY → CALL:maybeInitBuilder → IF_TRUE:attemptTokens == null → CALL:clearAppAttemptTokens → RETURN→EXIT → CALL:org.slf4j.Logger:error → EXIT",
    "log": "<log>[ERROR] Failed to convert Credentials to ByteBuffer.</log>"
  },
  "f7bf8632_5": {
    "exec_flow": "ENTRY → TRY → IF_TRUE:credentials != null → CALL:writeTokenStorageToStream → EXCEPTION:IOException → CALL:org.slf4j.Logger:error → EXIT",
    "log": "<log>[ERROR] Failed to convert Credentials to ByteBuffer.</log>"
  },
  "c6d15832_1": {
    "exec_flow": "ENTRY → CALL:createOutputCommitter → LOG: \"OutputCommitter set in config ...\" → IF_TRUE:newApiCommitter → CALL:org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:<init> → CALL:org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:getOutputFormatClass() → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → LOG: \"Handling deprecation for all properties in config...\" → CALL:getProps → CALL:addAll → FOREACH:keys → LOG: \"Handling deprecation for (String)item\" → CALL:handleDeprecation → FOREACH_EXIT → IF → CALL:readSplitMetaInfo → CALL:getNumReduceTasks → CALL:setupJob → CALL:getMapTaskRunnables → CALL:createMapExecutor → CALL:runTasks → LOG: \"Waiting for map tasks\" → LOG: \"map task executor complete.\" → IF → CALL:getReduceTaskRunnables → CALL:createReduceExecutor → CALL:runTasks → LOG: \"Waiting for reduce tasks\" → LOG: \"reduce task executor complete.\" → FINALLY → CALL:removeAll → CALL:commitJob → IF → CALL:localRunnerNotification → EXIT",
    "log": "<log> <level>INFO</level> <message>OutputCommitter set in config ...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for all properties in config...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for (String)item</message> </log> <log> <level>INFO</level> <message>Waiting for map tasks</message> </log> <log> <level>INFO</level> <message>map task executor complete.</message> </log> <log> <level>INFO</level> <message>Waiting for reduce tasks</message> </log> <log> <level>INFO</level> <message>reduce task executor complete.</message> </log> <log> <level>INFO</level> <message>OutputCommitter is ...</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)</message> </log> <log> <level>WARN</level> <message>JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)</message> </log> <log> <level>INFO</level> <message>Failed to createOutputCommitter</message> </log> <log> <level>INFO</level> <message>Error cleaning up job: [Exception Detail]</message> </log> <log> <level>WARN</level> <message>[JobID] [Exception Detail]</message> </log> <log> <level>WARN</level> <message>Error cleaning up [JobID]: [IOException Detail]</message> </log>"
  },
  "c6d15832_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:enterCommitter → CALL:createStageConfig → CALL:withOperations → CALL:build → CALL:new SetupJobStage(stageConfig).apply → CALL:logIOStatisticsAtDebug → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Committer statistics</message> </log>"
  },
  "c6d15832_3": {
    "exec_flow": "ENTRY → IF_TRUE:hasOutputPath → CALL:getJobAttemptPath → CALL:getFileSystem → CALL:mkdirs → IF_TRUE:!fs.mkdirs → CALL:LOG.ERROR → EXIT",
    "log": "<log> <level>ERROR</level> <message>Mkdirs failed to create ${jobAttemptPath}</message> </log>"
  },
  "c6d15832_4": {
    "exec_flow": "ENTRY → IF_FALSE:hasOutputPath → CALL:LOG.WARN → EXIT",
    "log": "<log> <level>WARN</level> <message>Output Path is null in setupJob()</message> </log>"
  },
  "c6d15832_5": {
    "exec_flow": "ENTRY → IF_FALSE:p.equals(\"/\") → IF_TRUE:status.isDirectory() → IF_TRUE:!recursive → CALL:listStatus → ENTRY → IF_FALSE:LOG.isDebugEnabled() → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_TRUE:LOG.isDebugEnabled() → LOG:Adding: rd (not a dir): {path} → CALL:add → CALL:toArray → CALL:size → CALL:size → RETURN → EXIT → CALL:getPath → CALL:getPath → IF_FALSE:statuses.length > 0 → CALL:maybeAddTrailingSlash → CALL:deleteObject → CALL:createFakeDirectoryIfNecessary → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Adding: rd (not a dir): {path}</message> </log>"
  },
  "055357aa_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {} [DEBUG] delete filesystem: {} path: {} recursive: {} [DEBUG] Couldn't delete {} - does not exist: {} [DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem"
  },
  "2003f395_1": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream</entry> <if_true>nodes.length == 0</if_true> <log>[INFO] nodes are empty for write pipeline of + block</log> <return>false</return> <exit/>",
    "log": "[INFO] nodes are empty for write pipeline of + block"
  },
  "2003f395_2": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream</entry> <if_false>nodes.length == 0</if_false> <if_false>LOG.isDebugEnabled()</if_false> <call>org.apache.hadoop.hdfs.DataStreamer:createSocketForPipeline</call> <call>persistBlocks.set</call> <while>true</while> <call>createSocketForPipeline</call> <call>saslDataTransferClient.socketSend</call> <call>getSmallBufferSize</call> <call>getPinnings</call> <call>new Sender</call> <ioexception>InvalidEncryptionKeyException</ioexception> <log>[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to node[0]</log> <call>org.apache.hadoop.hdfs.DFSClient:clearDataEncryptionKey</call> <log>[DEBUG] Clearing encryption key</log> <exit/>",
    "log": "[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to node[0] [DEBUG] Clearing encryption key"
  },
  "2003f395_3": {
    "exec_flow": "ENTRY→CALL:checkTrustAndSend→NEW:IOStreamPair→RETURN→EXIT <log>[DEBUG] SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}</log> <log>[DEBUG] SASL client doing encrypted handshake for addr = {}, datanodeId = {}</log> <log>[DEBUG] SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}</log>",
    "log": "[DEBUG] SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {} [DEBUG] SASL client doing encrypted handshake for addr = {}, datanodeId = {} [DEBUG] SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}"
  },
  "2003f395_4": {
    "exec_flow": "ENTRY→IF_FALSE:favoredNodes==null→FOR_INIT→FOR_COND:i<nodes.length→CALL:nodes[i].getXferAddrWithHostname→CALL:Logger.debug→FOR_EXIT→IF_TRUE:!favoredSet.isEmpty()→LOG:LOG.WARN:These favored nodes were specified but not chosen:+favoredSet+Specified favored nodes:+Arrays.toString(favoredNodes)→RETURN→EXIT <log>[DEBUG] {} was chosen by name node (favored={}).</log> <log>[WARN] These favored nodes were specified but not chosen: + favoredSet + Specified favored nodes: + Arrays.toString(favoredNodes)</log>",
    "log": "[DEBUG] {} was chosen by name node (favored={}). [WARN] These favored nodes were specified but not chosen: + favoredSet + Specified favored nodes: + Arrays.toString(favoredNodes)"
  },
  "2003f395_5": {
    "exec_flow": "ENTRY→IF_TRUE: sock != null→TRY→CALL: close→CATCH: IOException ignored→LOG: LOG.DEBUG: Ignoring exception while closing socket, ignored→EXIT",
    "log": "[DEBUG] Ignoring exception while closing socket"
  },
  "c770605e_1": {
    "exec_flow": "<seq> ENTRY→CALL:getTrimmed→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "c770605e_2": {
    "exec_flow": "<seq> ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "3329b409_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open(org.apache.hadoop.fs.Path,int)</step> <step>RETURN</step> <step>EXIT</step> <step>readTokenStorageStream</step> <step>readProto</step>",
    "log": "<log>Bad header found in token storage.</log> <log>Unsupported format {format}</log>"
  },
  "3329b409_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: scheme == null && authority == null</step> <step>IF_TRUE: scheme != null && authority == null</step> <step>CALL:getDefaultUri(org.apache.hadoop.conf.Configuration)</step> <step>IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null</step> <step>IF_TRUE: conf.getBoolean(disableCacheName, false)</step> <step>LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\")</step> <step>CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log>"
  },
  "3329b409_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: scheme == null && authority == null</step> <step>IF_FALSE: scheme != null && authority == null</step> <step>IF_TRUE: conf.getBoolean(disableCacheName, false)</step> <step>LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\")</step> <step>CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration)</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log>"
  },
  "3329b409_4": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:closeables</step> <step>IF(c != null)</step> <step>TRY</step> <step>CALL:c.close()</step> <step>CATCH:Throwable</step> <step>IF(logger != null)</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Exception in closing {}</log>"
  },
  "0dde253d_1": {
    "exec_flow": "ENTRY→CALL: ensureInitialized→IF_TRUE: !isInitialized()→SYNC: UserGroupInformation.class→IF_TRUE: !isInitialized() →LOG: Listing status for job →LOG: Handling deprecation for all properties in config... →CALL: getProps→CALL: addAll→FOREACH: keys →LOG: Handling deprecation for (String)item →CALL: handleDeprecation→FOREACH_EXIT→CALL: LOG_DEPRECATION.info→EXIT →CALL: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL: warn →LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY →CALL: get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL: warn →LOG: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) →ENTRY → IF_FALSE: dirs.length == 0 → CALL: TokenCache.obtainTokensForNamenodes → IF_TRUE: UserGroupInformation.isSecurityEnabled() → CALL: TokenCache.obtainTokensForNamenodesInternal → CALL: mergeBinaryTokens → CALL: fs.addDelegationTokens → IF_TRUE: !isTokenRenewalExcluded(fs, conf) → IF_TRUE: StringUtils.isEmpty(renewer) → THROW: IOException → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS) → LOG: LOG.INFO: Total input files to process : + result.size() → RETURN → EXIT",
    "log": "[DEBUG] Listing status for job [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [INFO] Total input files to process : ... [DEBUG] Time taken to get FileStatuses: ..."
  },
  "0dde253d_2": {
    "exec_flow": "ENTRY → IF_FALSE: dirs.length == 0 → CALL: TokenCache.obtainTokensForNamenodes → CALL: getInputPaths → CALL: Configuration.getInt → IF_TRUE: numThreads == 1 → CALL: singleThreadedListStatus → CALL: stop → IF_FALSE: LOG.isDebugEnabled() → LOG: LOG.INFO: Total input files to process : + result.size() → RETURN → EXIT",
    "log": "[INFO] Total input files to process : ..."
  },
  "0dde253d_3": {
    "exec_flow": "ENTRY → IF_FALSE: dirs.length == 0 → CALL: TokenCache.obtainTokensForNamenodes → CALL: getInputPaths → CALL: Configuration.getInt → IF_FALSE: numThreads == 1 → CALL: new LocatedFileStatusFetcher → TRY → CALL: getFileStatuses → CALL: newArrayList → CALL: stop → IF_FALSE: LOG.isDebugEnabled() → LOG: LOG.INFO: Total input files to process : + result.size() → RETURN → EXIT",
    "log": "[INFO] Total input files to process : ..."
  },
  "0dde253d_4": {
    "exec_flow": "ENTRY → IF_FALSE: dirs.length == 0 → CALL: TokenCache.obtainTokensForNamenodes → CALL: getInputPaths → CALL: Configuration.getInt → IF_FALSE: numThreads == 1 → CALL: new LocatedFileStatusFetcher → TRY → CALL: getFileStatuses → CALL: newArrayList → CALL: stop → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS) → LOG: LOG.INFO: Total input files to process : + result.size() → RETURN → EXIT",
    "log": "[DEBUG] Time taken to get FileStatuses: ... [INFO] Total input files to process : ..."
  },
  "43fbb0be_1": {
    "exec_flow": "<step>ENTRY</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>CATCH: Throwable e</step> <step>LOG: WARN - Exception while notifying listeners of this</step> <step>EXIT</step>",
    "log": "<log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "47b8f656_1": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.debug→CALL:org.slf4j.Logger:debug→SYNC:serviceList→CALL:serviceList.add→EXIT",
    "log": "[DEBUG] Adding service + service.getName()"
  },
  "c47b79f6_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG:[INFO] message → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3dbbb0d1_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:DEBUG:S3GetFileStatus {}</step> <step>CALL:Preconditions.checkArgument</step> <step>IF_TRUE:!key.isEmpty() && !key.endsWith(\"/\") && probes.contains(StatusProbeEnum.Head)</step> <step>TRY</step> <step>CALL:getObjectMetadata</step> <step>LOG:DEBUG:Found exact file: normal file {}</step> <step>CALL:getContentLength</step> <step>CALL:getUserMetaDataOf</step> <step>IF_FALSE:isCSEEnabled && meta.getUserMetaDataOf(Headers.CRYPTO_CEK_ALGORITHM) != null && contentLength ≥ CSE_PADDING_LENGTH</step> <step>CALL:dateToLong</step> <step>CALL:getDefaultBlockSize</step> <step>CALL:getETag</step> <step>CALL:getVersionId</step> <step>NEW:S3AFileStatus</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] S3GetFileStatus {}</log_entry> <log_entry>[DEBUG] Found exact file: normal file {}</log_entry>"
  },
  "3dbbb0d1_2": {
    "exec_flow": "<step>ENTRY</step> <step>[VIRTUAL_CALL] getFileStatus</step> <step>CALL:checkPathIsSlash</step> <step>NEW:FileStatus</step> <step>CALL:getShortUserName</step> <step>CALL:getPrimaryGroupName</step> <step>CALL:getGroups</step> <step>IF_TRUE:groups.isEmpty()</step> <step>THROW:new IOException(\"There is no primary group for UGI \" + this)</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Failed to get groups for user {}</log_entry>"
  },
  "3dbbb0d1_3": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>CALL:setLogEnabled</step> <step>CALL:getObjectMetadata(request)</step> <step>EXCEPTION:getObjectMetadata</step> <step>CATCH:OSSException osse</step> <step>LOG:DEBUG Exception thrown when get object meta: + key + , exception: + osse</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log_entry>"
  },
  "cc27d26c_1": {
    "exec_flow": "ENTRY → TRY → CALL: ensureInitialized → IF_FALSE: subject == null || subject.getPrincipals(User.class).isEmpty() → NEW: UserGroupInformation → RETURN → TRY → CALL: getCurrentUser → CATCH → LOG: LOG.WARN: Couldn't get current user → LOG: LOG.WARN: Exception + msg → CALL: RMAuditLogger.logFailure → THROW: IOException → EXIT",
    "log": "<log_entry>[WARN] Couldn't get current user</log_entry> <log_entry>[WARN] Exception + msg</log_entry>"
  },
  "cc27d26c_2": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → IF_TRUE: !isInitNodeLabelStoreInProgress() → CALL: checkRemoveFromClusterNodeLabelsOfQueue → CALL: cloneNodeMap → CALL: removeFromClusterNodeLabels → FOREACH: nodeCollections.entrySet() → FOREACH_EXIT → FOREACH: labelsToRemove → FOREACH_EXIT → IF_TRUE: null != dispatcher → CALL: dispatcher.getEventHandler().handle → CALL: org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event) → LOG: LOG.INFO: Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ] → WAIT → EXIT → CALL: updateResourceMappings → CALL: writeLock.unlock → EXIT",
    "log": "<log_entry>[INFO] Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ]</log_entry>"
  },
  "cc27d26c_3": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isInfoEnabled() → CALL: org.slf4j.Logger:isInfoEnabled() → CALL: org.slf4j.Logger:info(java.lang.String) → CALL: createSuccessLog(user, operation, target, null, null, null, null) → EXIT",
    "log": "<log_entry>[INFO] createSuccessLog(user, operation, target, null, null, null, null)</log_entry>"
  },
  "f703ecbd_1": {
    "exec_flow": "ENTRY→CALL: namedCallbacks.put → CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder → CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource → CALL:checkNotNull→CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>→ CALL:put→CALL:start→IF_TRUE:startMBeans→CALL:startMBeans→ LOG:LOG.DEBUG:Registering the metrics source→LOG:LOG.DEBUG:Registered source + name→EXIT",
    "log": "[DEBUG] Registering the metrics source [DEBUG] Registered source + name"
  },
  "4d97c0ff_1": {
    "exec_flow": "ENTRY → SWITCH: event.getType() → CASE: [NODE_UPDATE] → IF_FALSE: !(event instanceof NodeUpdateSchedulerEvent) → CALL: nodeMonitor.updateNode → CALL: org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode → LOG: LOG.DEBUG: Node update event from: {}, rmNode.getNodeID() → IF_TRUE: opportunisticContainersStatus==null → CALL: newInstance → CALL: writeLock.lock → TRY → IF_TRUE: currentNode==null → IF_TRUE: rmNode.getState()!=NodeState.DECOMMISSIONING&&(estimatedQueueWaitTime!=-1 || comparator==LoadComparator.QUEUE_LENGTH) → CALL: this.clusterNodes.put → CALL: LOG.INFO: Inserting ClusterNode [ + rmNode.getNodeID() + ] + with queue wait time [ + estimatedQueueWaitTime + ] and + wait queue length [ + waitQueueLength + ] → CALL: writeLock.unlock → EXIT",
    "log": "[DEBUG] Node update event from: {rmNode.getNodeID()} [INFO] Inserting ClusterNode [{rmNode.getNodeID()}] with queue wait time [{estimatedQueueWaitTime}] and wait queue length [{waitQueueLength}]"
  },
  "4d97c0ff_2": {
    "exec_flow": "ENTRY → LOG: LOG.INFO: Node delete event for: {}, removedRMNode.getNode().getName() → CALL: this.nodeByHostName.remove → CALL: removeFromNodeIdsByRack → CALL: writeLock.lock → TRY → CALL: remove → CALL: getNodeID → CALL: getNodeID → CALL: writeLock.unlock → IF_TRUE: LOG.isDebugEnabled() → CALL: LOG.DEBUG: Delete ClusterNode: + removedRMNode.getNodeID() → EXIT",
    "log": "[INFO] Node delete event for: {} [DEBUG] Delete ClusterNode:"
  },
  "4d97c0ff_3": {
    "exec_flow": "ENTRY → IF_TRUE: component == null → LOG: LOG.ERROR: No component exists for + event.getName() → RETURN → EXIT",
    "log": "[ERROR] No component exists for + event.getName()"
  },
  "4d97c0ff_4": {
    "exec_flow": "ENTRY → IF_FALSE: component == null → TRY → CALL: component.handle → EXCEPTION: component.handle → CATCH: Throwable t → LOG: LOG.ERROR: MessageFormat.format([COMPONENT {0}]: Error in handling event type {1}, component.getName(), event.getType()), t → EXIT",
    "log": "[ERROR] MessageFormat.format([COMPONENT {0}]: Error in handling event type {1}, component.getName(), event.getType()), t"
  },
  "4d97c0ff_5": {
    "exec_flow": "ENTRY → IF_FALSE: LOG.isDebugEnabled() → TRY → CALL: writeLock.lock → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState!=getInternalState() → LOG: LOG.INFO: taskId + Task Transitioned from + oldState + to + getInternalState() → CALL: writeLock.unlock → EXIT",
    "log": "[INFO] taskId + Task Transitioned from + oldState + to + getInternalState()"
  },
  "61bb83ba_1": {
    "exec_flow": "ENTRY → CALL: rpcServer.checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "61bb83ba_2": {
    "exec_flow": "ENTRY → CALL: rpcServer.checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "61bb83ba_3": {
    "exec_flow": "ENTRY → CALL: rpcServer.checkOperation → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "61bb83ba_4": {
    "exec_flow": "ENTRY → CALL: rpcServer.checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "cf0ef4c4_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → CALL: fs.open → [VIRTUAL_CALL] → ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → CALL: substituteVars → LOG: Unexpected SecurityException in Configuration → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → RETURN → FOREACH_EXIT → RETURN → EXIT → PRINT: \"Reading state from \" + stateFile.toString() → CALL: read → RETURN: true",
    "log": "<!-- Merged log sequence --> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "cf0ef4c4_2": {
    "exec_flow": "ENTRY → IF_TRUE: logAtInfo → CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object) → PRINT: [INFO] {} → EXIT",
    "log": "<log>[INFO] {}</log>"
  },
  "cf0ef4c4_3": {
    "exec_flow": "ENTRY → IF_FALSE: logAtInfo → IF_TRUE: log.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → PRINT: [DEBUG] {} → EXIT",
    "log": "<log>[DEBUG] {}</log>"
  },
  "cf0ef4c4_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → PRINT: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log>"
  },
  "e93f3c6e_1": {
    "exec_flow": "<step> <call>Parent.ENTRY</call> <log>Start execution</log> </step> <step> <call>Bypassing cache to create filesystem</call> <log>[DEBUG] Bypassing cache to create filesystem {uri}</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log> </step> <step> <call>Failed to initialize filesystem</call> <log>[WARN] Failed to initialize filesystem {}: {uri}, {e.toString()}</log> <log>[DEBUG] Failed to initialize filesystem</log> </step> <step> <entry>org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:getTotalNumRecords()</entry> <virtual_call>[VIRTUAL_CALL] org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext:acquire(org.apache.hadoop.mapreduce.TaskAttemptContext)</virtual_call> <call>LOG.INFO \"Acquiring pre-assigned chunk: \" + acquiredFilePath</call> <call>LOG.INFO taskId + \" acquired \" + chunkFile.getPath()</call> </step> <step> <call>ENTRY</call> <log>[DEBUG] Handling deprecation for all properties in config...</log> </step> <step> <call>FOREACH:keys</call> <log>[DEBUG] Handling deprecation for (String)item</log> <call>CALL:handleDeprecation</call> <call>FOREACH_EXIT</call> <call>RETURN</call> <log>WARN: Unexpected SecurityException in Configuration</log> <call>EXIT</call> </step> <step> <call>ENTRY</call> <log>[WARN] Bad checksum at {position}. Skipping entries.</log> </step> <step> <call>initialize</call> <log>[INFO] Got brand-new decompressor [<default extension>]</log> </step>",
    "log": "<log>Start execution</log> <log>[DEBUG] Bypassing cache to create filesystem {uri}</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log> <log>[WARN] Failed to initialize filesystem {}: {uri}, {e.toString()}</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[INFO] Acquiring pre-assigned chunk: {acquiredFilePath}</log> <log>[INFO] {taskId} acquired {chunkFile.getPath()}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>WARN: Unexpected SecurityException in Configuration</log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log>"
  },
  "21ca7ec4_1": {
    "exec_flow": "ENTRY→IF_FALSE:key.length()==0→CALL:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isDirectory()→LOG:[DEBUG] Path {f} is a folder., f.toString()→CALL:conditionalRedoFolderRename→RETURN→EXIT ENTRY→IF_FALSE:key.length()==0→CALL:retrieveMetadata→IF_TRUE:meta!=null→IF_FALSE:meta.isDirectory()→LOG:[DEBUG] Found the path: {f} as a file., f.toString()→CALL:updateFileStatusPath→RETURN→EXIT ENTRY→IF_TRUE: key.length() == 0→NEW: OSSFileStatus→RETURN→EXIT ENTRY→TRY→CALL:openConnection→CALL:setRequestMethod→CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:makeQualified→EXCEPTION:setRequestMethod→CATCH:Exception ex→THROW:new IOException(ex)→EXIT",
    "log": "<log>[DEBUG] Getting the file status for {f}</log> <log>[DEBUG] Path {f} is a folder.</log> <log>[DEBUG] Found the path: {f} as a file.</log> <log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "9e7e0d7e_1": {
    "exec_flow": "<log>[DEBUG] Failed to contact AM/History for job retrying..</log> <log>[WARN] ClientServiceDelegate invoke call interrupted</log>",
    "log": "[DEBUG] Failed to contact AM/History for job retrying.. [WARN] ClientServiceDelegate invoke call interrupted"
  },
  "9e7e0d7e_2": {
    "exec_flow": "ENTRY → IF_TRUE: hsProxy == null → TRY → CALL:instantiateHistoryProxy → IF_FALSE:StringUtils.isEmpty(serviceAddr) → CALL:get(java.lang.String) → LOG:debug(Connecting to HistoryServer at: + serviceAddr) → CALL:create(org.apache.hadoop.conf.Configuration) → LOG:debug(Connected to HistoryServer at: + serviceAddr) → CALL:getCurrentUser() → CALL:doAs(java.security.PrivilegedAction) → NEW:PrivilegedAction<MRClientProtocol> → CALL:getProxy → CALL:createSocketAddr → RETURN → EXIT",
    "log": "[DEBUG] Connecting to HistoryServer at: serviceAddr [DEBUG] Connected to HistoryServer at: serviceAddr [WARN] Could not connect to History server."
  },
  "0fa02022_1": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration →CALL:getRaw→ENTRY→LOG:Handling deprecation for all properties in config... →CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item →CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→FOREACH:names →CALL:getProps→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "013f7da7_1": {
    "exec_flow": "ENTRY → IF_FALSE:isInState(STATE.STARTED) → SYNC:stateChangeLock → IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL:currentTimeMillis → CALL:serviceStart → IF_TRUE:isInState(STATE.STARTED) → CALL:debug → CALL:notifyListeners → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC:this → CALL:toArray → CALL:size → FOREACH:callbacks → CALL:stateChanged → FOREACH_EXIT → CATCH:Throwable e → LOG:LOG.WARN:Exception while notifying listeners of {}, this, e → RETURN → EXIT → CALL:stopQuietly → LOG:When stopping the service {serviceName} → VIRTUAL_CALL → org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager$WriterFlushTask:run()",
    "log": "<log_entry> <level>DEBUG</level> <template>Service {} is started</template> </log_entry> <log_entry> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>noteFailure</template> </log_entry> <log_entry> <level>INFO</level> <template>Service {} failed in state {}</template> </log_entry> <log_entry> <level>WARN</level> <template>When stopping the service {}</template> </log_entry> <log_entry> <level>ERROR</level> <template>exception during timeline writer flush!</template> </log_entry>"
  },
  "013f7da7_2": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "013f7da7_3": {
    "exec_flow": "ENTRY→TRY→TRY→CALL:openListeners→LOG:LOG.DEBUG:opening listeners:{}→FOREACH:listeners→IF_TRUE:listener.getLocalPort() != -1 && listener.getLocalPort() != -2→CONTINUE→CALL:webServer.start→IF_TRUE:prometheusSupport→CALL:DefaultMetricsSystem.instance().register→FOREACH:hs→FOREACH_EXIT→IF_FALSE:unavailableException != null→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>opening listeners: {}</template> </log_entry> <log_entry> <level>INFO</level> <template>HttpServer.start() threw a non Bind IOException</template> </log_entry> <log_entry> <level>INFO</level> <template>HttpServer.start() threw a MultiException</template> </log_entry>"
  },
  "013f7da7_4": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Token file {} does not exist</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Failure to load login credentials</template> </log_entry> <log_entry> <level>DEBUG</level> <template>UGI loginUser: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry>"
  },
  "013f7da7_5": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CALL:org.apache.hadoop.security.UserGroupInformation.addCredentials → CALL:org.apache.hadoop.security.Credentials.addAll → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Token file {} does not exist</template> </log_entry> <log_entry> <level>DEBUG<log_entry> <template>Failure to load login credentials</template> </log_entry> <log_entry> <level>DEBUG</level> <template>UGI loginUser: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>WARN</level> <template>Null token ignored for {alias}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "013f7da7_6": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → EXCEPTION:readTokenStorageStream → CATCH:IOException ioe → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:new IOException(\"Exception reading \" + filename, ioe) → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "013f7da7_7": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → RETURN → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "edc60082_1": {
    "exec_flow": "ENTRY -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CATCH: PrivilegedActionException -> LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause -> THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException <entry>org.apache.hadoop.yarn.server.webapp.WebServices:getUser</entry> <call>VIRTUAL_CALL</call> <execute>org.apache.hadoop.security.UserGroupInformation:createRemoteUser</execute> <execute>org.apache.hadoop.security.User:<init></execute>",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "edc60082_2": {
    "exec_flow": "ENTRY -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CALL: Subject.doAs -> RETURN -> EXIT <entry>org.apache.hadoop.security.User:<init></entry>",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "edc60082_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)→IF_TRUE:finished > 0 && started > 0→IF_FALSE:elapsed >= 0→LOG:LOG.WARN: Finished time + finished + is ahead of started time + started→RETURN→EXIT",
    "log": "[WARN] Finished time + finished + is ahead of started time + started"
  },
  "edc60082_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)→IF_FALSE:finished > 0 && started > 0→IF_TRUE:isRunning→IF_FALSE:elapsed >= 0→LOG:LOG.WARN: Current time + current + is ahead of started time + started→RETURN→EXIT",
    "log": "[WARN] Current time + current + is ahead of started time + started"
  },
  "77b8e235_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length() == 0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta != null → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → RETURN → EXIT",
    "log": "<log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}].</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "77b8e235_2": {
    "exec_flow": "ENTRY → CALL:getUriPath → CALL:resolve → IF_FALSE:key.length() == 0 → IF_TRUE:meta == null && !key.endsWith(\"/\") → CALL:getObjectMetadata → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT",
    "log": "<log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "e94ada36_1": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.DFSUtilClient:getSmallBufferSize</entry> <call>getIoFileBufferSize</call> <comment>Optimized combination of execution flows from both nodes</comment> <entry>org.apache.hadoop.hdfs.DataStreamer:createSocketForPipeline</entry> <call>org.apache.hadoop.net.NetUtils:createSocketAddr</call> <call>org.apache.hadoop.hdfs.DFSClient:getRandomLocalInterfaceAddr</call> <call>NetUtils.connect</call> <call>setTcpNoDelay</call> <call>setSoTimeout</call> <call>setKeepAlive</call> <if_true>conf.getSocketSendBufferSize() > 0</if_true> <call>setSendBufferSize</call> <log>[DEBUG] Connecting to datanode {}</log> <log>[DEBUG] Send buf size {}</log> <return>EXIT</return>",
    "log": "[DEBUG] Connecting to datanode {} [DEBUG] Send buf size {}"
  },
  "e94ada36_2": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.DFSClient:getRandomLocalInterfaceAddr</entry> <if_false>localInterfaceAddrs.length == 0</if_false> <log>[DEBUG] Using local interface {}</log> <return>EXIT</return>",
    "log": "[DEBUG] Using local interface {}"
  },
  "22acf157_1": {
    "exec_flow": "<seq> ENTRY→IF_FALSE:firstListing→IF_FALSE:listing == null || !listing.isTruncated()→CALL:requestNextBatch→CALL:toString→CALL:toString→CALL:fetchNextBatchAsyncIfPresent→LOG:LOG.DEBUG: New listing status: {}, this→RETURN→EXIT→[VIRTUAL_CALL]→CALL:sourceHasNext→LOG:LOG.INFO: Next element fetched→RETURN </seq>",
    "log": "[DEBUG] New listing status: {} [INFO] Next element fetched"
  },
  "22acf157_2": {
    "exec_flow": "<seq> ENTRY→IF_FALSE:firstListing→IF_FALSE:listing == null || !listing.isTruncated()→CALL:requestNextBatch→CALL:toString→CALL:toString→CALL:fetchNextBatchAsyncIfPresent→LOG:LOG.DEBUG: New listing status: {}, this→RETURN→EXIT→[VIRTUAL_CALL]→CALL:sourceHasNext→LOG:LOG.INFO: Next element fetched→RETURN </seq>",
    "log": "[DEBUG] New listing status: {} [INFO] Next element fetched"
  },
  "22acf157_3": {
    "exec_flow": "<sequence> <step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>Child:fetch()</step> </sequence>",
    "log": "<!-- Merged log sequence derived from child logs -->"
  },
  "22acf157_4": {
    "exec_flow": "<entry>org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:hasNext()</entry> <execution> <step>sourceHasNext()</step> </execution>",
    "log": "<!-- Merged log sequence derived from child logs -->"
  },
  "492a835e_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log> <template>Handling deprecation for all properties in config...</template> <level>debug</level> </log> <log> <template>Handling deprecation for {item}</template> <level>debug</level> </log> <log> <template>message</template> <level>info</level> </log>"
  },
  "492a835e_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG: Bypassing cache to create filesystem {uri} → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "<log> <template>Bypassing cache to create filesystem {uri}</template> <level>debug</level> </log> <log> <template>Duplicate FS created for {}; discarding {}</template> <level>debug</level> </log>"
  },
  "492a835e_3": {
    "exec_flow": "ENTRY → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {}, property → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_FALSE: clazz == null → LOG: LOGGER.DEBUG: FS for {} is {}, scheme, clazz → RETURN → EXIT",
    "log": "<log> <template>Looking for FS supporting {}</template> <level>debug</level> </log> <log> <template>looking for configuration option {}</template> <level>debug</level> </log> <log> <template>Looking in service filesystems for implementation class</template> <level>debug</level> </log> <log> <template>FS for {} is {}</template> <level>debug</level> </log>"
  },
  "492a835e_4": {
    "exec_flow": "ENTRY→CALL:getInt→IF_TRUE:this.maxConcurrentTrackedNodes < 0→CALL:Configuration:getInt→LOG:ERROR {} is set to an invalid value, it must be zero or greater. Defaulting to {}→CALL:processConf→EXIT",
    "log": "<log> <template>{} is set to an invalid value, it must be zero or greater. Defaulting to {}</template> <level>error</level> </log>"
  },
  "1f8ae69e_1": {
    "exec_flow": "ENTRY → TRY → CALL:checkNotClosed → IF_FALSE: pos < 0 → IF_TRUE: this.pos > pos → IF_TRUE: in instanceof Seekable → CALL: ((Seekable) in).seek → LOG:LOG.DEBUG: Seek to position {}. Bytes skipped {}, pos, this.pos → EXCEPTION: debug → CATCH: IOException e → IF_TRUE: innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException) → THROW: new FileNotFoundException(String.format(\"%s is not found\", key)) → EXIT",
    "log": "[DEBUG] Seek to position {}. Bytes skipped {}, pos, this.pos"
  },
  "1f8ae69e_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:requested seek to position {}, n → IF_FALSE:closed → IF_FALSE:n < 0 → IF_FALSE:n > contentLength → IF_TRUE:streamStatistics != null → CALL:streamStatistics.seek → LOG:LOG.DEBUG:set nextReadPos to {}, nextReadPos → EXIT",
    "log": "[DEBUG] requested seek to position {} [DEBUG] set nextReadPos to {}"
  },
  "1f8ae69e_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:requested seek to position {}, n → IF_FALSE:closed → IF_FALSE:n < 0 → IF_FALSE:n > contentLength → IF_FALSE:streamStatistics != null → LOG:LOG.DEBUG:set nextReadPos to {}, nextReadPos → EXIT",
    "log": "[DEBUG] requested seek to position {} [DEBUG] set nextReadPos to {}"
  },
  "1f8ae69e_4": {
    "exec_flow": "ENTRY→CALL:checkNotClosed→IF_FALSE:position == pos→IF_FALSE:pos > position && pos < position + partRemaining→CALL:reopen→IF_FALSE: pos < 0→IF_FALSE: pos > contentLength→IF: pos + downloadPartSize > contentLength→CALL:org.slf4j.Logger:isDebugEnabled()→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:org.slf4j.Logger:warn(java.lang.String)→EXIT",
    "log": "[DEBUG] Aborting old stream to open at pos ... [WARN] interrupted when wait a read buffer"
  },
  "fbaf7e10_1": {
    "exec_flow": "ENTRY→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_COND: i < maxLength→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→IF_COND: roundingDirection→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→FOR_ITERATION→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:..."
  },
  "fbaf7e10_2": {
    "exec_flow": "ENTRY→CALL:multiplyAndRound→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_ITERATION→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:..."
  },
  "5f4a4516_1": {
    "exec_flow": "ENTRY → TRY → CALL:extractAndActivateSpanFromRequest → IF_TRUE:!logged.getAndSet(true) → CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[]) → LOG:log.WARN:format,args → RETURN → CALL:beforeMarshalling → IF_FALSE: counter == null → IF_FALSE: value < 0 → CALL: incAtomicLong → LOG: LOG.TRACE: Incrementing counter {} by {} with final value {}, key, value, l → RETURN → EXIT",
    "log": "[WARN] log.WARN:format,args [TRACE] Incrementing counter {} by {} with final value {}"
  },
  "5f4a4516_2": {
    "exec_flow": "ENTRY → CALL:retrieveAttachedSpan → IF_TRUE:span==null → LOG:LOG.DEBUG:No audit span attached to request {},request → CALL:getActiveAuditSpan → RETURN → EXIT",
    "log": "[DEBUG] No audit span attached to request {}"
  },
  "5f4a4516_3": {
    "exec_flow": "ENTRY → CALL:retrieveAttachedSpan → IF_FALSE:span==null → IF_FALSE:spaninstanceofWrappingAuditSpan → CALL:WARN_OF_SPAN_TYPE.warn → LOG:LOG.DEBUG:NOT_A_WRAPPED_SPAN+:{},span → RETURN → EXIT",
    "log": "[DEBUG] NOT_A_WRAPPED_SPAN + : {}"
  },
  "68d69014_1": {
    "exec_flow": "ENTRY → CALL: increment → CALL: merge → ENTRY → CALL: localIOStatistics → LOG: org.slf4j.Logger:debug → CALL: promoteInputStreamCountersToMetrics → CALL: snapshotIOStatistics → CALL: localIOStatistics → CALL: localIOStatistics → IF_FALSE: isClosed → EXIT → EXIT",
    "log": "[DEBUG] Merging statistics into FS statistics in {}: {}"
  },
  "68d69014_2": {
    "exec_flow": "ENTRY → IF_TRUE: !closed.getAndSet(true) → TRY → EXCEPTION: IOException|AbortedException → CALL: Logger.debug → CALL: IOUtils.cleanupWithLogger → CALL: streamStatistics.streamClose → CALL: streamStatistics.close → CALL: close → EXIT → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] While closing stream [DEBUG] Exception in closing {}"
  },
  "68d69014_3": {
    "exec_flow": "ENTRY → IF_TRUE: !closed.getAndSet(true) → TRY → EXCEPTION: IOException|AbortedException → CALL: Logger.debug → CALL: IOUtils.cleanupWithLogger → CALL: streamStatistics.streamClose → CALL: streamStatistics.close → CALL: close → EXIT",
    "log": "[DEBUG] While closing stream"
  },
  "2c4b4311_1": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "2c4b4311_2": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "2c4b4311_3": {
    "exec_flow": "ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:NetUtils.getSocketFactoryFromProperty→IF_TRUE:conf != null→IF_TRUE:theObject instanceof Configurable→CALL:((Configurable) theObject).setConf→LOG:Job end notification using proxy type \"ProxyType\" hostname \"hostname\" and port \"port\"→LOG:Job end notification couldn't parse configured proxy's port \"portConf\". Not going to use a proxy→EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] Job end notification using proxy type \"ProxyType\" hostname \"hostname\" and port \"port\"</log_entry> <log_entry>[WARN] Job end notification couldn't parse configured proxy's port \"portConf\". Not going to use a proxy</log_entry>"
  },
  "f63b1735_1": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT",
    "log": "<log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>FOREACH: [DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "d230363a_1": {
    "exec_flow": "ENTRY→TRY→CALL:nextOp→RETURN→EXIT <step>Check reader is not null</step> <step>Call readOp() with false parameter</step> <step>ENTRY→CALL:org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:nextOp→RETURN→EXIT</step>",
    "log": "[INFO] Next operation retrieved from backup input stream"
  },
  "d230363a_2": {
    "exec_flow": "ENTRY→TRY→CALL:nextOp→CATCH:Throwable→RETURN→EXIT",
    "log": "[ERROR] Got error reading edit log input stream..."
  },
  "5773bf30_1": {
    "exec_flow": "ENTRY → IF_FALSE: null == jobdesc → CALL:getNumberMaps → FOR_INIT → FOR_COND: i < maps → CALL:getTaskInfo → FOR_BODY → CALL:getInputBytes → CALL:getOutputBytes → CALL:getOutputRecords → FOR_UPDATE → FOR_COND: i < maps → FOR_EXIT → FOR_INIT → FOR_COND: i < reds → CALL:getTaskInfo → FOR_BODY → CALL:getInputBytes → CALL:getOutputRecords → FOR_UPDATE → FOR_COND: i < reds → FOR_EXIT → CALL:<init> → FOR_INIT → FOR_COND: i < maps → FOR_BODY → CALL:getTaskInfo → CALL:isDebugEnabled → IF_TRUE:LOG.debug → CALL:debug → CALL:getConfiguration → CALL:getUncompressedInputBytes → CALL:splitFor → CALL:new LoadSplit → FOR_UPDATE → FOR_COND: i < maps → FOR_EXIT → CALL:pushDescription → EXIT",
    "log": "[DEBUG] SPEC(%d) %d → %d %d %d %d %d %d %d"
  },
  "5773bf30_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "5773bf30_3": {
    "exec_flow": "ENTRY → CALL:getInt → IF_TRUE:this.maxConcurrentTrackedNodes < 0 → CALL:Configuration:getInt → LOG:ERROR {} is set to an invalid value, it must be zero or greater. Defaulting to {} → CALL:processConf → EXIT",
    "log": "[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT"
  },
  "5723bc42_1": {
    "exec_flow": "ENTRY → IF_FALSE: schedulingMode == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY && !accessibleToPartition(candidates.getPartition()) → IF_FALSE: !super.hasPendingResourceRequest(candidates.getPartition(), clusterResource, schedulingMode) → WHILE: canAssign(clusterResource, node) → IF_TRUE: LOG.isDebugEnabled() → LOG: [DEBUG] Trying to assign containers to child-queue of {} → CALL: assignContainersToChildQueues → CALL: org.apache.hadoop.yarn.util.resource.Resources:addTo(org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.api.records.Resource) → IF_TRUE: rootQueue → RETURN → EXIT",
    "log": "[DEBUG] Trying to assign containers to child-queue of {}"
  },
  "5723bc42_2": {
    "exec_flow": "ENTRY → CALL: readLock.lock → TRY → CALL: getCurrentLimitResource → CALL: queueUsage.getUsed → IF_TRUE: hasChildQueues → CALL: subtract → CALL: getTotalKillableResource → CALL: subtract → CALL: currentResourceLimits.setHeadroom → IF_TRUE: Resources.greaterThanOrEqual → IF_TRUE: reservationsContinueLooking → CALL: Resources.greaterThan → CALL: Resources.subtract → IF_TRUE: Resources.lessThan → IF_TRUE: LOG.isDebugEnabled → LOG: [DEBUG] try to use reserved: + getQueuePath() + usedResources: + queueUsage.getUsed() + , clusterResources: + clusterResource + , reservedResources: + resourceCouldBeUnreserved + , capacity-without-reserved: + newTotalWithoutReservedResource + , maxLimitCapacity: + currentLimitResource → RETURN: true → EXIT",
    "log": "[DEBUG] try to use reserved: + getQueuePath() + usedResources: + queueUsage.getUsed() + , clusterResources: + clusterResource + , reservedResources: + resourceCouldBeUnreserved + , capacity-without-reserved: + newTotalWithoutReservedResource + , maxLimitCapacity: + currentLimitResource"
  },
  "5723bc42_3": {
    "exec_flow": "ENTRY → CALL: updateCurrentResourceLimits → IF_TRUE: LOG.isDebugEnabled() → LOG: [DEBUG] assignContainers: partition= + candidates.getPartition() + #applications= + orderingPolicy.getNumSchedulableEntities → CALL: setPreemptionAllowed → CALL: allocateFromReservedContainer → IF_FALSE: null != assignment → IF_TRUE: schedulingMode == SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY && !accessibleToPartition(candidates.getPartition()) → CALL: ActivitiesLogger.QUEUE.recordQueueActivity → RETURN → EXIT",
    "log": "[DEBUG] assignContainers: partition= + candidates.getPartition() + #applications= + orderingPolicy.getNumSchedulableEntities"
  },
  "12c209c3_1": {
    "exec_flow": "<step>LOG.debug(\"noteFailure\", exception);</step> <step>if (exception == null) return;</step> <step>synchronized (this) {</step> <step>if (failureCause == null) {</step> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> <step>}</step> <step>}</step>",
    "log": "<log> <level>DEBUG</level> <template>noteFailure</template> </log> <log> <level>INFO</level> <template>Service {} failed in state {}</template> </log>"
  },
  "12c209c3_2": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CATCH: Throwable e</step> <step>LOG: WARN Exception while notifying listeners of {}, this, e</step> <step>EXIT</step>",
    "log": "<log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "7624a7d2_1": {
    "exec_flow": "<entry>ENTRY</entry> <sync>SYNC:deleteOnExit</sync> <for_init>FOR_INIT</for_init> <for_cond>FOR_COND:iter.hasNext()</for_cond> <call>CALL:exists</call> <if>IF</if> <call>CALL:delete</call> <catch>CATCH</catch> <call>CALL:info</call> <remove>REMOVE</remove> <for_continue>FOR_CONTINUE</for_continue> <exit>EXIT</exit> <entry>ENTRY</entry> <call>CALL:delete</call> <virtual_call>VIRTUAL_CALL</virtual_call> <entry>ENTRY</entry> <log>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log> <try>TRY</try> <call>CALL:getFileStatus</call> <if_false>IF_FALSE:key.compareToIgnoreCase(\"/\") == 0</if_false> <if_true>IF_TRUE:status.isDirectory()</if_true> <if_true>IF_TRUE:!key.endsWith(PATH_DELIMITER)</if_true> <call>CALL:createParent</call> <call>CALL:listStatus</call> <if_true>IF_TRUE:!recursive && listStatus(f).length > 0</if_true> <throw>THROW:new IOException(errMsg)</throw> <exit>EXIT</exit>",
    "log": "<log>[INFO] Ignoring failure to deleteOnExit for path {}</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log> <log>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log>"
  },
  "5e8bd156_1": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>IF_TRUE:fileStatus.isSymlink()</step> <step>IF_TRUE:options.isFollowLink()||(options.isFollowArgLink()&&(depth==0))</step> <step>CALL:resolvePath</step> <step>CALL:getFileSystem</step> <step>CALL:getFileStatus</step> <step>IF_FALSE:key.length() == 0</step> <step>CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE:meta != null</step> <step>IF_TRUE:meta.isFile()</step> <step>LOG:LOG.DEBUG: Path: [{}] is a file. COS key: [{}]</step> <step>CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile</step> <step>RETURN</step> <step>EXIT</step> </sequence>",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "9f40dd1e_1": {
    "exec_flow": "ENTRY→IF_FALSE: dirs.length == 0→CALL: TokenCache.obtainTokensForNamenodes→CALL: getInputPaths→CALL: Configuration.getInt→IF_TRUE: numThreads == 1→CALL: singleThreadedListStatus→CALL: stop→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS)→LOG: LOG.INFO: Total input files to process : + result.size()→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS)</log_entry> <log_entry>[INFO] Total input files to process : + result.size()</log_entry>"
  },
  "9f40dd1e_2": {
    "exec_flow": "ENTRY→IF_FALSE: dirs.length == 0→CALL: TokenCache.obtainTokensForNamenodes→CALL: getInputPaths→CALL: Configuration.getInt→IF_FALSE: numThreads == 1→CALL: new LocatedFileStatusFetcher→TRY→CALL: getFileStatuses→CALL: newArrayList→CALL: stop→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS)→LOG: LOG.INFO: Total input files to process : + result.size()→RETURN→EXIT",
    "log": "<log_entry>[DEBUG] Time taken to get FileStatuses: + sw.now(TimeUnit.MILLISECONDS)</log_entry> <log_entry>[INFO] Total input files to process : + result.size()</log_entry>"
  },
  "9f40dd1e_3": {
    "exec_flow": "ENTRY→IF_FALSE: dirs.length == 0→CALL: TokenCache.obtainTokensForNamenodes→CALL: getInputPaths→CALL: Configuration.getInt→IF_TRUE: numThreads == 1→CALL: singleThreadedListStatus→CALL: stop→IF_FALSE: LOG.isDebugEnabled()→LOG: LOG.INFO: Total input files to process : + result.size()→RETURN→EXIT",
    "log": "<log_entry>[INFO] Total input files to process : + result.size()</log_entry>"
  },
  "7e691af5_1": {
    "exec_flow": "ENTRY->[VIRTUAL_CALL]->ENTRY->TRY->IF_TRUE: LOG.isDebugEnabled()->LOG: [DEBUG] PrivilegedAction [as: {}][action: {}]->CATCH: PrivilegedActionException->LOG: [DEBUG] PrivilegedActionException as: {}->THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "<log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "7e691af5_2": {
    "exec_flow": "<step>Job.ENTRY</step> <step>[VIRTUAL_CALL]->ensureFreshStatus paths</step> <step>[VIRTUAL_CALL]->reduceProgress paths</step>",
    "log": "<log>[INFO]Job:ensureFreshStatus - Log details...</log> <log>[INFO]Job:reduceProgress - Log details...</log>"
  },
  "88d72e9e_1": {
    "exec_flow": "ENTRY→TRY→CALL:toBoolean→CATCH:TrileanConversionException→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable)→LOG:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled [DEBUG] \"isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call\"→CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)→CATCH:AbfsRestOperationException→LOG:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled [DEBUG] \"Get root ACL status\"→CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode()→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close()→RETURN→EXIT",
    "log": "<log>org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled [DEBUG] \"isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call\"</log> <log>org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled [DEBUG] \"Get root ACL status\"</log> <log>org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:listStatus [DEBUG] \"listStatus filesystem: {...} path: {...}, startFrom: {...}\"</log> <log>org.apache.hadoop.fs.azurebfs.utils.DateTimeUtils:parseLastModifiedTime [ERROR] \"Failed to parse the date {}\"</log>"
  },
  "be1ef823_1": {
    "exec_flow": "ENTRY→CALL:AddCachePoolOp.getInstance→CALL:AddCachePoolOp.setPool→CALL:logRpcIds→ LOG:LOG.DEBUG:doEditTx() op={} txid={}→CALL:org.slf4j.Logger:debug→TRY→ CALL:editLogStream.writeRaw→EXCEPTION:writeRaw→CATCH:IOException ex→ CALL:reset→CALL:endTransaction→CALL:shouldForceSync→RETURN→EXIT",
    "log": "[DEBUG] logRpcIds called [INFO] logEdit called [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "7cfeeebc_1": {
    "exec_flow": "ENTRY → TRY → CALL: checkNotClosed → IF_FALSE: pos < 0 → IF_FALSE: this.pos > pos → CALL: skip → LOG: LOG.DEBUG: Seek to position {}. Bytes skipped {}, pos, this.pos → EXCEPTION: debug → CATCH: IOException e → IF_TRUE: innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException) → THROW: new FileNotFoundException(String.format(\"%s is not found\", key)) → EXIT",
    "log": "[DEBUG] Seek to position {}. Bytes skipped {}, pos, this.pos"
  },
  "adea06dc_1": {
    "exec_flow": "ENTRY→TRY→CALL:readValue→CALL:org.slf4j.Logger:error(java.lang.String, java.lang.Throwable)→CALL:org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable)→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT→RETURN→EXIT",
    "log": "<log>[ERROR] Exception while parsing json input stream</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "9cc0091c_1": {
    "exec_flow": "ENTRY → IF_TRUE: newGroupName == null → CALL: filterGroupName → IF_TRUE: group == null → CALL: newGroup → IF_TRUE: isFGroup → CALL: put → IF_TRUE: groupNameInLegacyMap → LOG: Group + groupName + is deprecated. Use + newGroupName + instead → RETURN → EXIT",
    "log": "[WARN] Group + groupName + is deprecated. Use + newGroupName + instead"
  },
  "9cc0091c_2": {
    "exec_flow": "ENTRY → IF_FALSE: newGroupName == null → IF_TRUE: group == null → CALL: newGroup → IF_FALSE: isFGroup → CALL: checkGroups → CALL: put → IF_TRUE: groupNameInLegacyMap → LOG: Group + groupName + is deprecated. Use + newGroupName + instead → RETURN → EXIT",
    "log": "[WARN] Group + groupName + is deprecated. Use + newGroupName + instead"
  },
  "9cc0091c_3": {
    "exec_flow": "ENTRY → IF_TRUE: totalCounters != null → FOREACH: totalCounters → CALL: getGroup → IF_TRUE: newGroupName == null → CALL: filterGroupName → IF_TRUE: group == null → CALL: newGroup → IF_TRUE: isFGroup → CALL: put → IF_TRUE: groupNameInLegacyMap → LOG: Group + groupName + is deprecated. Use + newGroupName + instead → RETURN → FOREACH_EXIT → CALL: put → EXIT",
    "log": "[WARN] Group + groupName + is deprecated. Use + newGroupName + instead"
  },
  "9cc0091c_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapreduce.CounterGroup:findCounter(java.lang.String)→IF_TRUE: counters[i] == null→CALL: newCounter→CALL: org.slf4j.Logger:warn→RETURN→EXIT",
    "log": "[WARN] New counter created"
  },
  "9cc0091c_5": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.mapreduce.CounterGroup:findCounter(java.lang.String)→IF_FALSE: counters[i] == null→CALL: org.slf4j.Logger:warn→RETURN→EXIT",
    "log": "[WARN] Counter already exists"
  },
  "421dc2eb_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "421dc2eb_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "421dc2eb_3": {
    "exec_flow": "ENTRY → FOR_LOOP: newNames → GET: deprecatedKey → IF_TRUE: deprecatedKey != null && !getProps().containsKey(newName) → IF_TRUE: deprecatedValue != null → SET_PROPERTY: newName → EXIT",
    "log": "<!-- Inherited from child since parent has no logs -->"
  },
  "421dc2eb_4": {
    "exec_flow": "ENTRY → CALL: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) → IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null → CALL: warn → LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + \" Instead use \" + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + \" and \" + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY → CALL: get(JobConf.MAPRED_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null → CALL: warn → LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) → CALL: get(JobConf.MAPRED_MAP_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null → CALL: warn → LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) → CALL: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) → IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null → CALL: warn → LOG: LOG.WARN: JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT) → EXIT",
    "log": "[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + \" Instead use \" + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + \" and \" + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY [WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT) [WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)"
  },
  "421dc2eb_5": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → RETURN → EXIT",
    "log": "[LOG] getLoginUser"
  },
  "203b9f5d_1": {
    "exec_flow": "ENTRY → IF_FALSE: this.isAppendBlob → IF_FALSE: !blockToUpload.hasData() → CALL:outputStreamStatistics.bytesToUpload → CALL:outputStreamStatistics.writeCurrentBuffer → CALL:blockToUpload.startUpload → LOG: [DEBUG] Start datablock[{}] upload, index → CALL: verifyState → LOG: [DEBUG]: {}: entering state {}, this, next → CALL:executorService.submit → CALL:shrinkWriteOperationQueue → TRY → LOG: [TRACE] Fetch SAS token for {operation} on {path} → IF_TRUE: cachedSasToken == null → CALL: getSASToken → IF_FALSE: (sasToken == null) || sasToken.isEmpty() → IF_FALSE: sasToken.charAt(0) == '?' → CALL: queryBuilder.setSASToken → LOG: [TRACE] SAS token fetch complete for {operation} on {path} → RETURN → IF_TRUE: (op.isARetriedRequest()) && (op.getResult().getStatusCode() == HttpURLConnection.HTTP_BAD_REQUEST) → CALL: getPathStatus → IF_TRUE: destStatusOp.getResult().getStatusCode() == HttpURLConnection.HTTP_OK → IF_TRUE: length <= Long.parseLong(fileLength) → LOG: [DEBUG] Returning success response from append blob idempotency code → TRY → CALL:IOStatisticsBinding.trackDurationOfInvocation → CALL:decode → TRY → CALL:parse → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.azurebfs.services.AbfsClient</class> <method>append</method> <level>TRACE</level> <template>Fetch SAS token for {operation} on {path}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azurebfs.services.AbfsClient</class> <method>append</method> <level>TRACE</level> <template>SAS token fetch complete for {operation} on {path}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azurebfs.services.AbfsClient</class> <method>append</method> <level>DEBUG</level> <template>Returning success response from append blob idempotency code</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Start datablock[{}] upload, index</template> </log_entry> <log_entry> <level>DEBUG</level> <template>{State}: entering state {}, this, next</template> </log_entry>"
  },
  "203b9f5d_2": {
    "exec_flow": "ENTRY → TRY → CALL:IOStatisticsBinding.trackDurationOfInvocation → EXCEPTION:trackDurationOfInvocation → CATCH:AzureBlobFileSystemException aze → THROW:aze → IF_FALSE: token == sasToken → CALL:getExpiry → SYNC:this → IF_FALSE: isInvalid → ENTRY → IF_FALSE: token == null → IF_FALSE: start == -1 → TRY → CALL:decode → CALL:org.slf4j.Logger:error(java.lang.String, java.lang.Object, java.lang.Object) → RETURN → EXIT",
    "log": "<log_entry> <level>ERROR</level> <template>AzureBlobFileSystemException thrown</template> </log_entry>"
  },
  "203b9f5d_3": {
    "exec_flow": "ENTRY → TRY → CALL:urlEncode → CALL:append → CALL:append → CALL:append → TRY → NEW:URL → CALL:toString → CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Throwable) → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Unexpected error.</template> </log_entry>"
  },
  "203b9f5d_4": {
    "exec_flow": "ENTRY → TRY → CALL:IOStatisticsBinding.trackDurationOfInvocation → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>IOStatisticsBinding track duration completed</template> </log_entry>"
  },
  "78f2107c_1": {
    "exec_flow": "ENTRY→IF_TRUE:isPreUpgradableLayout(sd)→CALL:checkVersionUpgradable→IF_TRUE:oldVersion>LAST_UPGRADABLE_LAYOUT_VERSION→LOG(ERROR):*********** Upgrade is not supported from this older version {oldVersion} of storage to the current version. Please upgrade to {LAST_UPGRADABLE_HADOOP_VERSION} or a later version and then upgrade to current version. Old layout version is {oldVersion} and latest layout version this software version can upgrade from is {LAST_UPGRADABLE_LAYOUT_VERSION}. ************→THROW:IOException→EXIT",
    "log": "<log> <level>ERROR</level> <template>Unable to acquire file lock on path {}</template> </log> <log> <level>ERROR</level> <template>*********** Upgrade is not supported from this older version {oldVersion} of storage to the current version. Please upgrade to {LAST_UPGRADABLE_HADOOP_VERSION} or a later version and then upgrade to current version. Old layout version is {oldVersion} and latest layout version this software version can upgrade from is {LAST_UPGRADABLE_LAYOUT_VERSION}. ************</template> </log>"
  },
  "78f2107c_2": {
    "exec_flow": "ENTRY→IF_FALSE:isPreUpgradableLayout(sd)→EXIT",
    "log": ""
  },
  "85c12257_1": {
    "exec_flow": "ENTRY → IF_FALSE:stagingDirPath==null → CALL:getCurrentUser → IF_FALSE:user==null → IF_TRUE:writer==null → TRY → CALL:createEventWriter → LOG:LOG.INFO: Event Writer setup for JobId: [jobId], File: [historyFile] → CALL:get → CALL:writeXml → CALL:info → EXCEPTION:info → CATCH:IOException ioe → LOG:LOG.INFO: Could not create log file: [historyFile] for job [jobName] → THROW:ioe → EXIT",
    "log": "[INFO] Event Writer setup for JobId: [jobId], File: [historyFile] [INFO] Could not create log file: [historyFile] for job [jobName]"
  },
  "85c12257_2": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → FOREACH:tokenFileLocation → CALL:exists → IF_TRUE → CALL:isFile → CALL:readTokenStorageFile → TRY → CALL:readTokenStorageStream → LOG:Exception reading {filename} → CALL:addCredentials → LOG:Reading credentials from location {} → LOG:Loaded {} tokens from {} → IF_FALSE → LOG:Token file {} does not exist → LOG:Failure to load login credentials → LOG:UGI loginUser: {} → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[WARN] Null token ignored for {alias}</log> <log>[DEBUG] Exception reading {filename}</log>"
  },
  "b8adaa05_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOR EACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "608e260f_1": {
    "exec_flow": "ENTRY→CALL:get→IF_TRUE:aclStr!=null→IF_TRUE:keyOp==KeyOpType.ALL→LOG:warn→EXIT",
    "log": "[WARN] Invalid KEY_OP '{}' for {}, ignoring"
  },
  "608e260f_2": {
    "exec_flow": "ENTRY→CALL:get→IF_TRUE:aclStr!=null→IF_FALSE:keyOp==KeyOpType.ALL→IF_TRUE:aclStr.equals(\"*\")→LOG:info→CALL:<init>→EXIT",
    "log": "[INFO] {} for KEY_OP '{}' is set to '*'"
  },
  "608e260f_3": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "608e260f_4": {
    "exec_flow": "ENTRY→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG:DEBUG:Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "d578d533_1": {
    "exec_flow": "<flow>ENTRY→SYNC: elector→SYNC: this→IF_FALSE: millisToCede > 0→CALL:UserGroupInformation#getCurrentUser→CALL:Logger#debug→CALL:Server#getRemoteAddress→CALL:HAServiceTarget#getProxy→IF_FALSE: conf != null→ENTRY→LOG: Handling deprecation for all properties in config...→CALL: getProps→CALL: addAll→FOREACH: keys→LOG: Handling deprecation for (String)item→CALL: handleDeprecation→FOREACH_EXIT→CALL: getProps→FOREACH: names→CALL: substituteVars→CALL: getRaw→RETURN→EXIT→CALL:HAServiceProtocol#transitionToStandby→CALL:Logger#info→SYNC: elector→CALL:ActiveStandbyElector#quitElection→CALL:recheckElectability→EXIT</flow>",
    "log": "<log>[INFO] Requested by {currentUser} at {remoteAddress} to cede active role.</log> <log>[INFO] Successfully ensured local node is in standby mode</log> <log>[WARN] Unable to transition local node to standby: {exceptionMessage}</log> <log>[WARN] Quitting election but indicating that fencing is necessary</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Cleaning up resources</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[ERROR] Cannot add token {}: {}</log> <log>[DEBUG] Loaded {} base64 tokens</log> <log>[WARN] Null token ignored for {}</log> <log>[INFO] Yielding from election</log> <log>[INFO] Deleting bread-crumb of active node...</log> <log>[WARN] e.toString()</log> <log>[INFO] Would have joined master election, but this node is prohibited from doing so for ... more ms</log>"
  },
  "d578d533_2": {
    "exec_flow": "<flow>ENTRY→SYNC: elector→SYNC: this→IF_FALSE: remainingDelay > 0→SWITCH: lastHealthState→CASE: INITIALIZING→LOG: LOG.INFO→CALL: quitElection→EXIT</flow>",
    "log": "<log>[INFO] Ensuring that ... does not participate in active master election</log>"
  },
  "d578d533_3": {
    "exec_flow": "<flow>ENTRY→SYNC: elector→SYNC: this→IF_FALSE: remainingDelay > 0→SWITCH: lastHealthState→CASE: SERVICE_NOT_RESPONDING→LOG: LOG.INFO→CALL: quitElection→EXIT</flow>",
    "log": "<log>[INFO] Quitting master election for ... and marking that fencing is necessary</log>"
  },
  "d578d533_4": {
    "exec_flow": "ENTRY→IF_FALSE: data == null→IF_TRUE: wantToBeInElection→CALL: org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "<log>[INFO] Already in election. Not re-connecting.</log>"
  },
  "d578d533_5": {
    "exec_flow": "ENTRY→SWITCH: req.getSource()→CASE: [REQUEST_BY_USER_FORCED]→IF_TRUE: autoFailoverEnabled→CALL:org.slf4j.Logger:warn(java.lang.String)→LOG: [WARN] Allowing manual failover from + org.apache.hadoop.ipc.Server.getRemoteAddress() + even though automatic failover is enabled, because the user + specified the force flag→CALL:org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToStandby→ENTRY→IF_FALSE: rmContext.getHAServiceState() == HAServiceProtocol.HAServiceState.STANDBY→LOG: [INFO] Transitioning to standby state→CALL: rmContext.setHAServiceState→IF_FALSE: state == HAServiceProtocol.HAServiceState.ACTIVE→LOG: [INFO] Transitioned to standby state→EXIT→BREAK→EXIT",
    "log": "<log>[WARN] Allowing manual failover from + org.apache.hadoop.ipc.Server.getRemoteAddress() + even though automatic failover is enabled, because the user specified the force flag</log> <log>[INFO] Transitioning to standby state</log> <log>[INFO] Transitioned to standby state</log>"
  },
  "d578d533_6": {
    "exec_flow": "ENTRY→TRY→CALL:refreshAdminAcls→CALL:checkAccess→CALL:checkHaStateChange→TRY→CALL:transitionToStandby→EXCEPTION:transitionToStandby→CATCH:Exception e→CALL:RMAuditLogger.logFailure→THROW:new ServiceFailedException(\"Error when transitioning to Standby mode\", e)→EXIT",
    "log": "<log>[ERROR] Exception transitioning to standby</log>"
  },
  "d578d533_7": {
    "exec_flow": "ENTRY→TRY→CALL:refreshAdminAcls→CALL:checkAccess→CALL:checkHaStateChange→TRY→CALL:transitionToStandby→ENTRY→IF_FALSE: rmContext.getHAServiceState() == HAServiceProtocol.HAServiceState.STANDBY→LOG: [INFO] Transitioning to standby state→CALL: rmContext.setHAServiceState→IF_TRUE: state == HAServiceProtocol.HAServiceState.ACTIVE→CALL: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:stopActiveServices()→CALL: org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:reinitialize(boolean)→LOG: [INFO] Transitioned to standby state→EXIT→EXIT",
    "log": "<log>[INFO] Transitioning to standby state</log> <log>[INFO] Transitioned to standby state</log>"
  },
  "d578d533_8": {
    "exec_flow": "ENTRY→IF_TRUE: LOG.isInfoEnabled()→CALL:org.slf4j.Logger:isInfoEnabled()→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:createSuccessLog(user, operation, target, null, null, null, null)→EXIT",
    "log": "<log>[INFO] createSuccessLog(user, operation, target, null, null, null, null)</log>"
  },
  "d578d533_9": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→IF_TRUE:!authorizer.isAdmin(user)→LOG: LOG.WARN:User + user.getShortUserName() + doesn’t have permission + to call ' + method + '→CALL:RMAuditLogger.logFailure→THROW:new AccessControlException(\"User \" + user.getShortUserName() + \" doesn’t have permission\" + \" to call ' + method + '\")→EXIT",
    "log": "<log>[WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'</log>"
  },
  "d578d533_10": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→IF_FALSE:!authorizer.isAdmin(user)→IF_TRUE:LOG.isTraceEnabled()→LOG:LOG.TRACE:method + invoked by user + user.getShortUserName()→RETURN→EXIT",
    "log": "<log>[TRACE] ${method} invoked by user ${user.getShortUserName()}</log>"
  },
  "d578d533_11": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→CATCH→LOG:LOG.WARN:Couldn't get current user→CALL:RMAuditLogger.logFailure→THROW:IOException→EXIT",
    "log": "<log>[WARN] Couldn't get current user</log>"
  },
  "5859bccb_1": {
    "exec_flow": "<seq> ENTRY→CALL:org.apache.hadoop.yarn.util.resource.Resources:createResource→FOR_INIT→FOR_COND: i < maxLength →TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceInformation→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT <step> <description>Check if customResources is not null</description> <action> <condition>customResources != null</condition> <result> <log_sequence> <log>[INFO] Custom resources present, fetching max AM share values</log> <log>[DEBUG] Fetching custom max AM Share values</log> </log_sequence> </result> </action> <action> <condition>customResources == null</condition> <result> <log_sequence> <log>[INFO] No custom resources, using default max AM share</log> </log_sequence> </result> </action> </step> <step> <description>Return the created Resource instance</description> <action> <log_sequence> <log>[INFO] Returning resource instance with max AM share</log> </log_sequence> </action> </step> </seq>",
    "log": "<log>[INFO] Custom resources present, fetching max AM share values</log> <log>[DEBUG] Fetching custom max AM Share values</log>"
  },
  "5859bccb_2": {
    "exec_flow": "<seq> ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.ConfigurableResource:getResource(org.apache.hadoop.yarn.api.records.Resource)→CALL:org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)→IF_TRUE:!Resources.equals(maxResource, result)→LOG:LOG.WARN:String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)→RETURN→EXIT </seq>",
    "log": "<log>[INFO] No custom resources, using default max AM share</log>"
  },
  "0c1ac018_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:loginUser==null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser) → CALL:createLoginUser → ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → CALL:debug → CALL:loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND:loginUser==null → DO_EXIT → RETURN → EXIT → CALL:debug → VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String) → CALL:handleDeprecation → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → IF_TRUE:props!=null → CALL:loadResources → IF_TRUE:loadDefaults&&fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i<resources.size() → CALL:loadResource → FOR_EXIT → CALL:addTags → IF_TRUE:overlay!=null → CALL:putAll → IF_TRUE:backup!=null → FOREACH:overlay.entrySet() → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "feed500b_1": {
    "exec_flow": "ENTRY→CALL:getEditLog().logCloseFile→CALL:NameNode.stateChangeLog.debug→ENTRY→CALL:getInstance→CALL:setPath→CALL:setReplication→CALL:setModificationTime→CALL:setAccessTime→CALL:setBlockSize→CALL:setBlocks→CALL:setPermissionStatus→CALL:logEdit→ENTRY→CALL:beginTransaction→LOG:LOG.DEBUG:doEditTx() op={} txid={}→TRY→CALL:editLogStream.writeRaw→CALL:endTransaction→EXIT→CALL:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp)→LOG:LOG.INFO:Number of transactions: <numTransactions>→LOG:LOG.DEBUG:logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}",
    "log": "[DEBUG] closeFile: {} with {} blocks is persisted to the file system [DEBUG] doEditTx() op={} txid={} [INFO] Number of transactions: <numTransactions> [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}"
  },
  "3bc96240_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOR EACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → TRY → CALL: doAs → NEW: PrivilegedExceptionAction<AbstractFileSystem> → EXCEPTION: InterruptedException → CALL: error → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [ERROR] ex.toString()"
  },
  "3bc96240_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "694eade2_1": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>CALL:getUriPath</step> <step>CALL:resolve</step> <step>ENTRY → IF_FALSE: path.length <= 1</step> <step>IF_FALSE: root.isLink()</step> <step>CALL:Preconditions.checkState</step> <step>CALL:tryResolveInRegexMountpoint</step> <step>IF_TRUE: resolveResult != null</step> <step>RETURN</step> <step>EXIT</step> <step>CALL:getFileStatus</step> <step>IF_FALSE: key.length() == 0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE: meta != null</step> <step>IF_TRUE: meta.isFile()</step> <step>LOG: [DEBUG] Path: [{}] is a file. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile</step> <step>RETURN</step> <step>EXIT</step> <step>ENTRY→FOREACH:regexMountPointList→CALL:regexMountPoint.resolve→FOREACH:interceptorList→FOREACH_EXIT</step> <step>LOG:LOGGER.DEBUG</step> <step>WHILE:srcMatcher.find()→CALL:replaceRegexCaptureGroupInPath→FOREACH:groupRepresentationStrSetInDest→CALL:getRegexGroupValueFromMather</step> <step>LOG:org.slf4j.Logger:debug</step> <step>FOREACH_EXIT</step> <step>WHILE_EXIT</step> <step>IF_FALSE:0==mappedCount→FOREACH:interceptorList→FOREACH_EXIT</step> <step>CALL:buildResolveResultForRegexMountPoint→TRY→IF_TRUE:targetFs==null</step> <step>CALL:org.slf4j.Logger:error→RETURN→EXIT</step> </sequence>",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()</log> <log>[DEBUG] parsedDestPath value is:${parsedDestPath}</log> <log>[ERROR] Not able to initialize target file system. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null.</log>"
  },
  "694eade2_2": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>CALL:qualify</step> <step>LOG: [DEBUG] Getting path status for {} ({}) needEmptyDirectory={}</step> <step>CALL:s3GetFileStatus</step> <step>RETURN</step> <step>EXIT</step> <step>LOG: Handling deprecation for all properties in config...</step> <step>FOREACH:keys</step> <step>LOG: Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> </sequence>",
    "log": "<log>[DEBUG] Getting path status for {} ({}) needEmptyDirectory={}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "694eade2_3": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>IF_TRUE: client != null</step> <step>IF_FALSE: !client.isConnected()</step> <step>CALL:logout</step> <step>CALL:disconnect</step> <step>IF_TRUE: !logoutSuccess</step> <step>LOG: [WARN] Logout failed while disconnecting, error code -</step> <step>EXIT</step> </sequence>",
    "log": "<log>[WARN] Logout failed while disconnecting, error code -</log>"
  },
  "a98d1a41_1": {
    "exec_flow": "ENTRY→CALL:isDebugEnabled→IF_TRUE→CALL:debug→FOREACH:getProtocolImplMap(rpcKind).entrySet→FOREACH_EXIT→IF_TRUE:highest == null→RETURN→EXIT",
    "log": "[DEBUG] Call: Invocation object details [DEBUG] Size of protoMap for {rpcKind} = {getProtocolImplMap(rpcKind).size()}"
  },
  "85605c5f_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "85605c5f_2": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "85605c5f_3": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "85605c5f_4": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {} → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "85605c5f_5": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → IF_TRUE: types != null → CALL: getDeclaredMethod → RETURN → EXIT → FOREACH: locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod → CATCH → CALL:org.slf4j.Logger:error → FOREACH_EXIT → IF_TRUE: !thrownExceptions.isEmpty() → FOR_INIT → FOR_COND: i < thrownExceptions.size() → FOR_EXIT → THROW: thrownExceptions.get(0) → EXIT",
    "log": "[ERROR] Unexpected exception {} proxying {} to {}"
  },
  "85605c5f_6": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → IF_FALSE: types != null → CALL: getDeclaredMethod → RETURN → EXIT → FOREACH: locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod → CATCH → CALL:org.slf4j.Logger:error → FOREACH_EXIT → IF_TRUE: !thrownExceptions.isEmpty() → FOR_INIT → FOR_COND: i < thrownExceptions.size() → FOR_EXIT → THROW: thrownExceptions.get(0) → EXIT",
    "log": "[ERROR] Unexpected exception {} proxying {} to {}"
  },
  "85605c5f_7": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → IF_TRUE: types != null → CALL: getDeclaredMethod → CALL: org.slf4j.Logger:error(java.lang.String, java.lang.Object[]) → THROW: IOException → EXIT → FOREACH: locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod → CATCH → CALL:org.slf4j.Logger:error → FOREACH_EXIT → IF_FALSE: !thrownExceptions.isEmpty() → RETURN → EXIT",
    "log": "[ERROR] Cannot get method {} with types {} from {} [ERROR] Unexpected exception {} proxying {} to {}"
  },
  "85605c5f_8": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → IF_FALSE: types != null → CALL: getDeclaredMethod → CALL: org.slf4j.Logger:error(java.lang.String, java.lang.Object[]) → THROW: IOException → EXIT → FOREACH: locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod → CATCH → CALL:org.slf4j.Logger:error → FOREACH_EXIT → IF_FALSE: !thrownExceptions.isEmpty() → RETURN → EXIT",
    "log": "[ERROR] Cannot access method {} with types {} from {} [ERROR] Unexpected exception {} proxying {} to {}"
  },
  "4a9f2901_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Creating directory: {}, f.toString()→IF_FALSE: containsColon(f)→IF_FALSE: absolutePath.equals(ancestor)→CALL: performAuthCheck→IF_FALSE: noUmask→CALL: createPermissionStatus→CALL: applyUMask→CALL: createPermissionStatus→CALL: applyUMask→FOR_INIT→FOR_COND: parent != null→IF_TRUE: currentMetadata != null && !currentMetadata.isDirectory()→THROW: new FileAlreadyExistsException(\"Cannot create directory \" + f + \" because \" + current + \" is an existing file.\")→IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Creating directory: {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "cff41ab8_1": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNCHRONIZED → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT → CALL: noteFailure → LOG: LOG.DEBUG: noteFailure, exception → LOG: LOG.INFO: Service {getName()} failed in state {getServiceState()}, exception",
    "log": "[WARN] Exception while notifying listeners of {} [DEBUG] noteFailure, exception [INFO] Service {getName()} failed in state {getServiceState()}, exception"
  },
  "4bfb714c_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→COS Listing Logic→IF_FALSE - key.length()==0 → CALL - org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE - meta!=null → IF_TRUE - meta.isFile() → LOG - [DEBUG] Path: [{}] is a file. COS key: [{}] → CALL - org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "4bfb714c_2": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}]→CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log>"
  },
  "4bfb714c_3": {
    "exec_flow": "ENTRY→LOG: [DEBUG] List COS key: [{}] to check the existence of the path.→CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→LOG: [DEBUG] Path: [{}] is a directory. COS key: [{}]→CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→EXIT",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "f83962cb_1": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.startOp→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Proxying operation: {}, methodName→CALL:opCategory.set→IF_TRUE:op == OperationCategory.UNCHECKED || op == OperationCategory.READ→RETURN→EXIT→CALL:rpcServer.getLocationsForPath→IF_TRUE:rpcServer.isInvokeConcurrent→CALL:rpcClient.invokeConcurrent→EXIT",
    "log": "[DEBUG] Proxying operation: {} [INFO] Checked operation WRITE [DEBUG] Getting locations for path [INFO] Invoking method concurrently"
  },
  "f83962cb_2": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.startOp→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Proxying operation: {}, methodName→CALL:opCategory.set→IF_FALSE:op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL:checkSafeMode→EXIT→CALL:rpcServer.getLocationsForPath→IF_FALSE:rpcServer.isInvokeConcurrent→CALL:rpcClient.invokeSequential→EXIT",
    "log": "[DEBUG] Proxying operation: {} [INFO] Checked operation WRITE [DEBUG] Getting locations for path [INFO] Invoking method sequentially"
  },
  "f83962cb_3": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.startOp→IF_TRUE:LOG.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Proxying operation: {}, methodName→CALL:opCategory.set→IF_FALSE:op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL:checkSafeMode→EXIT→CALL:rpcServer.getLocationsForPath→IF_TRUE:subclusterResolver instanceof MountTableResolver→CALL:org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:getMountPoints→IF_FALSE:mountPoints != null→CALL:isPathAll→TRY→CALL:org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:getMountPoints→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {} [INFO] Checked operation WRITE [DEBUG] Getting locations for path [ERROR] Cannot get mount point"
  },
  "f83962cb_4": {
    "exec_flow": "ENTRY→FOREACH:locations→FOREACH_EXIT→IF_TRUE:!thrownExceptions.isEmpty()→FOR_INIT→FOR_COND:i < thrownExceptions.size()→IF_TRUE:isUnavailableException(ioe)→THROW:ioe→EXIT",
    "log": "[ERROR] Unexpected exception {exception} proxying {methodName} to {nameServiceId}"
  },
  "f90155ba_1": {
    "exec_flow": "<sequence> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:Filesystem {} defined in configuration option, scheme → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → EXIT → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:e → EXIT </sequence>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Looking for FS supporting {}</log> <log>[DEBUG] looking for configuration option {}</log> <log>[DEBUG] Filesystem {} defined in configuration option</log> <log>[DEBUG] FS for {} is {}</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[DEBUG] Failed to initialize filesystem, e</log>"
  },
  "f90155ba_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → RETURN → EXIT",
    "log": "<log>[DEBUG] Filesystem {} created while awaiting semaphore</log>"
  },
  "f90155ba_3": {
    "exec_flow": "ENTRY → CALL:FileSystem:get → IF_TRUE:scheme != null && authority == null → NOT(scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null) → IF_TRUE:conf.getBoolean(disableCacheName, false) → LOG:DEBUG: Bypassing cache to create filesystem {uri} → CALL:FileSystem:createFileSystem → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {uri}</log>"
  },
  "f90155ba_4": {
    "exec_flow": "ENTRY → CALL:FileSystem:get → NOT(scheme == null && authority == null) → NOT(scheme != null && authority == null) → IF_TRUE:conf.getBoolean(disableCacheName, false) → LOG:DEBUG: Bypassing cache to create filesystem {uri} → CALL:FileSystem:createFileSystem → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {uri}</log>"
  },
  "c4db6b32_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→CALL:processRawArguments→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] Interrupted"
  },
  "c4db6b32_2": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→CALL:processRawArguments→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead. [ERROR] IOException: errorMessage"
  },
  "c4db6b32_3": {
    "exec_flow": "ENTRY→TRY→IF_TRUE:isDeprecated()→CALL:displayWarning→CALL:processOptions→CALL:processRawArguments→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[WARN] DEPRECATED: Please use 'replacementCommand' instead."
  },
  "c4db6b32_4": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→CALL:processRawArguments→CATCH:CommandInterruptException→CALL:displayError→RETURN→EXIT",
    "log": "[ERROR] Interrupted"
  },
  "c4db6b32_5": {
    "exec_flow": "ENTRY→TRY→IF_FALSE:isDeprecated()→CALL:processOptions→CALL:processRawArguments→CATCH:IOException→CALL:displayError→CALL:exitCodeForError→RETURN→EXIT",
    "log": "[ERROR] IOException: errorMessage"
  },
  "c4db6b32_6": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "c4db6b32_7": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "9c7c2a65_1": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource LOG: Handling deprecation for all properties in config... → FOREACH: LOG: Handling deprecation for (String)item ]]>",
    "log": "<![CDATA[ LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item); ]]>"
  },
  "9c7c2a65_2": {
    "exec_flow": "<![CDATA[ ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT ]]>",
    "log": "<![CDATA[ [DEBUG] Handling deprecation for all properties in config... FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item); [INFO] message ]]>"
  },
  "200e38b2_1": {
    "exec_flow": "ENTRY→CALL:refreshIfNeeded→IF_TRUE: now - lastRefreshTime > REFRESH_INTERVAL_MS→CALL:reset→CALL:getSystemInfoInfoFromShell→IF_TRUE:sysInfoStr != null→IF_TRUE:index >= 0→IF_FALSE:sysInfo.length == sysInfoSplitCount→LOG:LOG.WARN:Expected split length of sysInfo to be + sysInfoSplitCount + . Got + sysInfo.length→RETURN→EXIT",
    "log": "[WARN] Expected split length of sysInfo to be 11. Got ..."
  },
  "200e38b2_2": {
    "exec_flow": "ENTRY→CALL:refreshIfNeeded→IF_TRUE: now - lastRefreshTime > REFRESH_INTERVAL_MS→CALL:reset→CALL:getSystemInfoInfoFromShell→IF_TRUE:sysInfoStr != null→IF_FALSE:index >= 0→LOG:LOG.WARN:Wrong output from sysInfo: + sysInfoStr→RETURN→EXIT",
    "log": "[WARN] Wrong output from sysInfo: ..."
  },
  "200e38b2_3": {
    "exec_flow": "ENTRY→CALL:refreshIfNeeded→TRY→CALL: shellExecutor.execute→EXCEPTION: execute→CATCH: IOException e→LOG: LOG.ERROR: StringUtils.stringifyException(e)→CALL: org.slf4j.Logger:error→RETURN→EXIT",
    "log": "[ERROR] {StringUtils.stringifyException(e)}"
  },
  "200e38b2_4": {
    "exec_flow": "ENTRY→CALL:refreshIfNeeded→IF_FALSE:readMemInfoFile && !readAgain→TRY→CALL:Files.newInputStream→CALL:org.slf4j.Logger:warn→RETURN→EXIT",
    "log": "[WARN] Couldn't read \"/proc/meminfo\"; can't determine memory settings"
  },
  "200e38b2_5": {
    "exec_flow": "ENTRY→CALL:refreshIfNeeded→IF_FALSE:readMemInfoFile && !readAgain→TRY→CALL:in.readLine→WHILE→IF_TRUE:mat.find→CALL:Long.parseLong→CALL:safeParseLong→CALL:org.slf4j.Logger:warn→FINALLY→RETURN→EXIT",
    "log": "[WARN] Error reading the stream [WARN] Error closing the stream"
  },
  "027fd100_1": {
    "exec_flow": "<step>ENTRY→TRY→CALL:connect→RETURN→FINALLY→CALL:disconnect→RETURN→EXIT</step> <step>Check if workingDir is null</step> <step>Execute getHomeDirectory</step> <step>Return workingDir/homeDirectory</step> <step>ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT</step>",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "027fd100_2": {
    "exec_flow": "<step>Check if workingDir is null</step> <step>Execute getHomeDirectory</step> <step>Return workingDir/homeDirectory</step>",
    "log": "<!-- Inherited logs from child node's getHomeDirectory -->"
  },
  "027fd100_3": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → LOG:org.apache.hadoop.util.DurationInfo:<init> → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL:loadFileSystems → LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme → IF_TRUE:conf != null → LOG:LOGGER.DEBUG:looking for configuration option {}, property → CALL:getClass → IF_TRUE:clazz == null → LOG:LOGGER.DEBUG:Looking in service filesystems for implementation class → CALL:get → IF_FALSE:clazz == null → LOG:LOGGER.DEBUG:FS for {} is {}, scheme, clazz → RETURN → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem, e → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH:closeables → IF(c!=null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger!=null) → LOG:org.slf4j.Logger:debug(\"Exception in closing {}\", c, e) → FOREACH_EXIT → EXIT",
    "log": "<log>[DEBUG] Looking for FS supporting {}</log> <log>[DEBUG] looking for configuration option {}</log> <log>[DEBUG] Looking in service filesystems for implementation class</log> <log>[DEBUG] FS for {} is {}</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "027fd100_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log>"
  },
  "027fd100_5": {
    "exec_flow": "Parent.ENTRY → LOGGER.warn(\"\\\"local\\\" is a deprecated filesystem name. Use \\\"file:///\\\" instead.\") → Parent.ENTRY → LOGGER.warn(\"\\\"<name>\\\" is a deprecated filesystem name. Use \\\"hdfs://\\\" + name + \\\"/\\\" instead.\")",
    "log": "<log>[WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead.</log> <log>[WARN] \"<name>\" is a deprecated filesystem name. Use \"hdfs://<name>/\" instead.</log>"
  },
  "cc9d37f0_1": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: size() == 0 → IF_TRUE: minSegment != null → CALL: adjustPriorityQueue → IF_FALSE: size() == 0 → CALL: top → CALL: getKey → IF_TRUE: !minSegment.inMemory() → CALL: minSegment.getValue → CALL: reset → CALL: mergeProgress.set → RETURN → EXIT",
    "log": "<log>[INFO] Progress updated</log>"
  },
  "cc9d37f0_2": {
    "exec_flow": "ENTRY → TRY → IF_FALSE: size() == 0 → IF_TRUE: minSegment != null → CALL: adjustPriorityQueue → IF_FALSE: size() == 0 → CALL: top → CALL: getKey → IF_TRUE: !minSegment.inMemory() → CALL: minSegment.getValue → CALL: reset → CALL: mergeProgress.set → IF_TRUE: Float.isNaN(progress) → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "<log>[INFO] Progress updated</log> <log>[DEBUG] Illegal progress value found, progress is Float.NaN. Progress will be changed to 0</log>"
  },
  "cc9d37f0_3": {
    "exec_flow": "ENTRY→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_TRUE: progress < 0→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "<log>[DEBUG] Illegal progress value found, progress is less than 0. Progress will be changed to 0</log>"
  },
  "cc9d37f0_4": {
    "exec_flow": "ENTRY→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_FALSE: progress < 0→IF_TRUE: progress > 1→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "<log>[DEBUG] Illegal progress value found, progress is larger than 1. Progress will be changed to 1</log>"
  },
  "cc9d37f0_5": {
    "exec_flow": "ENTRY→IF_FALSE: Float.isNaN(progress)→IF_FALSE: progress == Float.NEGATIVE_INFINITY→IF_FALSE: progress < 0→IF_FALSE: progress > 1→IF_TRUE: progress == Float.POSITIVE_INFINITY→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "<log>[DEBUG] Illegal progress value found, progress is Float.POSITIVE_INFINITY. Progress will be changed to 1</log>"
  },
  "56be7017_1": {
    "exec_flow": "ENTRY→IF_FALSE: conf == null→IF_FALSE: isInState(STATE.INITED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.INITED) != STATE.INITED→CALL: org.slf4j.Logger:debug(java.lang.String)→CALL: setConfig→TRY→CALL: serviceInit→IF_TRUE: isInState(STATE.INITED)→ENTRY→TRY→CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL: globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "<log> <fingerprint>org.slf4j.Logger:debug[DEBUG]Config has been overridden during init</fingerprint> <variables/> </log> <log> <fingerprint>LOG.WARN[WARN]Exception while notifying listeners of {}</fingerprint> <variables> <variable name=\"this\" /> <variable name=\"e\" /> </variables> </log>"
  },
  "56be7017_2": {
    "exec_flow": "SYNC: this→IF_TRUE: failureCause == null→SET: failureCause = exception→SET: failureState = getServiceState()→LOG: LOG.INFO: Service {getName()} failed in state {failureState}→EXIT",
    "log": "<log> <fingerprint>LOG.DEBUG[DEBUG]noteFailure</fingerprint> <variables/> </log> <log> <fingerprint>LOG.INFO[INFO]Service {getName()} failed in state {failureState}</fingerprint> <variables> <variable name=\"getName()\" /> <variable name=\"failureState\" /> </variables> </log>"
  },
  "56be7017_3": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:setConfig→EXIT",
    "log": "<log> <fingerprint>[DEBUG] Config has been overridden during init</fingerprint> <variables/> </log>"
  },
  "3edd1f98_1": {
    "exec_flow": "<step>ENTRY</step> <step>TRY → CALL:getFileController → IF_TRUE: fc != null → CALL:readAggregatedLogs → IF_TRUE:!foundAnyLogs → CALL:getRemoteAppLogDir → LOG:emptyLogDir → CALL:org.apache.hadoop.fs.RemoteIterator:next → LOG:[DEBUG] Next element retrieved from RemoteIterator → RETURN → EXIT</step> <step>TRY → CALL:getFileController → CALL:getAppId → CALL:getAppOwner → CALL:readAggregatedLogsMeta → FOREACH:containersLogMeta → FOREACH_EXIT → RETURN → EXIT → IF_TRUE:org.apache.hadoop.fs.RemoteIterator.hasNext() → CALL:org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:next() → TRY → CALL:org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → CALL:org.apache.hadoop.fs.s3a.Invoker:retry → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]) → CALL:getMultipartUploads → CALL:listIterator → CALL:hasNext → RETURN → EXIT</step>",
    "log": "<log>[ERROR] Exception logged from logContainerLogs: (ex.getMessage())</log> <log>[DEBUG] [{}], Requesting next {} uploads prefix {}, next key {}, next upload id {}</log> <log>[DEBUG] Listing found {} upload(s)</log> <log>[DEBUG] New listing state: {}</log> <log>[DEBUG] Next element retrieved from RemoteIterator</log> <log>System.err.println(\"Can not find any log file matching the pattern: \" + options.getLogTypes() + \" for the application: \" + options.getAppId());</log> <log>System.err.println(\"Can not find the logs for the application: \" + options.getAppId() + \" with the appOwner: \" + options.getAppOwner());</log> <log>[DEBUG] readAggregatedLogs</log> <log>[DEBUG] emptyLogDir</log>"
  },
  "3edd1f98_2": {
    "exec_flow": "<step>ENTRY</step> <step>TRY → CALL:getFileController → IF_TRUE: fc != null → CALL:readAggregatedLogs → IF_FALSE: !foundAnyLogs → CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogs → RETURN → EXIT</step>",
    "log": "<log>[DEBUG] readAggregatedLogs</log> <log>[DEBUG] Attempting to read aggregated logs using TFileController</log>"
  },
  "3edd1f98_3": {
    "exec_flow": "<step>ENTRY</step> <step>TRY → CALL:getFileController → IF_FALSE: fc != null → IF_TRUE: !foundAnyLogs → CALL:getRemoteAppLogDir → LOG:emptyLogDir → RETURN → EXIT</step>",
    "log": "<log>[DEBUG] emptyLogDir</log>"
  },
  "3edd1f98_4": {
    "exec_flow": "ENTRY→CALL:getContainerReportsFromRunningApplication→ENTRY →CALL:getApplicationAttempts→FOREACH: attempts→CALL:getContainers→FOREACH_EXIT →CALL:addAll→RETURN→EXIT→RETURN→EXIT→WebAppUtils.ENTRY →YarnConfiguration.ENTRY→LOG:Unexpected SecurityException in Configuration →CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw →LOG:Handling deprecation for all properties in config... →CALL:getProps→CALL:addAll→FOREACH: keys →LOG:Handling deprecation for + (String)item →CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log_entry>[WARN] Failed to fetch application attempts from ATS v2</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "fec2a3f1_1": {
    "exec_flow": "ENTRY→CALL:listXAttrs→CALL:rpcServer.checkOperation→CALL:getLocationsForPath→CALL:invokeSequential→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser→CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod→FOREACH: locations→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod→CATCH→CALL:org.slf4j.Logger:error→FOREACH_EXIT→IF_TRUE: !thrownExceptions.isEmpty()→FOR_INIT→FOR_COND: i < thrownExceptions.size()→IF_TRUE: isUnavailableException(ioe)→THROW: ioe→RETURN→EXIT",
    "log": "<log_statement>[ERROR] Unexpected exception {} proxying {} to {}</log_statement> <log_statement>The operation is not allowed because {...}</log_statement> <log_statement>Cannot find locations for {...}</log_statement> <log_statement>{...} is in a read only mount point</log_statement> <log_statement>routerFailureStateStore</log_statement>"
  },
  "fec2a3f1_2": {
    "exec_flow": "ENTRY→CALL:listXAttrs→CALL:rpcServer.checkOperation→CALL:getLocationsForPath→CALL:invokeSequential→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser→CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod→FOREACH: locations→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod→CATCH→CALL:org.slf4j.Logger:error→FOREACH_EXIT→IF_TRUE: !thrownExceptions.isEmpty()→FOR_INIT→FOR_COND: i < thrownExceptions.size()→FOR_EXIT→THROW: thrownExceptions.get(0)→RETURN→EXIT",
    "log": "<log_statement>[ERROR] Unexpected exception {} proxying {} to {}</log_statement> <log_statement>The operation is not allowed because {...}</log_statement> <log_statement>Cannot find locations for {...}</log_statement> <log_statement>{...} is in a read only mount point</log_statement> <log_statement>routerFailureStateStore</log_statement>"
  },
  "fec2a3f1_3": {
    "exec_flow": "ENTRY→CALL:listXAttrs→CALL:rpcServer.checkOperation→CALL:getLocationsForPath→CALL:invokeSequential→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser→CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod→FOREACH: locations→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod→CATCH→CALL:org.slf4j.Logger:error→FOREACH_EXIT→IF_FALSE: !thrownExceptions.isEmpty()→RETURN→EXIT",
    "log": "<log_statement>[ERROR] Unexpected exception {} proxying {} to {}</log_statement> <log_statement>The operation is not allowed because {...}</log_statement> <log_statement>Cannot find locations for {...}</log_statement> <log_statement>{...} is in a read only mount point</log_statement> <log_statement>routerFailureStateStore</log_statement>"
  },
  "5fc57a25_1": {
    "exec_flow": "ENTRY → SWITCH: eventType → CASE: [CLEANUP] → TRY → LOG: LOG.INFO: Cleaning master[APP_ID] → CALL: cleanup → BREAK → EXIT",
    "log": "[INFO] Cleaning master[APP_ID]"
  },
  "5fc57a25_2": {
    "exec_flow": "ENTRY → SWITCH: eventType → CASE: [CLEANUP] → TRY → LOG: LOG.INFO: Cleaning master[APP_ID] → CALL: cleanup → EXCEPTION: cleanup → CATCH: IOException ie → LOG: LOG.INFO: Error cleaning master , ie → BREAK → EXIT",
    "log": "[INFO] Cleaning master[APP_ID] [INFO] Error cleaning master , ie"
  },
  "5fc57a25_3": {
    "exec_flow": "ENTRY → SWITCH: eventType → CASE: [CLEANUP] → TRY → LOG: LOG.INFO: Cleaning master[APP_ID] → CALL: cleanup → EXCEPTION: cleanup → CATCH: YarnException e → CALL: sb.append(masterContainer.getId().toString()).append → IF_TRUE: !e.getMessage().contains(sb.toString()) → LOG: LOG.INFO: Error cleaning master , e → BREAK → EXIT",
    "log": "[INFO] Cleaning master[APP_ID] [INFO] Error cleaning master , e"
  },
  "5fc57a25_4": {
    "exec_flow": "ENTRY → SWITCH: eventType → CASE: [] → LOG: LOG.WARN: Received unknown event-type [eventType]. Ignoring. → BREAK → EXIT",
    "log": "[WARN] Received unknown event-type [eventType]. Ignoring."
  },
  "5e6ff362_1": {
    "exec_flow": "ENTRY → CALL:stopMetricsLogger → IF → CALL:p.stop → CALL:LOG.warn → FOR → CALL:blockPoolManager.getAllNamenodeThreads → IF → CALL:dataXceiverServer.sendOOBToPeers → TRY → CALL:dataXceiverServer.kill → CALL:LOG.warn → CATCH:IOException → TRY → CALL:dataXceiverServer.interrupt → IF → CALL:localDataXceiverServer.kill → CALL:localDataXceiverServer.interrupt → CALL:shutdownPeriodicScanners → CALL:shutdownDiskBalancer → IF → CALL:httpServer.close → CALL:volumeChecker.shutdownAndWait → IF → CALL:storageLocationChecker.shutdownAndWait → IF → CALL:pauseMonitor.stop → CALL:shutdownReconfigurationTask → CALL:LOG.info → CALL:HadoopExecutors.shutdown → IF → WHILE → CALL:LOG.info → CALL:threadGroup.interrupt → IF → CALL:dataXceiverServer.join → IF → CALL:localDataXceiverServer.join → IF → CALL:metrics.setDataNodeActiveXceiversCount → CALL:metrics.setDataNodePacketResponderCount → CALL:metrics.setDataNodeBlockRecoveryWorkerCount → IF → CALL:ipcServer.stop → IF → CALL:ecWorker.shutDown → IF → TRY → CALL:blockPoolManager.shutDownAll → IF → TRY → CALL:storage.unlockAll → IF → CALL:data.shutdown → IF → CALL:metrics.shutdown → IF → CALL:diskMetrics.shutdownAndWait → CALL:slowDiskDetectionDaemon.interrupt → TRY → CALL:slowDiskDetectionDaemon.join → CATCH:InterruptedException → LOG.error: Disk Outlier Detection daemon did not shutdown → IF → CALL:MBeans.unregister → IF → CALL:shortCircuitRegistry.shutdown → CALL:LOG.info → SYNC → CALL:tracer.close → EXIT",
    "log": "[INFO] Stopped plug-in {} [WARN] ServicePlugin {} could not be stopped [TRACE] Exception interrupting DataXceiverServer [WARN] Exception shutting down DataNode HttpServer [WARN] Received exception in BlockPoolManager#shutDownAll [WARN] Exception when unlocking storage [INFO] Waiting up to 30 seconds for transfer threads to complete [INFO] Waiting for threadgroup to exit, active threads is {} [WARN] {}:DataXceiverServer.kill(), datanode.getDisplayName(), ie [ERROR] Disk Balancer : Scheduler did not terminate. [ERROR] Disk Outlier Detection daemon did not shutdown [INFO] Shutdown complete. [DEBUG] Gracefully shutting down executor service {}. Waiting max {} {} [DEBUG] Executor service has not shutdown yet. Forcing. Will wait up to an additional {} {} for shutdown [DEBUG] Succesfully shutdown executor service"
  },
  "9980d2c9_1": {
    "exec_flow": "<step>Entry</step> <step>[VIRTUAL_CALL] Virtual Call to org.apache.hadoop.yarn.api.records.impl.LightWeightResource:<init>(long,int,org.apache.hadoop.yarn.api.records.ResourceInformation[])</step> <step>Call to org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfKnownResourceTypes()</step> <step>CALL:clone</step> <step>CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes</step> <step>FOR_COND: i < maxLength</step> <step>TRY</step> <step>CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation</step> <step>CATCH:ResourceNotFoundException</step> <step>CALL:org.slf4j.Logger:warn</step> <step>FOR_ITERATION</step> <step>FOR_EXIT</step> <step>RETURN</step> <step>EXIT</step> <step>ENTRY</step> <step>CALL:append</step> <step>CALL:getMaxShare</step> <step>ConfigurableResource:getResource:CALL</step> <step>componentwiseMax:ENTRY</step> <step>componentwiseMax:FOR_LOOP_ENTER</step> <step>componentwiseMax:GET_RHS_RESOURCE_INFO</step> <step>componentwiseMax:GET_LHS_RESOURCE_INFO</step> <step>componentwiseMax:SET_RESOURCE_INFO</step> <step>componentwiseMax:FOR_LOOP_EXIT</step> <step>componentwiseMax:RETURN</step> <step>IF_TRUE:!Resources.equals(maxResource, result)</step> <step>LOG:WARN:String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)</step> <step>CALL:computeMaxAMResource</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN] Resource is missing:...</log> <log>org.apache.hadoop.yarn.util.resource.Resources:[WARN]Resource is missing:{ye.getMessage()}</log> <log>[WARN] String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)</log>"
  },
  "96773209_1": {
    "exec_flow": "ENTRY -> CALL:getContainer -> CALL:doAs -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CALL: Subject.doAs -> RETURN -> EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "96773209_2": {
    "exec_flow": "ENTRY -> CALL:getContainer -> CALL:doAs -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CATCH: PrivilegedActionException -> LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause -> THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "96773209_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)→IF_TRUE:finished > 0 && started > 0→IF_FALSE:elapsed >= 0→LOG:LOG.WARN: Finished time + finished + is ahead of started time + started→RETURN→EXIT",
    "log": "[WARN] Finished time + finished + is ahead of started time + started"
  },
  "96773209_4": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)→IF_FALSE:finished > 0 && started > 0→IF_TRUE:isRunning→IF_FALSE:elapsed >= 0→LOG:LOG.WARN: Current time + current + is ahead of started time + started→RETURN→EXIT",
    "log": "[WARN] Current time + current + is ahead of started time + started"
  },
  "ea02af6d_1": {
    "exec_flow": "ENTRY → TRY → CALL:setLogEnabled → CALL:getObjectMetadata(request) → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT → ENTRY → LOG: [DEBUG] Creating directory: {}, f.toString() → TRY → CALL:getFileStatus → IF_TRUE:meta.isDirectory() → LOG: [DEBUG] Path: [{}] is a directory. COS key: [{}] → RETURN → EXIT → IF_FALSE:theInternalDir.isRoot()&&dir==null → IF_FALSE:theInternalDir.getChildren().containsKey(dir.toString().substring(1)) → IF_TRUE:this.fsState.getRootFallbackLink()!=null → TRY → CALL:org.apache.hadoop.fs.FileSystem:mkdirs → RETURN → EXIT",
    "log": "[DEBUG] Exception thrown when get object meta: + key + , exception: + osse [DEBUG] Creating directory: {} [DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}] [DEBUG] Making dir: [{}] in COS [DEBUG] createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {} [DEBUG] Incremented write operations count [DEBUG] Incremented storage statistics OpCounter [INFO] Applying UMask [DEBUG] FsPathBooleanRunner is running [INFO] S3AFileSystem:mkdirs - Path qualified and tracked for mkdirs operation [DEBUG] Failed to create <dirToCreate_value> at fallback : <linkedFallbackFs.getUri_value>"
  },
  "ea02af6d_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isDirectory() → LOG:LOG.DEBUG:Found the path: {} as a file., f.toString() → CALL:updateFileStatusPath → RETURN → EXIT",
    "log": "LOG.debug(\"Getting the file status for {}\", f.toString()) LOG.DEBUG: Found the path: {} as a file."
  },
  "199389b5_1": {
    "exec_flow": "ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Looking for Job + jobId -> CALL: getFileInfo -> IF_TRUE: fileInfo == null -> THROW: new HSFileRuntimeException(\"Unable to find job \" + jobId) -> EXIT",
    "log": "[DEBUG] Looking for Job + jobId"
  },
  "199389b5_2": {
    "exec_flow": "ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Looking for Job + jobId -> CALL: getFileInfo -> IF_FALSE: fileInfo == null -> CALL: fileInfo.waitUntilMoved() -> IF_TRUE: fileInfo.isDeleted() -> THROW: new HSFileRuntimeException(\"Cannot load deleted job \" + jobId) -> EXIT",
    "log": "[DEBUG] Looking for Job + jobId"
  },
  "199389b5_3": {
    "exec_flow": "ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Looking for Job + jobId -> CALL: getFileInfo -> IF_FALSE: fileInfo == null -> CALL: fileInfo.waitUntilMoved() -> IF_FALSE: fileInfo.isDeleted() -> CALL: loadJob -> RETURN -> EXIT",
    "log": "[DEBUG] Looking for Job + jobId"
  },
  "e6126b3b_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:key.length() == 0→CALL:retrieveMetadata→IF_TRUE:meta != null→IF_TRUE:meta.isFile()→LOG:LOG.DEBUG:Path: {} is a file. COS key: {}→CALL:newFile→RETURN→EXIT",
    "log": "<log>[DEBUG] Path: {} is a file. COS key: {}</log>"
  },
  "e6126b3b_2": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:List status for path:{path}→CALL:getFileStatus→IF_FALSE:fileStatus.isDirectory()→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Adding:rd(not a dir):{path}→CALL:add→CALL:toArray→RETURN→EXIT",
    "log": "<log>[DEBUG] List status for path: {path}</log> <log>[DEBUG] Adding: rd (not a dir): {path}</log>"
  },
  "e6126b3b_3": {
    "exec_flow": "ENTRY→IF_FALSE:LOG.isDebugEnabled()→CALL:getFileStatus→IF_TRUE:fileStatus.isDirectory()→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:listStatus:doing listObjects for directory {key}→WHILE:true→CALL:toArray→RETURN→EXIT",
    "log": "<log>[DEBUG] listStatus: doing listObjects for directory {key}</log>"
  },
  "e6126b3b_4": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG:AzureBlobFileSystem.delete path: {} recursive: {}→CALL:statIncrement→CALL:makeQualified→IF_TRUE:f.isRoot()→IF_FALSE:!recursive→CALL:deleteRoot→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}</log> <log>[DEBUG] Deleting root content</log>"
  },
  "e6126b3b_5": {
    "exec_flow": "ENTRY→TRY→CALL:qualify→CALL:createSpan→IF_FALSE:outcome→RETURN→EXIT",
    "log": "<log>[DEBUG] Couldn't delete {} - does not exist: {}</log>"
  },
  "ef4bd9c8_1": {
    "exec_flow": "ENTRY → IF_TRUE: subnet != null → CALL: org.apache.hadoop.conf.Configuration:get(java.lang.String) → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: logDeprecation → CALL: org.slf4j.Logger:info(java.lang.String) → LOG:Handling deprecation for all properties in config... → LOG:Handling deprecation for (String)item → EXIT",
    "log": "<log_entry>[INFO] message</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "57672376_1": {
    "exec_flow": "<sequence> ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "57672376_2": {
    "exec_flow": "<sequence> ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "57672376_3": {
    "exec_flow": "<sequence> ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "22ac07f5_1": {
    "exec_flow": "ENTRY → CALL:checkOperation → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:checkOperation → CALL:checkNameNodeSafeMode → CALL:FSDirSnapshotOp.renameSnapshot → CALL:writeUnlock → CALL:logSync → LOG:logAuditEvent(true, ...) → ENTRY→FOREACH: auditLoggers → IF: logger instanceof HdfsAuditLogger→ CALL:appendClientPortToCallerContextIfAbsent→ CALL:hdfsLogger.logAuditEvent→ TRY→ CALL:topMetrics.report→ EXCEPTION:report→ CATCH:Throwable→ LOG:LOG.error→ IF_FALSE:org.slf4j.Logger:isDebugEnabled()→ FOREACH_EXIT→EXIT",
    "log": "<log>[INFO] Audit success: renameSnapshot</log> <log>FSNamesystem.LOG.info(\"\\tNumber of suppressed write-lock reports: {}\" + \"\\n\\tLongest write-lock held at {} for {}ms via {}\" + \"\\n\\tTotal suppressed write-lock held time: {}\")</log> <log>[ERROR] An error occurred while reflecting the event in top service, event: (cmd={},userName={})</log>"
  },
  "22ac07f5_2": {
    "exec_flow": "ENTRY → CALL:checkOperation → CALL:FSPermissionChecker.setOperationType → TRY → CALL:writeLock → CALL:checkOperation → CALL:checkNameNodeSafeMode → CALL:FSDirSnapshotOp.renameSnapshot → CALL:writeUnlock → CATCH:AccessControlException ace → LOG:logAuditEvent(false, ...) → THROW:ace → ENTRY→FOREACH: auditLoggers → IF: logger instanceof HdfsAuditLogger→ CALL:appendClientPortToCallerContextIfAbsent→ CALL:hdfsLogger.logAuditEvent→ TRY→ CALL:topMetrics.report→ IF_TRUE:org.slf4j.Logger:isDebugEnabled()→ CALL:org.slf4j.Logger:debug→ FOREACH_EXIT→EXIT",
    "log": "<log>[INFO] Audit failed: renameSnapshot</log> <log>[DEBUG] ------------------- logged event for top service: allowed={boolean}\\tugi={userName}\\tip={addr}\\tcmd={cmd}\\tsrc={src}\\tdst={dst}\\tperm={perm}</log>"
  },
  "22ac07f5_3": {
    "exec_flow": "ENTRY→TRY→SYNC: this→TRY→CALL: printStatistics→WHILE: mytxid > synctxid && isSyncRunning→WHILE_COND: mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE: mytxid <= synctxid→CALL: getLastJournalledTxId→LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}→IF_FALSE: lastJournalledTxId <= synctxid→TRY→IF_TRUE: journalSet.isEmpty()→THROW: new IOException(\"No journals available to flush\")→CALL: org.apache.hadoop.util.ExitUtil:terminate→CALL: org.slf4j.Logger:error→CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger→EXIT",
    "log": "<log>[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid{}</log> <log>[ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}</log> <log>[INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()</log>"
  },
  "395b8e30_1": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "395b8e30_2": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "18737214_1": {
    "exec_flow": "ENTRY → LOG: [INFO] Got event {eventType} for appId {applicationId} → SWITCH: event.getType() → CASE: [APPLICATION_INIT] → LOG: [INFO] Got APPLICATION_INIT for service {serviceId} → TRY → CALL: get → CALL: getServiceID → CALL: initializeApplication → CATCH: Throwable th → LOG: [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_INIT → BREAK → CASE: [APPLICATION_STOP] → FOREACH: serviceMap.values() → TRY → CALL: stopApplication → CATCH: Throwable th → LOG: [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_STOP → FOREACH_EXIT → BREAK → CASE: [CONTAINER_INIT] → FOREACH: serviceMap.values() → TRY → CALL: initializeContainer → CATCH: Throwable th → LOG: [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_INIT → FOREACH_EXIT → BREAK → CASE: [CONTAINER_STOP] → FOREACH: serviceMap.values() → TRY → CALL: stopContainer → CATCH: Throwable th → LOG: [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_STOP → BREAK → CASE: [] → THROW: new RuntimeException(\"Unknown type: \" + event.getType()) → EXIT",
    "log": "[INFO] Got event {eventType} for appId {applicationId} [INFO] Got APPLICATION_INIT for service {serviceId} [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_INIT [WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_STOP [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_INIT [WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_STOP"
  },
  "18737214_2": {
    "exec_flow": "ENTRY → IF_FALSE: app != null → LOG: [WARN] Event + event + sent to absent application + event.getApplicationID() → EXIT",
    "log": "[WARN] Event + event + sent to absent application + event.getApplicationID()"
  },
  "18737214_3": {
    "exec_flow": "ENTRY → IF_TRUE: component == null → LOG: [ERROR] No component exists for {event.getName()} → RETURN → EXIT",
    "log": "[ERROR] No component exists for {event.getName()}"
  },
  "18737214_4": {
    "exec_flow": "ENTRY → CALL:writeLock.lock → TRY → CALL:this.stateMachine.doTransition → CATCH:InvalidStateTransitionException e → LOG: [ERROR] Can't handle this event at current state, e → CALL:writeLock.unlock → EXIT",
    "log": "[ERROR] Can't handle this event at current state, e"
  },
  "18737214_5": {
    "exec_flow": "ENTRY → IF_FALSE: c != null → LOG: [WARN] Event {event} sent to absent container {event.getContainerID()} → EXIT",
    "log": "[WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "d91108a0_1": {
    "exec_flow": "<entry>VIRTUAL_CALL</entry> <call_sequence> <call>org.apache.hadoop.yarn.client.RMProxy:<init>()</call> <call>getInterceptorChain</call> <sync>this.userPipelineMap</sync> <try>TRY -> LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user -> CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() -> CATCH: Exception e</try> <exception>createRequestInterceptorChain</exception> </call_sequence>",
    "log": "<log>[INFO] Initializing request processing pipeline for user: {}</log> <log>[ERROR] Init RMAdminRequestInterceptor error for user: {}</log>"
  },
  "d91108a0_2": {
    "exec_flow": "ENTRY → IF_TRUE: null == timeout → CALL: readDecommissioningTimeout → IF_TRUE: null == yarnConf → NEW: YarnConfiguration → CALL: YarnConfiguration:get → CALL: YarnConfiguration:get → LOG: LOG.INFO: refreshNodes excludesFile + excludesFile → IF_TRUE: graceful → CALL: HostsFileReader:lazyRefresh → CALL: printConfiguredHosts → LOG: LOG.INFO: hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + } → CALL: handleExcludeNodeList → LOG: LOG.WARN: Exception + msg → EXIT",
    "log": "<log>[INFO] refreshNodes excludesFile + excludesFile</log> <log>[INFO] hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + }</log> <log>[WARN] Exception + msg</log>"
  },
  "36ae91b7_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] ENTRY → CALL: ensureInitialized → IF_FALSE: (subject == null || subject.getPrincipals(User.class).isEmpty()) → CALL: getLoginUser → RETURN → EXIT → RouterRpcServer:checkOperation → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → RETURN → EXIT → RouterRpcServer:getLocationsForPath → FileSubclusterResolver:getMountPoints → MountTableResolver:getMountPoints → MountTableResolver:isTrashPath → MountTableResolver:subtractTrashCurrentPath → RouterRpcClient:invokeSequential",
    "log": "<log> <level>DEBUG</level> <template>Proxying operation: {}</template> </log> <log> <level>DEBUG</level> <template>Called RouterClientProtocol:getAdditionalDatanode</template> </log>"
  },
  "94710115_1": {
    "exec_flow": "ENTRY→CALL:genericForward→IF_TRUE:hsr!=null→CALL:RMWebAppUtil.getCallerUserGroupInformation→IF_FALSE:callerUGI==null→CALL:doAs→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}]→RETURN→EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "94710115_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:init→CALL:org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getInterceptorChain→CALL:org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getRootInterceptor→CALL:org.apache.hadoop.yarn.server.router.webapp.RESTRequestInterceptor:getClusterInfo [INHERITED CHILD LOGS]→LOG:Handling deprecation for all properties in config...→RETURN→EXIT",
    "log": "<log>[INFO] Request to start an already existing user: {} was received, so ignoring.</log> <log>[ERROR] Cannot get user: {}</log> <log>[INFO] Initializing request processing pipeline for user: {}</log> <log>[ERROR] Init RESTRequestInterceptor error for user: {}</log>"
  },
  "65d2e0e4_1": {
    "exec_flow": "ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → CALL:initialize → ENTRY → CALL:getAuthenticationMethod → IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → TRY → CALL:HadoopKerberosName.setConfiguration → EXCEPTION:setConfiguration → CATCH:IOException → THROW: new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe) → EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:LOG.DEBUG: Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT → EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "65d2e0e4_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "493c7510_1": {
    "exec_flow": "ENTRY→TRY→CALL:DurationInfo:<init>→CALL:getJobAttemptPath→CALL:getFileSystem→IF_FALSE: scheme == null && authority == null→IF_TRUE: scheme != null && authority == null→CALL:getDefaultUri→IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null→IF_TRUE: conf.getBoolean(disableCacheName, false)→LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\")→CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration)→RETURN→EXIT",
    "log": "[DurationInfo LOG] Listing pending uploads [DEBUG] Bypassing cache to create filesystem {}"
  },
  "bc43554a_1": {
    "exec_flow": "ENTRY→CALL:dispatcher.getEventHandler→CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→EXIT",
    "log": "[ERROR] Received event [ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties."
  },
  "bc43554a_2": {
    "exec_flow": "ENTRY→CALL:dispatcher.getEventHandler→CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for application + appID, t"
  },
  "bc43554a_3": {
    "exec_flow": "ENTRY→CALL:dispatcher.getEventHandler→CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→EXIT",
    "log": "[WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "bc43554a_4": {
    "exec_flow": "ENTRY→CALL:dispatcher.getEventHandler→CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→EXIT",
    "log": "[ERROR] No component exists for {event.getName()}"
  },
  "bc43554a_5": {
    "exec_flow": "ENTRY→CALL:dispatcher.getEventHandler→CALL:org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→EXIT",
    "log": "[ERROR] [COMPONENT {component.getName()}]: Error in handling event type {event.getType()}"
  },
  "cde939c7_1": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: enterStateResult != STATE.STOPPED → CALL: enterState → CALL: serviceStop() → CALL: noteFailure → CATCH: Exception e → IF: exception != null → LOG: LOG.DEBUG: noteFailure → SYNC: failureCause == null → SET: failureCause = exception → SET: failureState = getServiceState() → LOG: LOG.INFO: Service {} failed in state {} → FINALLY → EXIT",
    "log": "<log>LOG.debug(\"noteFailure\", exception)</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log>"
  },
  "cde939c7_2": {
    "exec_flow": "ENTRY → IF_TRUE: state == STATE.STOPPED → RETURN → EXIT",
    "log": "<log>LOG.debug(\"Ignoring re-entrant call to stop()\")</log>"
  },
  "cde939c7_3": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log>LOG.WARN: Exception while notifying listeners of {}</log>"
  },
  "b9cd5a73_1": {
    "exec_flow": "ENTRY→CALL:requireSelectSupport→IF_TRUE:fileStatus == null→CALL:innerGetFileStatus→IF_TRUE:fileStatus.isDirectory()→THROW:new FileNotFoundException(path.toString() + \" is a directory\")→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Stripping trailing '/' from {q}</log> <log>[DEBUG] Getting path status for {} ({}); needEmptyDirectory={}</log> <log>[DEBUG] S3GetFileStatus {}</log>"
  },
  "b9cd5a73_2": {
    "exec_flow": "ENTRY→CALL:requireSelectSupport→IF_FALSE:fileStatus == null→IF_TRUE:fileStatus.isDirectory()→THROW:new FileNotFoundException(path.toString() + \" is a directory\")→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Stripping trailing '/' from {q}</log> <log>[DEBUG] Getting path status for {} ({}); needEmptyDirectory={}</log> <log>[DEBUG] S3GetFileStatus {}</log>"
  },
  "cf2cc08c_1": {
    "exec_flow": "<step>Read a line.</step> <step>Check if file position is within limits or needs additional record.</step> <step>Set key position.</step> <step>Read line based on position.</step> <step>Log Skipped line of size if necessary.</step> <step>Check line length for continuation or completion.</step> <LOG>INFO: Skipped line of size {newSize} at pos {pos - newSize}</LOG> <LOG>INFO: Found UTF-8 BOM and skipped it</LOG>",
    "log": "<log>[INFO] Skipped line of size {newSize} at pos {pos - newSize}</log> <log>[INFO] Found UTF-8 BOM and skipped it</log>"
  },
  "cf2cc08c_2": {
    "exec_flow": "ENTRY→IF_TRUE: val instanceof Configurable→CALL: org.apache.hadoop.conf.Configurable:setConf→CALL: seekToCurrentValue→IF_FALSE: !blockCompressed→CALL: deserializeValue→IF_TRUE: (valLength < 0) AND org.slf4j.Logger:isDebugEnabled()→LOG: LOG.DEBUG: val + is a zero-length value→RETURN→EXIT ENTRY→IF_TRUE: this.conf.getBoolean(IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT)→CALL: org.apache.hadoop.conf.Configuration:getBoolean→LOG: LOG.WARN: Bad checksum at {position}. Skipping entries.→CALL: sync→CALL: org.apache.hadoop.conf.Configuration:getInt→EXIT",
    "log": "<log>[DEBUG] val + is a zero-length value</log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log>"
  },
  "cf2cc08c_3": {
    "exec_flow": "ENTRY→IF_TRUE: val instanceof Configurable→CALL: org.apache.hadoop.conf.Configurable:setConf→CALL: seekToCurrentValue→IF_TRUE: !blockCompressed→CALL: deserializeValue→IF_TRUE: valIn.read() > 0→LOG: LOG.INFO: available bytes: + valIn.available()→THROW: new IOException(val + \" read \" + (valBuffer.getPosition() - keyLength) + \" bytes, should read \" + (valBuffer.getLength() - keyLength))→EXIT ENTRY→IF_TRUE: this.conf.getBoolean(IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT)→CALL: org.apache.hadoop.conf.Configuration:getBoolean→LOG: LOG.WARN: Bad checksum at {position}. Skipping entries.→CALL: sync→CALL: org.apache.hadoop.conf.Configuration:getInt→EXIT",
    "log": "<log>[INFO] available bytes: <bytes></log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log>"
  },
  "cf2cc08c_4": {
    "exec_flow": "ENTRY→IF_FALSE: val instanceof Configurable→CALL: seekToCurrentValue→IF_TRUE: !blockCompressed→CALL: deserializeValue→IF_TRUE: valIn.read() > 0→LOG: LOG.INFO: available bytes: + valIn.available()→THROW: new IOException(val + \" read \" + (valBuffer.getPosition() - keyLength) + \" bytes, should read \" + (valBuffer.getLength() - keyLength))→EXIT ENTRY→IF_TRUE: this.conf.getBoolean(IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT)→CALL: org.apache.hadoop.conf.Configuration:getBoolean→LOG: LOG.WARN: Bad checksum at {position}. Skipping entries.→CALL: sync→CALL: org.apache.hadoop.conf.Configuration:getInt→EXIT",
    "log": "<log>[INFO] available bytes: <bytes></log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log>"
  },
  "cf2cc08c_5": {
    "exec_flow": "ENTRY→IF_FALSE: val instanceof Configurable→CALL: seekToCurrentValue→IF_FALSE: !blockCompressed→CALL: deserializeValue→IF_TRUE: (valLength < 0) AND org.slf4j.Logger:isDebugEnabled()→LOG: LOG.DEBUG: val + is a zero-length value→RETURN→EXIT ENTRY→IF_TRUE: this.conf.getBoolean(IO_SKIP_CHECKSUM_ERRORS_KEY, IO_SKIP_CHECKSUM_ERRORS_DEFAULT)→CALL: org.apache.hadoop.conf.Configuration:getBoolean→LOG: LOG.WARN: Bad checksum at {position}. Skipping entries.→CALL: sync→CALL: org.apache.hadoop.conf.Configuration:getInt→EXIT",
    "log": "<log>[DEBUG] val + is a zero-length value</log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log>"
  },
  "cf2cc08c_6": {
    "exec_flow": "<entry>org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:next</entry> <virtual_call>org.apache.hadoop.io.SequenceFile$Reader:nextRawKey</virtual_call> <virtual_call>org.apache.hadoop.io.SequenceFile$Reader:readBlock</virtual_call>",
    "log": "<!-- Inherited from child -->"
  },
  "0ce8d861_1": {
    "exec_flow": "ENTRY → FOREACH: elfs → IF_TRUE: elf.isInProgress() → IF_FALSE: !inProgressOk → TRY → CALL: scanLog → EXCEPTION: scanLog → CATCH: IOException e → LOG: LOG.ERROR: got IOException while trying to validate header of + elf + . Skipping., e → CONTINUE → EXIT",
    "log": "[ERROR] got IOException while trying to validate header of + elf + . Skipping., e"
  },
  "0ce8d861_2": {
    "exec_flow": "ENTRY → FOREACH: elfs → IF_TRUE: elf.isInProgress() → IF_FALSE: !inProgressOk → TRY → CALL: scanLog → IF_TRUE: elf.lastTxId < fromTxId → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: passing over + elf + because it ends at + elf.lastTxId + , but we only care about transactions + as new as + fromTxId → CONTINUE → EXIT",
    "log": "[DEBUG] passing over + elf + because it ends at + elf.lastTxId + , but we only care about transactions + as new as + fromTxId"
  },
  "0ce8d861_3": {
    "exec_flow": "ENTRY → FOREACH: elfs → IF_FALSE: elf.isInProgress() → IF_TRUE: elf.lastTxId < fromTxId → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: passing over + elf + because it ends at + elf.lastTxId + , but we only care about transactions + as new as + fromTxId → CONTINUE → EXIT",
    "log": "[DEBUG] passing over + elf + because it ends at + elf.lastTxId + , but we only care about transactions + as new as + fromTxId"
  },
  "0ce8d861_4": {
    "exec_flow": "ENTRY → FOREACH: elfs → IF_FALSE: elf.isInProgress() → IF_FALSE: elf.lastTxId < fromTxId → CALL: add → EXIT",
    "log": "[DEBUG] selecting edit log stream + elf"
  },
  "1bddcf5c_1": {
    "exec_flow": "ENTRY→CALL:FilterBase:accept→TRY→IF_FALSE: key instanceof Text→IF_TRUE: key instanceof BytesWritable→CALL:MD5Hashcode→IF_TRUE: hashcode / frequency * frequency == hashcode→RETURN→EXIT",
    "log": "[DEBUG] Configuring job jar"
  },
  "1bddcf5c_2": {
    "exec_flow": "ENTRY→CALL:FilterBase:accept→TRY→CATCH→CALL:warn→THROW:RuntimeException→EXIT",
    "log": "[WARN] Exception occurred"
  },
  "efc89b4b_1": {
    "exec_flow": "<step>ENTRY: getDomain(java.lang.String)</step> <step>CALL: getTimelineDomain(DBIterator, java.lang.String, byte[])</step> <step>TRY: LeveldbIterator</step> <step>NEW: LeveldbIterator</step> <step>CALL: seek</step> <step>EXCEPTION: seek</step> <step>CATCH: DBException e</step> <step>THROW: new IOException(e)</step> <step>CALL: IOUtils:cleanupWithLogger</step> <step>FOREACH:closeables</step> <step>IF(c != null)</step> <step>TRY</step> <step>CALL:c.close()</step> <step>CATCH:Throwable</step> <step>IF(logger != null)</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[INFO] Service stopped, return null for the storage</log> <log>[DEBUG] Exception in closing {}</log> <log>[ERROR] Unrecognized domain column: {column}</log>"
  },
  "efc89b4b_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>LOG: LOG.DEBUG: Verifying the access of + (callerUGI == null ? null : callerUGI.getShortUserName()) + on the timeline domain + domain</step> <step>IF_FALSE: !adminAclsManager.areACLsEnabled()</step> <step>IF_FALSE: owner == null || owner.length() == 0</step> <step>IF_TRUE: callerUGI != null && (adminAclsManager.isAdmin(callerUGI) || callerUGI.getShortUserName().equals(owner))</step> <step>CALL: adminAclsManager.isAdmin</step> <step>ENTRY</step> <step>CALL:isUserInList</step> <step>RETURN</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Verifying the access of [user]</log>"
  },
  "8a6f08cb_1": {
    "exec_flow": "ENTRY→CALL:checkOpen→LOG:LOG.DEBUG:{}:masked={}→CALL:DFSOutputStream.newStreamForCreate→CALL:beginFileLease→RETURN→EXIT",
    "log": "[DEBUG] {}: masked={}"
  },
  "95188b37_1": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→CALL: noteFailure→THROW: ServiceStateException.convert(e)→CALL: notifyListeners",
    "log": "[DEBUG] Service: {} entered state {} [WARN] Exception while notifying listeners of {} [DEBUG] noteFailure [INFO] Service {} failed in state {}"
  },
  "95188b37_2": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners",
    "log": "[DEBUG] Ignoring re-entrant call to stop()"
  },
  "d4bcfd9d_1": {
    "exec_flow": "<entry_point>Parent.ENTRY → [VIRTUAL_CALL] → org.apache.hadoop.mapreduce.counters.AbstractCounters:write(java.io.DataOutput)</entry_point> <steps> <step>WritableUtils.writeVInt(out, groupFactory.version());</step> <step>WritableUtils.writeVInt(out, fgroups.size());</step> <step>/* Loop over fgroups.values() */</step> <step>/* Handle FrameworkCounterGroup */</step> <step>WritableUtils.writeVInt(out, GroupType.FRAMEWORK.ordinal());</step> <step>/* Get and write Framework Group ID */</step> <step>group.write(out);</step> <step>/* Handle FileSystemCounterGroup */</step> <step>WritableUtils.writeVInt(out, GroupType.FILESYSTEM.ordinal());</step> <step>group.write(out);</step> <step>/* Conditional check for writeAllCounters */</step> <step>WritableUtils.writeVInt(out, groups.size());</step> <step>/* Loop and write group names */</step> <step>Text.writeString(out, group.getName());</step> <step>group.write(out);</step> </steps>",
    "log": "[DEBUG] Writing number of properties: + props.size() [DEBUG] Writing each key-value pair [DEBUG] Starting write operation"
  },
  "d4bcfd9d_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.io.GenericWritable:write → IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_TRUE: len > 0xffff → THROW: new IOException(\"string too long!\") → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "[DEBUG] Starting write operation [WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "d4bcfd9d_3": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.io.GenericWritable:write → IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "[DEBUG] Starting write operation [WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)"
  },
  "d4bcfd9d_4": {
    "exec_flow": "<step>CompositeInputSplit:write-ENTRY</step> <step>TRY → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance</step> <step>getSerialization: Loop through serializations</step> <step>Check if serialization accepts class</step> <step>If accepted, return serialization</step> <step>Return null if no serialization matches</step> <step>CATCH:ClassNotFoundException e → EXIT</step>",
    "log": "[WARN] Serialization class not found: , e"
  },
  "129189cf_1": {
    "exec_flow": "ENTRY→CALL:removeApplication→CALL:EventHandler:handle→TRY→IF_TRUE:applicationId==null→CALL:org.slf4j.Logger:error→ALIGN:stateMachine.doTransition→EXIT",
    "log": "<log> <level>DEBUG</level> <message>RMAppManager processing event for ApplicationId of type eventType</message> </log> <log> <level>ERROR</level> <message>RMAppManager received completed appId of null, skipping</message> </log> <log> <level>INFO</level> <message>[SERVICE] Transitioned from {} to {} on {} event.</message> </log>"
  },
  "129189cf_2": {
    "exec_flow": "ENTRY→CALL:removeApplication→CALL:EventHandler:handle→TRY→TRY→CALL:this.stateMachine.doTransition→EXCEPTION:doTransition→CATCH:InvalidStateTransitionException e→LOG:LOG.ERROR:Can't handle this event at current state, e→CALL:writeLock.unlock→EXIT",
    "log": "<log> <level>ERROR</level> <message>Can't handle this event at current state, e</message> </log>"
  },
  "129189cf_3": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[UPDATE_CONTAINER]→IF_FALSE:event instanceof UpdateContainerSchedulerEvent→LOG:LOG.ERROR:Unknown event type on UpdateContainer: {event.getType()}→BREAK→EXIT",
    "log": "<log> <level>ERROR</level> <message>Unknown event type on UpdateContainer: {event.getType()}</message> </log>"
  },
  "129189cf_4": {
    "exec_flow": "ENTRY→SWITCH:event.getType()→CASE:[]→LOG:LOG.ERROR:Unknown event arrived at ContainerScheduler: {event.toString()}→EXIT",
    "log": "<log> <level>ERROR</level> <message>Unknown event arrived at ContainerScheduler: {event.toString()}</message> </log>"
  },
  "129189cf_5": {
    "exec_flow": "ENTRY→CALL:removeApplication→CALL:EventHandler:handle→CALL:handleStoreEvent→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "<log> <level>WARN</level> <message>Unsupported operation</message> </log>"
  },
  "129189cf_6": {
    "exec_flow": "ENTRY→CALL:removeApplication→CALL:EventHandler:handle→CALL:handleStoreEvent→CALL:org.slf4j.Logger:error→THROW:YarnRuntimeException→EXIT",
    "log": "<log> <level>ERROR</level> <message>Failed to store attribute modification to storage</message> </log>"
  },
  "129189cf_7": {
    "exec_flow": "ENTRY→IF_TRUE: rmApp != null→TRY→CALL: rmApp.handle→EXCEPTION: handle→CATCH: Throwable t→LOG: LOG.ERROR: Error in handling event type + event.getType() + for application + appID, t→EXIT",
    "log": "<log> <level>ERROR</level> <message>Error in handling event type + event.getType() + for application + appID, t</message> </log>"
  },
  "84333f04_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT LOG: Setting up job parameters and classes → STEP: Check arguments → STEP: Job instance creation → STEP: Set job parameters (name, jar, input format) → CALL: JobClient:getConf() → CALL: Job.getInstance → CALL: TeraInputFormat.setInputPaths(job, new Path(args[0])) → CALL: FileOutputFormat.setOutputPath(job, new Path(args[1])) → IF_FALSE: args.length == 0 → IF_FALSE: numBytesToWritePerMap == 0 → LOG: [INFO] Job instance created → CALL: setNumReduceTasks → CALL: waitForCompletion → RETURN",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>INFO: MultiFileWordCount Job Configuration started</log> <log>INFO: Input/Output paths set</log> <log>INFO: Job completed successfully</log>"
  },
  "84333f04_2": {
    "exec_flow": "ENTRY→CALL:buildGeneralOptions→TRY→CALL:parse→CALL:preProcessForWindows→CALL:preProcessForWindows→CALL:processGeneralOptions→EXCEPTION:processGeneralOptions→CATCH:ParseException e→CALL:printHelp→RETURN→EXIT STEP: Check arguments → STEP: Job instance creation → STEP: Set job parameters (name, jar, input format) → IF_FALSE: args.length == 0 → IF_FALSE: numBytesToWritePerMap == 0 → LOG: [INFO] Job instance created → CALL: setNumReduceTasks → CALL: waitForCompletion → RETURN",
    "log": "<log>[WARN] options parsing failed: + e.getMessage()</log> <log>INFO: MultiFileWordCount Job Configuration started</log> <log>INFO: Input/Output paths set</log> <log>INFO: Job completed successfully</log>"
  },
  "1fc43784_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND→CALL:setCurrentStreamer→IF:si.isHealthy()→CALL:enqueueCurrentPacket→CALL:handleCurrentStreamerFailure→FOR_UPDATE→FOR_COND→FOR_EXIT→CALL:setCurrentStreamer→IF_TRUE: getStreamer().getAppendChunk() && getStreamer().getBytesCurBlock() % bytesPerChecksum == 0→CALL: getStreamer().setAppendChunk→CALL: resetChecksumBufSize→IF_TRUE: !getStreamer().getAppendChunk()→CALL: computePacketChunkSize→ENTRY→CALL:max→CALL:DFSClient.LOG.debug→EXIT",
    "log": "[DEBUG] computePacketChunkSize: src={}, chunkSize={}, chunksPerPacket={}, packetSize={}"
  },
  "1fc43784_2": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND→CALL:setCurrentStreamer→IF:si.isHealthy()→CALL:enqueueCurrentPacket→CALL:handleCurrentStreamerFailure→FOR_UPDATE→FOR_COND→FOR_EXIT→CALL:setCurrentStreamer→IF_FALSE: getStreamer().getAppendChunk() && getStreamer().getBytesCurBlock() % bytesPerChecksum == 0→IF_TRUE: !getStreamer().getAppendChunk()→CALL: computePacketChunkSize→ENTRY→CALL:max→CALL:DFSClient.LOG.debug→EXIT",
    "log": "[DEBUG] computePacketChunkSize: src={}, chunkSize={}, chunksPerPacket={}, packetSize={}"
  },
  "1fc43784_3": {
    "exec_flow": "ENTRY→LOG:LOG.WARN→CALL:streamer.getErrorState().setInternalError→CALL:streamer.close→FOREACH:streamers→FOREACH_EXIT→CALL:org.slf4j.Logger:isDebugEnabled()→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:checkStreamers:+streamers→LOG:LOG.DEBUG:healthy streamer count=+(numAllBlocks-failCount)→LOG:LOG.DEBUG:original failed streamers:+failedStreamers→LOG:LOG.DEBUG:newly failed streamers:+newFailed→IF_TRUE:failCount>(numAllBlocks-numDataBlocks)→CALL:closeAllStreamers→THROW:new IOException(\"Failed: the number of failed blocks = \"+failCount+\"> the number of parity blocks = \"+(numAllBlocks-numDataBlocks))→EXIT",
    "log": "[WARN] Failed: [err], [this], [e] [DEBUG] checkStreamers: ... [DEBUG] healthy streamer count= ... [DEBUG] original failed streamers: ... [DEBUG] newly failed streamers: ..."
  },
  "1fc43784_4": {
    "exec_flow": "ENTRY→LOG:LOG.WARN→CALL:streamer.getErrorState().setInternalError→CALL:streamer.close→FOREACH:streamers→FOREACH_EXIT→CALL:org.slf4j.Logger:isDebugEnabled()→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:checkStreamers:+streamers→LOG:LOG.DEBUG:healthy streamer count=+(numAllBlocks-failCount)→LOG:LOG.DEBUG:original failed streamers:+failedStreamers→LOG:LOG.DEBUG:newly failed streamers:+newFailed→IF_FALSE:failCount>(numAllBlocks-numDataBlocks)→RETURN→EXIT",
    "log": "[WARN] Failed: [err], [this], [e] [DEBUG] checkStreamers: ... [DEBUG] healthy streamer count= ... [DEBUG] original failed streamers: ... [DEBUG] newly failed streamers: ..."
  },
  "1fc43784_5": {
    "exec_flow": "ENTRY→CALL:getStreamer→SYNC:dataQueue→TRY→CALL:org.apache.hadoop.hdfs.DataStreamer:queuePacket→IF_FALSE:packet == null→CALL:addTraceParent→CALL:dataQueue.addLast→SET:lastQueuedSeqno→CALL:org.slf4j.Logger:debug→CALL:notifyAll→EXIT",
    "log": "[DEBUG] Closed channel exception [DEBUG] Queued {}, {}"
  },
  "a9ebe4e6_1": {
    "exec_flow": "ENTRY → CALL: Thread.setDefaultUncaughtExceptionHandler → TRY → CALL: org.apache.hadoop.security.UserGroupInformation:getCurrentUser() → IF_FALSE: !user.equals(uid) → CALL: org.apache.hadoop.fs.FileContext:getLocalFSFileContext() → CALL: org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider:getRecordFactory → CALL: org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:<init> → CALL: runLocalization → CALL: initDirs → CALL: org.apache.hadoop.security.UserGroupInformation:createRemoteUser → CALL: addToken → CALL: doAs → CALL: createDownloadThreadPool → TRY → CALL: localizeFiles → EXCEPTION: runLocalization → CATCH: Throwable e → CALL: printStackTrace → LOG: LOG.ERROR: Exception in main:, e → LOG: [ERROR] Heartbeat failed while dying: {exception} → CALL: System.exit → EXIT",
    "log": "<log>LOG.ERROR: Exception in main:, e</log> <log>[ERROR] Heartbeat failed while dying: {exception}</log>"
  },
  "a9ebe4e6_2": {
    "exec_flow": "ENTRY→IF_TRUE: defaultFsUri.getScheme() != null && !defaultFsUri.getScheme().trim().isEmpty()→CALL:getFileContext→ENTRY→SYNC:UserGroupInformation.class→CALL:initialize→ENTRY→CALL:getAuthenticationMethod→IF_TRUE:!isInitialized()→CALL:LOG_DEPRECATION.info→CALL:org.apache.hadoop.conf.Configuration:getBoolean→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT→CALL:ensureInitialized→IF_FALSE: (subject == null || subject.getPrincipals(User.class).isEmpty())→CALL:getLoginUser→RETURN→EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log> <log>[LOG] getLoginUser</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log>"
  },
  "a9ebe4e6_3": {
    "exec_flow": "ENTRY→IF_TRUE: defaultFsUri.getScheme() != null && !defaultFsUri.getScheme().trim().isEmpty()→CALL:getFileContext→ENTRY→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<AbstractFileSystem>→CALL:get→RETURN→EXIT→RETURN→EXIT",
    "log": "<log>[ERROR] ex.toString()</log>"
  },
  "59da4846_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:<init>→CALL:putIfAbsent→ENTRY→SYNC: collectors→CALL: get→IF_TRUE: collectorInTable == null→TRY→CALL: init→CALL: setWriter→CALL: start→CALL: put→CALL: postPut→LOG: info: the collector for + appId + was added→RETURN→EXIT→EXIT",
    "log": "[INFO] the collector for + appId + was added"
  },
  "59da4846_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:<init>→CALL:putIfAbsent→ENTRY→SYNC: collectors→CALL: get→IF_FALSE: collectorInTable == null→LOG: info: the collector for + appId + already exists!→RETURN→EXIT→EXIT",
    "log": "[INFO] the collector for + appId + already exists!"
  },
  "3299d8fe_1": {
    "exec_flow": "ENTRY->IF_TRUE: resource == null->LOG:LOG.ERROR: Invalid deduction of null resource for + rmNode.getNodeAddress()->RETURN->EXIT",
    "log": "<!-- [ERROR] Invalid deduction of null resource for rmNode.getNodeAddress() -->"
  },
  "3299d8fe_2": {
    "exec_flow": "ENTRY→FOR_INIT→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes()→FOR_COND: i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int)→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int)→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue(int,long)→FOR_BODY→FOR_UPDATE→FOR_COND: i < maxLength→EXCEPTION:ResourceNotFoundException→LOG.warn→FOR_UPDATE→FOR_COND: i < maxLength→FOR_EXIT→RETURN→EXIT",
    "log": "<!-- [WARN] Resource is missing: -->"
  },
  "c42e36ff_1": {
    "exec_flow": "ENTRY → CALL:httpConfig.setSecureScheme → CALL:httpConfig.addCustomizer → CALL:sslContextFactory.setNeedClientAuth → IF_FALSE:keyPassword!=null → IF_FALSE:keyStore!=null → IF_FALSE:trustStore!=null → IF_TRUE:null!=excludeCiphers&&!excludeCiphers.isEmpty() → CALL:sslContextFactory.setExcludeCipherSuites → LOG:[INFO] Excluded Cipher List: + excludeCiphers → CALL:setEnabledProtocols → CALL:getTrimmed → LOG:[DEBUG] Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:[DEBUG] Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → FOREACH:names → CALL:makeConfigurationChangeMonitor → IF_TRUE: keyStore != null → CALL:add → IF_TRUE: trustStore != null → CALL:add → CALL:schedule → LOG:[INFO] Reloading keystore and truststore certificates. → RETURN → EXIT",
    "log": "<log>[INFO] Excluded Cipher List: + excludeCiphers</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] Reloading keystore and truststore certificates.</log>"
  },
  "7d88b295_1": {
    "exec_flow": "<seq>ENTRY→IF_TRUE:StringUtils.isBlank(filename)→CALL:org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT</seq>",
    "log": "<log>[WARN] NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured.</log>"
  },
  "7d88b295_2": {
    "exec_flow": "<seq>ENTRY→IF_FALSE:StringUtils.isBlank(filename)→CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)→TRY→CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)→CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)→RETURN→EXIT</seq>",
    "log": "<log>[WARN] {filename} cannot be read.</log>"
  },
  "f5a05fd7_1": {
    "exec_flow": "ENTRY→IF_TRUE:excludeDatanodes.getValue()!=null→CALL:System.arraycopy→CALL:toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])→ENTRY→CALL:makeQualified→CALL:getAuthParameters→ENTRY→IF_TRUE: !op.getRequireAuth()→CALL:getDelegationToken→IF_TRUE: token != null→CALL:authParams.add→CALL:encodeToUrlString→CALL:toArray→RETURN→CALL:getNamenodeURL→LOG→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Delegation token encoded [INFO] Returning authentication parameters [TRACE] url={}"
  },
  "f5a05fd7_2": {
    "exec_flow": "ENTRY→IF_FALSE:excludeDatanodes.getValue()!=null→CALL:toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])→ENTRY→CALL:makeQualified→CALL:getAuthParameters→ENTRY→IF_TRUE: !op.getRequireAuth()→CALL:getDelegationToken→IF_TRUE: token != null→CALL:authParams.add→CALL:encodeToUrlString→CALL:toArray→RETURN→CALL:getNamenodeURL→LOG→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Delegation token encoded [INFO] Returning authentication parameters [TRACE] url={}"
  },
  "e362f21c_1": {
    "exec_flow": "<step>checkPath executed</step> <step>Path is absolute</step> <step>Converted path to File</step>",
    "log": "<log>Check path: execution started</log> <log>Path is determined to be absolute</log> <log>Path conversion to File completed</log>"
  },
  "73d70e1e_1": {
    "exec_flow": "ENTRY→CALL:incCounter→CALL:incAllGauges→CALL:incrementGauge→IF_TRUE: metric == null→CALL:org.slf4j.Logger:debug→EXIT→RETURN→EXIT",
    "log": "[DEBUG] No Gauge: + op"
  },
  "66919394_1": {
    "exec_flow": "ENTRY→IF_FALSE: serverProviderUri == null→TRY→CALL: get→CALL: createKeyProviderFromUri→EXCEPTION→CALL: error→RETURN→EXIT",
    "log": "[ERROR] Could not create KeyProvider for DFSClient !!"
  },
  "66919394_2": {
    "exec_flow": "ENTRY→IF_TRUE: feInfo != null→TRY→CALL:getTracer→CALL:newScope→CALL:getKeyProvider→CALL:invoke: org.apache.hadoop.hdfs.HdfsKMSUtil:createWrappedInputStream→CALL:HdfsKMSUtil.getCryptoProtocolVersion→IF_FALSE:suite.equals(CipherSuite.UNKNOWN)→CALL:org.apache.hadoop.crypto.CryptoCodec:getInstance→CALL:getInstance→CALL:convert→CALL:convert→CALL:getCodecClasses→CALL:newInstance→CALL:decryptEncryptedDataEncryptionKey→NEW:CryptoInputStream→NEW:HdfsDataInputStream→RETURN→EXIT",
    "log": "[INFO] Creating a Wrapped Input Stream since feInfo is not null [DEBUG] Codec classes obtained [DEBUG] New instance created"
  },
  "66919394_3": {
    "exec_flow": "ENTRY→IF_FALSE: feInfo != null→NEW:HdfsDataInputStream→RETURN→EXIT",
    "log": "[INFO] No encryption info; returning direct stream"
  },
  "0b29725a_1": {
    "exec_flow": "ENTRY→FOREACH:context.getAllPartitions()→IF_FALSE:null==queueNames→FOREACH:queueNames→IF_FALSE:null==leafQueue→IF_FALSE:leafQueue.getQueueCapacities().getUsedCapacity(partition)<context.getMinimumThresholdForIntraQueuePreemption()→CALL:org.apache.hadoop.yarn.util.resource.Resources:subtract→CALL:org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:computeAppsIdealAllocation→ENTRY→CALL:calculateUsedAMResourcesPerQueue→CALL:Resources.subtractFrom→IF_FALSE:apps.size()==1→CALL:createTempAppForResCalculation→CALL:calculateIdealAssignedResourcePerApp→CALL:Resources:multiply→IF_TRUE:Resources.greaterThan(rc,clusterResource,maxIntraQueuePreemptable,tq.getActuallyToBePreempted())→CALL:Resources.subtractFrom→CALL:Resources:min→CALL:Resources:clone→CALL:calculateToBePreemptedResourcePerApp→CALL:addAllApps→CALL:validateOutSameAppPriorityFromDemand→IF_TRUE:[DEBUG] Queue Name:queueName, partition:partition→FOREACH:tq.getApps→FOREACH_EXIT→EXIT→EXIT",
    "log": "[DEBUG] Queue Name:queueName, partition:partition"
  },
  "0b29725a_2": {
    "exec_flow": "<entry_point>Parent.ENTRY</entry_point> <virtual_call>true</virtual_call> <child_paths> <path> <id>P1-C1</id> <exec_flow> ENTRY→CALL:initializeUsageAndUserLimitForCompute→LOG:[DEBUG] Rolling resource usage for user:{} is : {}→FOREACH:tq.getApps→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Rolling resource usage for user:{} is : {}"
  },
  "0b29725a_3": {
    "exec_flow": "ENTRY→FOREACH:selectedCandidates.values()→FOREACH:containers→IF_FALSE:null==schedulerNode→IF_FALSE:null==res→CALL:org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition:deductActuallyToBePreempted→CALL:deductPreemptableResourcePerApp→FOREACH:tas→[VIRTUAL_CALL]→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND:i<maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→CONTINUE→EXIT",
    "log": "[DEBUG] Rolling resource usage for user:{} is : {}"
  },
  "ec74302a_1": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>CALL:clone</step> <step>CALL:subtractFrom</step> <step>CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes</step> <step>FOR_INIT</step> <step>FOR_COND: i < maxLength</step> <step>TRY</step> <step>CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation</step> <step>CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation</step> <step>CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue</step> <step>CATCH:ResourceNotFoundException</step> <step>CALL:org.slf4j.Logger:warn</step> <step>FOR_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "ec74302a_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: lhs.equals(rhs)</step> <step>IF_FALSE: isAllInvalidDivisor(clusterResource)</step> <step>TRY</step> <step>EXCEPTION:ArrayIndexOutOfBoundsException</step> <step>CALL:org.slf4j.Logger:error(java.lang.String)</step> <step>CALL:org.slf4j.Logger:error(java.lang.String)</step> <step>THROW:YarnRuntimeException</step>",
    "log": "[ERROR] A problem was encountered while calculating resource availability that should not occur under normal circumstances. Please report this error to the Hadoop community by opening a JIRA ticket at http://issues.apache.org/jira and including the following information: * Exception encountered: <stack_trace> * Cluster resources: <cluster_resources> * LHS resource: <lhs_resources> * RHS resource: <rhs_resources> [ERROR] The resource manager is in an inconsistent state. It is safe for the resource manager to be restarted as the error encountered should be transitive. If high availability is enabled, failing over to a standby resource manager is also safe."
  },
  "c23aa57b_1": {
    "exec_flow": "<step>ENTRY</step> <step>SYNC: stateChangeLock</step> <step>If: (isInState(STATE.STARTED))</step> <step>RETURN</step> <step>If: (stateModel.enterState(STATE.STARTED) != STATE.STARTED)</step> <step>setCurrentMillis: startTime</step> <step>CALL: serviceStart</step> <step>If: (isInState(STATE.STARTED))</step> <step>LOG: DEBUG - Service {getName()} is started</step> <step>[Parent-NotifyListeners]</step> <step>CALL: notifyListeners</step> <step>CATCH: Exception e</step> <step>CALL: noteFailure</step> <step>CALL: ServiceOperations.stopQuietly</step> <step>THROW: ServiceStateException.convert</step> <step>EXIT</step> <step>LOG: WARN - Exception while notifying listeners of {}</step>",
    "log": "<log> <level>DEBUG</level> <template>Service {getName()} is started</template> </log> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "6add765a_1": {
    "exec_flow": "ENTRY -> [VIRTUAL_CALL] -> CALL: rpcServer.checkOperation -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {methodName} -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "6add765a_2": {
    "exec_flow": "ENTRY -> [VIRTUAL_CALL] -> CALL: rpcServer.checkOperation -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {methodName} -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "9f7ed1b8_1": {
    "exec_flow": "<sequence> ENTRY IF_TRUE:NameNode.stateChangeLog.isDebugEnabled() CALL:org.slf4j.Logger:isDebugEnabled() CALL:NameNode.stateChangeLog.debug CALL:checkBlock CALL:org.apache.hadoop.hdfs.server.namenode.FSDirectory:resolvePath CALL:completeFileInternal TRY CALL:getLastINode CALL:checkLease IF_FALSE:!fsn.checkFileProgress(src, pendingFile, false) CALL:commitOrCompleteLastBlock IF_FALSE:!fsn.checkFileProgress(src, pendingFile, true) CALL:addCommittedBlocksToPending CALL:finalizeINodeFileUnderConstruction CALL:getEditLog().logCloseFile CALL:NameNode.stateChangeLog.debug ENTRY CALL:getInstance CALL:setPath CALL:setReplication CALL:setModificationTime CALL:setAccessTime CALL:setBlockSize CALL:setBlocks CALL:setPermissionStatus CALL:logEdit ENTRY CALL:beginTransaction LOG:LOG.DEBUG:doEditTx() op={} txid={} TRY CALL:editLogStream.writeRaw CALL:endTransaction EXIT CALL:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp) LOG:LOG.INFO:Number of transactions: <numTransactions> LOG:LOG.DEBUG:logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} ENTRY FOREACH:bc.getBlocks() IF:hasEnoughEffectiveReplicas CALL:neededReconstruction.add FOREACH_EXIT EXIT ENTRY FOREACH:bc.getBlocks() IF:shouldProcessExtraRedundancy CALL:processExtraRedundancyBlock FOREACH_EXIT CALL:org.slf4j.Logger:info RETURN EXIT </sequence>",
    "log": "[INFO] DIR* completeFile: ${src} is closed by ${holder} [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()} [DEBUG] DIR* NameSystem.completeFile: String srcArg for String holder [DEBUG] doEditTx() op={} txid={} [INFO] Number of transactions: <numTransactions> [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [DEBUG] Block added to reconstruction queue [INFO] Processing extra redundancy block [INFO] BLOCK* + err + (numNodes= + numNodes + (numNodes < min ? < : >= ) + minimum = + min + ) in file + src [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions} [DEBUG] Exception in closing {}"
  },
  "bf1328eb_1": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → CALL:checkPathIsSlash → NEW:FileStatus → CALL:getShortUserName → CALL:getPrimaryGroupName → CALL:isFile → IF_TRUE:groups.isEmpty() → THROW:new IOException(\"There is no primary group for UGI \" + this) → RETURN → EXIT",
    "log": "<log>[DEBUG] Looking for FS supporting {}</log> <log>[DEBUG] looking for configuration option {}</log> <log>[DEBUG] Looking in service filesystems for implementation class</log> <log>[DEBUG] FS for {} is {}</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[DEBUG] Exception in closing {}</log> <log>[DEBUG] Duplicate FS created for {}; discarding {}</log> <log>[INFO] Got dt for {fs.getUri()}; {token}</log> <log>[INFO] Binary tokens merged successfully</log> <log>[INFO] No binary token filename provided</log> <log>[ERROR] Failed to read token storage file</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log> <log_entry> <log_level>DEBUG</log_level> <template>Failed to get groups for user {}</template> </log_entry>"
  },
  "123c60c5_1": {
    "exec_flow": "<!-- Optimized execution flow structure based on the merging process logic --> ENTRY → WHILE: results.size() != totalReplicaExpected && numResultsOflastChoose != results.size() → WHILE_COND: results.size() != totalReplicaExpected && numResultsOflastChoose != results.size() → CALL: org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseOnce → CATCH: NotEnoughReplicasException → LOG: TRACE: Chosen nodes: {} → LOG: TRACE: Excluded nodes: {} → LOG: TRACE: New Excluded nodes: {} → WHILE_EXIT → IF_TRUE: numResultsOflastChoose != totalReplicaExpected → LOG: DEBUG: Best effort placement failed: expecting {} replicas, only chose {}. → THROW: lastException → EXIT",
    "log": "[TRACE] Chosen nodes: {} [TRACE] Excluded nodes: {} [TRACE] New Excluded nodes: {} [DEBUG] Best effort placement failed: expecting {} replicas, only chose {}."
  },
  "a0de6165_1": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH: keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "a0de6165_2": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "4165b977_1": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: mi == null</step> <step>IF_TRUE: mi.getHistoryFile() == null</step> <step>CALL: org.slf4j.Logger:warn</step> <step>IF_FALSE: mi.getConfFile() == null</step> <step>TRY</step> <step>CALL: makeQualified</step> <step>NEW: Path</step> <step>NEW: Path</step> <step>CALL: create</step> <step>CALL: summaryFileOut.writeUTF</step> <step>CLOSE: summaryFileOut</step> <step>CALL: setPermission</step> <step>CATCH: IOException e</step> <step>LOG: LOG.INFO: Unable to write out JobSummaryInfo to [qualifiedSummaryDoneFile], e</step> <step>THROW: e</step> <step>EXIT</step> <step>IF_TRUE: stagingDirFS.exists</step> <step>LOG: LOG.INFO: Copying fromPath.toString() to toPath.toString()</step> <step>CALL: org.apache.hadoop.fs.FileSystem:delete</step> <step>CALL: org.apache.hadoop.fs.FileUtil:copy</step> <step>CALL: doneDirFS.setPermission</step> <step>IF_TRUE: copied</step> <step>LOG: LOG.INFO: Copied from: fromPath.toString() to done location: toPath.toString()</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[WARN] No file for job-history with jobId found in cache!</log> <log>[INFO] Copying fromPath to toPath</log> <log>[INFO] Copied from: fromPath to done location: toPath</log>"
  },
  "4165b977_2": {
    "exec_flow": "<step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for +(String)item</step> <step>CALL:handleDeprecation(deprecations, (String)item)</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "a70f1d7e_1": {
    "exec_flow": "ENTRY→CALL:handleDeprecation→LOG:DEBUG Handling deprecation for all properties in config...→FOREACH:keys→LOG:DEBUG Handling deprecation for (String)item→CALL:org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)→CALL:org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:getOutputFormatClass()→CALL:ReflectionUtils:newInstance→CALL:handleDeprecation→IF_TRUE:newApiCommitter→CALL:loadResources→IF_TRUE:loadDefaults && fullReload→FOREACH:defaultResources→CALL:loadResource→FOREACH_EXIT→FOR_INIT→FOR_COND:i < resources.size()→CALL:loadResource→FOR_EXIT→CALL:addTags→IF_TRUE:overlay != null→CALL:putAll→IF_TRUE:backup != null→FOREACH:overlay.entrySet()→RETURN→EXIT→CALL:getLoginUser→ENTRY→CALL:ensureInitialized→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→ENTRY→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] OutputCommitter set in config ... [LOG] getLoginUser [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials"
  },
  "a70f1d7e_2": {
    "exec_flow": "ENTRY→CALL:getInt→IF_TRUE:this.maxConcurrentTrackedNodes < 0→CALL:Configuration:getInt→LOG.ERROR: {} is set to an invalid value, it must be zero or greater. Defaulting to {}→CALL:processConf→EXIT→[VIRTUAL_CALL]→ENTRY→CALL:getBoolean→CALL:loadMappingProviders→EXIT",
    "log": "[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT"
  },
  "f7b6a963_1": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → IF_TRUE: elapsedMs >= slowLookupThresholdMs → CALL: org.slf4j.Logger:warn(java.lang.String) → IF_TRUE: staticHost != null → CALL: getByAddress → CALL: getAddress → CALL: getAddress → NEW: InetSocketAddress → RETURN → EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "f7b6a963_2": {
    "exec_flow": "ENTRY → IF_TRUE: !addr.isUnresolved() && addr.getAddress().isAnyLocalAddress() → TRY → NEW: InetSocketAddress → CALL: getLocalHost → CALL: createSocketAddrForHost → CALL: org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → IF_FALSE: elapsedMs >= slowLookupThresholdMs → IF_TRUE: LOG.isTraceEnabled() → CALL: org.slf4j.Logger:trace(java.lang.String) → IF_FALSE: staticHost != null → NEW: InetSocketAddress → RETURN → EXIT",
    "log": "[TRACE] Name lookup for + hostname + took + elapsedMs + ms."
  },
  "976a611f_1": {
    "exec_flow": "ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→ENTRY→TRY→CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL:globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {}"
  },
  "a635a06a_1": {
    "exec_flow": "ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "a635a06a_2": {
    "exec_flow": "ENTRY → CALL:getCurrentUser → IF_TRUE: LOG.isDebugEnabled() → LOG:debug:The user credential is {} → CALL:doAs → TRY → CALL:GSSManager:getInstance → CALL:GSSManager:createName → CALL:GSSManager:createContext → CALL:GSSContext:requestMutualAuth → CALL:GSSContext:requestCredDeleg → CALL:GSSContext:initSecContext → CALL:GSSContext:dispose → CATCH: PrivilegedActionException → LOG:debug:GSSException as: {} → THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "<log_entry> <level>DEBUG</level> <template>The user credential is {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>GSSException as: {}</template> </log_entry>"
  },
  "1a504de1_1": {
    "exec_flow": "ENTRY → CALL:checkPath → CALL:delete → CALL:org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean) → RETURN → EXIT",
    "log": "<log> <template>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</template> <variable_chain>Path → f</variable_chain> </log> <log> <template>[DEBUG] Path: [{}] is a file. COS key: [{}]</template> <variable_chain>Path → f</variable_chain> </log> <log> <template>[DEBUG] Create parent key: {parentKey}</template> <variable_chain>-</variable_chain> </log>"
  },
  "1a504de1_2": {
    "exec_flow": "ENTRY → CALL:checkPath → CALL:delete → CALL:org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path,boolean) → RETURN → EXIT",
    "log": "<log> <template>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</template> <variable_chain>Path → f</variable_chain> </log> <log> <template>[DEBUG] Path: [{}] is a dir. COS key: [{}]</template> <variable_chain>Path → f</variable_chain> </log> <log> <template>[DEBUG] Delete the file: {}</template> <variable_chain>File → f</variable_chain> </log> <log> <template>[DEBUG] Create parent key: {parentKey}</template> <variable_chain>-</variable_chain> </log>"
  },
  "b6667488_1": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:eventHandler.handle→FOREACH:updatedNodes→FOR_INIT→FOR_COND:i<2→CALL:LOG.info→FOREACH:taskSet.entrySet→CALL:eventHandler.handle→FOREACH_EXIT→FOR_EXIT→EXIT",
    "log": "[INFO] Killing taskAttempt:tid because it is running on unusable node:taskAttemptNodeId [DEBUG] Processing [TaskAttemptID] of type [eventType] [INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]"
  },
  "b6667488_2": {
    "exec_flow": "ENTRY→IF_FALSE:!nodeBlacklistingEnabled→IF_TRUE:blacklistDisablePercent!=-1&&(blacklistedNodeCount!=blacklistedNodes.size()||clusterNmCount!=lastClusterNmCount)→CALL:writeLock.lock→TRY→TRY→CALL:size→IF_FALSE:clusterNmCount==0→IF_FALSE:val>=blacklistDisablePercent→IF_TRUE:ignoreBlacklisting.compareAndSet(true,false)→LOG:LOG.INFO:Ignore blacklisting set to false. Known: <clusterNmCount>, Blacklisted: <blacklistedNodeCount>, <val>%→CALL:blacklistAdditions.addAll→CALL:blacklistRemovals.clear→LOG:LOG.INFO:Received completed container {containerId}→LOG:LOG.INFO:[attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]→CALL:writeLock.unlock→EXIT",
    "log": "[INFO] Ignore blacklisting set to false. Known: <clusterNmCount>, Blacklisted: <blacklistedNodeCount>, <val>% [INFO] Received completed container {containerId} [INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]"
  },
  "b6667488_3": {
    "exec_flow": "ENTRY→CALL:handleStoreEvent→CALL:org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler:handle→EXIT",
    "log": "[INFO] Handling NodeAttributesStoreEvent"
  },
  "b6667488_4": {
    "exec_flow": "ENTRY→IF_FALSE:collectorInfo==null→IF_TRUE:collectorInfo.getCollectorAddr()!=null&& !collectorInfo.getCollectorAddr().isEmpty()&& !collectorInfo.getCollectorAddr().equals(timelineServiceAddress)→CALL:getCollectorAddr→LOG:LOG.INFO:Updated timeline service address to + timelineServiceAddress→EXIT",
    "log": "[INFO] Updated timeline service address to + timelineServiceAddress"
  },
  "b6667488_5": {
    "exec_flow": "ENTRY→IF_TRUE:clientService!=null→CALL:getBindAddress→TRY→IF_FALSE:serviceAddr!=null→CALL:registerApplicationMaster→CALL:getMaximumResourceCapability→CALL:this.context.getClusterInfo().setMaxContainerCapability→IF_TRUE:UserGroupInformation.isSecurityEnabled()→CALL:setClientToAMToken→CALL:getApplicationACLs→LOG:LOG.INFO:maxContainerCapability:+maxContainerCapability→EXCEPTION:info→CATCH:Exception are→LOG:LOG.ERROR:Exception while registering, are→THROW:new YarnRuntimeException(are)→EXIT",
    "log": "[INFO] maxContainerCapability: + maxContainerCapability [ERROR] Exception while registering"
  },
  "b6667488_6": {
    "exec_flow": "ENTRY→IF_FALSE:clientService!=null→TRY→CALL:getAMWebappScheme→CALL:setHost→CALL:setRpcPort→CALL:setTrackingUrl→CALL:registerApplicationMaster→CALL:getMaximumResourceCapability→CALL:this.context.getClusterInfo().setMaxContainerCapability→IF_FALSE:UserGroupInformation.isSecurityEnabled()→CALL:getApplicationACLs→LOG:LOG.INFO:maxContainerCapability:+maxContainerCapability→LOG:LOG.INFO:queue:+queue→CALL:setQueueName→CALL:this.schedulerResourceTypes.addAll→EXIT",
    "log": "[INFO] maxContainerCapability: + maxContainerCapability [INFO] queue: + queue"
  },
  "b6667488_7": {
    "exec_flow": "ENTRY→SYNC: RMContainerAllocator.this → IF_TRUE:changed → CALL:org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduleStats:log(java.lang.String) → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[INFO] msgPrefix + \"PendingReds:\" + numPendingReduces + \" ScheduledMaps:\" + numScheduledMaps + \" ScheduledReds:\" + numScheduledReduces + \" AssignedMaps:\" + numAssignedMaps + \" AssignedReds:\" + numAssignedReduces + \" CompletedMaps:\" + numCompletedMaps + \" CompletedReds:\" + numCompletedReduces + \" ContAlloc:\" + numContainersAllocated + \" ContRel:\" + numContainersReleased + \" HostLocal:\" + hostLocalAssigned + \" RackLocal:\" + rackLocalAssigned"
  },
  "b6667488_8": {
    "exec_flow": "ENTRY→CALL: clone → CALL: org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes → FOR_INIT → FOR_COND: i < maxLength → TRY → CALL: org.apache.hadoop.yarn.api.records.Resource:getResourceInformation → CALL: org.apache.hadoop.yarn.api.records.Resource:getResourceInformation → CALL: org.apache.hadoop.yarn.api.records.Resource:setResourceValue → CATCH: ResourceNotFoundException → CALL: org.slf4j.Logger:warn → FOR_EXIT → RETURN → EXIT",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "b6667488_9": {
    "exec_flow": "ENTRY→LOG: [INFO] Got allocated containers ...→ CALL: size → WHILE: it.hasNext() → WHILE_COND: it.hasNext() → CALL: org.apache.hadoop.yarn.api.records.Container:getResource() → CALL: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container) → LOG: [INFO] Finding containerReq for allocated container: → IF_FALSE: PRIORITY_FAST_FAIL_MAP.equals(priority) → IF_TRUE: PRIORITY_MAP.equals(priority) || PRIORITY_OPPORTUNISTIC_MAP.equals(priority)→ LOG: [INFO] Replacing MAP container → IF_TRUE: list != null && list.size() > 0 → IF_FALSE: maps.containsKey(tId) → LOG: [INFO] Found replacement: → RETURN → EXIT → LOG: [INFO] Cannot assign container...→ LOG: [WARN] Container allocated at unwanted priority → CALL: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:decContainerReq(...) → CALL: org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:addContainerReq(...) → CALL: assignContainers → CALL: iterator → WHILE: it.hasNext() → WHILE_COND: it.hasNext() → CALL: assignWithoutLocality → IF_TRUE: PRIORITY_FAST_FAIL_MAP.equals(priority) → LOG: [INFO] Assigning container <allocated> to fast fail map → CALL: assignToFailedMap → RETURN → IF_COND: assigned!=null → CALL: containerAssigned → CALL: decContainerReq → CALL: eventHandler.handle → CALL: assignedRequests.add → IF_TRUE: LOG.isDebugEnabled() → LOG: [DEBUG] Assigned container (allocated) to task assigned.attemptID on node allocated.getNodeId().toString() → EXIT → WHILE_EXIT → CALL: assignMapsWithLocality → ENTRY → WHILE: it.hasNext() && maps.size() > 0 && canAssignMaps() → WHILE_EXIT → LOG: [INFO] Releasing unassigned container ...→ CALL: containerNotAssigned → WHILE_EXIT → EXIT",
    "log": "[INFO] Got allocated containers ... [INFO] Finding containerReq for allocated container: [INFO] Replacing MAP container [INFO] Found replacement: [INFO] Cannot assign container... [WARN] Container allocated at unwanted priority: ... [INFO] Releasing unassigned container ... [INFO] Assigning container <allocated> to fast fail map [DEBUG] Assigned container (allocated) to task assigned.attemptID on node allocated.getNodeId().toString()"
  },
  "1c58f43a_1": {
    "exec_flow": "ENTRY→CALL:getApplication→CALL:checkAccess→EXCEPTION:checkAccess→CATCH:AuthorizationException|ApplicationAttemptNotFoundException e→IF_TRUE:e instanceof AuthorizationException→LOG:LOG.WARN→CALL:app.appReport.setDiagnostics→CALL:app.appReport.setCurrentApplicationAttemptId→RETURN→EXIT",
    "log": "[WARN] \"Failed to authorize when generating application report for \" + app.appReport.getApplicationId() + \". Use a placeholder for its latest attempt id.\""
  },
  "1c58f43a_2": {
    "exec_flow": "ENTRY→CALL:getApplication→CALL:checkAccess→EXCEPTION:checkAccess→CATCH:AuthorizationException|ApplicationAttemptNotFoundException e→IF_FALSE:e instanceof AuthorizationException→LOG:LOG.INFO→CALL:app.appReport.setDiagnostics→CALL:app.appReport.setCurrentApplicationAttemptId→RETURN→EXIT",
    "log": "[INFO] \"No application attempt found for \" + app.appReport.getApplicationId() + \". Use a placeholder for its latest attempt id.\""
  },
  "1c58f43a_3": {
    "exec_flow": "ENTRY→IF_TRUE: app.appViewACLs != null→CALL: aclsManager.addApplication→TRY→CALL: UserGroupInformation.getCurrentUser→LOG:LOG.DEBUG: Verifying access-type {} for {} on application {} owned by {} →IF_TRUE: !areACLsEnabled()→RETURN→CALL: org.apache.hadoop.yarn.security.AdminACLsManager:isAdmin(callerUGI)→IF_TRUE: this.adminAclsManager.isAdmin(callerUGI) || user.equals(applicationOwner) || applicationACL.isUserAllowed(callerUGI)→IF_TRUE: !aclsManager.checkAccess(UserGroupInformation.getCurrentUser(), ApplicationAccessType.VIEW_APP, app.appReport.getUser(), app.appReport.getApplicationId())→THROW: new AuthorizationException(\"User \" + UserGroupInformation.getCurrentUser().getShortUserName() + \" does not have privilege to see this application \" + app.appReport.getApplicationId())→EXIT",
    "log": "[DEBUG] Verifying access-type {} for {} on application {} owned by {} [INFO] User does not have privilege to see this application"
  },
  "1c58f43a_4": {
    "exec_flow": "ENTRY → CALL:getEntities → NEW:CheckAclImpl → IF_TRUE:entities == null → NEW:TimelineEntities → RETURN → EXIT",
    "log": "[DEBUG] Verifying access-type {} for {} on application {} owned by {}"
  },
  "1c58f43a_5": {
    "exec_flow": "ENTRY → CALL:getEntities → NEW:CheckAclImpl → IF_FALSE:entities == null → RETURN → EXIT",
    "log": "[DEBUG] Verifying access-type {} for {} on application {} owned by {}"
  },
  "1c58f43a_6": {
    "exec_flow": "ENTRY→CALL:getApplicationAttempts→TRY→CALL:getHistoryFileReader→WHILE:hfReader.hasNext()→WHILE_COND:hfReader.hasNext()→CALL:next→IF→IF→CALL:mergeApplicationAttemptHistoryData→IF→CALL:mergeApplicationAttemptHistoryData→WHILE_EXIT→LOG:LOG.INFO:Completed reading history information of all application attempts of application appId→CALL:hfReader.close→EXIT→ENTRY→CALL:IOUtils.cleanupWithLogger→EXIT→FOREACH_EXIT→RETURN→EXIT",
    "log": "[INFO] Completed reading history information of all application attempts of application appId"
  },
  "1c58f43a_7": {
    "exec_flow": "ENTRY→CALL:getApplicationAttempts→TRY→CALL:getHistoryFileReader→WHILE:hfReader.hasNext()→WHILE_COND:hfReader.hasNext()→CALL:next→IF→IF→CALL:mergeApplicationAttemptHistoryData→IF→CALL:mergeApplicationAttemptHistoryData→WHILE_EXIT→EXCEPTION:IOException→CATCH:IOException→LOG:LOG.INFO:Error when reading history information of some application attempts of application appId→CALL:hfReader.close→EXIT→ENTRY→CALL:IOUtils.cleanupWithLogger→EXIT→FOREACH_EXIT→RETURN→EXIT",
    "log": "[INFO] Error when reading history information of some application attempts of application appId"
  },
  "fa46f676_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addTags → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "fa46f676_2": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "fa46f676_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → CALL:getFileStatus → IF_FALSE: fs.isDirectory() → IF_FALSE: key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE: meta!=null → IF_TRUE: meta.isFile() → LOG:LOG.INFO: Open the file: [{f}] for reading. → LOG:LOG.DEBUG: Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → NEW:FSDataInputStream → CALL:getConf → NEW:BufferedFSInputStream → NEW:CosNInputStream → EXIT",
    "log": "<log>[INFO] Open the file: [{f}] for reading.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "fa46f676_4": {
    "exec_flow": "ENTRY → IF_TRUE: client != null → IF_FALSE: !client.isConnected() → CALL:logout → CALL:disconnect → IF_TRUE: !logoutSuccess → LOG:LOG.WARN: Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "fa46f676_5": {
    "exec_flow": "ENTRY → CALL:statIncrement → TRY → CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open → CALL:org.slf4j.Logger:debug → NEW:FSDataInputStream → RETURN → EXIT",
    "log": "<log>[DEBUG] Incrementing stat CALL_OPEN</log> <log>[INFO] File qualified</log> <log>[INFO] File opened for read</log>"
  },
  "fa46f676_6": {
    "exec_flow": "<node>org.apache.hadoop.fs.s3a.S3AFileSystem:open</node> <action>executeOpen(qualify(f), openFileHelper.openSimpleFile(bufferSize))</action> <node>org.apache.hadoop.fs.s3a.S3AFileSystem:executeOpen</node>",
    "log": "<log>LOG.debug(\"Opening '{}'{}\", readContext)</log>"
  },
  "4a07a2c8_1": {
    "exec_flow": "ENTRY→TRY→CALL:setupQueueConfigs→RETURN→EXIT",
    "log": "[INFO] Initializing [Queue Path Details With Associated Attributes]"
  },
  "df73778b_1": {
    "exec_flow": "<context>org.apache.hadoop.service.AbstractService:init</context> <entry>Parent.ENTRY</entry> <flow>IF_FALSE: conf == null</flow> <flow>IF_FALSE: isInState(STATE.INITED)</flow> <sync>SYNC: stateChangeLock</sync> <flow>IF_TRUE: enterState(STATE.INITED) != STATE.INITED</flow> <call>CALL: setConfig</call> <try>TRY</try> <call>CALL: serviceInit</call> <flow>IF_TRUE: isInState(STATE.INITED)</flow> <call>CALL: notifyListeners</call> <exit>EXIT</exit> <entry>VIRTUAL_CALL</entry> <flow>IF_TRUE: service != null</flow> <call>CALL: stop</call> <exit>EXIT</exit> <flow>ENTRY</flow> <flow>TRY</flow> <call>CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners</call> <call>CALL: globalListeners.notifyListeners</call> <catch>CATCH: Throwable e</catch> <log>LOG: LOG.WARN: Exception while notifying listeners of {}, this, e</log> <exit>EXIT</exit>",
    "log": "<log_entry> <log> <level>DEBUG</level> <template>Service: {} entered state {}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>When stopping the service {service_name}</template> </log> </log_entry> <log_entry> <log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log> </log_entry>"
  },
  "2d127aa0_1": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>SYNC: this</step> <step>CALL: toArray</step> <step>CALL: size</step> <step>FOREACH: callbacks</step> <step>CALL: stateChanged</step> <step>FOREACH_EXIT</step> <step>CALL: globalListeners.notifyListeners</step> <step>CATCH: Throwable e</step> <step>LOG: LOG.WARN: Exception while notifying listeners of {}, this, e</step> <step>EXIT</step>",
    "log": "<log> <level>WARN</level> <template>Exception while notifying listeners of {}</template> </log>"
  },
  "2d127aa0_2": {
    "exec_flow": "<step>ENTRY</step> <step>SYNCHRONIZED: stateChangeLock</step> <step>CONDITION_CHECK: isInState(STATE.STARTED)</step> <step>BRANCH_EXIT: not in state</step> <step>ENTRY: noteFailure</step> <step>CONDITION: exception == null</step> <branch> <step>LOG: LOG.DEBUG: noteFailure</step> <step>LOG: LOG.DEBUG: Service {} is started, getName()</step> <step>CALL: notifyListeners</step> </branch> <branch> <condition>synchronized(this)</condition> <condition>failureCause == null</condition> <step>LOG: LOG.INFO: Service {getName()} failed in state {failureState}</step> </branch>",
    "log": "<log> <level>DEBUG</level> <message>noteFailure</message> </log> <log> <level>DEBUG</level> <template>Service {} is started</template> </log> <log> <level>INFO</level> <template>Service {getName()} failed in state {failureState}</template> </log>"
  },
  "2c656d6c_1": {
    "exec_flow": "ENTRY -> IF_FALSE: args.length == 0 -> TRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() -> CALL: Subject.doAs -> RETURN -> EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "2c656d6c_2": {
    "exec_flow": "CATCH: PrivilegedActionException -> LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause -> THROW: IOException|Error|RuntimeException| InterruptedException|UndeclaredThrowableException",
    "log": "[DEBUG] PrivilegedActionException as: {}"
  },
  "2c656d6c_3": {
    "exec_flow": "ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "7b5b8c53_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>DEBUG</level> <template>org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration) - Initializing with Configuration</template> </log_entry>"
  },
  "7b5b8c53_2": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CALL:org.apache.hadoop.security.UserGroupInformation.addCredentials → CALL:org.apache.hadoop.security.Credentials.addAll → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Token file {} does not exist</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Failure to load login credentials</template> </log_entry> <log_entry> <level>DEBUG</level> <template>UGI loginUser: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>WARN</level> <template>Null token ignored for {alias}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "7b5b8c53_3": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → EXCEPTION:readTokenStorageStream → CATCH:IOException ioe → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:new IOException(\"Exception reading \" + filename, ioe) → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "7b5b8c53_4": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → RETURN → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "6b26f51f_1": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Processing {event.getNodeId()} of type {event.getType()} → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → IF_TRUE: oldState != getState() → LOG: LOG.INFO: {nodeId} Node Transitioned from {oldState} to {getState()} → CALL: writeLock.unlock → EXIT",
    "log": "[DEBUG] Processing {event.getNodeId()} of type {event.getType()} [INFO] {nodeId} Node Transitioned from {oldState} to {getState()}"
  },
  "6b26f51f_2": {
    "exec_flow": "ENTRY → IF_TRUE: rmApp != null → IF_TRUE: rmAppAttempt != null → TRY → CALL: rmAppAttempt.handle → CALL: handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent) → EXCEPTION: handle → CATCH: Throwable t → LOG: LOG.ERROR: Error in handling event type {event.getType()} for applicationAttempt {appAttemptId} → EXIT",
    "log": "[ERROR] Error in handling event type {event.getType()} for applicationAttempt {appAttemptId}"
  },
  "6b26f51f_3": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → TRY → CALL: stateMachine.doTransition → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: LOG.ERROR: MessageFormat.format([COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState), e → IF_TRUE: oldState != getState() → LOG: LOG.INFO: [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType() → CALL: writeLock.unlock → EXIT",
    "log": "[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState [INFO] [COMPONENT {}] Transitioned from {} to {} on {} event."
  },
  "bdd79628_1": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Ready to delete path: [{}]. recursive: [{}]. → TRY → CALL:getFileStatus → IF_FALSE:key.compareToIgnoreCase(\"/\") == 0 → IF_FALSE:status.isDirectory() → LOG:LOG.DEBUG:Delete the file: {}, f → CALL:createParent → CALL:store.delete → RETURN → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}]. [DEBUG] Delete the file: {}"
  },
  "bdd79628_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Ready to delete path: [{}]. recursive: [{}]. → TRY → CALL:getFileStatus → IF_FALSE:key.compareToIgnoreCase(\"/\") == 0 → IF_TRUE:status.isDirectory() → IF_TRUE:!key.endsWith(PATH_DELIMITER) → CALL:createParent → DO-WHILE:priorLastKey != null → CALL:list → FOR:FileMetadata file → CALL:store.delete(file.getKey()) → FOR:FileMetadata commonPrefix → CALL:store.delete(commonPrefix.getKey()) → TRY → CALL:store.delete(key) → CATCH:Exception e → LOG:LOG.ERROR:Deleting the COS key: [{}] occurs an exception., key, e → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}]. [DEBUG] Create parent key: {parentKey} [ERROR] Deleting the COS key: [{}] occurs an exception."
  },
  "bdd79628_3": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[INFO] message"
  },
  "3effd8db_1": {
    "exec_flow": "ENTRY → IF_FALSE: null == storageInteractionLayer → LOG: LOG.DEBUG: Retrieving metadata for {}, key → TRY → IF_FALSE: checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist → IF_TRUE: key.equals(\"/\") → NEW: FileMetadata → CALL: defaultPermissionNoBlobMetadata → RETURN → EXIT",
    "log": "[DEBUG] Retrieving metadata for {}"
  },
  "3effd8db_2": {
    "exec_flow": "ENTRY → IF_FALSE: null == storageInteractionLayer → LOG: LOG.DEBUG: Retrieving metadata for {}, key → TRY → IF_FALSE: checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist → IF_FALSE: key.equals(\"/\") → CALL: getBlobReference → IF_TRUE: null != blob && blob.exists(getInstrumentedContext()) → LOG: LOG.DEBUG: Found {} as an explicit blob. Checking if it's a file or folder., key → TRY → CALL: downloadAttributes → EXCEPTION: downloadAttributes → CATCH: StorageException e → IF_TRUE: !NativeAzureFileSystemHelper.isFileNotFoundException(e) → THROW: e → EXIT",
    "log": "[DEBUG] Retrieving metadata for {} [DEBUG] Found {} as an explicit blob. Checking if it's a file or folder."
  },
  "3effd8db_3": {
    "exec_flow": "ENTRY → IF_FALSE: null == storageInteractionLayer → LOG: LOG.DEBUG: Retrieving metadata for {}, key → TRY → IF_FALSE: checkContainer(ContainerAccessType.PureRead) == ContainerState.DoesntExist → IF_FALSE: key.equals(\"/\") → CALL: getBlobReference → IF_FALSE: null != blob && blob.exists(getInstrumentedContext()) → FOREACH: objects → IF_TRUE: blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper → LOG: LOG.DEBUG: Found blob as a directory-using this file under it to infer its properties {}, blobItem.getUri() → NEW: FileMetadata → CALL: getTime → CALL: getLastModified → CALL: getPermissionStatus → RETURN → EXIT",
    "log": "[DEBUG] Retrieving metadata for {} [DEBUG] Found blob as a directory-using this file under it to infer its properties {}"
  },
  "ec969c99_1": {
    "exec_flow": "ENTRY→IF_TRUE: nativeHandlerAddr != 0→CALL: NativeRuntime.releaseNativeObject→CALL: IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "51fba431_1": {
    "exec_flow": "ENTRY → IF_TRUE: paths != null → IF_TRUE: withLinks != null → FOREACH: withLinks → CALL: getFileSystem → org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path) → ENTRY → org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus → org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal → CALL: getFileStatus → ENTRY → VIRTUAL_CALL → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE: meta != null → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE: listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → RETURN → EXIT → IF_FALSE: name.equals(DistributedCache.WILDCARD) → IF_TRUE: (wildcard != null) && (u.getFragment() != null) → THROW: IOException → EXIT → FOREACH_EXIT → FOREACH: paths → CALL: getFileSystem → CALL: resolvePath → CALL: crossPlatformifyMREnv → CALL: addToEnvironment → FOREACH_EXIT → EXIT → ENTRY → CALL: getTrimmed → CALL: handleDeprecation → LOG: Handling deprecation for all properties in config... → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[WARN] The same path is included more than once with different links or wildcards</log> <log>[DEBUG] Getting the file status for {f}</log> <log>[DEBUG] Found the path: {f} as a file.</log> <log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}]</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log> <log>[DEBUG] List objects. prefix: [{}], delimiter: [{}], maxListLength: [{}], priorLastKey: [{}]</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "51fba431_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→CALL:addKVAnnotation→CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass→ENTRY →IF_TRUE:!FILE_SYSTEMS_LOADED→CALL:loadFileSystems→LOG:Looking for FS supporting {}, scheme →IF_TRUE:conf != null→LOG:looking for configuration option {}, property→CALL:getClass →IF_TRUE:clazz == null→LOG:Looking in service filesystems for implementation class→CALL:get →IF_TRUE:clazz == null→THROW:new UnsupportedFileSystemException(\"No FileSystem for scheme \"+\"\\\\\"\"+scheme+\"\\\\\"\")→EXIT →CALL:org.apache.hadoop.util.ReflectionUtils:newInstance→ENTRY→IF_FALSE:conf != null→EXIT →TRY→CALL:fs.initialize→CATCH:IOException|RuntimeException→LOG:Failed to initialize filesystem {}: {} →LOG:Failed to initialize filesystem→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→FOREACH:closeables →IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null) →CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT→EXIT",
    "log": "<log>[DEBUG] Looking for FS supporting {}</log> <log>[DEBUG] looking for configuration option {}</log> <log>[DEBUG] Looking in service filesystems for implementation class</log> <log>[WARN] Failed to initialize filesystem {}: {}</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "54c7fde3_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → IF_FALSE:isInState(STATE.STARTED) → SYNC:stateChangeLock → IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL:currentTimeMillis → CALL:serviceStart → IF_TRUE:isInState(STATE.STARTED) → CALL:debug → CALL:notifyListeners → CATCH:Exception e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → CALL:noteFailure → CALL:ServiceOperations.stopQuietly → THROW: ServiceStateException.convert(e) → EXIT",
    "log": "<log> <template>[DEBUG] Service {} is started</template> <params/> </log> <log> <template>[WARN] Exception while notifying listeners of {}</template> <params/> </log> <log> <template>org.apache.hadoop.service.ServiceOperations:stopQuietly[WARN]When stopping the service {service.getName()}</template> <params> <param>service.getName()</param> </params> </log> <log> <template>org.apache.hadoop.service.AbstractService:noteFailure[DEBUG]noteFailure</template> <params> <param>{exception}</param> </params> </log> <log> <template>org.apache.hadoop.service.AbstractService:noteFailure[INFO]Service {} failed in state {}</template> <params> <param>getName()</param> <param>failureState</param> <param>exception</param> </params> </log>"
  },
  "f6662704_1": {
    "exec_flow": "ENTRY→IF_FALSE: !sd.getVersionFile().exists()→IF_TRUE: sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE)→CALL: exists→CALL: getStorageFile→CALL: imageDirs.add→IF_FALSE: sd.getStorageDirType().isOfType(NameNodeDirType.EDITS)→CALL: checkpointTimes.add→CALL: readCheckpointTime→IF_TRUE: sd.getStorageDirType().isOfType(NameNodeDirType.IMAGE) && (latestNameCheckpointTime < checkpointTime) && imageExists→IF_TRUE: sd.getStorageDirType().isOfType(NameNodeDirType.EDITS) && (latestEditsCheckpointTime < checkpointTime) && editsExists→IF_TRUE: checkpointTime <= 0L→CALL: exists→CALL: getPreviousDir→EXIT",
    "log": "[DEBUG] Checking file + f [DEBUG] Exception in closing {}"
  },
  "6fb34b5a_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL:getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → [VIRTUAL_CALL]: propagateOptions → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {}"
  },
  "31f0d49d_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation→CALL:checkReservationSystem→CALL:ReservationInputValidator:validateReservationSubmissionRequest→IF_FALSE:allocation != null→CALL:checkReservationACLs→TRY→IF_TRUE: (contract.getArrival() - clock.getTime()) < reservationSystem.getPlanFollowerTimeStep()→CALL:ReservationAgent:createReservation→IF_TRUE:result→CALL:reservationSystem.setQueueForReservation→CALL:reservationSystem.synchronizePlan→LOG: [DEBUG] Reservation is within threshold so attempting to create synchronously.→LOG: [INFO] Created reservation {0} synchronously., reservationId→LOG: [INFO] Reservation created successfully→LOG: [INFO] createSuccessLog(user, operation, target, null, null, null, null)→RETURN→EXIT",
    "log": "[INFO] Submitting reservation to ResourceManager [DEBUG] Reservation is within threshold so attempting to create synchronously. [INFO] Created reservation {0} synchronously., reservationId [INFO] Reservation created successfully [INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "31f0d49d_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation→CALL:checkReservationSystem→CALL:ReservationInputValidator:validateReservationSubmissionRequest→IF_FALSE:allocation != null→CALL:checkReservationACLs→TRY→CALL:ReservationAgent:createReservation→IF_FALSE:result→LOG: [ERROR] Unable to create the reservation→THROW:RPCUtil.getRemoteException→EXIT",
    "log": "[INFO] Submitting reservation to ResourceManager [ERROR] Unable to create the reservation"
  },
  "31f0d49d_3": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation→CALL:checkReservationSystem→CALL:ReservationInputValidator:validateReservationSubmissionRequest→IF_FALSE:allocation != null→CALL:checkReservationACLs→TRY→CALL:ReservationAgent:createReservation→IF_FALSE:result→LOG: [INFO] Reservation creation failed→RETURN→EXIT",
    "log": "[INFO] Submitting reservation to ResourceManager [INFO] Reservation creation failed"
  },
  "31f0d49d_4": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation→IF_FALSE: reservationId == null→IF_FALSE: queue == null || queue.isEmpty()→CALL:ReservationSystem.getPlan→IF_FALSE: plan == null→IF_TRUE: contract == null→CALL: RMAuditLogger.logFailure→THROW: RPCUtil.getRemoteException(message)→EXIT",
    "log": "[INFO] Submitting reservation to ResourceManager <log>[LOG] RMAuditLogger.logFailure: Missing reservation definition. Please try again by specifying a reservation definition.</log>"
  },
  "31f0d49d_5": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null,newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds DO_EXIT → RETURN → EXIT → LOG: [WARN] Unexpected SecurityException in Configuration → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → LOG: [DEBUG] Handling deprecation for all properties in config... EXIT → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → LOG: [DEBUG] Handling deprecation for all properties in config... CALL: getProps → CALL: addAll → FOREACH: keys → LOG: [DEBUG] Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → EXIT IF_FALSE: !isInitialized() → VIRTUAL_CALL: TO ClientRequestInterceptor.submitReservation",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[INFO] Submitting reservation to ResourceManager</log_entry>"
  },
  "a06f1990_1": {
    "exec_flow": "ENTRY → CALL: getLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → LOG: Reading credentials from location {} → LOG: Loaded {} tokens from {} → LOG: Token file {} does not exist → LOG: Failure to load login credentials → LOG: UGI loginUser: {} → CALL: spawnAutoRenewalThreadForUserCreds(false) → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → FOREACH: names → CALL: substituteVars → FOREACH_EXIT → RETURN → EXIT → CALL: getCurrentUser → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → RETURN → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[LOG] getLoginUser</log>"
  },
  "d21ec39a_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:submit → CALL:listObjects → CALL:pairedTrackerFactory → CALL:getDurationTrackerFactory → CALL:getDurationTrackerFactory → CALL:pairedTrackerFactory → CALL:getDurationTrackerFactory → CALL:getDurationTrackerFactory → CALL:lambda$0 → CALL:listObjects → CALL:pairedTrackerFactory → CALL:getDurationTrackerFactory → CALL:getDurationTrackerFactory → CALL:pairedTrackerFactory → CALL:getDurationTrackerFactory → CALL:getDurationTrackerFactory → RETURN → EXIT",
    "log": "[DEBUG] All entries in batch were filtered...continuing [DEBUG] New listing status: {} [DEBUG] [New listing status: {}], Requesting next {} objects under {} [DEBUG] {}: {} [DEBUG] Adding: {} [DEBUG] Ignoring: {} [DEBUG] Adding directory: {} [DEBUG] Ignoring directory: {} [DEBUG] Added {} entries; ignored {}; hasNext={}; hasMoreObjects={}"
  },
  "d21ec39a_2": {
    "exec_flow": "ENTRY→WHILE:source.hasNext()→IF_TRUE:buildNextStatusBatch(source.next())→LOG:LOG.DEBUG:All entries in batch were filtered...continuing→CALL:org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:next()→ENTRY→IF_FALSE:firstListing→IF_FALSE:objectsPrev != null && !objectsPrev.isTruncated()→CALL:onceInTheFuture→TRY→CALL:DurationInfo:<init>→CALL:awaitFuture→CALL:DurationInfo:close→RETURN→IF_TRUE:objects.isTruncated()→LOG:LOG.DEBUG:[DEBUG] [New listing status: {}], Requesting next {} objects under {}→CALL:continueListObjectsAsync→EXIT→RETURN→EXIT",
    "log": "[DEBUG] All entries in batch were filtered...continuing [DEBUG] New listing status: {} [DEBUG] [New listing status: {}], Requesting next {} objects under {} [DEBUG] {}: {} [DEBUG] Adding: {} [DEBUG] Ignoring: {} [DEBUG] Adding directory: {} [DEBUG] Ignoring directory: {} [DEBUG] Added {} entries; ignored {}; hasNext={}; hasMoreObjects={}"
  },
  "58a318f9_1": {
    "exec_flow": "<step>ENTRY: getDomain(java.lang.String)</step> <step>CALL: getTimelineDomain(DBIterator, java.lang.String, byte[])</step> <step>TRY: LeveldbIterator</step> <step>CALL: seek</step> <step>EXCEPTION: seek</step> <step>CATCH: DBException e</step> <step>THROW: new IOException(e)</step> <step>CALL: IOUtils.cleanupWithLogger</step> <step>FOREACH:closeables</step> <step>IF(c != null)</step> <step>TRY</step> <step>CALL:c.close()</step> <step>CATCH:Throwable</step> <step>IF(logger != null)</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object)</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Exception in closing {}</log> <log>[ERROR] Unrecognized domain column: {column}</log>"
  },
  "9de82ba1_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.service.AbstractService:stop → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → LOG: LOG.DEBUG: Service: {} entered state {} → CALL: recordLifecycleEvent → CATCH: Exception e → LOG: LOG.DEBUG: noteFailure → CALL: noteFailure(e) → LOG: LOG.INFO: Service {} failed in state {} → THROW: ServiceStateException.convert(e) → FINALLY → TERMINATION_NOTIFICATION.SET(true) → SYNC: terminationNotification → NOTIFYALL → CALL: notifyListeners → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → FINALLY → EXIT → ELSE → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → EXIT <conditional_execution> <condition>exception != null</condition> <log>LOG.debug(\"noteFailure\", exception)</log> <sync_block> <condition>failureCause == null</condition> <actions> <set failureCause>exception</set> <set failureState>getServiceState()</set> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log> </actions> </sync_block> </conditional_execution>",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] noteFailure [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {} [DEBUG] Ignoring re-entrant call to stop() <log>LOG.debug(\"noteFailure\", exception)</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log>"
  },
  "9de82ba1_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.service.AbstractService:stop → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) == STATE.STOPPED → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "9de82ba1_3": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "e603378e_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getTrimmed → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → TRY → CALL:org.apache.hadoop.util.DurationInfo:<init> → CALL:addKVAnnotation → CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL:org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL:initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString() → LOG:LOGGER.DEBUG:Failed to initialize filesystem → CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger → THROW:e → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry> <log_entry>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log_entry> <log_entry>[DEBUG] Failed to initialize filesystem</log_entry> <log_entry>Finishing reading the block. Release all resources.</log_entry> <log_entry>Resources released in RBlockState:finish()</log_entry>"
  },
  "1629f3b3_1": {
    "exec_flow": "ENTRY → FOREACH: req.hosts → IF_FALSE: reqMap == null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: BEFORE decResourceRequest: + applicationId= + applicationId.getId() + priority= + priority.getPriority() + resourceName= + resourceName + numContainers= + remoteRequest.getNumContainers() + #asks= + ask.size() → IF_TRUE: remoteRequest.getNumContainers() > 0 → CALL: remoteRequest.setNumContainers → IF_TRUE: remoteRequest.getNumContainers() == 0 → CALL: reqMap.remove → IF_TRUE: reqMap.size() == 0 → CALL: remoteRequests.remove → IF_TRUE: remoteRequests.size() == 0 → CALL: remoteRequestsTable.remove → CALL: addResourceRequestToAsk → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: AFTER decResourceRequest: + applicationId= + applicationId.getId() + priority= + priority.getPriority() + resourceName= + resourceName + numContainers= + remoteRequest.getNumContainers() + #asks= + ask.size() → EXIT → FOREACH_EXIT → <!-- Similar repetition for req.racks and ResourceRequest.ANY -->",
    "log": "[DEBUG] BEFORE decResourceRequest: applicationId= applicationId.getId() priority= priority.getPriority() resourceName= resourceName numContainers= remoteRequest.getNumContainers() #asks= ask.size() [DEBUG] AFTER decResourceRequest: applicationId= applicationId.getId() priority= priority.getPriority() resourceName= resourceName numContainers= remoteRequest.getNumContainers() #asks= ask.size()"
  },
  "ad52211b_1": {
    "exec_flow": "ENTRY→IF_TRUE: finished > 0 && started > 0→CALL:org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Finished time + finished + is ahead of started time + started"
  },
  "ad52211b_2": {
    "exec_flow": "ENTRY→IF_FALSE: finished > 0 && started > 0→IF_TRUE: isRunning→CALL:org.slf4j.Logger:warn(java.lang.String)→RETURN→EXIT",
    "log": "[WARN] Current time + current + is ahead of started time + started"
  },
  "7e21eca5_1": {
    "exec_flow": "ENTRY→CALL:checkOperation→CALL:FSPermissionChecker.setOperationType→TRY→CALL:getPermissionChecker→CALL:writeLock→TRY→CALL:checkOperation→CALL:checkNameNodeSafeMode→CALL:FSDirMkdirOp.mkdirs→CALL:writeUnlock→CALL:logSync→LOG:logAuditEvent(success,operationName,src,null,auditStat)→IF_FALSE: !pc.isSuperUser()→CALL:unprotectedMkdir→CALL:allocateNewInodeId→CALL:now→CALL:allocateNewInodeId→CALL:now→IF_FALSE: existing == null→CALL:NameNode.getNameNodeMetrics().incrFilesCreated→CALL:fsd.getEditLog().logMkDir→IF_TRUE: NameNode.stateChangeLog.isDebugEnabled()→CALL:NameNode.stateChangeLog.debug→RETURN→EXIT→Parent.ENTRY→[VIRTUAL_CALL]→FOREACH: auditLoggers→IF: logger instanceof HdfsAuditLogger→CALL:appendClientPortToCallerContextIfAbsent→CALL:hdfsLogger.logAuditEvent→FOREACH_EXIT→ENTRY→TRY→SYNC: this→TRY→CALL: printStatistics→WHILE: mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE: mytxid <= synctxid→CALL: getLastJournalledTxId→LOG: LOG.DEBUG: logSync(tx)→IF_FALSE: lastJournalledTxId <= synctxid→TRY→IF_TRUE: journalSet.isEmpty()→THROW: new IOException(\"No journals available to flush\")→CALL:logSync→IF_FALSE: (resources.isEmpty() && lastPrintTime + 60000 > now && !force)→FOREACH:resources→IF_TRUE: !resource.isRequired()→CALL:isResourceAvailable→CALL:buf.append(\"Number of transactions:\").append(numTransactions).append(\"Total time for transactions(ms):\").append(totalTimeTransactions).append(\"Number of transactions batched in Syncs:\").append(numTransactionsBatchedInSync.longValue()).append(\"Number of syncs:\").append(editLogStream.getNumSync()).append(\"SyncTimes(ms):\").append(journalSet.getSyncTimes())→LOG:LOG.INFO:buf.toString()→CALL:terminate→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→FOREACH_EXIT→IF_TRUE: redundantResourceCount == 0→RETURN→EXIT",
    "log": "[INFO] logAuditEvent: success, operationName, src, null, auditStat [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()} [DEBUG] mkdirs: created directory {cur} [DEBUG] logEdit called [DEBUG] ------------------- logged event for top service: allowed={boolean}\\tugi={userName}\\tip={addr}\\tcmd={cmd}\\tsrc={src}\\tdst={dst}\\tperm={perm} [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()> [INFO] Logging exit info [DEBUG] Detailed exit debug info"
  },
  "7e21eca5_2": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty()→CALL:getLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser == null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→IF_TRUE:readTokenStorageStream succeeded→CALL:addCredentials→CALL:debug→CALL:cleanupWithLogger→RETURN→EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [INFO] Cleaning up resources"
  },
  "7e21eca5_3": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_FALSE:subject == null || subject.getPrincipals(User.class).isEmpty()→NEW:UserGroupInformation→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "3ef3175b_1": {
    "exec_flow": "ENTRY → LOG:DEBUG:Rename source path: [{}] to dest path: [{}]. → IF_FALSE:src.isRoot() → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus → IF_FALSE:src.equals(dst) → LOG:DEBUG:Call the getFileStatus to obtain the metadata for the file: [{}]. → IF_FALSE:srcStatus==null → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:retrieveMetadata → LOG:DEBUG:Path: [{}] is a dir. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → LOG:DEBUG:Moving {} to {}, src, dst → LOG:DEBUG:Renamed {} to {} successfully. → LOG: \"AzureBlobFileSystem.getFileStatus path: {path}\" → CALL: statIncrement → TRY → CALL: makeQualified → CALL: abfsStore.getFileStatus → IF_TRUE: meta != null → IF_TRUE: meta.isDirectory() → LOG: [DEBUG] \"Path {} is a folder.\", f.toString() → CALL: conditionalRedoFolderRename → ELSE: LOG: [DEBUG] \"Found the path: {} as a file.\", f.toString() → CALL: updateFileStatusPath → LOG: org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\" → LOG: LOG.DEBUG: \"Found the path: {} as a file.\" → RETURN → EXIT",
    "log": "<log>[DEBUG] Rename source path: [{}] to dest path: [{}].</log> <log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}].</log> <log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log> <log>[DEBUG] Moving {} to {}, src, dst</log> <log>[DEBUG] Renamed {} to {} successfully.</log> <log>AzureBlobFileSystem.getFileStatus path: {path}</log> <log>[DEBUG] \"Path {} is a folder.\"</log> <log>[DEBUG] \"Found the path: {} as a file.\"</log> <log>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</log> <log>LOG.DEBUG: \"Found the path: {} as a file.\"</log>"
  },
  "92690038_1": {
    "exec_flow": "ENTRY→IF_TRUE: this.identifier == null→LOG: LOG.WARN: The identifier for the State Store connection is not set→CALL:initDriver→IF_FALSE: !success→FOREACH: records→CALL:getRecordName→CALL:initRecordStorage→IF_TRUE: !initRecordStorage→LOG: LOG.ERROR: Cannot initialize record store for {}, cls.getSimpleName()→RETURN→EXIT",
    "log": "[WARN] The identifier for the State Store connection is not set [ERROR] Cannot initialize record store for simpleName"
  },
  "92690038_2": {
    "exec_flow": "ENTRY→IF_FALSE: this.identifier == null→CALL:initDriver→IF_FALSE: !success→FOREACH: records→CALL:getRecordName→CALL:initRecordStorage→IF_TRUE: !initRecordStorage→LOG: LOG.ERROR: Cannot initialize record store for {}, cls.getSimpleName()→RETURN→EXIT",
    "log": "[ERROR] Cannot initialize record store for simpleName"
  },
  "92690038_3": {
    "exec_flow": "ENTRY→IF_TRUE: this.identifier == null→LOG: LOG.WARN: The identifier for the State Store connection is not set→CALL:initDriver→IF_FALSE: !success→FOREACH: records→CALL:getRecordName→CALL:initRecordStorage→FOREACH_EXIT→RETURN→EXIT",
    "log": "[WARN] The identifier for the State Store connection is not set"
  },
  "705b7a99_1": {
    "exec_flow": "ENTRY→TRY→SYNC: responseQueue→CALL: size→IF_FALSE: numElements == 0→CALL: removeFirst→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Thread.currentThread().getName() + : responding to + call→CALL: channelWrite→IF_TRUE: numBytes < 0→RETURN→EXIT",
    "log": "[DEBUG] responding to call"
  },
  "705b7a99_2": {
    "exec_flow": "ENTRY→TRY→SYNC: responseQueue→CALL: size→IF_FALSE: numElements == 0→CALL: removeFirst→IF_TRUE: numBytes >= 0→IF_TRUE: !call.rpcResponse.hasRemaining()→CALL: decRpcCount→IF_TRUE: numElements == 1→SET: done = true→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Thread.currentThread().getName() + : responding to + call + Wrote + numBytes + bytes.→RETURN→EXIT",
    "log": "[DEBUG] responding to call Wrote numBytes bytes."
  },
  "705b7a99_3": {
    "exec_flow": "ENTRY→TRY→SYNC: responseQueue→CALL: size→IF_TRUE: error && call != null→LOG: LOG.WARN: Thread.currentThread().getName(), call call: output error→CALL: closeConnection→RETURN→EXIT",
    "log": "[WARN] Thread.currentThread().getName(), call call: output error"
  },
  "bbfe72ef_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "bbfe72ef_2": {
    "exec_flow": "Parent.ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "8324cdb5_1": {
    "exec_flow": "<step>AzureBlobFileSystemStore ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>Logger trace</step> <step>[VIRTUAL_CALL]</step> <step>LOG: TRACE: Fetching token provider</step> <step>CALL: getTokenProvider</step> <step>LOG: TRACE: Initializing AbfsClient for {}, baseUrl</step> <step>NEW: AbfsClient</step> <step>[VIRTUAL_CALL]</step> <step>LOG: DEBUG: Handling deprecation for all properties in config...</step> <step>CALL: getProps</step> <step>CALL: addAll</step> <step>FOREACH: keys</step> <step>LOG: DEBUG: Handling deprecation for {item}</step> <step>CALL: handleDeprecation</step> <step>LOG_DEPRECATION.info</step> <step>CALL: org.slf4j.Logger:info(java.lang.String)</step> <step>FOREACH_EXIT</step> <step>TRY</step> <step>LOG: WARN: Unexpected SecurityException in Configuration</step> <step>CATCH: SecurityException</step> <step>EXIT</step>",
    "log": "<log>[AzureBlobFileSystemStore:<init>] TRACE: Initializing AzureBlobFileSystemStore</log> <log>[Logger:trace] TRACE: Message logged by Logger</log> <log_entry> <level>TRACE</level> <template>Fetching token provider</template> </log_entry> <log_entry> <level>TRACE</level> <template>Initializing AbfsClient for {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "8324cdb5_2": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>ENTRY</step> <step>CALL:ensureInitialized</step> <step>TRY</step> <step>CALL:doSubjectLogin</step> <step>IF_TRUE:proxyUser==null</step> <step>CALL:getProperty</step> <step>CALL:createProxyUser</step> <step>CALL:tokenFileLocations.addAll</step> <step>CALL:getTrimmedStringCollection</step> <step>CALL:get</step> <step>CALL:getTrimmedStringCollection</step> <step>CALL:getTokenFileLocation</step> <step>CALL:exists</step> <step>CALL:isFile</step> <step>CALL:readTokenStorageFile</step> <step>CALL:addCredentials</step> <step>CALL:debug</step> <step>LOG:Reading credentials from location {}</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:handleDeprecation</step> <step>CALL:LOG_DEPRECATION.info</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>FOREACH_EXIT</step> <step>TRY</step> <step>LOG:Unexpected SecurityException in Configuration</step> <step>CATCH:SecurityException</step> <step>EXIT</step> <step>IF_TRUE:subject==null||subject.getPrincipals(User.class).isEmpty()</step> <step>CALL:getLoginUser</step> <step>IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()</step> <step>CALL:handleDeprecation</step> <step>FOREACH:names</step> <step>CALL:getProps</step> <step>CALL:substituteVars</step> <step>FOREACH_EXIT</step> <step>IF_TRUE:GROUPS == null</step> <step>IF_TRUE:LOG.isDebugEnabled()</step> <step>CALL:isDebugEnabled</step> <step>LOG:Creating new Groups object</step> <step>NEW:Groups</step> <step>CALL:<init></step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry>"
  },
  "1f139dd6_1": {
    "exec_flow": "ENTRY→CALL:listStatusWithRetries→FOREACH:newChildNodes→CALL:replaceFile→IF_FALSE:existsWithRetries→LOG:org.slf4j.Logger:info→CALL:renameFileWithRetries→FOREACH_EXIT→EXIT",
    "log": "[INFO] File doesn't exist. Skip deleting the file + dstPath"
  },
  "1f139dd6_2": {
    "exec_flow": "ENTRY→NEW:FSAction<FileStatus[]>→CALL:runWithRetries→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH:IOException→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)→IF:retry > fsNumRetries→LOG:org.slf4j.Logger:info(\"Maxed out FS retries. Giving up!\")→THROW:IOException→CALL:deleteFile→RETURN→EXIT",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Maxed out FS retries. Giving up! [INFO] Deleting file with retries"
  },
  "1f139dd6_3": {
    "exec_flow": "ENTRY→NEW:FSAction<FileStatus[]>→CALL:runWithRetries→WHILE:true→WHILE_COND:true→TRY→CALL:run→CATCH:IOException→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Throwable)→IF:retry <= fsNumRetries→LOG:org.slf4j.Logger:info(\"Retrying operation on FS. Retry no. \" + retry)→CALL:Thread.sleep→WHILE_COND:true→RETURN→EXIT",
    "log": "[INFO] Exception while executing an FS operation. [INFO] Retrying operation on FS. Retry no. X"
  },
  "48d7bf7a_1": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.ClientContext:get(java.lang.String,org.apache.hadoop.conf.Configuration)</entry> <call>org.apache.hadoop.hdfs.client.impl.DfsClientConf:<init>(org.apache.hadoop.conf.Configuration)</call> <entry>org.apache.hadoop.hdfs.client.impl.DfsClientConf:getChecksumOptFromConf</entry> <call>substituteVars</call> <call>getRaw</call> <log>Handling deprecation for all properties in config...</log> <call>getProps</call> <call>addAll</call> <foreach>keys</foreach> <log>Handling deprecation for (String)item</log> <call>handleDeprecation</call> <foreach_exit/> <call>LOG_DEPRECATION.info</call> <call>org.slf4j.Logger:info(java.lang.String)</call> <return/> <exit/>",
    "log": "<log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration, se</log> <log>[INFO] message</log> <log>[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}</log>"
  },
  "48d7bf7a_2": {
    "exec_flow": "<entry>org.apache.hadoop.hdfs.ClientContext:get(java.lang.String,org.apache.hadoop.conf.Configuration)</entry> <call>org.apache.hadoop.hdfs.client.impl.DfsClientConf:<init>(org.apache.hadoop.conf.Configuration)</call> <entry>org.apache.hadoop.hdfs.client.impl.DfsClientConf:getChecksumOptFromConf</entry> <call>substituteVars</call> <call>getRaw</call> <log>Handling deprecation for all properties in config...</log> <call>getProps</call> <call>addAll</call> <foreach>keys</foreach> <log>Handling deprecation for (String)item</log> <call>handleDeprecation</call> <foreach_exit/> <call>LOG_DEPRECATION.info</call> <call>org.slf4j.Logger:info(java.lang.String)</call> <return/> <exit/> <entry>P1-C1</entry> <exec_flow>ENTRY → IF_TRUE: !existing.equals(requested) → IF_TRUE: !printedConfWarning → CALL: org.slf4j.Logger:warn(java.lang.String) → EXIT",
    "log": "<log>[WARN] Existing client context ' + name + ' does not match + requested configuration. Existing: + existing + , Requested: + requested</log>"
  },
  "b914ab3f_1": {
    "exec_flow": "ENTRY→LOG:[DEBUG] Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:[DEBUG] Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "b914ab3f_2": {
    "exec_flow": "ENTRY→CALL:getInt→CALL:getLong→CALL:getBoolean→IF_TRUE: numReduceTasks == 0→IF_TRUE: isUber→LOG:[INFO] Uberizing job + jobId + \": \" + numMapTasks + \"m+\" + numReduceTasks + \"r tasks (\" + dataInputLength + \" input bytes) will run sequentially on single node.→CALL:setFloat→CALL:setInt→CALL:setInt→CALL:setBoolean→CALL:setBoolean→EXIT",
    "log": "<log>[INFO] Uberizing job + jobId + \": \" + numMapTasks + \"m+\" + numReduceTasks + \"r tasks (\" + dataInputLength + \" input bytes) will run sequentially on single node.</log>"
  },
  "b914ab3f_3": {
    "exec_flow": "ENTRY→IF_FALSE: scheme == null && authority == null→IF_FALSE: scheme != null && authority == null→IF_TRUE: conf.getBoolean(disableCacheName, false)→LOG:[DEBUG] Bypassing cache to create filesystem {}→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>→CALL:addKVAnnotation→CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass→CALL:org.apache.hadoop.util.ReflectionUtils:newInstance→TRY→CALL:initialize→RETURN→CALL:org.apache.hadoop.util.DurationInfo:close→EXIT",
    "log": "<log>[DEBUG] Bypassing cache to create filesystem {}</log>"
  },
  "b914ab3f_4": {
    "exec_flow": "ENTRY→CALL:get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY)→IF_TRUE: get(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) != null→CALL:warn→LOG:[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY→CALL:get(JobConf.MAPRED_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_TASK_ULIMIT) != null→CALL:warn→LOG:[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_MAP_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_MAP_TASK_ULIMIT) != null→CALL:warn→LOG:[WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)→CALL:get(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→IF_TRUE: get(JobConf.MAPRED_REDUCE_TASK_ULIMIT) != null→CALL:warn→LOG:[WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)→EXIT",
    "log": "<log>[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY</log> <log>[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)</log> <log>[WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)</log> <log>[WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)</log>"
  },
  "af9e0d0b_1": {
    "exec_flow": "ENTRY → CALL:writeLock → TRY → IF_TRUE:!isInSafeMode() → CALL:NameNode.stateChangeLog.info → RETURN → CALL:writeUnlock → EXIT",
    "log": "[INFO] STATE* Safe mode is already OFF"
  },
  "af9e0d0b_2": {
    "exec_flow": "ENTRY → IF_TRUE → CALL:doConsistencyCheck → ENTRY → IF_FALSE:!assertsOn → SYNC:this → IF_TRUE:blockTotal!=activeBlocks && !(blockSafe>=0 && blockSafe<=blockTotal) → CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[]) → LOG:LOG.WARN:SafeMode is in inconsistent filesystem state. + BlockManagerSafeMode data: blockTotal={}, blockSafe={}; + BlockManager data: activeBlocks={} → EXIT → RETURN → EXIT",
    "log": "[WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}"
  },
  "af9e0d0b_3": {
    "exec_flow": "ENTRY → IF_TRUE:!isSecurityEnabled() → IF_TRUE:shouldUseDelegationTokens() && !isInSafeMode() && getEditLog().isOpenForWrite() → IF_TRUE:dtSecretManager != null → TRY → CATCH:IOException",
    "log": "[INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()}"
  },
  "fabe3ec9_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → RETURN → EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "fabe3ec9_2": {
    "exec_flow": "ENTRY → TRY → CALL:userUgi.doAs → CALL:getRemoteRootLogDir → CALL:getRemoteRootLogDirSuffix → CALL:getFileSystem → CALL:LogAggregationUtils.getRemoteAppLogDir → CALL:Path.makeQualified → CALL:checkExists → CALL:Path.getParent → CALL:createDir → RETURN → EXIT",
    "log": "<log_entry> <level>ERROR</level> <template>Failed to setup application log directory for [appId]</template> </log_entry> <log_entry> <level>DEBUG</level> <template>PrivilegedAction [as: {}][action: {}]</template> </log_entry>"
  },
  "fabe3ec9_3": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE: meta != null → IF_FALSE: meta.isFile() → LOG: LOG.DEBUG: Path: [{}] is a dir. COS key: [{}] → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Path: [{}] is a dir. COS key: [{}]</template> </log_entry>"
  },
  "fabe3ec9_4": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:retrieveMetadata → IF_FALSE:meta!=null → LOG:DEBUG:List COS key: [{}] to check the existence of the path. → CALL:list → IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0 → IF_TRUE:LOG.isDebugEnabled() → LOG:DEBUG:Path: [{}] is a directory. COS key: [{}] → CALL:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>List COS key: [{}] to check the existence of the path.</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Path: [{}] is a directory. COS key: [{}]</template> </log_entry>"
  },
  "fabe3ec9_5": {
    "exec_flow": "ENTRY → CALLOTHER: FileSystem:getWorkingDirectory → EXIT",
    "log": "<!-- Inherited child log sequence as parent has no logs -->"
  },
  "fabe3ec9_6": {
    "exec_flow": "ENTRY → CALL: incrementWriteOps → CALL: adlClient.createDirectory → CALL: applyUMask → CALL: toRelativeFilePath → CALL: Integer.toOctalString → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Applying UMask</template> </log_entry>"
  },
  "fabe3ec9_7": {
    "exec_flow": "ENTRY → CALL: incrementWriteOps → CALL: adlClient.createDirectory → CALL: applyUMask → CALL: toRelativeFilePath → CALL: Integer.toOctalString → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Applying UMask</template> </log_entry>"
  },
  "5a1c6e80_1": {
    "exec_flow": "ENTRY→CALL:readLock.lock→TRY→CALL:getLabelsToNodesMapping→FOREACH:labels→ IF_FALSE:label.equals(NO_LABEL)→CALL:getAssociatedNodeIds→ IF_FALSE:nodeIds.isEmpty()→CALL:org.slf4j.Logger:warn(java.lang.String)→ FOREACH_EXIT→CALL:unmodifiableMap→RETURN→EXIT",
    "log": "[WARN] getLabelsToNodes : Label [label] cannot be found"
  },
  "918feacc_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "b3dbfd7b_1": {
    "exec_flow": "<step> <call>Parent.ENTRY</call> <log>Start execution</log> </step> <step> <call>Bypassing cache to create filesystem</call> <log>[DEBUG] Bypassing cache to create filesystem {uri}</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log> </step> <step> <call>Failed to initialize filesystem</call> <log>[WARN] Failed to initialize filesystem {}: {uri}, {e.toString()}</log> <log>[DEBUG] Failed to initialize filesystem</log> </step> <step> <entry>org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:getTotalNumRecords()</entry> <virtual_call>[VIRTUAL_CALL] org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext:acquire(org.apache.hadoop.mapreduce.TaskAttemptContext)</virtual_call> <call>LOG.INFO \"Acquiring pre-assigned chunk: \" + acquiredFilePath</call> <call>LOG.INFO taskId + \" acquired \" + chunkFile.getPath()</call> </step> <step> <call>ENTRY</call> <log>[DEBUG] Handling deprecation for all properties in config...</log> </step> <step> <call>FOREACH:keys</call> <log>[DEBUG] Handling deprecation for (String)item</log> <call>CALL:handleDeprecation</call> <call>FOREACH_EXIT</call> <call>RETURN</call> <log>WARN: Unexpected SecurityException in Configuration</log> <call>EXIT</call> </step> <step> <call>ENTRY</call> <log>[WARN] Bad checksum at {position}. Skipping entries.</log> </step> <step> <call>initialize</call> <log>[INFO] Got brand-new decompressor [<default extension>]</log> </step> <step> <call>ENTRY→IF_TRUE: chunk != null→CALL: DynamicInputChunk:close()→CALL:IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT</call> <log>[DEBUG] Exception in closing {}</log> </step>",
    "log": "<log>Start execution</log> <log>[DEBUG] Bypassing cache to create filesystem {uri}</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log> <log>[WARN] Failed to initialize filesystem {}: {uri}, {e.toString()}</log> <log>[DEBUG] Failed to initialize filesystem</log> <log>[INFO] Acquiring pre-assigned chunk: {acquiredFilePath}</log> <log>[INFO] {taskId} acquired {chunkFile.getPath()}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>WARN: Unexpected SecurityException in Configuration</log> <log>[WARN] Bad checksum at {position}. Skipping entries.</log> <log>[INFO] Got brand-new decompressor [<default extension>]</log> <log>[DEBUG] Exception in closing {}</log>"
  },
  "b3dbfd7b_2": {
    "exec_flow": "ENTRY→CALL: org.apache.hadoop.util.Preconditions:checkArgument→CALL: org.apache.hadoop.util.Preconditions:checkArgument→CALL: trim→IF_TRUE: deprecations.getDeprecatedKeyMap().isEmpty()→CALL: getProps→CALL: getOverlay().setProperty→CALL: getProps().setProperty→IF_FALSE: !isDeprecated(name)→CALL: handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)→FOREACH: names→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [INFO] message"
  },
  "65edba59_1": {
    "exec_flow": "ENTRY → SYNC:this → CALL:obtainContext → IF_TRUE:obtainContext succeeded → [VIRTUAL_CALL] → TRY → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isDirectory() → LOG:LOG.DEBUG:Found the path: {} as a file., f.toString() → CALL:updateFileStatusPath → RETURN → EXIT",
    "log": "<log> <statement>LOG.DEBUG: Found the path: {} as a file.</statement> </log>"
  },
  "65edba59_2": {
    "exec_flow": "ENTRY → SYNC:this → CALL:obtainContext → IF_TRUE:obtainContext succeeded → [VIRTUAL_CALL] → TRY → CALL:setLogEnabled → CALL:getObjectMetadata(request) → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT",
    "log": "<log> <statement>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</statement> </log>"
  },
  "3180c7a9_1": {
    "exec_flow": "ENTRY → CALL: incrementCounter → CALL: org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter → RETURN → EXIT",
    "log": "<!-- No logs were generated, hence none in this sequence -->"
  },
  "3180c7a9_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: {}: {} '{}' to '{}', getName(), operation, source, dest → CALL: requireNonNull → CALL: requireNonNull → TRY → CALL: apply → IF_FALSE: !success → CALL: close → IF_FALSE: !success → ENTRY → LOG:TRACE:{}: getFileStatus('{}'), getName(), path → CALL:requireNonNull → CALL:trackDuration → CALL:getIOStatistics → CALL:operations.getFileStatus → RETURN → CATCH:FileNotFoundException → LOG:TRACE:{}: getFileStatus('{}') [EXCEPTION] → RETURN → EXIT → CALL:getFileStatus → CALL:getFileStatusOrNull → LOG:ERROR:{}: failure to {} {} to {} with source status {} and destination status {} → NEW:PathIOException → CALL:toString → RETURN → EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest' [TRACE] getName(): getFileStatus('{}'), path [TRACE] getName(): getFileStatus('{}') [EXCEPTION] [ERROR] getName(): failure to {} {} to {} with source status {} and destination status {}"
  },
  "3180c7a9_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: {}: {} '{}' to '{}', getName(), operation, source, dest → CALL: requireNonNull → CALL: requireNonNull → TRY → CALL: apply → IF_TRUE: !success → CALL: failed → CALL: close → IF_TRUE: !success → THROW: escalateRenameFailure(operation, source, dest) → ENTRY → LOG:TRACE:{}: getFileStatus('{}'), getName(), path → CALL:requireNonNull → CALL:trackDuration → CALL:getIOStatistics → CALL:operations.getFileStatus → RETURN → CATCH:FileNotFoundException → LOG:TRACE:{}: getFileStatus('{}') [EXCEPTION] → RETURN → EXIT → CALL:getFileStatus → CALL:getFileStatusOrNull → LOG:ERROR:{}: failure to {} {} to {} with source status {} and destination status {} → NEW:PathIOException → CALL:toString → RETURN → EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest' [TRACE] getName(): getFileStatus('{}'), path [TRACE] getName(): getFileStatus('{}') [EXCEPTION] [ERROR] getName(): failure to {} {} to {} with source status {} and destination status {}"
  },
  "3180c7a9_4": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: {}: {} '{}' to '{}', getName(), operation, source, dest → CALL: requireNonNull → CALL: requireNonNull → TRY → CALL: apply → IF_TRUE: !success → CALL: failed → LOG: LOG.INFO: {}: {} raised an exception: {}, getName(), operation, e.toString() → LOG: LOG.DEBUG: {}: {} stack trace, getName(), operation, e → CALL: failed → THROW: escalateRenameFailure(operation, source, dest) → EXIT → ENTRY → LOG:TRACE:{}: getFileStatus('{}'), getName(), path → CALL:requireNonNull → CALL:trackDuration → CALL:getIOStatistics → CALL:operations.getFileStatus → RETURN → CATCH:FileNotFoundException → LOG:TRACE:{}: getFileStatus('{}') [EXCEPTION] → RETURN → EXIT → CALL:getFileStatus → CALL:getFileStatusOrNull → LOG:ERROR:{}: failure to {} {} to {} with source status {} and destination status {} → NEW:PathIOException → CALL:toString → RETURN → EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest' [INFO] getName(): operation raised an exception: exceptionMessage [DEBUG] getName(): operation stack trace [TRACE] getName(): getFileStatus('{}'), path [TRACE] getName(): getFileStatus('{}') [EXCEPTION] [ERROR] getName(): failure to {} {} to {} with source status {} and destination status {}"
  },
  "a18cbeb4_1": {
    "exec_flow": "<seq> ENTRY → [VIRTUAL_CALL] → org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:serviceStart → IF_TRUE: !addr.isUnresolved() && addr.getAddress().isAnyLocalAddress() → TRY → NEW: InetSocketAddress → CALL: getLocalHost → IF_TRUE: logSlowLookups || LOG.isTraceEnabled() → CALL: org.slf4j.Logger:isTraceEnabled() → IF_TRUE: elapsedMs >= slowLookupThresholdMs → CALL: org.slf4j.Logger:warn(java.lang.String) → IF_TRUE: staticHost != null → CALL: getByAddress → CALL: getAddress → CALL: getAddress → NEW: InetSocketAddress → RETURN → EXIT </seq>",
    "log": "<log> [WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms. [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "a18cbeb4_2": {
    "exec_flow": "<seq> ENTRY → [VIRTUAL_CALL] → LOG: Creating YarnRPC for {Configuration} → CALL: get → ENTRY → LOG: Unexpected SecurityException in Configuration → CALL: getRaw → ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → FOREACH: names → CALL: getProps → FOREACH_EXIT → RETURN → EXIT </seq>",
    "log": "<log> [DEBUG] Creating a HadoopYarnProtoRpc server for protocol {} with {} handlers [DEBUG] Creating YarnRPC for {Configuration} [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item </log>"
  },
  "a18cbeb4_3": {
    "exec_flow": "<seq> ENTRY→CALL:totalRequests.increment→CALL:set→CALL:get→CALL:get→CALL:set→CALL:rpcMetrics.addRpcQueueTime→IF_FALSE:call.isResponseDeferred() || connDropped→CALL:rpcMetrics.addRpcLockWaitTime→CALL:rpcMetrics.addRpcProcessingTime→CALL:getDetailedMetricsName→CALL:rpcDetailedMetrics.addProcessingTime→CALL:callQueue.addResponseTime→IF_TRUE:isLogSlowRPC→CALL:logSlowRpcCalls→EXIT→TRY→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}], this, action, new Exception()→CALL:Subject.doAs→RETURN→CATCH:PrivilegedActionException→LOG:LOG.DEBUG:PrivilegedActionException as: {}, this, cause→THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException→RETURN→EXIT ENTRY → LOG: [INFO] Thread.currentThread().getName(): starting → CALL: SERVER.set → CALL: connectionManager.startIdleScan → WHILE: running → WHILE_COND: running → TRY → CALL: getSelector().select → CALL: getSelector().selectedKeys → CALL: key.isValid → IF_COND: key.isValid → CALL: key.isAcceptable → IF_COND: key.isAcceptable → CALL: doAccept → CATCH: IOException → CATCH_END → OUTER_TRY_END → CATCH: OutOfMemoryError → LOG: [WARN] \"Out of Memory in server select\" → CALL: closeCurrentConnection → CALL: connectionManager.closeIdle → CALL: Thread.sleep → CATCH_END → OUTER_CATCH: Exception → CALL: closeCurrentConnection → OUTER_CATCH_END → WHILE_END → LOG: [INFO] \"Stopping \" + Thread.currentThread().getName() → SYNC: this → TRY → CALL: acceptChannel.close → CALL: selector.close → CATCH: IOException → CATCH_END → SYNC_END → CALL: connectionManager.stopIdleScan → CALL: FOREACH: toArray() → CALL: org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection) → FOREACH_EXIT → EXIT </seq>",
    "log": "<log> [DEBUG] Thread.currentThread().getName(): starting [DEBUG] Thread.currentThread().getName(): call for RpcKind + call.rpcKind [INFO] Thread.currentThread().getName() unexpectedly interrupted [INFO] Thread.currentThread().getName() caught an exception [DEBUG] Served: [{}] [DEBUG] Thread.currentThread().getName(): exiting [DEBUG] Exception in closing {} [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {} [INFO] Thread.currentThread().getName(): starting [WARN] Out of Memory in server select [INFO] Stopping Thread.currentThread().getName() </log>"
  },
  "2fc23b04_1": {
    "exec_flow": "<![CDATA[ ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG: Unexpected SecurityException in Configuration → CATCH: SecurityException → THROW: new IOException(\"Running in secure mode, but config doesn't have a keytab\") → CATCH_EXIT → ENTRY → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: LOG.DEBUG: Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → EXIT ]]>",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] Unexpected SecurityException in Configuration [DEBUG] Creating new Groups object"
  },
  "2388674f_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → DO_WHILE → CALL:add → CALL:getParent → DO_COND:absolutePath != null → DO_EXIT → FOREACH:paths → IF_FALSE:path.equals(New Path(CosNFileSystem.PATH_DELIMITER)) → TRY → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus → IF_FALSE:fileStatus.isFile() → IF_FALSE:fileStatus.isDirectory() → CATCH:FileNotFoundException → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:storeEmptyFile → FOREACH_EXIT → RETURN → EXIT → TRY → CALL:getIsNamespaceEnabled → CALL:createPath → CALL:registerResult → CALL:close → CALL: abfsStore.createDirectory → CATCH: AzureBlobFileSystemException ex → CALL: checkException → RETURN → EXIT → ENTRY → IF_FALSE:theInternalDir.isRoot()&&dir==null → IF_FALSE:theInternalDir.getChildren().containsKey(dir.toString().substring(1)) → IF_TRUE:this.fsState.getRootFallbackLink()!=null → TRY → CALL:org.apache.hadoop.fs.FileSystem:mkdirs → RETURN → EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}] [DEBUG] Making dir: [{}] in COS [DEBUG] createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {} [DEBUG] Incremented write operations count [DEBUG] Incremented storage statistics OpCounter [INFO] Applying UMask [DEBUG] FsPathBooleanRunner is running [INFO] S3AFileSystem:mkdirs - Path qualified and tracked for mkdirs operation [DEBUG] Failed to create <dirToCreate_value> at fallback : <linkedFallbackFs.getUri_value>"
  },
  "eb8f7b07_1": {
    "exec_flow": "ENTRY→IF_TRUE:LOG.isDebugEnabled()→CALL:debugLoggingBuilder.get().setLength→CALL:debugLoggingBuilder.get().append(\"[\")→CALL:CHOOSE_RANDOM_REASONS.get().clear→WHILE:numOfReplicas > 0→CALL:chooseDataNode→IF_TRUE:storage != null→CALL:chooseStorage4Block→WHILE_EXIT→IF_TRUE:numOfReplicas > 0→IF_FALSE:LOG.isDebugEnabled()→IF_TRUE:!reasonMap.isEmpty()→LOG:LOG.INFO:Not enough replicas was chosen. Reason: {}, reasonMap→THROW:new NotEnoughReplicasException(detail)→EXIT",
    "log": "LOG.INFO: Not enough replicas was chosen. Reason: {}, reasonMap"
  },
  "eb8f7b07_2": {
    "exec_flow": "ENTRY→CALL:chooseRandomWithStorageTypeTwoTrial→CALL:netlock.readLock().lock→TRY→IF_TRUE:scope.startsWith(\"~\")→CALL:substring→IF_FALSE:n == null→CALL:Preconditions.checkArgument→IF_FALSE:dnDescriptor.hasStorageType(type)→CALL:LOG.debug:First trial failed, node has no type {}, making second trial carrying this type→CALL:chooseRandomWithStorageType→RETURN→EXIT",
    "log": "[DEBUG] First trial failed, node has no type {}, making second trial carrying this type"
  },
  "4309d337_1": {
    "exec_flow": "ENTRY → TRY → CALL:qualify → CALL:createSpan → IF_FALSE: outcome → RETURN → EXIT",
    "log": "[DEBUG] Couldn't delete {} - does not exist: {}"
  },
  "4309d337_2": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>TRY</step> <step>CALL:create</step> <step>CALL:open</step> <step>CALL:IOUtils.copyBytes</step> <step>CALL:fsIn.close</step> <step>CALL:delete</step> <step>CALL:fsOut.write</step> <step>CALL:fsOut.close</step> <step>CALL:rename</step> <step>CALL:org.apache.hadoop.fs.FileSystem:rename</step> <step>LOG:[DEBUG] Moving {} to {}, src, dst</step> <step>IF_FALSE:containsColon(dst)</step> <step>IF_FALSE:srcParentFolder == null</step> <step>IF_FALSE:srcKey.length() == 0</step> <step>CALL:performAuthCheck</step> <step>IF_FALSE:this.azureAuthorization</step> <step>TRY</step> <step>CALL:retrieveMetadata</step> <step>IF_FALSE:dstMetadata != null && dstMetadata.isDirectory()</step> <step>IF_FALSE:dstMetadata != null</step> <step>TRY</step> <step>CALL:retrieveMetadata</step> <step>IF_FALSE:parentOfDestMetadata == null</step> <step>IF_FALSE:!parentOfDestMetadata.isDirectory()</step> <step>CALL:prepareAtomicFolderRename</step> <step>CALL:execute</step> <step>LOG:[DEBUG] Renamed {} to {} successfully.</step> <step>EXCEPTION:rename</step> <step>CATCH:IOException ie</step> <step>LOG:[ERROR] Got an exception while writing file, ie</step> <step>EXIT</step> </sequence>",
    "log": "[DEBUG] Moving {} to {}, src, dst [DEBUG] Renamed {} to {} successfully. [ERROR] Got an exception while writing file"
  },
  "c085d325_1": {
    "exec_flow": "ENTRY → FOREACH:args → TRY → CALL:processArgument → CATCH(IOException) → CALL:displayError → CALL:displayWarning → CALL:org.slf4j.Logger:debug(java.lang.String) → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "c085d325_2": {
    "exec_flow": "ENTRY → FOREACH:args → TRY → CALL:processArgument → CATCH(IOException) → CALL:displayError → CALL:displayWarning → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "958f1ce8_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→CALL:namedCallbacks.put→CALL:org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder→CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource→CALL:checkNotNull→CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>→CALL:put→CALL:start→LOG:LOG.DEBUG:Registering the metrics source→LOG:LOG.DEBUG:Registered source + name→EXIT",
    "log": "[DEBUG] Registering the metrics source [DEBUG] Registered source + name"
  },
  "021879cc_1": {
    "exec_flow": "ENTRY → CALL: this.readLock.lock → TRY → CALL: org.slf4j.Logger:debug → IF_FALSE: !appAttemptSet.contains(applicationAttemptId) → IF_FALSE: identifier.getKeyId() == this.currentMasterKey.getMasterKey().getKeyId() → IF_TRUE: nextMasterKey != null && identifier.getKeyId() == this.nextMasterKey.getMasterKey().getKeyId() → CALL: createPassword → CALL: org.apache.hadoop.yarn.security.AMRMTokenIdentifier:getBytes → CALL: getSecretKey → RETURN → EXIT",
    "log": "<log>[DEBUG] Trying to retrieve password for {applicationAttemptId}</log>"
  },
  "021879cc_2": {
    "exec_flow": "ENTRY → CALL: this.readLock.lock → TRY → LOG: LOG.DEBUG: Trying to retrieve password for {}, applicationAttemptId → IF_TRUE: !appAttemptSet.contains(applicationAttemptId) → THROW: new InvalidToken(applicationAttemptId + \" not found in AMRMTokenSecretManager.\") → EXIT",
    "log": "[DEBUG] Trying to retrieve password for {applicationAttemptId}"
  },
  "021879cc_3": {
    "exec_flow": "ENTRY → CALL: this.readLock.lock → TRY → LOG: LOG.DEBUG: Trying to retrieve password for {}, applicationAttemptId → IF_FALSE: !appAttemptSet.contains(applicationAttemptId) → IF_TRUE: identifier.getKeyId() == this.currentMasterKey.getMasterKey().getKeyId() → CALL: createPassword → CALL: getBytes → CALL: getSecretKey → RETURN → EXIT",
    "log": "[DEBUG] Trying to retrieve password for {applicationAttemptId}"
  },
  "021879cc_4": {
    "exec_flow": "ENTRY → CALL: this.readLock.lock → TRY → LOG: LOG.DEBUG: Trying to retrieve password for {}, applicationAttemptId → IF_FALSE: !appAttemptSet.contains(applicationAttemptId) → IF_FALSE: identifier.getKeyId() == this.currentMasterKey.getMasterKey().getKeyId() → IF_TRUE: nextMasterKey != null && identifier.getKeyId() == this.nextMasterKey.getMasterKey().getKeyId() → CALL: createPassword → CALL: getBytes → CALL: getSecretKey → RETURN → EXIT",
    "log": "[DEBUG] Trying to retrieve password for {applicationAttemptId}"
  },
  "021879cc_5": {
    "exec_flow": "ENTRY → CALL: this.readLock.lock → TRY → LOG: LOG.DEBUG: Trying to retrieve password for {}, applicationAttemptId → IF_FALSE: !appAttemptSet.contains(applicationAttemptId) → IF_FALSE: identifier.getKeyId() == this.currentMasterKey.getMasterKey().getKeyId() → IF_FALSE: nextMasterKey != null && identifier.getKeyId() == this.nextMasterKey.getMasterKey().getKeyId() → THROW: new InvalidToken(\"Invalid AMRMToken from \" + applicationAttemptId) → EXIT",
    "log": "[DEBUG] Trying to retrieve password for {applicationAttemptId}"
  },
  "021879cc_6": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>Child.PATH_A</step>",
    "log": "<log>org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager:retrievePasswordInternal [DEBUG] Retrieving password for {identifier.getContainerID()} for user {identifier.getUser()} to be run on NM {identifier.getNmHostAddress()}</log>"
  },
  "461eb906_1": {
    "exec_flow": "ENTRY→CALL:getClosest→ IF_FALSE: this.keys != null → TRY → CALL: org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable) → WHILE: true → WHILE_COND: true → CALL: org.apache.hadoop.io.WritableComparator:newKey() → IF_FALSE: !index.next(k, position) → IF_FALSE: lastKey != null && comparator.compare(lastKey, k) > 0 → WHILE_EXIT → CALL: toArray → CALL: copyOf → CALL: close → IF_TRUE: seekIndex != -1 && seekIndex + 1 < count && comparator.compare(key, keys[seekIndex + 1]) < 0 && comparator.compare(key, nextKey) >= 0 → ELSE: seekIndex = binarySearch(key) → IF_TRUE: seekIndex < 0 → SEEKING: (advance seekPosition based on conditions) → IF_FALSE: !blockCompressed → CALL: deserializeValue → IF_TRUE: (valLength < 0) AND LOG.isDebugEnabled() → LOG.DEBUG: val + is a zero-length value → RETURN → EXIT",
    "log": "[DEBUG] val + is a zero-length value"
  },
  "22d23912_1": {
    "exec_flow": "ENTRY→CALL:checkToken→IF_TRUE:info==null→CALL:getRealUser→LOG:LOG.WARN:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→EXIT",
    "log": "[WARN] {}, Token={}"
  },
  "22d23912_2": {
    "exec_flow": "ENTRY→CALL:checkToken→IF_FALSE:info==null→IF_TRUE:info.getRenewDate()<now→CALL:getRealUser→CALL:formatTime→CALL:formatTime→CALL:getRenewDate→CALL:getRenewDate→LOG:LOG.INFO:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→EXIT",
    "log": "[INFO] {}, Token={}"
  },
  "4b187cd5_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.util.Map,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType)→IF_TRUE: acl == null→LOG: LOG.DEBUG: No ACL available for key, denying access for {}, opType→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→RETURN→EXIT",
    "log": "[DEBUG] No ACL available for key, denying access for {}"
  },
  "4b187cd5_2": {
    "exec_flow": "<step> <description>Call unauthorized method</description> <action>op(OpStatus.UNAUTHORIZED, op, user, key, \"Unknown\", \"\")</action> </step> <step> <description>Evaluate if user and key information are not empty and operation is valid</description> <condition>!Strings.isNullOrEmpty(user) && !Strings.isNullOrEmpty(key) && (op != null) && AGGREGATE_OPS_WHITELIST.contains(op)</condition> <then> <step> <description>Invalidate cache and log unauthorized event</description> <action>cache.invalidate(cacheKey)</action> <log>logEvent(opStatus, new AuditEvent(op, ugi, key, remoteHost, extraMsg))</log> </step> </then> <else> <step> <description>Log unauthorized event directly</description> <log>logEvent(opStatus, new AuditEvent(op, ugi, key, remoteHost, extraMsg))</log> </step> </else> </step>",
    "log": "<log>logEvent(OpStatus.UNAUTHORIZED, new AuditEvent(op, ugi, key, \"Unknown\", \"\"))</log>"
  },
  "13478294_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:UserGroupInformation.getLoginUser</step> <step>CALL:UserGroupInformation.setConfiguration</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>RETURN</step> <step>FOREACH_EXIT</step> <step>CALL:doAs</step> <step>TRY</step> <step>IF_TRUE: LOG.isDebugEnabled()</step> <step>LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}]</step> <step>CATCH: PrivilegedActionException</step> <step>LOG: LOG.DEBUG: PrivilegedActionException as: {}</step> <step>THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException</step> <step>NEW:PrivilegedExceptionAction<Integer></step> <step>CALL:runJob</step> <step>CALL:System.out.print</step> <step>CALL:System.out.println</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log> <level>INFO</level> <template>DefaultRuleAssigned</template> </log> <log> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log> <log> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log> <log> <level>DEBUG</level> <template>Creating new Groups object</template> </log> <log> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log> <log> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log> <log> <level>INFO</level> <template>Token file {} does not exist</template> </log> <log> <level>DEBUG</level> <template>Failure to load login credentials</template> </log> <log> <level>DEBUG</level> <template>PrivilegedAction [as: {}][action: {}]</template> </log> <log> <level>DEBUG</level> <template>PrivilegedActionException as: {}</template> </log>"
  },
  "174ea629_1": {
    "exec_flow": "<sequence> Check auto throttling Check account throttling Initialize singleton if null Acquire lock Create singleton with configuration Release lock ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→CALL:substituteVars→RETURN→EXIT <flow>org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory:getInstance.ENTRY → [VIRTUAL_CALL] → AbfsClientThrottlingIntercept.ENTRY → [VIRTUAL_CALL] → Logger:debug paths</flow> </sequence>",
    "log": "<log>[Class:AbfsClientThrottlingIntercept][DEBUG] Client-side throttling is enabled for the ABFS file system.</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[Class:Logger][DEBUG] Debug log message</log>"
  },
  "174ea629_2": {
    "exec_flow": "ENTRY→CALL:entriesCreatedCount.incrementAndGet→DO_WHILE→CALL:put→CALL:resolve→IF_TRUE:resolvedStrongRef == null→CALL:referenceLostDuringCreation.warn→CALL:noteLost→DO_COND:resolvedStrongRef == null→DO_EXIT→IF_FALSE:strongRef != resolvedStrongRef→RETURN→EXIT",
    "log": "<log>[WARN] reference to %s lost during creation, key</log>"
  },
  "174ea629_3": {
    "exec_flow": "ENTRY→CALL:entriesCreatedCount.incrementAndGet→DO_WHILE→CALL:put→CALL:resolve→IF_FALSE:resolvedStrongRef == null→DO_COND:resolvedStrongRef == null→DO_EXIT→IF_TRUE:strongRef != resolvedStrongRef→LOG:LOG.DEBUG: Created instance for key {}: {} overwritten by {}, key, strongRef, resolvedStrongRef→RETURN→EXIT",
    "log": "<log>[DEBUG] Created instance for key {}: {} overwritten by {}, key, strongRef, resolvedStrongRef</log>"
  },
  "174ea629_4": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Initializing SSL Context to channel mode {}, preferredChannelMode→SWITCH: preferredChannelMode→CASE: [OpenSSL]→LOG: LOG.DEBUG: Attempting to register OpenSSL provider→CALL: bindToOpenSSLProvider→BREAK→CHECK if defaultCiphers[i].contains(\"_GCM_\")→LOG: LOG.DEBUG: Removed Cipher - {} from list of enabled SSLSocket ciphers→EXIT",
    "log": "<log>[DEBUG] Initializing SSL Context to channel mode OpenSSL</log> <log>[DEBUG] Attempting to register OpenSSL provider</log> <log>[DEBUG] Removed Cipher - CipherName from list of enabled SSLSocket ciphers</log>"
  },
  "3621884f_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CALL: Subject.doAs → RETURN → EXIT CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause → THROW: IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException",
    "log": "<log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "3621884f_2": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → IF_TRUE: loginUser==null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser==null → DO_EXIT → RETURN → EXIT → LOG: Unexpected SecurityException in Configuration → CALL: getRaw → ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[LOG] getLoginUser</log>"
  },
  "c9e3a49d_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "c9e3a49d_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "c9e3a49d_3": {
    "exec_flow": "ENTRY → CALL:getTrimmed → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → LOG:Handling deprecation for (String)item → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "25ae6275_1": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>CALL:getHistoryFileReader</step> <step>WHILE: hfReader.hasNext()</step> <step>CALL:next</step> <step>IF: entry.key.id.startsWith(ConverterUtils.CONTAINER_PREFIX)</step> <step>CALL:ContainerId.fromString</step> <step>CALL:equals</step> <step>IF_TRUE</step> <step>LOG:LOG.INFO: Error when reading history information of some containers of application attempt + appAttemptId</step> <step>CALL:close</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[INFO] Error when reading history information of some containers of application attempt [appAttemptId]"
  },
  "25ae6275_2": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>CALL:getHistoryFileReader</step> <step>WHILE: hfReader.hasNext()</step> <step>CALL:next</step> <step>IF: entry.key.id.startsWith(ConverterUtils.CONTAINER_PREFIX)</step> <step>CALL:ContainerId.fromString</step> <step>CALL:equals</step> <step>IF_TRUE</step> <step>IF: historyData == null</step> <step>CALL:ContainerHistoryData.newInstance</step> <step>CALL:put</step> <step>IF: entry.key.suffix.equals(START_DATA_SUFFIX)</step> <step>CALL:parseContainerStartData</step> <step>CALL:mergeContainerHistoryData</step> <step>LOG:LOG.INFO: Completed reading history information of all containers of application attempt + appAttemptId</step> <step>CALL:close</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[INFO] Completed reading history information of all containers of application attempt [appAttemptId]"
  },
  "25ae6275_3": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>CALL:getHistoryFileReader</step> <step>WHILE: hfReader.hasNext()</step> <step>CALL:next</step> <step>IF: entry.key.id.startsWith(ConverterUtils.CONTAINER_PREFIX)</step> <step>CALL:ContainerId.fromString</step> <step>CALL:equals</step> <step>IF_TRUE</step> <step>IF: historyData == null</step> <step>CALL:ContainerHistoryData.newInstance</step> <step>CALL:put</step> <step>IF: entry.key.suffix.equals(FINISH_DATA_SUFFIX)</step> <step>CALL:parseContainerFinishData</step> <step>CALL:mergeContainerHistoryData</step> <step>LOG:LOG.INFO: Completed reading history information of all containers of application attempt + appAttemptId</step> <step>CALL:close</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[INFO] Completed reading history information of all containers of application attempt [appAttemptId]"
  },
  "776ea4b4_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:releaseContainer</step> <step>CALL:org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getResource</step> <step>IF_FALSE:container.containerMetrics != null</step> <step>CALL:sendFinishedEvents</step> <step>IF_TRUE:container.getCurrentState() != org.apache.hadoop.yarn.api.records.ContainerState.NEW</step> <step>CALL:container.dispatcher.getEventHandler().handle</step> <step>CALL:container.context.getNodeStatusUpdater().sendOutofBandHeartBeat</step> <step>EXIT</step>",
    "log": "<log>[INFO] Container metrics released</log> <log>[DEBUG] Handle called for event</log> <log> <class>org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl</class> <method>initResources</method> <level>WARN</level> <template>Got unknown resource type: {entry.getKey()}; skipping</template> </log>"
  },
  "776ea4b4_2": {
    "exec_flow": "ENTRY→IF_TRUE:container.wasLaunched→CALL:endRunningContainer→CALL:completedContainer→CALL:NMAuditLogger.logSuccess→IF_TRUE: LOG.isInfoEnabled()→LOG: LOG.INFO:createSuccessLog(user, operation, target, null, null)→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:transition→CALL:ContainerImpl$ContainerDoneTransition:transition→EXIT",
    "log": "[INFO] createSuccessLog(user, operation, target, null, null)"
  },
  "776ea4b4_3": {
    "exec_flow": "ENTRY→IF_FALSE:container.wasLaunched→LOG:LOG.WARN:Container exited with success despite being killed and not actually running→CALL:completedContainer→CALL:NMAuditLogger.logSuccess→IF_TRUE: LOG.isInfoEnabled()→LOG: LOG.INFO:createSuccessLog(user, operation, target, null, null)→CALL:org.slf4j.Logger:info(java.lang.String)→CALL:transition→CALL:ContainerImpl$ContainerDoneTransition:transition→EXIT",
    "log": "[WARN] Container exited with success despite being killed and not actually running [INFO] createSuccessLog(user, operation, target, null, null)"
  },
  "fb2b3cdf_1": {
    "exec_flow": "ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "fb2b3cdf_2": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG→CALL:statIncrement →CALL:trailingPeriodCheck→CALL:makeQualified→TRY →EXCEPTION:createFile→CATCH:AzureBlobFileSystemException →CALL:checkException→RETURN→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</template> </log_entry>"
  },
  "fb2b3cdf_3": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:fileStatus.isDirectory() →IF_FALSE:!overwrite→CALL:org.slf4j.Logger:debug →NEW:FSDataOutputStream→NEW:CosNOutputStream→CALL:getConf→RETURN→EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Creating a new file: [{}] in COS.</template> </log_entry>"
  },
  "fb2b3cdf_4": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:status.isDirectory() → IF_FALSE:!overwrite → LOG:debug → EXCEPTION:debug → CATCH:FileNotFoundException → CALL:getMultipartSizeProperty → NEW:FSDataOutputStream → NEW:AliyunOSSBlockOutputStream → CALL:getConf → NEW:SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Overwriting file {}</template> </log_entry>"
  },
  "cc21933b_1": {
    "exec_flow": "<!-- Aggregated execution flow for enhanced context --> <step>ENTRY</step> <step>VIRTUAL_CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:setupConfigurableCapacities</step> <step>CALL:loadCapacitiesByLabelsFromConf</step> <step>CALL:updateAbsoluteCapacitiesByNodeLabels</step> <step>CALL:capacitiesSanityCheck</step> <step>EXIT</step>",
    "log": "[DEBUG] CSConf - getCapacityOfLabel: prefix= + getNodeLabelPrefix(queue, label) + , capacity= + capacity"
  },
  "981ad082_1": {
    "exec_flow": "ENTRY→CALL:obtainContext→IF_TRUE:pathStr.startsWith(\"/\")→CALL:substring→CALL:getLocalPathForWrite→RETURN→EXIT",
    "log": "[DEBUG] mkdirs of {}={} [WARN] Failed to create [dirStrings[i]] [WARN] [dirStrings[i]] is not writable"
  },
  "b76eddd0_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY -> IF_FALSE: conf == null -> IF_FALSE: isInState(STATE.INITED) -> SYNC: stateChangeLock -> IF_TRUE: enterState(STATE.INITED) != STATE.INITED -> CALL: setConfig -> TRY -> CALL: serviceInit -> CALL: org.slf4j.Logger:debug(java.lang.String) -> CALL: setConfig -> IF_TRUE: isInState(STATE.INITED) -> CALL: notifyListeners -> TRY -> CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners -> CALL: globalListeners.notifyListeners -> CATCH: Throwable e -> LOG: LOG.WARN: Exception while notifying listeners of {}, this, e -> CALL: stopQuietly -> EXIT",
    "log": "<!-- Merged log sequence --> <log> <event>Service: {} entered state {}</event> <level>DEBUG</level> </log> <log> <event>noteFailure</event> <level>DEBUG</level> </log> <log> <event>Service {} failed in state {}</event> <level>INFO</level> <parameters> <parameter>getName()</parameter> <parameter>failureState</parameter> <parameter>exception</parameter> </parameters> </log> <log> <event>Exception while notifying listeners of {}</event> <level>WARN</level> </log> <log> <event>Config has been overridden during init</event> <level>DEBUG</level> </log> <log> <class>org.apache.hadoop.service.ServiceOperations</class> <method>stopQuietly</method> <level>WARN</level> <template>When stopping the service {service_name}</template> </log>"
  },
  "e66aeff6_1": {
    "exec_flow": "ENTRY→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}"
  },
  "e66aeff6_2": {
    "exec_flow": "ENTRY→CALL:org.slf4j.Logger:debug→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite→EXCEPTION:AzureBlobFileSystemException→CALL:checkException→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}"
  },
  "e66aeff6_3": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:authorize→RETURN→EXIT→TRY→CALL:retrieveMetadata→IF_TRUE:meta != null→CALL:getOwner→LOG:LOG.DEBUG:Retrieved '{}' as owner for path - {}, owner, absolutePath→RETURN→EXIT",
    "log": "[DEBUG] Retrieved '{}' as owner for path - {}"
  },
  "e66aeff6_4": {
    "exec_flow": "ENTRY→TRY→CALL:retrieveMetadata→IF_FALSE:meta != null→LOG:LOG.DEBUG:Cannot find file/folder - '{}'. Returning owner as empty string, absolutePath→RETURN→EXIT",
    "log": "[DEBUG] Cannot find file/folder - '{}'. Returning owner as empty string"
  },
  "e66aeff6_5": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → FOREACH:names → LOG:Handling deprecation for (String)item → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "04d1847e_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → CALL: noteFailure → CATCH: Exception e → IF: exception != null → LOG: LOG.DEBUG: noteFailure → SYNC: failureCause == null → SET: failureCause = exception → SET: failureState = getServiceState() → LOG: LOG.INFO: Service {} failed in state {} → FINALLY → TERMINATION_NOTIFICATION_SET: true → SYNC: terminationNotification → NOTIFYALL → CALL: notifyListeners → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → FINALLY → EXIT",
    "log": "<log>LOG.debug(\"noteFailure\", exception)</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log> <log>LOG.warn(\"Exception while notifying listeners of {}\")</log>"
  },
  "2a4bb787_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <block type=\"try-catch\"> <log_sequence> <log> <class>org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices</class> <method>getDomains</method> <level>ERROR</level> <template>Error getting domains</template> </log> </log_sequence> </block> ENTRY→IF_TRUE: getServiceStopped()→LOG: LOG.INFO: Service stopped, return null for the storage→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Verifying the access of [user]→IF_FALSE: !adminAclsManager.areACLsEnabled()→IF_FALSE: owner == null || owner.length() == 0→IF_TRUE: callerUGI != null && (adminAclsManager.isAdmin(callerUGI) || callerUGI.getShortUserName().equals(owner))→CALL: adminAclsManager.isAdmin→RETURN→EXIT",
    "log": "<log> <class>org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices</class> <method>getDomains</method> <level>ERROR</level> <template>Error getting domains</template> </log>"
  },
  "528e05e6_1": {
    "exec_flow": "ENTRY→CALL:set→IF_TRUE: // grow buffer\\nbytes == null || length > bytes.length→CALL: System.arraycopy→CALL: org.slf4j.Logger:warn→CALL:write→EXIT",
    "log": "<!-- No logs generated in this path -->"
  },
  "528e05e6_2": {
    "exec_flow": "ENTRY→CALL:set→IF_FALSE: // grow buffer\\nbytes == null || length > bytes.length→CALL: System.arraycopy→CALL: org.slf4j.Logger:warn→CALL:write→EXIT",
    "log": "<!-- No logs generated in this path -->"
  },
  "983fae0f_1": {
    "exec_flow": "ENTRY→IF_FALSE: newRecords == null || currentDriverTime <= 0→FOREACH: newRecords→IF_TRUE: record.shouldBeDeleted(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: getDriver().remove→IF_FALSE: remove!=success→CALL: Logger.warn→FOREACH_EXIT→IF_TRUE: commitRecords.size() > 0→CALL: verifyDriverReady→IF_FALSE: records.isEmpty()→FOREACH: records→CALL: getPathForClass→CALL: exists→IF_TRUE: exists(recordPath)→IF_FALSE: allowUpdate→IF_TRUE: errorIfExists→LOG: LOG.ERROR: Attempt to insert record {} that already exists→CALL: addFailure→RETURN→EXIT",
    "log": "<log>Warn: Couldn’t delete State Store record {}: {}</log> <log>[ERROR] Attempt to insert record {} that already exists</log>"
  },
  "983fae0f_2": {
    "exec_flow": "ENTRY→IF_TRUE: newRecords == null || currentDriverTime <= 0→LOG: LOG.ERROR: Cannot check overrides for record→RETURN→EXIT",
    "log": "<log>Error: Cannot check overrides for record</log>"
  },
  "983fae0f_3": {
    "exec_flow": "ENTRY→IF_FALSE: newRecords == null || currentDriverTime <= 0→FOREACH: newRecords→IF_TRUE: record.shouldBeDeleted(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: getDriver().remove→IF_TRUE: remove==success→CALL: Logger.info→FOREACH_EXIT→IF_TRUE: commitRecords.size() > 0→CALL: putAll→IF_TRUE: deleteRecords.size() > 0→CALL: newRecords.removeAll→EXIT",
    "log": "<log>Info: Deleted State Store record {}: {}</log>"
  },
  "983fae0f_4": {
    "exec_flow": "ENTRY→IF_FALSE: newRecords == null || currentDriverTime <= 0→FOREACH: newRecords→IF_FALSE: record.shouldBeDeleted(currentDriverTime)→IF_TRUE: record.checkExpired(currentDriverTime)→CALL: StateStoreUtils.getRecordName→CALL: Logger.info→FOREACH_EXIT→IF_TRUE: commitRecords.size() > 0→CALL: verifyDriverReady→IF_FALSE: records.isEmpty()→FOREACH: records→CALL: getPathForClass→CALL: exists→IF_FALSE: exists(recordPath)→CALL: getWriter→TRY→CALL: serializeString→CATCH →LOG: LOG.ERROR: Cannot write {}→CALL: rename→RETURN→EXIT",
    "log": "<log>Info: Override State Store record {}: {}</log> <log>[ERROR] Cannot write {}</log>"
  },
  "fcda25a1_1": {
    "exec_flow": "ENTRY → LOG:DEBUG:Opening file:{f.toString()} → CALL:performAuthCheck → TRY → CALL:retrieveMetadata → IF_FALSE:meta==null → IF_FALSE:meta.isDirectory() → TRY → CALL:retrieve → NEW:FSDataInputStream → NEW:BufferedFSInputStream → NEW:NativeAzureFsInputStream → CALL:getLen → RETURN → EXIT",
    "log": "<log>[DEBUG] Opening file: {f.toString()}</log>"
  },
  "fcda25a1_2": {
    "exec_flow": "<step>org.apache.hadoop.fs.s3a.S3AFileSystem:open → org.apache.hadoop.fs.s3a.S3AFileSystem:executeOpen</step> <step>org.apache.hadoop.fs.s3a.S3AFileSystem:executeOpen → org.apache.hadoop.fs.s3a.S3AFileSystem:qualify</step> <step>org.apache.hadoop.fs.s3a.S3AFileSystem:qualify → org.apache.hadoop.fs.s3a.S3AFileSystem:makeQualified</step>",
    "log": "<log>DEBUG: Opening '{readContext}'</log> <log>DEBUG: Stripping trailing '/' from {q}</log> <log>DEBUG: Opening '{readContext}'</log>"
  },
  "fcda25a1_3": {
    "exec_flow": "ENTRY → TRY → CALL:openConnection → CALL:setRequestMethod → IF_FALSE:method.equals(HTTP_POST) || method.equals(HTTP_PUT) → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[INFO] Cleaning up resources</log> <log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log> <log>[DEBUG] PrivilegedActionException as: {}</log>"
  },
  "fcda25a1_4": {
    "exec_flow": "ENTRY→LOG:INFO:Open the file: [{}] for reading.→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:DEBUG:Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log>[INFO] Open the file: [{}] for reading.</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "fcda25a1_5": {
    "exec_flow": "ENTRY→LOG:INFO:Open the file: [{}] for reading.→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_FALSE:meta.isFile()→LOG:DEBUG:Path: [{}] is a dir. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "<log>[INFO] Open the file: [{}] for reading.</log> <log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log>"
  },
  "fcda25a1_6": {
    "exec_flow": "ENTRY→LOG:INFO:Open the file: [{}] for reading.→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta!=null→LOG:DEBUG:List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0→LOG:DEBUG:Path: [{}] is a directory. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "<log>[INFO] Open the file: [{}] for reading.</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "fcda25a1_7": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "fcda25a1_8": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "fcda25a1_9": {
    "exec_flow": "ENTRY→IF_TRUE: loadDefaults && fullReload→FOREACH: defaultResources→CALL: loadResource→FOREACH_EXIT→FOR_INIT→FOR_COND: i < resources.size()→CALL: loadResource→FOR_EXIT→CALL: addTags→EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "fcda25a1_10": {
    "exec_flow": "ENTRY→IF_FALSE: loadDefaults && fullReload→FOR_INIT→FOR_COND: i < resources.size()→CALL: loadResource→FOR_EXIT→CALL: addTags→EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "fcda25a1_11": {
    "exec_flow": "ENTRY→IF_FALSE: loadDefaults && fullReload→FOR_INIT→FOR_COND: i < resources.size()→CALL: loadResource→FOR_EXIT→CALL: addTags→EXIT",
    "log": "<!-- Inherited from child path as parent has no logs -->"
  },
  "a598dba1_1": {
    "exec_flow": "ENTRY→IF_TRUE: stream != null→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "a598dba1_2": {
    "exec_flow": "<step> <call>Parent.ENTRY</call> <log>Start execution</log> </step> <step> <call>Child opt() method</call> <log>Set an array of string values as optional parameter for the Builder</log> </step>",
    "log": "<log>Start execution</log> <log>Set an array of string values as optional parameter for the Builder</log>"
  },
  "81ac90e6_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: [DEBUG] Bypassing cache to create filesystem {} → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: [WARN] Failed to initialize filesystem {}: {} → LOG: [DEBUG] Failed to initialize filesystem → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → THROW: e → RETURN → EXIT → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC: this → CALL: get → IF_TRUE: fs != null → LOG: [DEBUG] Filesystem {} created while awaiting semaphore → RETURN → EXIT → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: [DEBUG] Duplicate FS created for {}; discarding {} → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT → [VIRTUAL_CALL] → ENTRY → LOG: [DEBUG] Handling deprecation for all properties in config... → CALL: getProps → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → [RETURN_FROM_CALL] → EXIT → CALL: org.apache.hadoop.fs.FileSystem:getFileStatus → IF_TRUE: status == null → LOG: [DEBUG] Getting the file status for {} → RETURN → LOG: [DEBUG] Path to resolve: {} , srcPattern: {} → LOG: [ERROR] Got Exception while build resolve result. ResultKind: {}, resolvedPathStr: {}, targetOfResolvedPathStr: {}, remainingPath: {}, will return null. → LOG: [DEBUG] Path {} is a folder. → LOG: [DEBUG] Found the path: {} as a file. → LOG: [DEBUG] Exception thrown when get object meta: {}, exception: {} → LOG: [DEBUG] Path: [{}] is a file. COS key: [{}] → LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}] → LOG: [DEBUG] List COS key: [{}] to check the existence of the path. → LOG: [DEBUG] Path: [{}] is a directory. COS key: [{}] → EXIT → [VIRTUAL_CALL] → EXIT",
    "log": "<log_entry>[DEBUG] Bypassing cache to create filesystem {}</log_entry> <log_entry>[WARN] Failed to initialize filesystem {}: {}</log_entry> <log_entry>[DEBUG] Failed to initialize filesystem</log_entry> <log_entry>[DEBUG] Filesystem {} created while awaiting semaphore</log_entry> <log_entry>[DEBUG] Duplicate FS created for {}; discarding {}</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Getting the file status for {}</log_entry> <log_entry>[DEBUG] Path to resolve: {} , srcPattern: {}</log_entry> <log_entry>[ERROR] Got Exception while build resolve result. ResultKind: {}, resolvedPathStr: {}, targetOfResolvedPathStr: {}, remainingPath: {}, will return null.</log_entry> <log_entry>[DEBUG] Path {} is a folder.</log_entry> <log_entry>[DEBUG] Found the path: {} as a file.</log_entry> <log_entry>[DEBUG] Exception thrown when get object meta: {}, exception: {}</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] List COS key: [{}] to check the existence of the path.</log_entry> <log_entry>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log_entry>"
  },
  "c86dc9e0_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE:LOG.isDebugEnabled() → LOG:DEBUG:Cannot rename the root of a filesystem → IF_TRUE:src.isRoot() → LOG:DEBUG:Cannot rename the root directory of a filesystem → RETURN → EXIT",
    "log": "[DEBUG] Cannot rename the root of a filesystem [DEBUG] Cannot rename the root directory of a filesystem."
  },
  "c86dc9e0_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL:getUriPath → CALL:resolve → IF_TRUE:resSrc.isInternalDir() → IF_TRUE:src.equals(dst) → LOG:DEBUG:Source path and dest path refer to the same file or directory: {src} → THROW:IOException → EXIT",
    "log": "[DEBUG] Source path and dest path refer to the same file or directory: {src}"
  },
  "c86dc9e0_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL:getUriPath → CALL:resolve → IF_TRUE:resSrc.isInternalDir() → IF_TRUE:!src.equals(dst) → FOR → IF_TRUE:commonParent → LOG:DEBUG:It is not allowed to rename a parent directory: {src} to its subdirectory: {dst} → THROW:IOException → EXIT",
    "log": "[DEBUG] It is not allowed to rename a parent directory: {src} to its subdirectory: {dst}"
  },
  "c86dc9e0_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → LOG:DEBUG:Moving {} to {}, src, dst → IF_FALSE:containsColon(dst) → IF_FALSE:srcParentFolder == null → IF_FALSE:srcKey.length() == 0 → CALL:performAuthCheck → IF_FALSE:this.azureAuthorization → TRY → CALL:retrieveMetadata → IF_FALSE:dstMetadata != null && dstMetadata.isDirectory() → IF_FALSE:dstMetadata != null → TRY → CALL:retrieveMetadata → IF_FALSE:parentOfDestMetadata == null → IF_FALSE:!parentOfDestMetadata.isDirectory() → CALL:prepareAtomicFolderRename → CALL:store.isAtomicRenameKey → IF_TRUE → CALL:leaseSourceFolder → CALL:FolderRenamePending:<init> → CALL:renamePending.writeFile → CALL:getSourceMetadata → IF_TRUE:srcMetadata2.getBlobMaterialization() == BlobMaterialization.Explicit → CALL:fs.getStoreInterface().rename → CALL:updateParentFolderLastModifiedTime → CALL:updateParentFolderLastModifiedTime → LOG:DEBUG:Renamed {} to {} successfully. → RETURN → EXIT",
    "log": "[DEBUG] Moving {} to {}, src, dst [DEBUG] Renamed {} to {} successfully."
  },
  "c86dc9e0_5": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → LOG:DEBUG:Moving {} to {}, src, dst → IF_FALSE:containsColon(dst) → IF_FALSE:srcParentFolder == null → IF_FALSE:srcKey.length() == 0 → CALL:performAuthCheck → IF_FALSE:this.azureAuthorization → TRY → CALL:retrieveMetadata → IF_FALSE:dstMetadata != null && dstMetadata.isDirectory() → IF_TRUE:dstMetadata != null → LOG:DEBUG:Destination {} is already existing file, failing the rename. → RETURN → EXIT",
    "log": "[DEBUG] Moving {} to {}, src, dst [DEBUG] Destination {} is already existing file, failing the rename."
  },
  "c86dc9e0_6": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG→CALL:makeQualified→CALL:statIncrement→CALL:trailingPeriodCheck→IF_FALSE:parentFolder==null→IF_FALSE:makeQualified(parentFolder).equals(qualifiedDstPath)→IF_FALSE:qualifiedSrcPath.equals(qualifiedDstPath)→TRY→CALL:makeQualified→CALL:abfsStore.rename→EXCEPTION:AzureBlobFileSystemException→LOG:LOG.DEBUG→CALL:checkException→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.rename src: {} dst: {} [DEBUG] Rename operation failed."
  },
  "e4e3cda2_1": {
    "exec_flow": "ENTRY → CALL:serviceStart → TRY → CALL:createRootRegistryPaths → EXCEPTION:createRootRegistryPaths → CATCH:NoPathPermissionsException e → LOG:LOG.ERROR:Failure {}, e, e → LOG:LOG.ERROR:message → CALL:dumpRegistryRobustly → THROW:new NoPathPermissionsException(e.getPath().toString(), message, e) → EXIT",
    "log": "LOG.ERROR: Failure {}, e, e LOG.ERROR: message"
  },
  "e4e3cda2_2": {
    "exec_flow": "ENTRY → CALL:serviceStart → CALL:serviceStart → CALL:createCurator → CALL:createEnsembleProvider → LOG:LOG.INFO:Creating CuratorService with connection {}, connectionDescription → CALL:registrySecurity.applySecurityEnvironment → ENTRY → IF_TRUE:isSecureRegistry() → SWITCH:access → CASE:[sasl] → IF_TRUE:existingJaasConf==null||existingJaasConf.isEmpty() → IF_FALSE:principal==null||keytab==null → LOG:LOG.INFO:Enabling ZK sasl client: jaasClientEntry=+jaasClientEntry+,principal= + principal + ,keytab= + keytab → CALL:javax.security.auth.login.Configuration.setConfiguration → CALL:setSystemPropertyIfUnset → CALL:setSystemPropertyIfUnset → BREAK → RETURN → EXIT",
    "log": "[INFO] Creating CuratorService with connection {} [INFO] Enabling ZK sasl client: jaasClientEntry= + jaasClientEntry + , principal= + principal + , keytab= + keytab"
  },
  "e4e3cda2_3": {
    "exec_flow": "ENTRY → CALL:serviceStart → CALL:serviceStart → CALL:createCurator → CALL:createEnsembleProvider → LOG:LOG.INFO:Creating CuratorService with connection {}, connectionDescription → CALL:registrySecurity.applySecurityEnvironment → ENTRY → IF_TRUE:isSecureRegistry() → SWITCH:access → CASE:[sasl] → IF_FALSE:existingJaasConf==null||existingJaasConf.isEmpty() → LOG:LOG.INFO:Using existing ZK sasl configuration:jaasClientEntry=+System.getProperty(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY,Client)+,sasl client=+System.getProperty(ZKClientConfig.ENABLE_CLIENT_SASL_KEY,ZKClientConfig.ENABLE_CLIENT_SASL_DEFAULT)+,jaas=+existingJaasConf → BREAK → RETURN → EXIT",
    "log": "[INFO] Creating CuratorService with connection {} [INFO] Using existing ZK sasl configuration: jaasClientEntry= + System.getProperty(ZKClientConfig.LOGIN_CONTEXT_NAME_KEY, \"Client\") + , sasl client= + System.getProperty(ZKClientConfig.ENABLE_CLIENT_SASL_KEY, ZKClientConfig.ENABLE_CLIENT_SASL_DEFAULT) + , jaas= + existingJaasConf"
  },
  "e4e3cda2_4": {
    "exec_flow": "ENTRY → IF_TRUE: LOG.isDebugEnabled() → CALL:org.slf4j.Logger:isDebugEnabled() → CALL:org.slf4j.Logger:debug(java.lang.String) → FOREACH: services → CALL:org.apache.hadoop.service.Service:start() → FOREACH_EXIT → CALL:serviceStart → EXIT",
    "log": "[DEBUG] CompositeService: starting services, size=..."
  },
  "e4e3cda2_5": {
    "exec_flow": "ENTRY → TRY → CALL:dumpPath → CALL:toString → RETURN → EXIT",
    "log": "[DEBUG] Ignoring exception: {}"
  },
  "a4d22b4f_1": {
    "exec_flow": "<entry>org.apache.hadoop.fs.FilterFileSystem:getFileStatus</entry> <call>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal</call> <sequence> <log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <if condition=\"key.length() != 0\"> <try> <call>retrieveMetadata</call> <if condition=\"meta != null\"> <if condition=\"meta.isDirectory()\"> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <call>conditionalRedoFolderRename</call> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </if> <else> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log> <call>updateFileStatusPath</call> <return>EXIT</return> </else> </if> </try> </if> <else> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </else> </sequence>",
    "log": "<log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log>"
  },
  "a4d22b4f_2": {
    "exec_flow": "<entry>ENTRY</entry> <if condition=\"key.length()==0\"> <else> <call>org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</call> <if condition=\"meta!=null\"> <else> <log>LOG.DEBUG:Path: [{}] is a dir. COS key: [{}]</log> <call>org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</call> <return>EXIT</return> </else> </if> </else> </if>",
    "log": "<log>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log>"
  },
  "a4d22b4f_3": {
    "exec_flow": "<entry>ENTRY</entry> <log>LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}</log> <call>statIncrement</call> <call>makeQualified</call> <try> <call>abfsStore.getFileStatus</call> <return>EXIT</return> </try>",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "a4d22b4f_4": {
    "exec_flow": "<entry>ENTRY</entry> <log>LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}</log> <call>statIncrement</call> <call>makeQualified</call> <try> <call>abfsStore.getFileStatus</call> <catch>AzureBlobFileSystemException</catch> <call>checkException</call> <return>EXIT</return> </try>",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "a4d22b4f_5": {
    "exec_flow": "<sequence> <call_site> <method>getFileStatus</method> <context> <![CDATA[ // Iterate over MRNflyNode instances for (int i = 0; i < mrNodes.length; i++) { MRNflyNode nflyNode = mrNodes[i]; try { nflyNode.updateFileStatus(f); if (nflyFlags.contains(NflyKey.readMostRecent)) { final long nflyTime = nflyNode.status.getModificationTime(); if (nflyTime > maxMtime) { maxMtime = nflyTime; maxMtimeIdx = i; } } else { return nflyNode.nflyStatus(); } } catch (FileNotFoundException fnfe) { numNotFounds++; processThrowable(nflyNode, \"getFileStatus\", fnfe, ioExceptions, f); } catch (Throwable t) { processThrowable(nflyNode, \"getFileStatus\", t, ioExceptions, f); } } ]]> </context> </call_site> </sequence>",
    "log": "<log>[MRNflyNode:access$2] Log sequence not directly available.</log>"
  },
  "3226da8c_1": {
    "exec_flow": "ENTRY→IF_FALSE:!statistics.isAlive()→IF_TRUE:stat==null→CALL:GridmixJob.getJobSeqId→LOG:LOG.ERROR:[Statistics] Missing entry for job + job.getJob().getJobID()→RETURN→EXIT",
    "log": "[ERROR] [Statistics] Missing entry for job + job.getJob().getJobID()"
  },
  "3226da8c_2": {
    "exec_flow": "ENTRY→IF_FALSE:!statistics.isAlive()→IF_FALSE:stat==null→CALL:GridmixJob.getJobSeqId→CALL:subtractFromNumMapsSubmitted→CALL:subtractFromNumReducesSubmitted→IF_TRUE:completedJobsInCurrentInterval>=maxJobCompletedInInterval→CALL:Logger.isDebugEnabled()→IF_TRUE:Logger.isDebugEnabled()→LOG:LOG.DEBUG:Reached maximum limit of jobs in a polling interval + completedJobsInCurrentInterval→CALL:lock→TRY→FOREACH:jobStatListeners→CALL:StatListener.update→FOREACH_EXIT→CALL:this.jobCompleted.signalAll→CALL:unlock→EXIT",
    "log": "[DEBUG] Reached maximum limit of jobs in a polling interval + completedJobsInCurrentInterval"
  },
  "2fcd288e_1": {
    "exec_flow": "<seq> ENTRY→CALL:makeQualified→NEW:Path→CALL:getHomeDirectory→CALL:getPath→CALL:toUri→CALL:getHomeDirectory→RETURN→EXIT </seq>",
    "log": "<log>[LOG] getLoginUser</log> <log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[WARN] Unable to get user name. Fall back to system property user.name</log>"
  },
  "76bb94b3_1": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Refresh superuser groups configuration in Router.→CALL: rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: [DEBUG] Proxying operation: {}, methodName→CALL: opCategory.set→IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: namenodeResolver.getNamespaces()→IF_TRUE: nss.isEmpty()→CALL: ProxyUsers.refreshSuperUserGroupsConfiguration→EXIT",
    "log": "<log>[DEBUG] Refresh superuser groups configuration in Router.</log> <log>[DEBUG] Proxying operation: {}, methodName</log>"
  },
  "76bb94b3_2": {
    "exec_flow": "ENTRY→LOG: [DEBUG] Refresh superuser groups configuration in Router.→CALL: rpcServer.checkOperation→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: [DEBUG] Proxying operation: {}, methodName→CALL: opCategory.set→IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: namenodeResolver.getNamespaces()→IF_FALSE: nss.isEmpty()→CALL: rpcClient.invokeConcurrent→EXIT",
    "log": "<log>[DEBUG] Refresh superuser groups configuration in Router.</log> <log>[DEBUG] Proxying operation: {}, methodName</log>"
  },
  "76bb94b3_3": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "7b723d23_1": {
    "exec_flow": "ENTRY→IF_TRUE: getServiceStopped()→LOG: LOG.INFO: Service stopped, return null for the storage→RETURN→EXIT",
    "log": "[INFO] Service stopped, return null for the storage"
  },
  "7189a8c2_1": {
    "exec_flow": "ENTRY→IF_FALSE: c == null→CALL: setLastContact→TRY→CALL: readAndProcess→CATCH: InterruptedException→CALL: org.slf4j.Logger:info→THROW: InterruptedException→EXIT",
    "log": "[INFO] Thread.currentThread().getName() + \": readAndProcess caught InterruptedException\""
  },
  "7189a8c2_2": {
    "exec_flow": "ENTRY→IF_FALSE: c == null→CALL: setLastContact→TRY→CALL: readAndProcess→CATCH: Exception→CALL: org.slf4j.Logger:info→CALL: access$9→IF_TRUE: count < 0 || c.shouldClose()→CALL: closeConnection→EXIT",
    "log": "[INFO] Thread.currentThread().getName() + \": readAndProcess from client \" + c + \" threw exception [\" + e + \"]\""
  },
  "7189a8c2_3": {
    "exec_flow": "ENTRY→IF_FALSE: c == null→CALL: setLastContact→TRY→CALL: readAndProcess→CATCH: Exception→CALL: org.slf4j.Logger:info→CALL: access$9→IF_FALSE: count < 0 || c.shouldClose()→CALL: setLastContact→EXIT",
    "log": "[INFO] Thread.currentThread().getName() + \": readAndProcess from client \" + c + \" threw exception [\" + e + \"]\""
  },
  "7189a8c2_4": {
    "exec_flow": "ENTRY → WHILE: !shouldClose() → WHILE_COND: !shouldClose() → CALL:channelRead → IF_FALSE:data.remaining() == 0 → CALL:warn → CALL:setupBadVersionResponse → RETURN → EXIT",
    "log": "<log> <source>org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse</source> <level>WARN</level> <message>Incorrect RPC Header length from {hostName}:{remotePort} / {hostAddress}:{remotePort}. Expected: {RpcConstants.HEADER}. Actual: {dataLengthBuffer}</message> </log> <log> <source>org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse</source> <level>WARN</level> <message>Version mismatch from {hostName}:{remotePort} / {hostAddress}:{remotePort}. Expected version: {CURRENT_VERSION}. Actual version: {version}</message> </log>"
  },
  "7189a8c2_5": {
    "exec_flow": "ENTRY-IF_TRUE: dataLength < 0-CALL:org.slf4j.Logger:warn(java.lang.String)-THROW: new IOException(error)-EXIT",
    "log": "<log> <source>org.apache.hadoop.ipc.Server$Connection:checkDataLength</source> <level>WARN</level> <message>Unexpected data length x!! from y</message> </log>"
  },
  "7189a8c2_6": {
    "exec_flow": "ENTRY-IF_FALSE: dataLength < 0-IF_TRUE: dataLength > maxDataLength-CALL:org.slf4j.Logger:warn(java.lang.String)-THROW: new IOException(error)-EXIT",
    "log": "<log> <source>org.apache.hadoop.ipc.Server$Connection:checkDataLength</source> <level>WARN</level> <message>Requested data length x is longer than maximum configured RPC length y. RPC came from z</message> </log>"
  },
  "5c662484_1": {
    "exec_flow": "ENTRY→TRY→CALL:lock→IF_TRUE:isPreUpgradableLayout(sd)→CALL:checkVersionUpgradable→IF_TRUE:oldVersion>LAST_UPGRADABLE_LAYOUT_VERSION→CALL:org.slf4j.Logger:error(java.lang.String)→THROW:IOException→RETURN→EXIT",
    "log": "[ERROR] Unable to acquire file lock on path {oldF} [ERROR] *********** Upgrade is not supported from this older version {oldVersion} of storage to the current version. Please upgrade to {LAST_UPGRADABLE_HADOOP_VERSION} or a later version and then upgrade to current version. Old layout version is {oldVersion} and latest layout version this software version can upgrade from is {LAST_UPGRADABLE_LAYOUT_VERSION}. ************"
  },
  "5c662484_2": {
    "exec_flow": "ENTRY→TRY→CALL:lock→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)→IF_TRUE:!root.mkdirs()→THROW new IOException(\"Cannot create directory \" + rootPath)→EXIT",
    "log": "[INFO] {} does not exist. Creating ..."
  },
  "5c662484_3": {
    "exec_flow": "ENTRY→TRY→CALL:lock→CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object)→RETURN→EXIT",
    "log": "[WARN] {} is not a directory"
  },
  "5c662484_4": {
    "exec_flow": "ENTRY→TRY→CALL:lock→CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object,java.lang.Object)→RETURN→EXIT",
    "log": "[WARN] Cannot access storage directory {}"
  },
  "79d6410f_1": {
    "exec_flow": "ENTRY → IF_TRUE:volumeFailureSummary!=null&&this.volumeFailureSummary!=null → IF_TRUE:checkFailedStorages → CALL:org.slf4j.Logger:info → SYNC:storageMap → CALL:org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:updateFailedStorage → FOREACH: failedStorageInfos → IF:storageInfo.getState()!=FAILED → CALL:org.slf4j.Logger:info → CALL:storageInfo.setState → FOREACH_EXIT → CALL:setCacheCapacity → CALL:setCacheUsed → CALL:setXceiverCount → FOREACH:reports → SYNC:storageMap → CALL:get → CALL:getStorageID → CALL:receivedHeartbeat → SYNC:storageMap → IF_TRUE:storageMapSize!=reports.length → CALL:org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:pruneStorageMap → EXIT",
    "log": "[INFO] Number of failed storages changes from {} to {} [DEBUG] Number of storages reported in heartbeat={}; Number of storages in storageMap={} [DEBUG] child remove storage:{}:{} [INFO] Removed storage {} from DataNode {} [DEBUG] Deferring removal of stale storage {} with {} blocks [INFO] {storageInfo} failed."
  },
  "0ef51e87_1": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE:isInState(STATE.STARTED)</step> <step>SYNC:stateChangeLock</step> <step>IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED</step> <step>TRY</step> <step>CALL:currentTimeMillis</step> <step>CALL:serviceStart</step> <step>EXCEPTION:serviceStart</step> <step>CATCH:Exception</step> <step>CALL:noteFailure</step> <step>LOG.debug(\"noteFailure\", exception)</step> <step>if (exception == null) { return; }</step> <step> <sync> <condition>failureCause == null</condition> <steps> <step>failureCause = exception;</step> <step>failureState = getServiceState();</step> <step>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</step> </steps> </sync> </step> <step>CALL:ServiceOperations.stopQuietly</step> <step>THROW:ServiceStateException.convert(e)</step> <step>EXIT</step>",
    "log": "<log> <level>DEBUG</level> <message>noteFailure</message> <exception>{exception}</exception> </log> <log> <level>INFO</level> <message>Service {getName()} failed in state {failureState}</message> <exception>{exception}</exception> </log>"
  },
  "91c6e758_1": {
    "exec_flow": "ENTRY → IF_FALSE:isInState(STATE.STARTED) → SYNC:stateChangeLock → IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL:currentTimeMillis → CALL:serviceStart → IF_TRUE:isInState(STATE.STARTED) → CALL:debug → CALL:notifyListeners → TRY → CALL:org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC:this → CALL:toArray → CALL:size → FOREACH:callbacks → CALL:stateChanged → FOREACH_EXIT → CATCH:Throwable e → LOG:LOG.WARN:Exception while notifying listeners of {}, this, e → RETURN → EXIT → CALL:stopQuietly → LOG:When stopping the service {serviceName}",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {} LOG.debug(\"noteFailure\", exception) LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) [WARN] When stopping the service {}"
  },
  "fde265b0_1": {
    "exec_flow": "ENTRY→IF_TRUE: counters[i] == null→CALL: newCounter→CALL: org.slf4j.Logger:warn→RETURN→EXIT",
    "log": "[WARN] New counter created"
  },
  "fde265b0_2": {
    "exec_flow": "ENTRY→IF_FALSE: counters[i] == null→CALL: org.slf4j.Logger:warn→RETURN→EXIT",
    "log": "[WARN] Counter already exists"
  },
  "468048b0_1": {
    "exec_flow": "ENTRY→CALL:generateKey→CALL:org.apache.hadoop.crypto.key.KeyProviderExtension:createKey→RETURN→EXIT",
    "log": "[DEBUG] Generating key material [INFO] Key creation with KeyProviderExtension successful"
  },
  "60196480_1": {
    "exec_flow": "ENTRY→LOG: Getting the file status for {filePath}→CALL: getFileStatusInternal→IF_TRUE: meta != null→IF_TRUE: meta.isDirectory()→LOG: Path {filePath} is a folder.→CALL: conditionalRedoFolderRename→ENTRY→IF_FALSE: filePath.getName().equals(\"\")→IF_FALSE: existsInternal(absoluteRenamePendingFile)→RETURN→EXIT→RETURN→EXIT→IF_FALSE: meta.isDirectory()→LOG: Found the path: {filePath} as a file.→CALL: updateFileStatusPath→RETURN→EXIT",
    "log": "<log>[DEBUG] Getting the file status for {filePath}</log> <log>[DEBUG] Path {filePath} is a folder.</log> <log>[DEBUG] Found the path: {filePath} as a file.</log>"
  },
  "60196480_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path→CALL:statIncrement→CALL:makeQualified→TRY→CALL:abfsStore.getFileStatus→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "60196480_3": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path→CALL:statIncrement→CALL:makeQualified→TRY→CALL:abfsStore.getFileStatus→CATCH:AzureBlobFileSystemException→CALL:checkException→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "60196480_4": {
    "exec_flow": "ENTRY→IF_TRUE: client != null→IF_FALSE: !client.isConnected()→CALL: logout→CALL: disconnect→IF_TRUE: !logoutSuccess→LOG: LOG.WARN: Logout failed while disconnecting, error code - + client.getReplyCode()→EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code -</log>"
  },
  "f3578e7e_1": {
    "exec_flow": "ENTRY→CALL:logExpireDelegationToken→ASSERT:!isInSafeMode()→ASSERT:hasReadLock()→CALL:logCancelDelegationToken→ENTRY→CALL:beginTransaction→TRY→CALL:editLogStream.writeRaw→CALL:endTransaction→EXIT→CALL:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp)→CALL:logSync()→TRY→CALL:printStatistics→WHILE:mytxid > synctxid && isSyncRunning→WHILE_COND:mytxid > synctxid && isSyncRunning→WHILE_EXIT→IF_FALSE:mytxid <= synctxid→CALL:getLastJournalledTxId→LOG:LOG.DEBUG:logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}→IF_FALSE:lastJournalledTxId <= synctxid→TRY→IF_TRUE:journalSet.isEmpty()→THROW:new IOException(\"No journals available to flush\")→CALL:org.apache.hadoop.util.ExitUtil:terminate→CALL:org.slf4j.Logger:error→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger→EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}"
  },
  "f3578e7e_2": {
    "exec_flow": "FSNamesystem.ENTRY → IF_TRUE → CALL:isInManualOrResourceLowSafeMode → IF_FALSE:!blockManager.isInSafeMode → BlockManager.ENTRY → IF_TRUE → CALL:doConsistencyCheck → ENTRY → IF_FALSE:!assertsOn → SYNC:this → IF_TRUE:blockTotal!=activeBlocks && !(blockSafe>=0 && blockSafe<=blockTotal) → CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[]) → LOG:LOG.WARN:SafeMode is in inconsistent filesystem state.+BlockManagerSafeMode data:blockTotal={},blockSafe={};+BlockManager data:activeBlocks={},blockTotal,blockSafe,activeBlocks → EXIT → RETURN → EXIT",
    "log": "[WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}"
  },
  "3044ac0f_1": {
    "exec_flow": "ENTRY→IF_TRUE: !this.running→LOG: LOG.ERROR: Cannot get a connection to {} because the manager isn't running, nnAddress→RETURN→EXIT",
    "log": "[ERROR] Cannot get a connection to {} because the manager isn't running"
  },
  "3044ac0f_2": {
    "exec_flow": "ENTRY→IF_FALSE: !this.running→CALL: readLock.lock→TRY→CALL: get→CALL: readLock.unlock→IF_FALSE: pool == null→IF_TRUE: conn == null || !conn.isUsable()→IF_TRUE: !this.creatorQueue.offer(pool)→LOG: LOG.ERROR: Cannot add more than {} connections at the same time, this.creatorQueueMaxSize→IF_TRUE: conn != null && conn.isClosed()→LOG: LOG.ERROR: We got a closed connection from {}, pool→RETURN→EXIT",
    "log": "[ERROR] Cannot add more than {} connections at the same time [ERROR] We got a closed connection from {}"
  },
  "3044ac0f_3": {
    "exec_flow": "ENTRY→IF_FALSE: !this.running→CALL: readLock.lock→TRY→CALL: get→CALL: readLock.unlock→IF_FALSE: pool == null→IF_TRUE: conn == null || !conn.isUsable()→IF_TRUE: !this.creatorQueue.offer(pool)→LOG: LOG.ERROR: Cannot add more than {} connections at the same time, this.creatorQueueMaxSize→IF_FALSE: conn != null && conn.isClosed()→RETURN→EXIT",
    "log": "[ERROR] Cannot add more than {} connections at the same time"
  },
  "3044ac0f_4": {
    "exec_flow": "ENTRY→IF_FALSE: !this.running→CALL: readLock.lock→TRY→CALL: get→CALL: readLock.unlock→IF_FALSE: pool == null→IF_TRUE: conn == null || !conn.isUsable()→IF_FALSE: !this.creatorQueue.offer(pool)→IF_TRUE: conn != null && conn.isClosed()→LOG: LOG.ERROR: We got a closed connection from {}, pool→RETURN→EXIT",
    "log": "[ERROR] We got a closed connection from {}"
  },
  "3044ac0f_5": {
    "exec_flow": "ENTRY→IF_FALSE: !this.running→CALL: readLock.lock→TRY→CALL: get→CALL: readLock.unlock→IF_FALSE: pool == null→IF_FALSE: conn == null || !conn.isUsable()→IF_TRUE: conn != null && conn.isClosed()→LOG: LOG.ERROR: We got a closed connection from {}, pool→RETURN→EXIT",
    "log": "[ERROR] We got a closed connection from {}"
  },
  "3044ac0f_6": {
    "exec_flow": "ENTRY→IF_TRUE: !PROTO_MAP.containsKey(proto)→LOG: LOG.ERROR: Unsupported protocol for connection to NameNode: null→THROW: new IllegalStateException(msg)→EXIT",
    "log": "[ERROR] Unsupported protocol for connection to NameNode: null"
  },
  "8977751c_1": {
    "exec_flow": "ENTRY→IF_TRUE:raPool != null && inFd != null && readahead→CALL:readaheadStream→ENTRY→CALL:Preconditions.checkArgument→IF_FALSE: readaheadLength <= 0→IF_TRUE: lastReadahead != null→CALL:getOffset→IF_TRUE: curPos >= nextOffset→IF_TRUE: lastReadahead != null→CALL:lastReadahead.cancel→IF_FALSE: length <= 0→CALL:submitReadahead→CALL:execute→CALL:org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>→IF_TRUE:LOG.isTraceEnabled()→CALL:LOG.trace→RETURN→EXIT→EXIT",
    "log": "[TRACE] submit readahead: + req"
  },
  "8977751c_2": {
    "exec_flow": "ENTRY→IF_TRUE:raPool != null && inFd != null && readahead→CALL:readaheadStream→ENTRY→CALL:Preconditions.checkArgument→IF_FALSE: readaheadLength <= 0→IF_TRUE: lastReadahead != null→CALL:getOffset→IF_TRUE: curPos >= nextOffset→IF_FALSE: lastReadahead != null→IF_FALSE: length <= 0→CALL:submitReadahead→CALL:execute→CALL:org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>→IF_TRUE:LOG.isTraceEnabled()→CALL:LOG.trace→RETURN→EXIT→EXIT",
    "log": "[TRACE] submit readahead: + req"
  },
  "8977751c_3": {
    "exec_flow": "ENTRY→IF_TRUE:raPool != null && inFd != null && readahead→CALL:readaheadStream→ENTRY→CALL:Preconditions.checkArgument→IF_FALSE: readaheadLength <= 0→IF_FALSE: lastReadahead != null→IF_TRUE: curPos >= nextOffset→IF_TRUE: lastReadahead != null→CALL:lastReadahead.cancel→IF_FALSE: length <= 0→CALL:submitReadahead→CALL:execute→CALL:org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>→IF_TRUE:LOG.isTraceEnabled()→CALL:LOG.trace→RETURN→EXIT→EXIT",
    "log": "[TRACE] submit readahead: + req"
  },
  "8977751c_4": {
    "exec_flow": "ENTRY→IF_TRUE:raPool != null && inFd != null && readahead→CALL:readaheadStream→ENTRY→CALL:Preconditions.checkArgument→IF_FALSE: readaheadLength <= 0→IF_FALSE: lastReadahead != null→IF_TRUE: curPos >= nextOffset→IF_FALSE: lastReadahead != null→IF_FALSE: length <= 0→CALL:submitReadahead→CALL:execute→CALL:org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>→IF_TRUE:LOG.isTraceEnabled()→CALL:LOG.trace→RETURN→EXIT→EXIT",
    "log": "[TRACE] submit readahead: + req"
  },
  "e64ddcb5_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "e64ddcb5_2": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→CALL:getProps→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "e64ddcb5_3": {
    "exec_flow": "ENTRY→CALL: LOG_DEPRECATION.info→CALL: org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[INFO] message"
  },
  "4a358422_1": {
    "exec_flow": "<step>Entry: ViewFileSystem:getUriPath(Path)</step> <step>Process: checkPath(Path)</step> <step>Return: makeAbsolute(p).toUri().getPath()</step> <step>ENTRY</step> <step>LOG:Moving {} to {}, src, dst</step> <step>IF_FALSE:containsColon(dst)</step> <step>IF_FALSE:srcParentFolder == null</step> <step>IF_FALSE:srcKey.length() == 0</step> <step>CALL:performAuthCheck</step> <step>IF_FALSE:azureAuth</step> <step>TRY</step> <step>CALL:retrieveMetadata</step> <step>IF_FALSE:dstMetadata != null && dstMetadata.isDirectory()</step> <step>IF_TRUE:dstMetadata != null</step> <step>LOG:Destination {} is already existing file, failing the rename</step> <step>RETURN</step> <step>CALL:updateParentFolderLastModifiedTime</step> <step>EXIT</step>",
    "log": "<log_entry> <log_type>INFO</log_type> <description>Start processing URI path</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Path checked for validity</description> </log_entry> <log_entry> <log_type>INFO</log_type> <description>Completed URI path transformation</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Moving {} to {}, src, dst</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>Destination {} is already existing file, failing the rename</description> </log_entry>"
  },
  "4a358422_2": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:regexMountPointList</step> <step>CALL:regexMountPoint.resolve</step> <step>FOREACH:interceptorList</step> <step>FOREACH_EXIT</step> <step>LOG:Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()</step> <step>WHILE:srcMatcher.find()</step> <step>CALL:replaceRegexCaptureGroupInPath</step> <step>IF_FALSE:groupValue==null</step> <step>FOREACH:groupRepresentationStrSetInDest</step> <step>LOG:parsedDestPath value is:${parsedDestPath}</step> <step>FOREACH_EXIT</step> <step>WHILE_EXIT</step> <step>IF_TRUE:targetFs==null</step> <step>CALL:org.slf4j.Logger:error</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log_entry> <log_type>DEBUG</log_type> <description>Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()</description> </log_entry> <log_entry> <log_type>DEBUG</log_type> <description>parsedDestPath value is:${parsedDestPath}</description> </log_entry> <log_entry> <log_type>ERROR</log_type> <description>Not able to initialize target file system. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null.</description> </log_entry>"
  },
  "7e1389bc_1": {
    "exec_flow": "<entry> <call_site>org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getAclStatus</call_site> <conditions>(checkOperation succeeded) AND (getLocationsForPath returned valid List) AND (invokeSequential succeeded) AND failIfLocked = true</conditions> <virtual_call> <callee>org.apache.hadoop.hdfs.server.federation.resolver.FileSubclusterResolver:getDestinationForPath</callee> <fall_through>false</fall_through> </virtual_call> </entry> <call>rpcServer.checkOperation(NameNode.OperationCategory.READ)</call> <call>rpcServer.getLocationsForPath(src, false, false)</call> <call>invokeSequential</call> <return>RETURN</return> <exit>EXIT</exit>",
    "log": "<log>[DEBUG] Check operation</log> <log>[INFO] Get locations for path</log> <log>org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.getLocationsForPath - AccessControlException thrown: {reasons}</log> <log>[INFO] Invoke sequential execution</log> <log>[DEBUG] User {} NN {} is using connection {}</log>"
  },
  "7e1389bc_2": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "7e1389bc_3": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "7e1389bc_4": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "7e1389bc_5": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ -> CALL: checkSafeMode -> EXIT",
    "log": "<log>[DEBUG] Proxying operation: {}</log>"
  },
  "be5e9fc9_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → TRY → TRY → CALL:buildSplits → EXCEPTION:buildSplits → CATCH:IOException e → LOG:WARN:Failed to submit [JobName] as [Ugi], e → CALL:submissionFailed → RETURN → EXIT",
    "log": "<!-- Merged log sequence --> [WARN] Failed to submit [JobName] as [Ugi] INFO: Job submission failed notification for job {jobID}"
  },
  "be5e9fc9_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "be5e9fc9_3": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "<!-- Merged log sequence --> [INFO] message"
  },
  "ab947378_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <step>ENTRY</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:getProps</step> <step>FOREACH:names</step> <step>CALL:substituteVars</step> <step>CALL:getRaw</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration, se</log>"
  },
  "ab947378_2": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>NEW: LineReader</step> <step>WHILE: lineReader.readLine(line) > 0</step> <step>IF: matchesReporter(lineStr)</step> <step>IF: matchesCounter(lineStr)</step> <step>CALL: incrCounter</step> <step>CALL: reporter.progress</step> <step>WHILE_EXIT</step> <step>IF_TRUE: lineReader != null</step> <step>CALL: lineReader.close</step> <step>IF_TRUE: clientErr_ != null</step> <step>CALL: clientErr_.close</step> <step>LOG:LOG.INFO: MRErrorThread done</step> <step>EXIT</step>",
    "log": "<log>[INFO] MRErrorThread done</log>"
  },
  "ab947378_3": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE: columns.length == 3</step> <step>TRY</step> <step>CALL:org.apache.hadoop.mapred.Reporter:incrCounter</step> <step>EXCEPTION:NumberFormatException</step> <step>CATCH:NumberFormatException e</step> <step>LOG:LOG.WARN: Cannot parse counter increment ' + columns[2] + ' from line: + line</step> <step>EXIT</step>",
    "log": "<log>[WARN] Cannot parse counter increment ' + columns[2] + ' from line: + line</log>"
  },
  "ab947378_4": {
    "exec_flow": "<step>ENTRY</step> <step>IF_FALSE: columns.length == 3</step> <step>LOG:LOG.WARN: Cannot parse counter line: + line</step> <step>EXIT</step>",
    "log": "<log>[WARN] Cannot parse counter line: + line</log>"
  },
  "ab947378_5": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:LOG_DEPRECATION.info</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "ac81d765_1": {
    "exec_flow": "ENTRY → CALL: getLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → LOG: Reading credentials from location {} → LOG: Loaded {} tokens from {} → LOG: Token file {} does not exist → LOG: Failure to load login credentials → LOG: UGI loginUser: {} → CALL: spawnAutoRenewalThreadForUserCreds(false) → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getProps → FOREACH: names → CALL: substituteVars → FOREACH_EXIT → IF_FALSE: !(groups instanceof TestingGroups) → CALL: getUserToGroupsMappingService → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → IF_FALSE: metrics.getGroupsQuantiles == null → EXIT → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → RETURN → EXIT",
    "log": "<log>[DEBUG] Reading credentials from location {}</log> <log>[DEBUG] Loaded {} tokens from {}</log> <log>[INFO] Token file {} does not exist</log> <log>[DEBUG] Failure to load login credentials</log> <log>[DEBUG] UGI loginUser: {}</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log> <log>[LOG] getLoginUser</log>"
  },
  "133084f6_1": {
    "exec_flow": "ENTRY→CALL:getDestinationFileSystem→IF_TRUE: fs instanceof S3AFileSystem→CALL:createTaskCommitter→LOG:[DEBUG] Committer option is {name}→IF_TRUE:factory != null→LOG:[INFO] Using committer {name} to output data to {outputPath}→RETURN→EXIT",
    "log": "<log> <template>Using Committer {} for {}, outputCommitter, outputPath</template> <level>info</level> </log> <log> <template>[DEBUG] Committer option is {name}</template> <level>debug</level> </log> <log> <template>[INFO] Using committer {name} to output data to {outputPath}</template> <level>info</level> </log>"
  },
  "95c37475_1": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState→CALL:setLastHealthState→CALL:org.slf4j.Logger:info→CALL:access$4→CALL:recheckElectability→EXIT",
    "log": "<log>[INFO] Local service {localTarget} entered state: {newState}</log> <log>[INFO] Entered state: newState</log>"
  },
  "95c37475_2": {
    "exec_flow": "ENTRY→SYNC: electror→SYNC: this→IF_TRUE: remainingDelay > 0→IF_TRUE: healthy→LOG: LOG.INFO→CALL: scheduleRecheck→RETURN→EXIT",
    "log": "<log>[INFO] Would have joined master election, but this node is prohibited from doing so for ... more ms</log>"
  },
  "95c37475_3": {
    "exec_flow": "ENTRY→SYNC: elector→SYNC: this→IF_FALSE: remainingDelay > 0→SWITCH: lastHealthState→CASE: INITIALIZING→LOG: LOG.INFO→CALL: quitElection→EXIT",
    "log": "<log>[INFO] Ensuring that ... does not participate in active master election</log>"
  },
  "95c37475_4": {
    "exec_flow": "ENTRY→SYNC: elector→SYNC: this→IF_FALSE: remainingDelay > 0→SWITCH: lastHealthState→CASE: SERVICE_NOT_RESPONDING→LOG: LOG.INFO→CALL: quitElection→EXIT",
    "log": "<log>[INFO] Quitting master election for ... and marking that fencing is necessary</log>"
  },
  "60e2de51_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:fetchDatanodes → SYNC: this → NEW: ArrayList<> → CALL: size → CALL: Collections.sort → IF_TRUE: listDeadNodes → FOREACH: includedNodes → FOREACH_EXIT → CALL: LOG.isDebugEnabled → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: getDatanodeListForReport with includedNodes = ..., excludedNodes = ..., foundNodes = ..., nodes = ... → RETURN → EXIT",
    "log": "[DEBUG] getDatanodeListForReport with includedNodes = ..., excludedNodes = ..., foundNodes = ..., nodes = ..."
  },
  "60e2de51_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:fetchDatanodes → SYNC: this → NEW: ArrayList<> → CALL: size → CALL: Collections.sort → IF_FALSE: listDeadNodes → CALL: LOG.isDebugEnabled → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: getDatanodeListForReport with includedNodes = ..., excludedNodes = ..., foundNodes = ..., nodes = ... → RETURN → EXIT",
    "log": "[DEBUG] getDatanodeListForReport with includedNodes = ..., excludedNodes = ..., foundNodes = ..., nodes = ..."
  },
  "7a7f8c31_1": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: [DEBUG] Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "7a7f8c31_2": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: [DEBUG] Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> LOG: [DEBUG] Renew delegation token -> TRY -> IF_FALSE: !isAllowedDelegationTokenOp() -> CALL: getRemoteUser -> CALL: renewToken -> CALL: DFSUtil.decodeDelegationToken -> CALL: toStringStable -> IF_TRUE: AccessControlException -> CALL: DFSUtil.decodeDelegationToken -> CALL: toStringStable -> CATCH: AccessControlException -> LOG: [DEBUG] Operation: renewDelegationToken Status: false TokenId: tokenId -> EXIT",
    "log": "[DEBUG] Proxying operation: {} [DEBUG] Renew delegation token [DEBUG] Operation: renewDelegationToken Status: false TokenId: tokenId"
  },
  "0e1d2902_1": {
    "exec_flow": "ENTRY → IF_TRUE:container.isMarkedForKilling → LOG:LOG.INFO:Container + containerId + not launched as it has already + been marked for Killing → CALL:getExitCode → RETURN → EXIT",
    "log": "[INFO] Container + containerId + not launched as it has already + been marked for Killing"
  },
  "0e1d2902_2": {
    "exec_flow": "ENTRY → IF_FALSE:container.isMarkedForKilling → CALL:dispatcher.getEventHandler().handle → CALL:context.getNMStateStore().storeContainerLaunched → LOG: [DEBUG] storeContainerLaunched: containerId={} → CALL:removeContainerQueued → TRY → CALL:put → EXIT → IF_TRUE:!containerAlreadyLaunched.compareAndSet(false, true) → LOG:LOG.INFO:Container + containerId + not launched as + cleanup already called → CALL:getExitCode → RETURN → EXIT",
    "log": "[INFO] Container + containerId + not launched as + cleanup already called [DEBUG] storeContainerLaunched: containerId={}"
  },
  "0bee1cb9_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "0bee1cb9_2": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_FALSE: overlay != null → EXIT → FOREACH:names → RETURN → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "0bee1cb9_3": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\") → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "0bee1cb9_4": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\") → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "0bee1cb9_5": {
    "exec_flow": "ENTRY -> IF_TRUE: this.fs == null -> TRY -> VIRTUAL_CALL: doAs -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}] -> CATCH: PrivilegedActionException -> LOG: LOG.DEBUG: PrivilegedActionException as: {} -> CALL: RETURN_PRIVILEGED_EXCEPTION -> CALL: NEW: Path -> CALL: getSystemDir -> CALL: getFileSystem -> CALL: getConf -> RETURN -> EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "b55b7772_1": {
    "exec_flow": "ENTRY→TRY→CALL:KMSConfiguration:getKMSConf→CALL:UserGroupInformation:setConfiguration→CALL:KMSACLs:<init>→CALL:KMSACLs:startReloader→CALL:MetricRegistry:<init>→CALL:JmxReporter:forRegistry→CALL:JmxReporter:build→CALL:JmxReporter:start→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:MetricRegistry:register→CALL:KMSAudit:<init>→CALL:Configuration:get→CALL:KeyProviderFactory:get→CALL:Preconditions:checkNotNull→CALL:Configuration:getBoolean→CALL:Configuration:getLong→CALL:Configuration:getLong→CALL:CachingKeyProvider:<init>→CALL:Logger:info→CALL:KeyProviderCryptoExtension:createKeyProviderCryptoExtension→CALL:EagerKeyGeneratorKeyProviderCryptoExtension:<init>→CALL:Configuration:getBoolean→CALL:KeyAuthorizationKeyProvider:<init>→CALL:Logger:info→CALL:Configuration:getInt→CALL:Logger:info→CALL:Logger:info→CATCH(Throwable)→System:println→System:println→System:println→System:println→System:println→Throwable:printStackTrace→System:println→System:println→System:exit→EXIT",
    "log": "[INFO] Java runtime version : {} [INFO] User: {} [INFO] KMS Hadoop Version: [INFO] Initialized KeyProvider [INFO] Initialized KeyProviderCryptoExtension [INFO] Default key bitlength is {} [INFO] KMS Started [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "b55b7772_2": {
    "exec_flow": "ENTRY→CALL:getTrimmedStringCollection→IF_TRUE:classes.isEmpty()→LOG:LOG.INFO:No audit logger configured, using default.→CALL:add→RETURN→EXIT→CALL:getAuditLoggerClasses→CALL:Preconditions.checkState→FOREACH:classes→CALL:ReflectionUtils.newInstance→FOREACH_EXIT→FOREACH:auditLoggers→TRY→LOG:LOG.INFO:Initializing audit logger {}, logger.getClass()→CALL:initialize→CATCH:Exception ex→THROW:new RuntimeException(\"Failed to initialize \" + logger.getClass().getName(), ex)→EXIT",
    "log": "[INFO] No audit logger configured, using default. [INFO] Initializing audit logger {logger.getClass()}"
  },
  "b55b7772_3": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "5402729c_1": {
    "exec_flow": "ENTRY→CALL:checkNNStartup→CALL:checkOperation→IF_FALSE:cacheEntry != null && cacheEntry.isSuccess()→CALL:org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache)→TRY→CALL:FSNamesystem:setXAttr→CALL:RetryCache.setState→EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions} [LOG] logAuditEvent [DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)] [INFO] Set xattr for [getPath(src)] [DEBUG] logRpcIds [DEBUG] logEdit"
  },
  "5402729c_2": {
    "exec_flow": "ENTRY→FOREACH: auditLoggers→IF: logger instanceof HdfsAuditLogger→CALL:appendClientPortToCallerContextIfAbsent→CALL:hdfsLogger.logAuditEvent→FOREACH_EXIT→EXIT",
    "log": "[LOG] logAuditEvent"
  },
  "5402729c_3": {
    "exec_flow": "ENTRY→FOREACH: auditLoggers→IF: !(logger instanceof HdfsAuditLogger)→CALL:logger.logAuditEvent→FOREACH_EXIT→EXIT",
    "log": "[LOG] logAuditEvent"
  },
  "befe8e3e_1": {
    "exec_flow": "<entry> <operation> <action>Check timerFunctionality</action> <path> <condition>RESUME</condition> <sequence> <log>resuming timer</log> <action>Execute resumeTimer()</action> <log>log statements from resumeTimer</log> </sequence> </path> <path> <condition>SUSPEND</condition> <sequence> <log>suspending if conditions met</log> </sequence> </path> </operation> </entry>",
    "log": "<log>resuming timer</log> <log>log statements from resumeTimer</log> <log>suspending if conditions met</log>"
  },
  "befe8e3e_2": {
    "exec_flow": "ENTRY → IF_TRUE: errorPercentage < MIN_ACCEPTABLE_ERROR_PERCENTAGE → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → IF_TRUE: logDebugEnabled() → LOG: LOG.DEBUG: String.format(...) → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "<log>[DEBUG] %5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d</log>"
  },
  "befe8e3e_3": {
    "exec_flow": "ENTRY → IF_FALSE: errorPercentage < MIN_ACCEPTABLE_ERROR_PERCENTAGE → IF_TRUE: errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: String.format(...) → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "<log>[DEBUG] %5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d</log>"
  },
  "befe8e3e_4": {
    "exec_flow": "ENTRY → IF_FALSE: errorPercentage < MIN_ACCEPTABLE_ERROR_PERCENTAGE → IF_FALSE: errorPercentage < MAX_EQUILIBRIUM_ERROR_PERCENTAGE → IF_TRUE: bytesSuccessful > 0 → CALL: max → CALL: min → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: String.format(...) → CALL: org.slf4j.Logger:debug(java.lang.String) → RETURN → EXIT",
    "log": "<log>[DEBUG] %5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d</log>"
  },
  "f387726a_1": {
    "exec_flow": "ENTRY→CALL:getCapacityAtTime→CALL:getCapacityAtTime→CALL:add→FOR_INIT →CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes() →FOR_COND: i < maxLength →TRY →CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) →CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation(int) →CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue(int,long) →FOR_BODY→FOR_UPDATE→FOR_COND: i < maxLength →EXCEPTION:ResourceNotFoundException→LOG.warn →FOR_UPDATE→FOR_COND: i < maxLength→FOR_EXIT→RETURN→EXIT",
    "log": "[WARN] Resource is missing:"
  },
  "a568d945_1": {
    "exec_flow": "<call_sequence> <call> <caller>org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:serviceStop</caller> <callee>org.apache.hadoop.yarn.client.api.TimelineReaderClient:stop</callee> </call> <call> <caller>org.apache.hadoop.service.CompositeService:stop</caller> <callee>org.apache.hadoop.service.AbstractService:stop</callee> </call> <call> <caller>org.apache.hadoop.service.AbstractService:noteFailure</caller> <callee>org.slf4j.Logger:debug</callee> <log_statement>LOG.debug(\"noteFailure\", exception);</log_statement> </call> </call_sequence>",
    "log": "<log> <template>Stopping service #i: {service}</template> <level>DEBUG</level> </log> <log> <template>noteFailure</template> <level>DEBUG</level> </log>"
  },
  "5ef4bf5b_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:getBoolean</step> <step>IF_TRUE</step> <step>CALL:setBoolean</step> <step>CALL:getClasses</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:LOG_DEPRECATION.info</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>CALL:get</step> <step>CALL:isSecurityEnabled</step> <step>IF_TRUE</step> <step>CALL:add</step> <step>CONTINUE</step> <step>CALL:info</step> <step>CALL:set</step> <step>EXIT</step>",
    "log": "<log>[INFO] Using RM authentication filter(kerberos/delegation-token) for RM webapp authentication</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "5ef4bf5b_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:LOG_DEPRECATION.info</step> <step>CALL:org.slf4j.Logger:info(java.lang.String)</step> <step>EXIT</step>",
    "log": "<log>[INFO] message</log>"
  },
  "154d542c_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE → CALL:doConsistencyCheck → IF_FALSE:!assertsOn → SYNC:this → IF_TRUE:blockTotal!=activeBlocks && !(blockSafe>=0 && blockSafe<=blockTotal) → CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[]) → RETURN → EXIT → RETURN → EXIT",
    "log": "[WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}"
  },
  "a00a55ec_1": {
    "exec_flow": "ENTRY → SWITCH: cmd.getAction() → CASE: [DatanodeProtocol.DNA_TRANSFER] → CALL: dn.transferBlocks → CATCH(IOException) → LOG: WARN: Failed to transfer block {block}, {EXCEPTION} → BREAK → EXIT → CASE: [DatanodeProtocol.DNA_INVALIDATE] → TRY → CALL: dn.getFSDataset().invalidate → CALL: incrBlocksRemoved → BREAK → EXIT → ENTRY → CALL: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:invalidate → IF_FALSE: !enabled → IF_TRUE: !affectedSlots.isEmpty() → LOG: INFO: Block <blockId> has been invalidated. Marking short-circuit slots as invalid: <slotDetails> → CALL: bld.append(\"Block \").append(blockId).append(\" has been invalidated. \").append → FOREACH: affectedSlots → FOREACH_EXIT → CALL: LOG.info(bld.toString()) → EXIT → RETURN → EXIT → ENTRY → FOR_INIT → FOR_COND: i < blockIds.length → CALL: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:uncacheBlock → IF_TRUE: cacheLoader.isTransientCache() && !dataset.datanode.getShortCircuitRegistry().processBlockMunlockRequest(key) → IF_TRUE: prevValue == null → LOG: DEBUG: Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap., blockId, bpid → CALL: numBlocksFailedToUncache.increment → RETURN → FOR_EXIT → EXIT",
    "log": "<log>[WARN] Failed to transfer block {}, {}</log> <log>[INFO] Invalidating blocks in dataset</log> <log>[INFO] Block <blockId> has been invalidated. Marking short-circuit slots as invalid: <slotDetails></log> <log>[DEBUG] Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.</log> <log>[INFO] Block recovery started for {who}</log> <log>[INFO] Recovering block {block}</log> <log>[WARN] Failed to recover block (block={block}, datanode={id})</log> <log>[INFO] Setting block keys</log>"
  },
  "c45ae68c_1": {
    "exec_flow": "ENTRY -> CALL: addCustomerProvidedKeyHeaders -> CALL: requestHeaders.add -> IF_FALSE: leaseId != null -> CALL: abfsUriQueryBuilder.addQuery -> CALL: abfsUriQueryBuilder.addQuery -> CALL: abfsUriQueryBuilder.addQuery -> CALL: abfsUriQueryBuilder.addQuery -> CALL: appendSASTokenToQuery -> CALL: createRequestUrl -> ENTRY -> TRY -> CALL: urlEncode -> CALL: append -> CALL: append -> CALL: append -> TRY -> NEW: URL -> CALL: toString -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) -> RETURN -> EXIT -> ENTRY → TRY → CALL: IOStatisticsBinding.trackDurationOfInvocation → EXCEPTION: trackDurationOfInvocation → CATCH: IOException e → THROW: new UncheckedIOException(\"Error while tracking Duration of an AbfsRestOperation call\", e) → EXIT -> RETURN -> EXIT",
    "log": "[ERROR] UncheckedIOException thrown [DEBUG] Unexpected error."
  },
  "c45ae68c_2": {
    "exec_flow": "ENTRY→IF_FALSE: token == sasToken→CALL:getExpiry→IF_FALSE: token == null→IF_FALSE: start == -1→TRY→CALL: decode→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Object,java.lang.Object)→RETURN→SYNC: this→IF_TRUE: isInvalid→EXIT",
    "log": "[ERROR] Error decoding se query parameter ({}) from SAS."
  },
  "c45ae68c_3": {
    "exec_flow": "ENTRY→IF_FALSE: token == sasToken→CALL:getExpiry→IF_FALSE: token == null→IF_FALSE: start == -1→TRY→CALL: decode→TRY→CALL: parse→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Object,java.lang.Object)→RETURN→SYNC: this→IF_TRUE: isInvalid→EXIT",
    "log": "[ERROR] Error parsing se query parameter ({}) from SAS."
  },
  "455584af_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_TRUE: this.cacheUpdater != null → LOG: LOG.DEBUG: Removing service {}, service.getName() → CALL: this.cacheUpdater.stop → CALL: removeService → EXIT",
    "log": "[DEBUG] Removing service {}"
  },
  "455584af_2": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → LOG: LOG.DEBUG: Service: {} entered state {} → CALL: recordLifecycleEvent → CATCH: Exception e → LOG: LOG.DEBUG: noteFailure → CALL: noteFailure(e) → LOG: LOG.INFO: Service {} failed in state {} → THROW: ServiceStateException.convert(e) → FINALLY → TERMINATION_NOTIFICATION.SET(true) → SYNC: terminationNotification → NOTIFYALL → CALL: notifyListeners → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → FINALLY → EXIT → ELSE → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] noteFailure [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {} [DEBUG] Ignoring re-entrant call to stop()"
  },
  "455584af_3": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) == STATE.STOPPED → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "d9380ece_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:cloneAndGetContainerStatus</step> <step>LOG:DEBUG:Cloning and getting container status</step> <step>CALL:eventHandler.handle</step> <step>LOG:DEBUG:Processing {resourcePath} of type {event.getType()}</step> <step>IF_TRUE: qSize != 0 && qSize % 1000 == 0 && lastEventQueueSizeLogged != qSize</step> <step>LOG:INFO: Size of event-queue is + qSize</step> <step>TRY</step> <step>CALL:doTransition</step> <step>THROW:InvalidStateTransitionException</step> <step>CATCH</step> <step>LOG:ERROR:Can't handle this event at current state</step> <step>CALL:eventHandler.handle</step> <step>LOG:INFO:Handling application container finished event</step> <step>CALL:eventHandler.handle</step> <step>LOG:INFO:Scheduler container completed</step> <step>CALL:eventHandler.handle</step> <step>LOG:INFO:Container stop monitoring</step> <step>CALL:eventHandler.handle</step> <step>LOG:INFO:Log handler container finished</step> <step>CALL:this.writeLock.unlock</step> <step>EXIT</step>",
    "log": "[DEBUG] Cloning and getting container status [DEBUG] Processing {resourcePath} of type {event.getType()} [INFO] Size of event-queue is + qSize [ERROR] Can't handle this event at current state [INFO] Handling application container finished event [INFO] Scheduler container completed [INFO] Container stop monitoring [INFO] Log handler container finished"
  },
  "d9380ece_2": {
    "exec_flow": "<sequence> <step>ENTRY</step> <step>IF_TRUE: instance == null</step> <step>LOG:ERROR:No component instance exists for + event.getContainerId()</step> <step>RETURN</step> <step>EXIT</step> <step>ENTRY</step> <step>IF_FALSE: instance == null</step> <step>TRY</step> <step>CALL:handle</step> <step>EXCEPTION:handle</step> <step>CATCH:Throwable t</step> <step>LOG:instance.getCompInstanceId() + \": Error in handling event type \" + event.getType(), t</step> <step>EXIT</step> <step>ENTRY</step> <step>CALL:this.writeLock.lock</step> <step>TRY</step> <step>LOG:DEBUG:Processing {resourcePath} of type {event.getType()}</step> <step>TRY</step> <step>CALL:doTransition</step> <step>THROW:InvalidStateTransitionException</step> <step>CATCH</step> <step>LOG:ERROR:Can't handle this event at current state</step> <step>CALL:this.writeLock.unlock</step> <step>EXIT</step> </sequence>",
    "log": "[ERROR] No component instance exists for + event.getContainerId() [ERROR] instance.getCompInstanceId() + \": Error in handling event type \" + event.getType(), t [DEBUG] Processing {resourcePath} of type {event.getType()} [ERROR] Can't handle this event at current state"
  },
  "0f9260fc_1": {
    "exec_flow": "ENTRY→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {methodName}→CALL: opCategory.set→IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "0f9260fc_2": {
    "exec_flow": "ENTRY→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.startOp→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {methodName}→CALL: opCategory.set→IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: checkSafeMode→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "0f9260fc_3": {
    "exec_flow": "ENTRY→IF_FALSE: rpcMonitor != null→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {methodName}→CALL: opCategory.set→IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "0f9260fc_4": {
    "exec_flow": "ENTRY→IF_FALSE: rpcMonitor != null→IF_TRUE: LOG.isDebugEnabled()→LOG: LOG.DEBUG: Proxying operation: {methodName}→CALL: opCategory.set→IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ→CALL: checkSafeMode→EXIT",
    "log": "[DEBUG] Proxying operation: {methodName}"
  },
  "94676f78_1": {
    "exec_flow": "ENTRY→TRY→CALL:getCurrentUser→[EXCEPTION:getCurrentUser]→CALL:warn→CALL:makeQualified→NEW:Path→RETURN→EXIT",
    "log": "[WARN] Unable to get user name. Fall back to system property user.name"
  },
  "e8a63041_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "e8a63041_2": {
    "exec_flow": "ENTRY→CALL:getInt→IF_TRUE:this.maxConcurrentTrackedNodes < 0→CALL:Configuration:getInt→LOG:LOG.ERROR {} is set to an invalid value, it must be zero or greater. Defaulting to {}→CALL:processConf→EXIT",
    "log": "[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT"
  },
  "e1e0eccd_1": {
    "exec_flow": "ENTRY→CALL:coarseLock.readLock().unlock→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→EXIT",
    "log": "[INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "e1e0eccd_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.hdfs.protocol.ClientProtocol:isFileClosed→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser→CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod→FOREACH: locations→TRY→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice→CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod→CATCH→CALL:org.slf4j.Logger:error→FOREACH_EXIT→IF_TRUE: !thrownExceptions.isEmpty()→FOR_INIT→FOR_COND: i < thrownExceptions.size()→FOR_EXIT→THROW: thrownExceptions.get(0)→EXIT",
    "log": "<log>[ERROR] Unexpected exception {} proxying {} to {}</log>"
  },
  "e1e0eccd_3": {
    "exec_flow": "ENTRY→CALL:checkOperation→CALL:FSPermissionChecker.setOperationType→TRY→CALL:getPermissionChecker→CALL:readLock→EXCEPTION:readLock→CATCH:AccessControlException e→LOG:logAuditEvent→FOREACH: auditLoggers →IF: !(logger instanceof HdfsAuditLogger)→CALL:logger.logAuditEvent→FOREACH_EXIT→THROW:e→EXIT",
    "log": "[AUDIT] Access denied for operation isFileClosed on source"
  },
  "1c35bf00_1": {
    "exec_flow": "ENTRY→IF_TRUE: LOGGER.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled()→LOG: LOGGER.DEBUG: FileSystem.close() by method: close); Key: someKey; URI: someUri; Object Identity Hash: someHash→TRY→CALL:processDeleteOnExit→CALL:CACHE.remove→EXIT",
    "log": "[DEBUG] FileSystem.close() by method: close); Key: someKey; URI: someUri; Object Identity Hash: someHash"
  },
  "1c35bf00_2": {
    "exec_flow": "ENTRY→SYNC:deleteOnExit→FOR_INIT→FOR_COND:iter.hasNext()→CALL:exists→IF→CALL:delete→CATCH→CALL:info→REMOVE→FOR_CONTINUE→EXIT",
    "log": "[INFO] Ignoring failure to deleteOnExit for path {}"
  },
  "a06cc986_1": {
    "exec_flow": "ENTRY → CALL:rpcServer.checkOperation → IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.startOp → IF_TRUE:LOG.isDebugEnabled() → CALL:org.slf4j.Logger:isDebugEnabled() → LOG:LOG.DEBUG: Proxying operation: {}, methodName → CALL:opCategory.set → IF_FALSE:op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL:checkSafeMode → CALL:namenodeResolver.getNamespaces → IF_FALSE:locations.isEmpty() → IF_FALSE:locations.size() == 1 && timeOutMs <= 0 → FOREACH:locations → FOREACH_EXIT → IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.proxyOp → TRY → IF_TRUE:timeOutMs > 0 → CALL:invokeAll → FOR_INIT → FOR_COND:i < futures.size() → FOR_EXIT → RETURN → EXIT → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addTags → IF_TRUE:overlay != null → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT",
    "log": "<log> [DEBUG] Proxying operation: {}, methodName [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [WARN] Unexpected SecurityException in Configuration [INFO] Concurrent invocation success [DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [INFO] Token file {} does not exist [DEBUG] Loaded {} base64 tokens [DEBUG] Reading credentials from location {} [WARN] Null token ignored for {} [INFO] Rolled edit log using RouterClientProtocol [INFO] Rolled edit log using RouterRpcServer </log>"
  },
  "76800554_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config</log> <log>[DEBUG] Handling deprecation for (String) item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "76800554_2": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → CALL: org.slf4j.Logger:warn → CATCH: ClassNotFoundException e → EXIT",
    "log": "<log>[WARN] Serialization class not found: , e</log>"
  },
  "76800554_3": {
    "exec_flow": "ENTRY → CALL: getCompressor → CALL: org.slf4j.Logger:isDebugEnabled() → CALL: org.slf4j.Logger:debug → CALL: org.apache.hadoop.io.compress.CompressionCodec:createCompressor → CALL: org.apache.hadoop.io.compress.Compressor:reinit → CALL: org.apache.hadoop.io.compress.CompressionCodec:getDefaultExtension → CALL: org.slf4j.Logger:info → RETURN → EXIT",
    "log": "<log>[DEBUG] Compression codec created</log> <log>[INFO] Compressor initialized</log>"
  },
  "76800554_4": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → TRY → CALL: getGroups → CALL: getShortUserName → CALL: org.slf4j.Logger:debug → EXIT",
    "log": "<log>[DEBUG] Failed to get groups for user {}</log>"
  },
  "76800554_5": {
    "exec_flow": "ENTRY → FOR_LOOP: newNames → GET: deprecatedKey → IF_TRUE: deprecatedKey != null && !getProps().containsKey(newName) → IF_TRUE: deprecatedValue != null → SET_PROPERTY: newName → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "1174975e_1": {
    "exec_flow": "ENTRY → CALL:stop → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → LOG: LOG.DEBUG: Service: {} entered state {} → CALL: recordLifecycleEvent → CATCH: Exception e → LOG: LOG.DEBUG: noteFailure → CALL: noteFailure(e) → LOG: LOG.INFO: Service {} failed in state {} → THROW: ServiceStateException.convert(e) → FINALLY → TERMINATION_NOTIFICATION.SET(true) → SYNC: terminationNotification → NOTIFYALL → CALL: notifyListeners → TRY → CALL: listeners.notifyListeners(this) → CALL: globalListeners.notifyListeners(this) → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {} → FINALLY → EXIT → ELSE → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → CALL:closeInternal → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] noteFailure [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {} [DEBUG] Ignoring re-entrant call to stop()"
  },
  "1c192ee9_1": {
    "exec_flow": "<seq> ENTRY→CALL:append→CALL:getInt→IF_TRUE: props != null→CALL:getProps→CALL:addTags→IF_TRUE: overlay != null→CALL:putAll→IF_TRUE: backup != null→FOREACH: overlay.entrySet()→FOREACH_EXIT→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info [DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}→CALL:org.apache.hadoop.fs.FilterFileSystem:append→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:append→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:statIncrement→TRY→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:makeQualified→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite→CALL:startTracking→CALL:client.getPathStatus→CALL:perfInfo.registerResult→IF_FALSE:parseIsDirectory(resourceType)→CALL:perfInfo.registerSuccess→IF_TRUE:isAppendBlobKey→CALL:isAppendBlobKey→CALL:maybeCreateLease→NEW:AbfsOutputStream→CALL:populateAbfsOutputStreamContext→CALL:perfInfo.close→RETURN→EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize} [DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite </log>"
  },
  "551b11d2_1": {
    "exec_flow": "ENTRY → CALL:failedContainer → IF_TRUE: LOG.isWarnEnabled() → CALL: org.slf4j.Logger:isWarnEnabled() → CALL: LOG.WARN: createFailureLog(user, operation, target, description, null, null) → CALL: org.slf4j.Logger:warn(java.lang.String) → CALL:ContainerImpl$ContainerDoneTransition:transition → CALL:transition → EXIT",
    "log": "[INFO] Container failed with state: <state> [WARN] createFailureLog(user, operation, target, description, null, null) [DEBUG] Cloning and getting container status [INFO] Handling application container finished event [INFO] Scheduler container completed [INFO] Container stop monitoring [INFO] Log handler container finished"
  },
  "551b11d2_2": {
    "exec_flow": "ENTRY → IF_FALSE: c != null → LOG: [WARN] Event {event} sent to absent container {event.getContainerID()} → EXIT",
    "log": "[WARN] Event {event} sent to absent container {event.getContainerID()}"
  },
  "0e2e82e1_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> <seq> ENTRY→TRY→LOG: LOG.INFO: Reconfiguring {} to {}, property, newVal →IF_TRUE: property.equals(DFS_BLOCKREPORT_INTERVAL_MSEC_KEY) →CALL: Preconditions.checkNotNull →CALL: toString →CALL: dnConf.setBlockReportInterval →FOREACH: blockPoolManager.getAllNamenodeThreads() →FOREACH_EXIT →LOG: LOG.INFO: RECONFIGURE* changed {} to {}, property, newVal →RETURN→EXIT </seq>",
    "log": "<log> [INFO] Reconfiguring {} to {} [INFO] RECONFIGURE* changed {} to {} </log>"
  },
  "0e2e82e1_2": {
    "exec_flow": "<!-- Optimized execution flow structure --> <seq> ENTRY→TRY→LOG: LOG.INFO: Reconfiguring {} to {}, property, newVal →IF_FALSE: property.equals(DFS_BLOCKREPORT_INTERVAL_MSEC_KEY) →IF_TRUE: property.equals(DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY) →CALL: Preconditions.checkNotNull →CALL: toString →CALL: dnConf.setBlockReportSplitThreshold →LOG: LOG.INFO: RECONFIGURE* changed {} to {}, property, newVal →RETURN→EXIT </seq>",
    "log": "<log> [INFO] Reconfiguring {} to {} [INFO] RECONFIGURE* changed {} to {} </log>"
  },
  "0e2e82e1_3": {
    "exec_flow": "<!-- Optimized execution flow structure --> <seq> ENTRY→TRY→LOG: LOG.INFO: Reconfiguring {} to {}, property, newVal →IF_FALSE: property.equals(DFS_BLOCKREPORT_INTERVAL_MSEC_KEY) →IF_FALSE: property.equals(DFS_BLOCKREPORT_SPLIT_THRESHOLD_KEY) →IF_TRUE: property.equals(DFS_BLOCKREPORT_INITIAL_DELAY_KEY) →CALL: Preconditions.checkNotNull →CALL: toString →CALL: dnConf.setInitBRDelayMs →LOG: LOG.INFO: RECONFIGURE* changed {} to {}, property, newVal →RETURN→EXIT </seq>",
    "log": "<log> [INFO] Reconfiguring {} to {} [INFO] RECONFIGURE* changed {} to {} </log>"
  },
  "eed79d5d_1": {
    "exec_flow": "<seq> ENTRY→LOG:Unexpected SecurityException in Configuration→CALL:getRaw→ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→RETURN→EXIT </seq>",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "fc6af5a0_1": {
    "exec_flow": "<!-- Optimized execution flow structure combining parent and child behaviors --> ENTRY→IF_FALSE: inode == INodeId.ROOT_INODE_ID→IF_FALSE: !dirPathCache.containsKey(parent)→CALL: get→RETURN→EXIT",
    "log": "[DEBUG] No snapshot name found for inode {}"
  },
  "b09fff71_1": {
    "exec_flow": "ENTRY → LOG: \"Handling deprecation for all properties in config...\" → CALL:fixName → [VIRTUAL_CALL] → (LOG: \"Handling deprecation for all properties in config...\" → CALL:handleDeprecation → FOREACH:names → CALL:getProps → IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → RETURN) → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Handling deprecation for all properties in config...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for (String)item</message> </log>"
  },
  "b09fff71_2": {
    "exec_flow": "ENTRY → IF_TRUE:LOG.isDebugEnabled()→LOG:running sort pass→CALL:sortPass.setProgressable→NEW: MergeSort→NEW: SeqFileComparator→TRY→CALL:run→RETURN→EXIT",
    "log": "<log> <level>DEBUG</level> <message>running sort pass</message> </log>"
  },
  "b09fff71_3": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: Bypassing cache to create filesystem {uri} → CALL: createFileSystem → RETURN → EXIT → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE:!FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: Looking for FS supporting {scheme} → IF_TRUE: conf != null → LOG: looking for configuration option {property} → CALL: getClass → IF_TRUE: clazz == null → LOG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \" \"+\" \"\\\"\" + scheme + \"\\\" \") → CATCH: IOException | RuntimeException e → LOG: Failed to initialize filesystem {uri}: {e.toString()} → LOG: Failed to initialize filesystem → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Bypassing cache to create filesystem {uri}</message> </log> <log> <level>DEBUG</level> <message>Looking for FS supporting {scheme}</message> </log> <log> <level>DEBUG</level> <message>looking for configuration option {property}</message> </log> <log> <level>DEBUG</level> <message>Looking in service filesystems for implementation class</message> </log> <log> <level>WARN</level> <message>Failed to initialize filesystem {uri}: {e.toString()}</message> </log> <log> <level>DEBUG</level> <message>Failed to initialize filesystem</message> </log> <log> <level>DEBUG</level> <message>Exception in closing {}</message> </log>"
  },
  "b09fff71_4": {
    "exec_flow": "ENTRY → LOG: \"local\" is a deprecated filesystem name. Use \"file:///\" instead. → CALL:fixName → [VIRTUAL_CALL] → (CALL:handleDeprecation → CALL:getProps → FOREACH:names → CALL:substituteVars → FOREACH_EXIT → RETURN) → EXIT",
    "log": "<log> <level>WARN</level> <message>\"local\" is a deprecated filesystem name. Use \"file:///\" instead.</message> </log> <log> <level>WARN</level> <message>\"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead.</message> </log>"
  },
  "b09fff71_5": {
    "exec_flow": "Parent.ENTRY→LOG: Call the getFileStatus to obtain the metadata for the file: [{f}].→ IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→ IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG: Path: [{f}] is a file. COS key: [{key}].→ CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log> <level>DEBUG</level> <message>Call the getFileStatus to obtain the metadata for the file: [{f}].</message> </log> <log> <level>DEBUG</level> <message>Path: [{f}] is a file. COS key: [{key}].</message> </log>"
  },
  "b09fff71_6": {
    "exec_flow": "ENTRY → LOG: Ready to delete path: [{}]. recursive: [{}] → TRY → CALL:getFileStatus → LOG:Path: [{}] is a file. COS key: [{}] → LOG:Path: [{}] is a dir. COS key: [{}] → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Ready to delete path: [{}]. recursive: [{}]</message> </log> <log> <level>DEBUG</level> <message>Path: [{}] is a file. COS key: [{}]</message> </log> <log> <level>DEBUG</level> <message>Path: [{}] is a dir. COS key: [{}]</message> </log>"
  },
  "e5aa3368_1": {
    "exec_flow": "ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> RETURN -> ENTRY -> IF_FALSE: locations.isEmpty() -> IF_FALSE: locations.size() == 1 && timeOutMs <= 0 -> FOREACH: locations -> FOREACH_EXIT -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.proxyOp -> TRY -> IF_TRUE: timeOutMs > 0 -> CALL: invokeAll -> FOR_INIT -> FOR_COND: i < futures.size() -> FOR_EXIT -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "e5aa3368_2": {
    "exec_flow": "ENTRY -> IF_FALSE: rpcMonitor != null -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> ENTRY -> IF_FALSE: locations.isEmpty() -> IF_FALSE: locations.size() == 1 && timeOutMs <= 0 -> FOREACH: locations -> FOREACH_EXIT -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.proxyOp -> TRY -> IF_FALSE: timeOutMs > 0 -> CALL: invokeAll -> FOR_INIT -> FOR_COND: i < futures.size() -> FOR_EXIT -> RETURN -> EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "3ad7a7b0_1": {
    "exec_flow": "ENTRY → LOG: Getting the file status for {f} → ENTRY → IF_FALSE: key.length() == 0 → TRY → CALL: retrieveMetadata → IF_TRUE: meta != null → IF_FALSE: meta.isDirectory() → LOG: Found the path: {f} as a file. → CALL: updateFileStatusPath → RETURN → EXIT",
    "log": "Getting the file status for {f} [DEBUG] Found the path: {f} as a file."
  },
  "3ad7a7b0_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → TRY → CALL: makeQualified → CALL: abfsStore.getFileStatus → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.getFileStatus path: {}"
  },
  "3ad7a7b0_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path → CALL: statIncrement → CALL: makeQualified → TRY → CALL: abfsStore.getFileStatus → CATCH: AzureBlobFileSystemException → CALL: checkException → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.getFileStatus path: {}"
  },
  "3ad7a7b0_4": {
    "exec_flow": "ENTRY→CALL:getUriPath→CALL:resolve→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs→LOG: LOG.DEBUG: AzureBlobFileSystem.mkdirs path: {} permissions: {}→CALL:statIncrement→CALL:trailingPeriodCheck→IF_TRUE:parentFolder==null→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.mkdirs path: {} permissions: {}"
  },
  "3ad7a7b0_5": {
    "exec_flow": "ENTRY→CALL:getUriPath→CALL:resolve→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs→LOG: LOG.DEBUG: AzureBlobFileSystem.mkdirs path: {} permissions: {}→CALL:statIncrement→CALL:trailingPeriodCheck→IF_FALSE:parentFolder==null→CALL:makeQualified→TRY→CALL:getUMask→CALL:abfsStore.createDirectory→CALL:statIncrement→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.mkdirs path: {} permissions: {}"
  },
  "3ad7a7b0_6": {
    "exec_flow": "ENTRY→CALL:getUriPath→CALL:resolve→CALL:org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs→LOG: LOG.DEBUG: AzureBlobFileSystem.mkdirs path: {} permissions: {}→CALL:statIncrement→CALL:trailingPeriodCheck→IF_FALSE:parentFolder==null→CALL:makeQualified→TRY→CALL:getUMask→CALL:abfsStore.createDirectory→EXCEPTION:createDirectory→CATCH:AzureBlobFileSystemException ex→CALL:checkException→RETURN→EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.mkdirs path: {} permissions: {}"
  },
  "5da0ed3f_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics:getMetrics() → SWITCH:rmNode.getState → CASE:DEFAULT → LOG:LOG.DEBUG:Unexpected node state → EXIT",
    "log": "[DEBUG] Unexpected node state"
  },
  "a2966b23_1": {
    "exec_flow": "ENTRY→CALL:delete→CALL:org.apache.hadoop.fs.FileSystem:delete→RETURN→EXIT ENTRY→LOG:DEBUG: Ready to delete path: [{}]. recursive: [{}]. LOG:DEBUG: AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive CALL:statIncrement→CALL:makeQualified→IF_FALSE:f.isRoot()→TRY→ENTRY LOG:DEBUG: delete filesystem: {} path: {} recursive: {}→DO_WHILE→TRY CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:deletePath CALL:perfInfo.registerResult→CALL:getResponseHeader→CALL:getResult CALL:perfInfo.registerSuccess→CALL:isEmpty→IF_FALSE:!shouldContinue CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close DO_COND:shouldContinue→DO_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}]. [DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive [DEBUG] delete filesystem: {} path: {} recursive: {}"
  },
  "4a248e0e_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: Listing status for {}, f.toString()→CALL: performAuthCheck→TRY→CALL: retrieveMetadata→IF_FALSE: meta == null→IF_FALSE: !meta.isDirectory()→CALL: listWithErrorHandling→IF_FALSE: renamed→CALL: conditionalRedoFolderRenames→IF_TRUE: key.equals(\"/\")→FOREACH: listing→IF_TRUE: fileMetadata.isDirectory()→IF_TRUE: fileMetadata.getKey().equals(AZURE_TEMP_FOLDER)→CONTINUE→EXIT",
    "log": "[DEBUG] Listing status for {f}"
  },
  "4a248e0e_2": {
    "exec_flow": "ENTRY→IF_FALSE: key.length() == 0→CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE: meta != null→LOG:DEBUG: List COS key: [{}] to check the existence of the path.→CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE: listing.getFiles().length>0 || listing.getCommonPrefixes().length>0→IF_TRUE: LOG.isDebugEnabled()→LOG:DEBUG: Path: [{}] is a directory. COS key: [{}]→CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "4a248e0e_3": {
    "exec_flow": "ENTRY→IF_FALSE: key.length() == 0→CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE: meta != null→IF_TRUE: meta.isFile()→LOG:DEBUG: Path: [{}] is a file. COS key: [{}]→CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "4a248e0e_4": {
    "exec_flow": "ENTRY→IF_TRUE: key.length() > 0→CALL: getFileStatus→IF_FALSE: fileStatus.isFile()→IF_FALSE: !key.endsWith(PATH_DELIMITER)→CALL: list→FOREACH: listing.getFiles()→CALL: debug→FOREACH_EXIT→FOREACH: listing.getCommonPrefixes()→CALL: newFile→CALL: newDirectory→FOREACH_EXIT→CALL: getPriorLastKey→DO_COND: priorLastKey != null→DO_EXIT→CALL: toArray→CALL: size→RETURN→EXIT",
    "log": "[DEBUG] The file list contains the COS key [{}] to be listed."
  },
  "4a248e0e_5": {
    "exec_flow": "<step> <description>Qualify the input path</description> <source>org.apache.hadoop.fs.s3a.S3AFileSystem:qualify</source> </step> <step> <description>List the statuses of the files/directories</description> <source>org.apache.hadoop.fs.s3a.S3AFileSystem:listStatus</source> </step>",
    "log": "<log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:listStatus</class_method> <level>INFO</level> <message>Starting listStatus for path: {path}</message> </log_entry>"
  },
  "4a248e0e_6": {
    "exec_flow": "<!-- Optimized execution flow structure for listStatus and connect methods -->",
    "log": "<!-- Merged log sequence if any exist -->"
  },
  "21324a1c_1": {
    "exec_flow": "ENTRY → CALL:executeOnlyOnce → CALL:progress → CALL:getStageConfig().enterStage → LOG:INFO:{}:Executing Stage {}, getName(), stageName → CALL:createTracker → CALL:getIOStatistics → TRY → CALL:executeStage → LOG:INFO:{}:Stage {} completed after {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()) → CALL:setCounter → LOG:DEBUG:Setting counter {} to {}, key, value → RETURN → CALL:progress → CALL:getStageConfig().exitStage → CALL:org.apache.hadoop.fs.statistics.DurationTracker:close → EXIT",
    "log": "[INFO] {}: Executing Stage {}, getName(), stageName [INFO] {}: Stage {} completed after {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()) [DEBUG] Setting counter {} to {}, key, value"
  },
  "21324a1c_2": {
    "exec_flow": "ENTRY → CALL:executeOnlyOnce → CALL:progress → CALL:getStageConfig().enterStage → LOG:INFO:{}:Executing Stage {}, getName(), stageName → CALL:createTracker → CALL:getIOStatistics → TRY → CALL:executeStage → EXCEPTION → LOG:ERROR:{}:Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString() → LOG:DEBUG:{}:Stage failure:, getName(), e → CALL:progress → CALL:getStageConfig().exitStage → CALL:org.apache.hadoop.fs.statistics.DurationTracker:close → EXIT",
    "log": "[INFO] {}: Executing Stage {}, getName(), stageName [ERROR] {}: Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString() [DEBUG] {}: Stage failure:, getName(), e"
  },
  "21324a1c_3": {
    "exec_flow": "ENTRY → CALL:executeOnlyOnce → CALL:progress → CALL:getStageConfig().enterStage → LOG:INFO:{}:Executing Stage {}, getName(), stageName → CALL:createTracker → CALL:getIOStatistics → TRY → CALL:executeStage → LOG:INFO:{}:Stage {} completed after {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()) → RETURN → CALL:progress → CALL:getStageConfig().exitStage → CALL:org.apache.hadoop.fs.statistics.DurationTracker:close → EXIT",
    "log": "[INFO] {}: Executing Stage {}, getName(), stageName [INFO] {}: Stage {} completed after {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis())"
  },
  "21324a1c_4": {
    "exec_flow": "ENTRY → CALL:executeOnlyOnce → CALL:progress → CALL:getStageConfig().enterStage → LOG:INFO:{}:Executing Stage {}, getName(), stageName → CALL:createTracker → CALL:getIOStatistics → TRY → CALL:executeStage → EXCEPTION → LOG:ERROR:{}:Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString() → LOG:DEBUG:{}:Stage failure:, getName(), e → CALL:progress → CALL:getStageConfig().exitStage → CALL:org.apache.hadoop.fs.statistics.DurationTracker:close → EXIT",
    "log": "[INFO] {}: Executing Stage {}, getName(), stageName [ERROR] {}: Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString() [DEBUG] {}: Stage failure:, getName(), e"
  },
  "590460eb_1": {
    "exec_flow": "<call_context>noteFailure</call_context> <conditional_execution> <condition>exception != null</condition> <log>LOG.debug(\"noteFailure\", exception)</log> <sync_block> <condition>failureCause == null</condition> <actions> <set failureCause>exception</set> <set failureState>getServiceState()</set> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log> </actions> </sync_block> </conditional_execution>",
    "log": "<log>LOG.debug(\"noteFailure\", exception)</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log>"
  },
  "931ae0d7_1": {
    "exec_flow": "ENTRY→IF_FALSE:this.closed→CALL:this.currentBlockOutputStream.flush→CALL:this.currentBlockOutputStream.close→LOG:LOG.INFO:The output stream has been close, and begin to upload the last block: [{}].,this.currentBlockId→CALL:this.blockCacheBuffers.add→IF_FALSE:this.blockCacheBuffers.size()==1→IF_TRUE:this.blockWritten>0→LOG:LOG.INFO:Upload the last part...,blockId:[{}],written bytes:[{}]→CALL:store.uploadPart→CALL:waitForFinishPartUploads→IF_FALSE:null==futurePartETagList→CALL:store.completeMultipartUpload→TRY→CALL:BufferPool.getInstance().returnBuffer→LOG:LOG.INFO:The outputStream for key: [{}] has been uploaded.,key→EXIT",
    "log": "[INFO] The output stream has been close, and begin to upload the last block: [{}]. [INFO] Upload the last part..., blockId: [{}], written bytes: [{}] [INFO] The outputStream for key: [{}] has been uploaded."
  },
  "931ae0d7_2": {
    "exec_flow": "ENTRY→CALL: uploadPartRequest.setBucketName→CALL: uploadPartRequest.setUploadId→CALL: uploadPartRequest.setInputStream→CALL: uploadPartRequest.setPartNumber→CALL: uploadPartRequest.setPartSize→CALL: uploadPartRequest.setKey→TRY→CALL: callCOSClientWithRetry→ENTRY→WHILE: true→WHILE_COND: true→TRY→IF_TRUE: request instanceof UploadPartRequest→CALL: uploadPart→CATCH: CosServiceException→CALL: org.slf4j.Logger:info→WHILE: retry→CALL: org.slf4j.Logger:error→RETURN→CALL: getPartETag→EXIT",
    "log": "[DEBUG] Current thread: [%d], COS key: [%s], upload id: [%s], part num: [%d], exception: [%s] [INFO] Call cos sdk failed, retryIndex: [1 / max], call method: uploadPart, exception: ... [ERROR] Call cos sdk failed, retryIndex: [max / max], call method: uploadPart, exception: ..."
  },
  "0e05e196_1": {
    "exec_flow": "ENTRY→WHILE→SWITCH→CASE:SKIP_UNTIL→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:skipUntil→IF_TRUE:prevTxId!=HdfsServerConstants.INVALID_TXID→CALL:org.slf4j.Logger:info→RETURN→EXIT",
    "log": "[INFO] Fast-forwarding stream..."
  },
  "0e05e196_2": {
    "exec_flow": "ENTRY→WHILE→SWITCH→CASE:STREAM_FAILED_RESYNC→TRY→CALL:org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:resync→CATCH→IF_FALSE:curIdx+1==streams.length→CALL:org.slf4j.Logger:error→SET_STATE:SKIP_UNTIL→RETURN→EXIT",
    "log": "[ERROR] Failing over to edit log..."
  },
  "5dc21754_1": {
    "exec_flow": "ENTRY→IF_FALSE: getStreamer().getAppendChunk() && getStreamer().getBytesCurBlock() % bytesPerChecksum == 0→IF_TRUE: !getStreamer().getAppendChunk()→CALL: computePacketChunkSize→CALL: DFSClient.LOG.debug→EXIT",
    "log": "[DEBUG] computePacketChunkSize: src={}, chunkSize={}, chunksPerPacket={}, packetSize={}"
  },
  "81562dea_1": {
    "exec_flow": "ENTRY → TRY → NEW: DataInputStream → NEW: BufferedInputStream → CALL: newInputStream → CALL: toPath → CALL: toPath → CALL: readTokenStorageStream → EXCEPTION: readTokenStorageStream → CATCH: IOException ioe → CALL: IOUtils.cleanupWithLogger → EXIT <step>addToken Execution: Check for null token and handle appropriately</step> <step>Log: \"Null token ignored for {alias}\"</step> <step>Token Update Logic</step>",
    "log": "<log>[INFO] Cleaning up resources</log> <log>[DEBUG] Exception in closing {}</log> <log>org.apache.hadoop.security.Credentials:addToken[WARN]\"Null token ignored for {alias}\"</log>"
  },
  "c26ef4ae_1": {
    "exec_flow": "ENTRY→SYNC:UserGroupInformation.class→CALL:initialize→ENTRY→CALL:getAuthenticationMethod→IF_TRUE:!isInitialized()→CALL:LOG_DEPRECATION.info→CALL:org.apache.hadoop.conf.Configuration:getBoolean→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:Groups:getUserToGroupsMappingService→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→LOG:Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Creating new Groups object</log>"
  },
  "c26ef4ae_2": {
    "exec_flow": "ENTRY→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "826fe5c8_1": {
    "exec_flow": "ENTRY → LOG: [DEBUG] AzureBlobFileSystem.getFileStatus path: {} → CALL: setRequestMethod → CALL: org.apache.hadoop.fs.http.client.HttpFSFileSystem:makeQualified → EXCEPTION: setRequestMethod → CATCH: Exception ex → THROW: new IOException(ex) → EXIT → IF_FALSE: key.length()==0 → LOG: [DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}]. → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE: meta!=null → LOG: [DEBUG] List COS key: [{}] to check the existence of the path. → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE: listing.getFiles().length>0||listing.getCommonPrefixes().length>0 → LOG: [DEBUG] Path: [{}] is a directory. COS key: [{}] → RETURN → EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.getFileStatus path: {} [DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}]. [DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "826fe5c8_2": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → IF_TRUE: meta == null && !key.endsWith(\"/\") → CALL: getObjectMetadata → EXCEPTION: getObjectMetadata → CATCH: OSSException osse → CALL: LOG.debug → RETURN → EXIT",
    "log": "[DEBUG] Exception thrown when get object meta: + key + , exception: + osse"
  },
  "13e02f35_1": {
    "exec_flow": "ENTRY->CALL:qualify->CALL:getActiveAuditSpan->LOG:[DEBUG] List status for path: {path}->CALL:getFileStatusesAssumingNonEmptyDir->CALL:hasNext->IF_FALSE:!statusIt.hasNext()->RETURN->EXIT",
    "log": "<log>[DEBUG] List status for path: {path}</log>"
  },
  "13e02f35_2": {
    "exec_flow": "ENTRY->CALL:qualify->CALL:getActiveAuditSpan->LOG:[DEBUG] List status for path: {path}->CALL:getFileStatusesAssumingNonEmptyDir->CALL:hasNext->IF_TRUE:!statusIt.hasNext()->CALL:innerGetFileStatus->IF_TRUE:fileStatus.isFile()->LOG:[DEBUG] Adding: rd (not a dir): {path}->CALL:createProvidedFileStatusIterator->RETURN->EXIT",
    "log": "<log>[DEBUG] List status for path: {path}</log> <log>[DEBUG] Adding: rd (not a dir): {path}</log>"
  },
  "13e02f35_3": {
    "exec_flow": "ENTRY->TRY->CALL:setRequestMethod->CALL:org.apache.hadoop.security.UserGroupInformation:getCurrentUser->IF_TRUE:method.equals(HTTP_POST) || method.equals(HTTP_PUT)->CALL:setDoOutput->ENTRY->TRY->IF_TRUE:LOG.isDebugEnabled()->LOG:LOG.DEBUG:PrivilegedAction [as: {}][action: {}]->CALL:Subject.doAs->RETURN->EXIT->RETURN->EXIT",
    "log": "<log>[LOG] getLoginUser</log> <log>[DEBUG] PrivilegedAction [as: {}][action: {}]</log>"
  },
  "aaf8208f_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→CALL:substituteVars→LOG:Unexpected SecurityException in Configuration→CALL:findSubVariable→CALL:getenv→CALL:getProperty→CALL:getRaw→RETURN→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "aaf8208f_2": {
    "exec_flow": "ENTRY→CALL: LOG_DEPRECATION.info→CALL: org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "aaf8208f_3": {
    "exec_flow": "ENTRY→CALL:unnestUri→CALL:org.slf4j.Logger:isDebugEnabled()→IF_TRUE:LOG.isDebugEnabled()→LOG:org.slf4j.Logger:debug→EXIT",
    "log": "<log>[DEBUG] backing jks path initialized to ...</log>"
  },
  "b28919a5_1": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource ]]>",
    "log": "<!-- Integrated log sequence adhering to parent's log structure -->"
  },
  "b28919a5_2": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource ]]>",
    "log": "<!-- Integrated log sequence merging P4 and loadProps logs -->"
  },
  "b28919a5_3": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource ]]>",
    "log": "<!-- Log sequence accounting for potential logs under 'loadProps' -->"
  },
  "078f6f50_1": {
    "exec_flow": "ENTRY → CALL: readTokensFromFiles → CALL: org.apache.hadoop.conf.Configuration:getStrings → LOG: LOG.DEBUG: adding the following namenodes' delegation tokens: + Arrays.toString(nameNodes) → IF_TRUE: nameNodes != null → FOR_INIT → FOR_COND: i < nameNodes.length → CALL: TokenCache.obtainTokensForNamenodes → ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → THROW: e → EXIT → FOR_EXIT → EXIT",
    "log": "[DEBUG] adding the following namenodes' delegation tokens: [nameNode1, nameNode2...] [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem"
  },
  "078f6f50_2": {
    "exec_flow": "ENTRY → CALL: readTokensFromFiles → CALL: org.apache.hadoop.conf.Configuration:getStrings → LOG: LOG.DEBUG: adding the following namenodes' delegation tokens: + Arrays.toString(nameNodes) → IF_TRUE: nameNodes != null → FOR_INIT → FOR_COND: i < nameNodes.length → CALL: TokenCache.obtainTokensForNamenodes → ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → RETURN → CALL: org.apache.hadoop.util.DurationInfo:close → EXIT → FOR_EXIT → EXIT",
    "log": "[DEBUG] adding the following namenodes' delegation tokens: [nameNode1, nameNode2...] [DEBUG] Bypassing cache to create filesystem {}"
  },
  "078f6f50_3": {
    "exec_flow": "ENTRY → CALL: readTokensFromFiles → CALL: org.apache.hadoop.conf.Configuration:getStrings → LOG: LOG.DEBUG: adding the following namenodes' delegation tokens: + Arrays.toString(nameNodes) → IF_FALSE: nameNodes != null → EXIT",
    "log": "[DEBUG] adding the following namenodes' delegation tokens: null"
  },
  "1485d206_1": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→LOG: DEBUG: AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive →CALL: statIncrement →CALL: makeQualified →IF_FALSE: f.isRoot() →TRY →CALL: abfsStore.delete →RETURN →EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive"
  },
  "1485d206_2": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→LOG: DEBUG: AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive →CALL: statIncrement →CALL: makeQualified →IF_FALSE: f.isRoot() →TRY →CALL: abfsStore.delete →EXCEPTION: delete →CATCH: AzureBlobFileSystemException ex →CALL: checkException →RETURN →EXIT",
    "log": "[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive"
  },
  "1485d206_3": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→LOG: [DEBUG] Deleting root content →CALL: makeQualified →CALL: listStatus →FOREACH: ls →CALL: delete →RETURN →FOREACH_EXIT →CALL: executorService.shutdownNow →EXIT",
    "log": "[DEBUG] Deleting root content"
  },
  "3f5cf33b_1": {
    "exec_flow": "ENTRY → TRY → CALL:toBoolean → CATCH:TrileanConversionException → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext) → CATCH:AbfsRestOperationException → CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode() → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close() → ENTRY → VIRTUAL_CALL → LOG:LOG.DEBUG: createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {} → CALL:createDefaultHeaders → IF_TRUE: isFile → CALL: addCustomerProvidedKeyHeaders → IF_TRUE: !overwrite → CALL: requestHeaders.add → IF_FALSE: permission != null && !permission.isEmpty() → IF_FALSE: umask != null && !umask.isEmpty() → IF_FALSE: eTag != null && !eTag.isEmpty() → CALL: abfsUriQueryBuilder.addQuery → IF_FALSE: isAppendBlob → CALL: appendSASTokenToQuery → TRY → CALL: IOStatisticsBinding.trackDurationOfInvocation → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:lambda$execute$0 → CATCH:AzureBlobFileSystemException ex → IF_TRUE: !op.hasResult() → THROW: ex → CALL: perfInfo.registerResult(op.getResult()).registerSuccess → CALL: maybeCreateLease → ENTRY → VIRTUAL_CALL → LOG:LOG.DEBUG: Attempting to acquire lease on {}, retry {}, path, numRetries → IF_FALSE: future != null && !future.isDone() → CALL: schedule → CALL: acquireLease → CALL: addCallback → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsLease:lambda$0(tracingContext) → EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}</log> <log>[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call</log> <log>[DEBUG] Get root ACL status</log> <log>[DEBUG] IOStatisticsBinding track duration started</log> <log>[TRACE] Fetch SAS token for {operation} on {path}</log> <log>[TRACE] SAS token fetch complete for {operation} on {path}</log> <log>[DEBUG] First execution of REST operation - {}</log> <log>[DEBUG] Retrying REST operation {}. RetryCount = {}</log> <log>[DEBUG] Unexpected error.</log> <log>[TRACE] {} REST operation complete</log> <log>[DEBUG] IOStatisticsBinding track duration completed</log> <log>[DEBUG] Attempting to acquire lease on {}, retry {}</log>"
  },
  "3f5cf33b_2": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:create → RETURN → EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "3f5cf33b_3": {
    "exec_flow": "ENTRY → IF_FALSE:!enabled → IF_TRUE:isValidInstant(perfInfo.getAggregateStart())&&perfInfo.getAggregateCount()>0 → CALL:recordClientLatency → ENTRY → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:String.format → CALL:offerToQueue → IF_TRUE:LOG.isDebugEnabled() → CALL:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Queued latency info [{} ms]: {}, elapsed, latencyDetails → RETURN → EXIT → EXIT",
    "log": "<log>[DEBUG] Queued latency info [{} ms]: {}</log>"
  },
  "3f5cf33b_4": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE: containsColon(f) → CALL: performAuthCheck → CALL: createInternal → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "3f5cf33b_5": {
    "exec_flow": "ENTRY → NEW: FTPClient → CALL: connect → CALL: org.apache.hadoop.conf.Configuration:get → LOG:Handling deprecation for all properties in config... → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL: org.apache.hadoop.conf.Configuration:getInt → IF_FALSE: !FTPReply.isPositiveCompletion(reply) → CALL: client.login → IF_TRUE: client.login(user, password) → CALL: org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode → IF_FALSE:mode == null → IF_FALSE:upper.equals(\"STREAM_TRANSFER_MODE\") → IF_FALSE:upper.equals(\"COMPRESSED_TRANSFER_MODE\") → IF_TRUE:!upper.equals(\"BLOCK_TRANSFER_MODE\") → CALL: org.slf4j.Logger:warn → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Cannot parse the value for FS_FTP_TRANSFER_MODE: mode. Using default.</log>"
  },
  "3f5cf33b_6": {
    "exec_flow": "ENTRY → IF_TRUE:client != null → IF_FALSE:!client.isConnected() → CALL:logout → CALL:disconnect → IF_TRUE:!logoutSuccess → LOG:LOG.WARN:Logout failed while disconnecting, error code - + client.getReplyCode() → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "111d9450_1": {
    "exec_flow": "ENTRY -> IF_TRUE: !reReservation -> CALL: queue.getMetrics().reserveResource -> CALL: org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:reserve -> CALL: reserveResource -> ENTRY -> CALL: writeLock.lock -> TRY -> IF_TRUE: rmContainer == null -> NEW: RMContainerImpl -> CALL: getResource -> CALL: getApplicationAttemptId -> CALL: getNodeID -> CALL: getUser -> IF_TRUE: rmContainer.getState() == RMContainerState.NEW -> CALL: attemptResourceUsage.incReserved -> IF_FALSE: scheduler instanceof CapacityScheduler -> CALL: ((RMContainerImpl) rmContainer).setQueueName -> CALL: resetReReservations -> CALL: rmContainer.handle -> IF_TRUE: reservedContainers == null -> NEW: HashMap<NodeId, RMContainer> -> CALL: this.reservedContainers.put -> CALL: reservedContainers.put -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource -> CALL: org.slf4j.Logger:debug(java.lang.String) -> CALL: commonReserve -> RETURN -> EXIT -> EXIT",
    "log": "[DEBUG] Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource"
  },
  "111d9450_2": {
    "exec_flow": "ENTRY -> IF_TRUE: !reReservation -> CALL: queue.getMetrics().reserveResource -> CALL: org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:reserve -> CALL: reserveResource -> ENTRY -> CALL: writeLock.lock -> TRY -> IF_TRUE: rmContainer == null -> NEW: RMContainerImpl -> CALL: getResource -> CALL: getApplicationAttemptId -> CALL: getNodeID -> CALL: getUser -> IF_TRUE: rmContainer.getState() == RMContainerState.NEW -> CALL: attemptResourceUsage.incReserved -> IF_TRUE: scheduler instanceof CapacityScheduler -> CALL: normalizeQueueName -> CALL: ((RMContainerImpl) rmContainer).setQueueName -> CALL: resetReReservations -> CALL: rmContainer.handle -> IF_FALSE: reservedContainers == null -> CALL: reservedContainers.put -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource -> CALL: org.slf4j.Logger:debug(java.lang.String) -> CALL: commonReserve -> RETURN -> EXIT -> EXIT",
    "log": "[DEBUG] Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource"
  },
  "234641e7_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.s3a.tools.MarkerToolOperations:listObjects → WHILE:listing.hasNext() → WHILE_COND:listing.hasNext() → CALL:org.apache.hadoop.fs.RemoteIterator:hasNext() → CALL:org.apache.hadoop.fs.RemoteIterator:next() → CALL:org.slf4j.Logger:debug → LOG:LOG.DEBUG:\"{}\", key → WHILE_EXIT → LOG:LOG.DEBUG:\"Listing summary {}\", listing → IF_TRUE:verbose → CALL:println → RETURN → EXIT",
    "log": "[DEBUG] \"{}\", key [DEBUG] Listing summary {}"
  },
  "234641e7_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.s3a.tools.MarkerToolOperations:listObjects → WHILE:listing.hasNext() → WHILE_COND:listing.hasNext() → CALL:org.apache.hadoop.fs.RemoteIterator:hasNext() → CALL:org.apache.hadoop.fs.RemoteIterator:next() → CALL:org.slf4j.Logger:debug → LOG:LOG.DEBUG:\"{}\", key → WHILE_EXIT → LOG:LOG.DEBUG:\"Listing summary {}\", listing → IF_FALSE:verbose → RETURN → EXIT",
    "log": "[DEBUG] \"{}\", key [DEBUG] Listing summary {}"
  },
  "275b61a0_1": {
    "exec_flow": "ENTRY → CALL:writeLock.lock → TRY → IF_TRUE:rmContainer==null → CALL:rmContext.getDispatcher().getEventHandler().handle → RETURN → EXIT",
    "log": "<!-- No logs from parent path -->"
  },
  "275b61a0_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Processing {event.getContainerId()} of type {event.getType()} → CALL:writeLock.lock → TRY → TRY → CALL:stateMachine.doTransition → EXCEPTION:doTransition → CATCH:InvalidStateTransitionException e → LOG:LOG.ERROR:Can't handle this event at current state → CALL:onInvalidStateTransition → CALL:writeLock.unlock → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Processing {event.getContainerId()} of type {event.getType()}</message> </log> <log> <level>ERROR</level> <message>Can't handle this event at current state</message> </log>"
  },
  "275b61a0_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Processing {event.getContainerId()} of type {event.getType()} → CALL:writeLock.lock → TRY → TRY → CALL:stateMachine.doTransition → IF_TRUE:oldState != getState() → LOG:LOG.INFO:{event.getContainerId()} Container Transitioned from {oldState} to {getState()} → CALL:writeLock.unlock → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Processing {event.getContainerId()} of type {event.getType()}</message> </log> <log> <level>INFO</level> <message>{event.getContainerId()} Container Transitioned from {oldState} to {getState()}</message> </log>"
  },
  "275b61a0_4": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Processing {event.getContainerId()} of type {event.getType()} → CALL:writeLock.lock → TRY → TRY → CALL:stateMachine.doTransition → IF_FALSE:oldState != getState() → CALL:writeLock.unlock → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Processing {event.getContainerId()} of type {event.getType()}</message> </log>"
  },
  "275b61a0_5": {
    "exec_flow": "ENTRY → CALL:writeLock.lock → TRY → CALL:stateMachine.doTransition → IF_TRUE:oldState!=getState() → LOG:LOG.INFO:[SERVICE] Transitioned from {} to {} on {} event., oldState, getState(), event.getType() → CALL:writeLock.unlock → EXIT",
    "log": "<log> <level>INFO</level> <message>[SERVICE] Transitioned from {} to {} on {} event.</message> </log>"
  },
  "6ba18247_1": {
    "exec_flow": "ENTRY→TRY→WHILE: bytesRemaining > 0→WHILE_COND: bytesRemaining > 0→CALL:in.read→CALL:out.write→WHILE_EXIT→IF_TRUE: close→CALL: out.close→CALL: in.close→CALL:closeStream(out)→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→CALL:closeStream(in)→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "6ba18247_2": {
    "exec_flow": "ENTRY→TRY→WHILE: bytesRemaining > 0→WHILE_COND: bytesRemaining > 0→CALL:in.read→CALL:out.write→WHILE_EXIT→IF_FALSE: close→CALL:closeStream(out)→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→CALL:closeStream(in)→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "dd3ea435_1": {
    "exec_flow": "ENTRY→FOREACH:interceptorList→FOREACH_EXIT→LOG:LOGGER.DEBUG→WHILE:srcMatcher.find()→CALL:replaceRegexCaptureGroupInPath→IF_FALSE:groupValue==null→CALL:replaceRegexCaptureGroupInPath→LOG:org.slf4j.Logger:debug→WHILE_EXIT→IF_FALSE:0==mappedCount→FOREACH:interceptorList→FOREACH_EXIT→CALL:buildResolveResultForRegexMountPoint→TRY→IF_TRUE:targetFs==null→CALL:org.slf4j.Logger:error→RETURN→EXIT",
    "log": "<log> [DEBUG] Path to resolve: {pathStrToResolve}, srcPattern: {getSrcPathRegex()} </log> <log> [DEBUG] parsedDestPath value is:${parsedDestPath} </log> <log> [ERROR] Not able to initialize target file system. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null. </log>"
  },
  "dd3ea435_2": {
    "exec_flow": "ENTRY→FOREACH:interceptorList→FOREACH_EXIT→LOG:LOGGER.DEBUG→WHILE:srcMatcher.find()→CALL:replaceRegexCaptureGroupInPath→IF_FALSE:groupValue==null→CALL:replaceRegexCaptureGroupInPath→LOG:org.slf4j.Logger:debug→WHILE_EXIT→IF_TRUE:0==mappedCount→TRY→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→RETURN→EXIT",
    "log": "<log> [DEBUG] Path to resolve: {pathStrToResolve}, srcPattern: {getSrcPathRegex()} </log> <log> [DEBUG] parsedDestPath value is:${parsedDestPath} </log> <log> [ERROR] Got Exception while build resolve result. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null. </log>"
  },
  "5dfdfdfd_1": {
    "exec_flow": "ENTRY→LOG:LOG.INFO:Starting plan for Node : {}:{}, node.getDataNodeName(), node.getDataNodePort() →WHILE:node.isBalancingNeeded(this.threshold) →CALL:Preconditions.checkNotNull →CALL:Preconditions.checkNotNull →CALL:Preconditions.checkNotNull →WHILE:currentSet.isBalancingNeeded(this.threshold) →CALL:removeSkipVolumes →CALL:getSortedQueue →CALL:first →CALL:getSortedQueue →CALL:last →IF:!lowVolume.isSkip() && !highVolume.isSkip() →CALL:computeMove →ENDIF →CALL:applyStep →IF:nextStep != null →LOG:LOG.DEBUG:Step : , nextStep →CALL:addStep →ENDIF →WHILE_COND:currentSet.isBalancingNeeded(this.threshold)→WHILE_EXIT →LOG:LOG.INFO:Disk Volume set {} - Type : {} plan completed., currentSet.getSetID(), currentSet.getVolumes().get(0).getStorageType() →CALL:setNodeName →CALL:setNodeUUID →CALL:setTimeStamp →CALL:setPort →WHILE_EXIT →LOG:LOG.INFO:Compute Plan for Node : {}:{} took {} ms, node.getDataNodeName(), node.getDataNodePort(), endTime - startTime →RETURN →EXIT",
    "log": "[INFO] Starting plan for Node : {}:{}, node.getDataNodeName(), node.getDataNodePort() [DEBUG] Step : {} [INFO] Disk Volume set {} - Type : {} plan completed. [INFO] Compute Plan for Node : {}:{} took {} ms, node.getDataNodeName(), node.getDataNodePort(), endTime - startTime"
  },
  "b63f7992_1": {
    "exec_flow": "ENTRY→CALL:main→TRY→CALL:parseOptions→CALL:checkConf→CATCH:MissingArgumentException→CATCH:ParseException→CALL:terminate→EXCEPTION:terminate→RETURN→EXIT",
    "log": "[WARN] options parsing failed: + e.getMessage()"
  },
  "b63f7992_2": {
    "exec_flow": "ENTRY→CALL:main→TRY→CALL:parseOptions→CALL:checkConf→CALL:setTokensFile→CALL:addResource→CALL:terminate→EXIT",
    "log": "[DEBUG] setting conf tokensFile: [fileName value]"
  },
  "a3a8ddeb_1": {
    "exec_flow": "ENTRY → IF_TRUE: service != null → CALL: stop → CATCH_EXCEPTION → CALL: noteFailure → CALL: log.warn → EXIT",
    "log": "<log>org.apache.hadoop.service.ServiceOperations:stopQuietly - WARN When stopping the service {service_name}</log> <log>LOG.debug(\"noteFailure\", exception)</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception)</log>"
  },
  "a3a8ddeb_2": {
    "exec_flow": "ENTRY → IF_TRUE: oldState != newState → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → RETURN → EXIT",
    "log": "<log>[DEBUG] Service: {} entered state {}</log>"
  },
  "a3a8ddeb_3": {
    "exec_flow": "ENTRY → IF_TRUE: config != existingConfig → CALL: org.slf4j.Logger:debug(java.lang.String) → CALL: setConfig → EXIT",
    "log": "<log>[DEBUG] Config has been overridden during init</log>"
  },
  "ab551f41_1": {
    "exec_flow": "ENTRY -> IF_TRUE:!isRMActive() -> LOG:LOG.WARN:createFailureLog -> CALL:throwStandbyException -> EXIT",
    "log": "[DEBUG] ResourceManager is not active. Can not [message] [WARN] createFailureLog"
  },
  "ab551f41_2": {
    "exec_flow": "ENTRY -> CALL:Groups.getUserToGroupsMappingService(getConfiguration(new Configuration(false), YarnConfiguration.CORE_SITE_CONFIGURATION_FILE)).refresh -> CALL:org.apache.hadoop.yarn.server.resourcemanager.AdminService:checkAcls -> ENTRY -> CALL:verifyAdminAccess -> ENTRY -> TRY -> CALL:getCurrentUser -> IF_TRUE:!authorizer.isAdmin(user) -> LOG:LOG.WARN:User {user.getShortUserName()} doesn't have permission to call '{method}' -> CALL:RMAuditLogger.logFailure -> THROW:new AccessControlException(\"User \" + user.getShortUserName() + \" doesn't have permission to call '\" + method + \"'\") -> EXIT -> IF_TRUE:LOG.isTraceEnabled() -> LOG:LOG.TRACE:{method} invoked by user {user.getShortUserName()} -> RETURN -> EXIT -> CALL:org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess -> CALL:org.slf4j.Logger:isInfoEnabled() -> IF_TRUE:LOG.isInfoEnabled() -> CALL:org.slf4j.Logger:info(java.lang.String) -> CALL:createSuccessLog(user, operation, target, null, null, null, null) -> CALL:org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshUserToGroupsMappings -> CALL:org.apache.hadoop.yarn.server.resourcemanager.AdminService:checkRMStatus -> EXIT",
    "log": "[WARN] User {user.getShortUserName()} doesn't have permission to call '{method}' [TRACE] {method} invoked by user {user.getShortUserName()} [INFO] createSuccessLog(user, operation, target, null, null, null, null)"
  },
  "aae05c71_1": {
    "exec_flow": "ENTRY → CALL: checkNNStartup → IF_FALSE: snapshotNewName == null || snapshotNewName.isEmpty() → CALL: checkOperation → CALL: incrRenameSnapshotOps → IF_FALSE: cacheEntry != null && cacheEntry.isSuccess() → TRY → ENTRY → CALL: checkOperation → CALL: FSPermissionChecker.setOperationType → TRY → CALL: writeLock → CALL: checkOperation → CALL: checkNameNodeSafeMode → CALL: FSDirSnapshotOp:renameSnapshot → CALL: writeUnlock → CALL: logSync → LOG: logAuditEvent(true, ...) → CATCH: AccessControlException ace → LOG: logAuditEvent(false, ...) → RetriableException thrown if HA is enabled and active in safe mode, SafeModeException thrown if NameNode is in SafeMode. → EXIT",
    "log": "<log>[TRACE] Execution trace</log> <log>[INFO] Audit success: renameSnapshot</log> <log>RetriableException thrown if HA is enabled and active in safe mode.</log> <log>SafeModeException thrown if NameNode is in SafeMode.</log> <log>[DEBUG] RenameSnapshotOp created</log> <log>[DEBUG] logRpcIds executed</log> <log>[DEBUG] logEdit executed</log> <log>[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}</log> <log>[ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}</log> <log>[DEBUG] Exception in closing {}</log> <log>[INFO] Audit failed: renameSnapshot</log>"
  },
  "682acd20_1": {
    "exec_flow": "<step>ENTRY→CALL:discardSegments</step> <step>TRY→CALL:waitFor</step> <step>WHILE: true→WHILE_COND: true→CALL: restartQuorumStopWatch</step> <step>CALL: checkAssertionErrors</step> <step>IF_FALSE: minResponses > 0 && countResponses() >= minResponses</step> <step>IF_FALSE: minSuccesses > 0 && countSuccesses() >= minSuccesses</step> <step>IF_FALSE: maxExceptions >= 0 && countExceptions() > maxExceptions</step> <step>IF_TRUE: now > nextLogTime</step> <step>[INFO] Waited X ms (timeout=Y ms) for a response for Z...</step> <step>[WARN] Waited A ms (timeout=B ms) for a response for C...</step> <step>CALL: QuorumJournalManager.LOG.info</step> <step>IF_TRUE: rem <= 0</step> <step>[INFO] Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of {pauseTime} ms.</step> <step>IF_FALSE: timeoutIncrease > 0</step> <step>THROW: new TimeoutException()</step> <step>EXIT</step>",
    "log": "[INFO] Waited X ms (timeout=Y ms) for a response for Z... [WARN] Waited A ms (timeout=B ms) for a response for C... [INFO] Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of {pauseTime} ms. [INFO] Discard the EditLog files, the given start txid is {startTxId} [INFO] Trash the EditLog file {elf}"
  },
  "682acd20_2": {
    "exec_flow": "<step>ENTRY→CALL:matchEditLogs</step> <step>[ERROR] Edits file + f + has improperly formatted + transaction ID</step> <step>[ERROR] In-progress edits file + f + has improperly formatted transaction ID</step>",
    "log": "[ERROR] Edits file + f + has improperly formatted + transaction ID [ERROR] In-progress edits file + f + has improperly formatted transaction ID"
  },
  "41cbf1b8_1": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → CALL:debug → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT → IF_TRUE:subject==null||subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:Creating new Groups object → NEW:Groups → CALL:<init> RETURN → EXIT → ENTRY → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → TRY → CALL:doAs → NEW:PrivilegedExceptionAction → CALL:get → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>WARN</level> <template>Unable to get user name. Fall back to system property user.name</template> </log_entry> <log_entry> <template>getLoginUser</template> </log_entry> <log_entry> <level>ERROR</level> <template>ex.toString()</template> </log_entry> <log_entry> <level>ERROR</level> <template>Failed to get the AbstractFileSystem for path: + uri</template> </log_entry>"
  },
  "c1258d0c_1": {
    "exec_flow": "ENTRY → TRY → CALL:getCurrentUser → RETURN → EXIT → CALL:verifyUserAccessForRMApp → ENTRY → IF_TRUE: application == null → CALL: RMAuditLogger.logFailure → THROW: new ApplicationNotFoundException(\"Application with id '\" + applicationId + \"' doesn't exist in RM. Please check that the job submission was successful.\") → EXIT",
    "log": "RMAuditLogger.logFailure(callerUGI.getUserName(), operation, \"UNKNOWN\", \"ClientRMService\", \"Trying to \" + operation + \" of an absent application\", applicationId)"
  },
  "c1258d0c_2": {
    "exec_flow": "ENTRY → CALL:getCallerUgi → CALL:verifyUserAccessForRMApp → ENTRY → IF_FALSE: application == null → IF_TRUE: needCheckAccess → IF_TRUE: !checkAccess(callerUGI, application.getUser(), accessType, application) → CALL: RMAuditLogger.logFailure → THROW: RPCUtil.getRemoteException(new AccessControlException(\"User \" + callerUGI.getShortUserName() + \" cannot perform operation \" + accessType.name() + \" on \" + applicationId)) → EXIT",
    "log": "RMAuditLogger.logFailure(callerUGI.getShortUserName(), operation, \"User doesn't have permissions to \" + accessType.toString(), \"ClientRMService\", AuditConstants.UNAUTHORIZED_USER, applicationId)"
  },
  "c1258d0c_3": {
    "exec_flow": "ENTRY → TRY → CALL:getCurrentUser → CATCH:IOException → CALL:info → CALL:logFailure → CALL:getRemoteException → THROW → EXIT → CALL:verifyUserAccessForRMApp → ENTRY → IF_TRUE: application == null → CALL: RMAuditLogger.logFailure → THROW: new ApplicationNotFoundException(\"Application with id '\" + applicationId + \"' doesn't exist in RM. Please check that the job submission was successful.\") → EXIT",
    "log": "[INFO] Error getting UGI RMAuditLogger.logFailure(\"UNKNOWN\", operation, \"UNKNOWN\", \"ClientRMService\", \"Error getting UGI\", applicationId)"
  },
  "0e392301_1": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "0e392301_2": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_TRUE: rpcMonitor != null→CALL: rpcMonitor.proxyOp→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "0e392301_3": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_TRUE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "0e392301_4": {
    "exec_flow": "ENTRY→IF_FALSE: locations.isEmpty()→IF_FALSE: locations.size() == 1 && timeOutMs <= 0→FOREACH: locations→FOREACH_EXIT→IF_FALSE: rpcMonitor != null→TRY→IF_FALSE: timeOutMs > 0→CALL: invokeAll→FOR_INIT→FOR_COND: i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "550f610f_1": {
    "exec_flow": "ENTRY→IF_TRUE: fields == null || fields.isEmpty()→CALL:add→CALL:mergeFilters→CALL:doGetUri→ENTRY→IF_TRUE:resp==null||resp.getStatusInfo().getStatusCode()!=ClientResponse.Status.OK.getStatusCode()→LOG:LOG.ERROR:msg→CALL:org.slf4j.Logger:error→THROW:new IOException(msg)→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Fields parameter is empty, defaulting to INFO [ERROR] Response from the timeline reader server is ..."
  },
  "550f610f_2": {
    "exec_flow": "ENTRY→IF_TRUE: fields == null || fields.isEmpty()→CALL:add→CALL:mergeFilters→CALL:doGetUri→ENTRY→IF_FALSE:resp==null||resp.getStatusInfo().getStatusCode()!=ClientResponse.Status.OK.getStatusCode()→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Fields parameter is empty, defaulting to INFO"
  },
  "550f610f_3": {
    "exec_flow": "ENTRY→IF_FALSE: fields == null || fields.isEmpty()→CALL:add→CALL:mergeFilters→CALL:doGetUri→ENTRY→IF_TRUE:resp==null||resp.getStatusInfo().getStatusCode()!=ClientResponse.Status.OK.getStatusCode()→LOG:LOG.ERROR:msg→CALL:org.slf4j.Logger:error→THROW:new IOException(msg)→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Fields parameter provided [ERROR] Response from the timeline reader server is ..."
  },
  "550f610f_4": {
    "exec_flow": "ENTRY→IF_FALSE: fields == null || fields.isEmpty()→CALL:add→CALL:mergeFilters→CALL:doGetUri→ENTRY→IF_FALSE:resp==null||resp.getStatusInfo().getStatusCode()!=ClientResponse.Status.OK.getStatusCode()→RETURN→EXIT→RETURN→EXIT",
    "log": "[DEBUG] Fields parameter provided"
  },
  "550f610f_5": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManager:getApplicationAttempt→CATCH→CALL:org.slf4j.Logger:error→THROW→EXIT",
    "log": "[ERROR] ${e.getMessage()}"
  },
  "744f0248_1": {
    "exec_flow": "ENTRY → TRY → CALL:container.nmClientAsync.getClient().stopContainer → TRY → CALL:container.nmClientAsync.getCallbackHandler().onContainerStopped → EXCEPTION:onContainerStopped → CATCH:Throwable thr → CALL:org.slf4j.Logger:info → RETURN → EXIT",
    "log": "[INFO] Unchecked exception is thrown from onContainerStopped for Container {event.getContainerId()}, thr [ERROR] onContainerStopped received unknown container ID: [ContainerId]"
  },
  "744f0248_2": {
    "exec_flow": "ENTRY → TRY → CALL:onStopContainerError → CATCH:Throwable → LOG:LOG.INFO → RETURN → EXIT",
    "log": "[ERROR] Exception raised while stopping container [ERROR] onStopContainerError received unknown containerID: + containerId [ERROR] Failed to stop NameNode container ID + containerId [ERROR] Failed to stop DataNode Container + containerId [INFO] Unchecked exception is thrown from onStopContainerError for Container event.getContainerId()"
  },
  "744f0248_3": {
    "exec_flow": "<step> <description>Check if ContainerManagementProtocolProxyData is present and refresh if token is updated</description> <details>If !proxy.token.getIdentifier().equals(nmTokenCache.getToken(containerManagerBindAddr).getIdentifier()) and !proxy.scheduledForClose, removeProxy; else wait()</details> </step> <step> <description>If proxy is null, create new and add to LRU cache</description> <details>proxy = new ContainerManagementProtocolProxyData(...) and addProxyToCache(containerManagerBindAddr, proxy)</details> </step> <step> <description>Synchronize and check container state</description> <details>If startedContainer is not null, synchronized and check if state is RUNNING</details> </step> <step> <description>Stop container internally</description> <details>Execute stopContainerInternal(containerId, nodeId)</details> </step> <step> <description>Update container state and remove from cache if successful</description> <details>Set state to COMPLETE and remove container from startedContainers</details> </step>",
    "log": "<log>DEBUG - Refreshing proxy as NMToken got updated for node : containerManagerBindAddr</log>"
  },
  "aad12832_1": {
    "exec_flow": "<flow_step>Parent.ENTRY</flow_step> <flow_step>[VIRTUAL_CALL]</flow_step> <flow_step>ENTRY→CALL:schemeFromPath→[ENTRY→IF_TRUE: scheme == null→IF_TRUE: fs != null→CALL:getScheme→CALL:getUri→RETURN→EXIT]→CALL:authorityFromPath→LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString→FOREACH:flattenedPatterns→CALL:fixRelativePart→LOG:LOG.DEBUG:Pattern: {}→CALL:getPathComponents→TRY→CALL:FileSystem:listStatus→EXCEPTION:FileNotFoundException→CALL:Logger:debug→RETURN→EXIT</flow_step> <flow_step>CALL:finished</flow_step> <flow_step>IF_TRUE:logAtInfo</flow_step> <flow_step>CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)</flow_step> <flow_step>LOG:[INFO] {} </flow_step> <flow_step>EXIT</flow_step> <flow_step>IF_FALSE:logAtInfo</flow_step> <flow_step>IF_TRUE:log.isDebugEnabled()</flow_step> <flow_step>CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)</flow_step> <flow_step>LOG:[DEBUG] {} </flow_step>",
    "log": "<log>org.slf4j.Logger:debug - [LOG] Execution started with parameters</log> <log>[INFO] glob path pattern</log> <log>[INFO] {}</log> <log>[DEBUG] {}</log> <log>[DEBUG] Filesystem glob {}</log> <log>[DEBUG] Pattern: {}</log> <log>[DEBUG] listStatus({}) failed; returning empty array</log>"
  },
  "130e9dec_1": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→CALL: noteFailure→THROW: ServiceStateException.convert(e)→CALL: notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "130e9dec_2": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→TRY→CALL: serviceStop→FINALLY→terminationNotification.set(true)→SYNC: terminationNotification→terminationNotification.notifyAll()→CALL: notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}, this, e→EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "130e9dec_3": {
    "exec_flow": "ENTRY→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners→EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop()"
  },
  "020b96f2_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → EXIT → CALL: org.apache.hadoop.fs.FileContext:getFSofPath → CALL: org.apache.hadoop.fs.FSLinkResolver:next → TRY → FOR_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration [INFO] Resolving file system path [DEBUG] Attempting symlink resolution"
  },
  "020b96f2_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "020b96f2_3": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → IF_TRUE: confUmask != null → CALL: UmaskParser.getUMask() → LOG: LOG.warn(...) → EXIT",
    "log": "WARN: Unable to parse configuration {@code UMASK_LABEL} with value {@code confUmask} as decimal or octal or symbolic umask. WARN: An IllegalArgumentException was thrown"
  },
  "8ba261e4_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→RETURN→EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "8ba261e4_2": {
    "exec_flow": "ENTRY→CALL: LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "[INFO] message"
  },
  "8ba261e4_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "8ba261e4_4": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_TRUE: overlay != null → CALL:putAll → IF_FALSE: backup != null → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "8ba261e4_5": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT → IF_FALSE: overlay != null → EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "18c157a8_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "18c157a8_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0 → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Path: [{}] is a directory. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "18c157a8_3": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_FALSE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0 → THROW:FileNotFoundException → EXIT",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log>"
  },
  "18c157a8_4": {
    "exec_flow": "ENTRY → TRY → CALL: checkNotClosed → IF_FALSE: pos < 0 → IF_TRUE: this.pos > pos → IF_TRUE: in instanceof Seekable → CALL: ((Seekable) in).seek → LOG:LOG.DEBUG: Seek to position {}. Bytes skipped {}, pos, this.pos → EXCEPTION: debug → CATCH: IOException e → IF_TRUE: innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException) → THROW: new FileNotFoundException(String.format(\"%s is not found\", key)) → EXIT",
    "log": "<log>[DEBUG] Seek to position {}. Bytes skipped {}, pos, this.pos</log>"
  },
  "18c157a8_5": {
    "exec_flow": "<![CDATA[ ENTRY→IF_TRUE: key.length() == 0→NEW: OSSFileStatus→RETURN→EXIT→IF_FALSE: key.length() != 0→CALL:getFileStatus→TRY→CALL:setLogEnabled→CALL:getObjectMetadata(request)→EXCEPTION:getObjectMetadata→CATCH:OSSException osse→CALL:LOG.debug→RETURN→EXIT→IF_TRUE: fileStatus.isDirectory()→THROW: FileNotFoundException→RETURN→EXIT→IF_FALSE→NEW: FSDataInputStream→RETURN→EXIT ]]>",
    "log": "<log>[DEBUG] Exception thrown when get object meta: + key + , exception: + osse</log>"
  },
  "61875063_1": {
    "exec_flow": "ENTRY→CALL:checkNNStartup→CALL:rpcServer.checkOperation→IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.startOp→IF_TRUE:LOG.isDebugEnabled → CALL:org.slf4j.Logger:isDebugEnabled()→LOG:LOG.DEBUG:Proxying operation:{}→CALL:opCategory.set→IF_TRUE:op==OperationCategory.UNCHECKED OR op==OperationCategory.READ→RETURN→EXIT→CALL:rpcServer.isInvokeConcurrent→IF_TRUE:rpcServer.isInvokeConcurrent→CALL:rpcServer.getLocationsForPath→LOG:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.getLocationsForPath - AccessControlException thrown: {reasons}→CALL:invokeConcurrent →ENTRY→IF_FALSE:locations.isEmpty()→IF_FALSE:locations.size() == 1 && timeOutMs <= 0→FOREACH:locations→FOREACH_EXIT→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.proxyOp→TRY→IF_TRUE:timeOutMs > 0→CALL:invokeAll→FOR_INIT→FOR_COND:i < futures.size()→FOR_EXIT→RETURN→EXIT →EXIT→CALL:FSPermissionChecker.setOperationType→TRY→CALL:writeLock→TRY→CALL:checkOperation→CALL:checkNameNodeSafeMode→CALL:FSDirAttrOp.setReplication→CALL:writeUnlock→IF_TRUE:success→CALL:getEditLog→CALL:logSync→CALL:logAuditEvent→RETURN→EXIT",
    "log": "[INFO] Audit Event: setReplication [DEBUG] Decreasing replication from {} to {} for {} [DEBUG] Adding set replication record to edit log [DEBUG] Proxying operation: {} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [INFO] Checking operation category WRITE [DEBUG] Concurrent invocation on src [INFO] Checking invocation result [DEBUG] User {} NN {} is using connection {} [ERROR] Unexpected exception {} proxying {} to {} [ERROR] Cannot get method {} with types {} from {} [ERROR] Cannot access method {} with types {} from {} org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.getLocationsForPath - AccessControlException thrown: {reasons}"
  },
  "61875063_2": {
    "exec_flow": "ENTRY→IF_TRUE:subclusterResolver instanceof MountTableResolver→TRY→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→RETURN→EXIT",
    "log": "[ERROR] Cannot get mount point"
  },
  "549867d8_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→CALL:handleDeprecation→FOREACH:names→LOG:Handling deprecation for (String)item→CALL:getProps→CALL:substituteVars→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→RETURN→EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "bfa7e3c7_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → LOG: Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG: Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "8f0ecfee_1": {
    "exec_flow": "ENTRY → SYNC: elector → SYNC: this → IF_TRUE: remainingDelay > 0 → IF_TRUE: healthy → LOG: LOG.INFO → CALL: scheduleRecheck → RETURN → EXIT",
    "log": "[INFO] Would have joined master election, but this node is prohibited from doing so for ... more ms"
  },
  "8f0ecfee_2": {
    "exec_flow": "ENTRY → SYNC: elector → SYNC: this → IF_FALSE: remainingDelay > 0 → SWITCH: lastHealthState → CASE: INITIALIZING → LOG: LOG.INFO → CALL: quitElection → EXIT",
    "log": "[INFO] Ensuring that ... does not participate in active master election"
  },
  "8f0ecfee_3": {
    "exec_flow": "ENTRY → SYNC: elector → SYNC: this → IF_FALSE: remainingDelay > 0 → SWITCH: lastHealthState → CASE: SERVICE_NOT_RESPONDING → LOG: LOG.INFO → CALL: quitElection → EXIT",
    "log": "[INFO] Quitting master election for ... and marking that fencing is necessary"
  },
  "1e8bb39e_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:key.length()==0→CALL:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isDirectory()→LOG:LOG.DEBUG:Path {} is a folder., f.toString()→CALL:conditionalRedoFolderRename→THROW:FileNotFoundException(absolutePath+\": No such file or directory.\")→RETURN→EXIT ENTRY→TRY→CALL:getFileStatus→IF_FALSE:key.length()==0→CALL:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isDirectory()→LOG:LOG.DEBUG:Path {} is a folder., f.toString()→CALL:conditionalRedoFolderRename→RETURN→EXIT ENTRY→TRY→CALL:getFileStatus→IF_FALSE:key.length()==0→CALL:retrieveMetadata→IF_TRUE:meta!=null→IF_FALSE:meta.isDirectory()→LOG:LOG.DEBUG:Found the path: {} as a file., f.toString()→CALL:updateFileStatusPath→RETURN→EXIT",
    "log": "<log>[DEBUG] Getting the file status for {f}</log> <log>[DEBUG] Path {f} is a folder.</log> <log>[DEBUG] Found the path: {f} as a file.</log>"
  },
  "1a64bf5d_1": {
    "exec_flow": "ENTRY→PRINT_USAGE→RETURN",
    "log": "<!-- No logs, inherited child path --> [DEBUG] Failing attempt with id: attId [INFO] Attempt failed for application: appId [INFO] Failing application attempt + attemptId [INFO] Attempt to fail application initiated with ResourceManager"
  },
  "1a64bf5d_2": {
    "exec_flow": "ENTRY→PRINT_USAGE→RETURN",
    "log": "<!-- No logs, inherited child path --> [DEBUG] Failing attempt with id: attId [INFO] Attempt failed for application: appId [INFO] Failing application attempt + attemptId [INFO] Attempt to fail application routed through ClientRMService"
  },
  "9ec376aa_1": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:org.slf4j.Logger:debug(java.lang.String)→CALL:setConfig→CALL:requireNonNull→EXIT",
    "log": "[DEBUG] Config has been overridden during init"
  },
  "a3dd3d46_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.datanode.FileIoProvider:getFileInputStream → CALL:readDataChecksum → CALL:getIoFileBufferSize → TRY → IF_TRUE:parentFile != null → IF_TRUE:!parentFile.mkdirs() && !parentFile.isDirectory() → THROW:new IOException(\"Destination '\" + parentFile + \"' directory cannot be created\") → CALL:org.apache.hadoop.io.IOUtils:closeStream → ENTRY → IF_TRUE: stream != null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "DEBUG: Handling deprecation for all properties in config… DEBUG: Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration DEBUG: Exception in closing {} WARN: Unexpected meta-file version for + name + : version in file is + header.getVersion() + but expected version is + VERSION"
  },
  "a3dd3d46_2": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.hdfs.server.datanode.FileIoProvider:getFileInputStream → CALL:readDataChecksum → CALL:getIoFileBufferSize → TRY → IF_TRUE:parentFile != null → CALL:org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:getDataInputStream → FOR → CALL:calculateChunkedSums → CALL:write → CALL:calculateChunkedSums → CALL:write → CALL:org.apache.hadoop.io.IOUtils:closeStream → ENTRY → IF_TRUE: stream != null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "INFO: message WARN: Unexpected meta-file version for + name + : version in file is + header.getVersion() + but expected version is + VERSION"
  },
  "a3dd3d46_3": {
    "exec_flow": "ENTRY → CALL:checkDiskErrorAsync → CALL:volumeChecker.checkVolume → CALL:lambda$0 → EXIT",
    "log": "WARN: checkDiskErrorAsync callback got {} failed volumes: {} DEBUG: checkDiskErrorAsync: no volume failures detected"
  },
  "a3dd3d46_4": {
    "exec_flow": "ENTRY → CALL:checkDiskErrorAsync → CALL:volumeChecker.checkVolume → CALL:lambda$0 → EXIT",
    "log": "DEBUG: checkDiskErrorAsync: no volume failures detected"
  },
  "9ee3985f_1": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:writeLock.lock → TRY → WHILE → CALL:swapContainer → CALL:removeFromOutstandingUpdate → CALL:updateContainerAndNMToken → WHILE_EXIT → WHILE → CALL:asyncContainerRelease → LOG: Processing {} of type {}, event.getNodeId(), event.getType() → CALL: this.rmContext.getDispatcher → CALL: getEventHandler → CALL: org.apache.hadoop.yarn.event.EventHandler:handle → EXCEPTION: doTransition → CATCH: InvalidStateTransitionException e → LOG: Can't handle this event at current state, e → LOG: Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState → IF_TRUE: oldState != getState() → LOG: nodeId + Node Transitioned from + oldState + to + getState() → CALL: writeLock.unlock → WHILE_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Processing {} of type {} [ERROR] Can't handle this event at current state [ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState [INFO] nodeId + Node Transitioned from + oldState + to + getState()"
  },
  "614c82b6_1": {
    "exec_flow": "ENTRY → IF_TRUE: defaultFsUri.getScheme() != null && !defaultFsUri.getScheme().trim().isEmpty() → CALL: getFileContext → ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → CALL:initialize → ENTRY → LOG:Handling deprecation for all properties in config... → IF_TRUE:props != null → IF_TRUE:loadDefaults && fullReload → FOREACH:defaultResources → CALL:loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND:i < resources.size() → CALL:loadResource → FOR_EXIT → CALL:putAll → IF_TRUE:backup != null → FOREACH:overlay.entrySet() → FOREACH_EXIT → CALL:addTags → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → CALL:isDebugEnabled → LOG:LOG.DEBUG: Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT → EXIT → IF_TRUE:loginUserRef == null → DO_WHILE → IF_TRUE:loginUserRef.compareAndSet(null, newLoginUser) → CALL:createLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser == null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → EXCEPTION:readTokenStorageStream → CATCH:IOException ioe → CALL:IOUtils.cleanupWithLogger → THROW:new IOException(\"Exception reading \" + filename, ioe) → EXIT → CALL:debug → CALL:loginUser.spawnAutoRenewalThreadForUserCreds(false) → DO_COND:loginUser == null → DO_EXIT → RETURN → EXIT → RETURN → EXIT",
    "log": "<log_entry>[INFO] message</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for {item}</log_entry> <log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Reading credentials from location {}</log_entry> <log_entry>[DEBUG] Loaded {} tokens from {}</log_entry> <log_entry>[INFO] Token file {} does not exist</log_entry> <log_entry>[INFO] Cleaning up resources</log_entry> <log_entry>[DEBUG] Failure to load login credentials</log_entry> <log_entry>[WARN] auth_to_local rule mechanism not set. Using default of DEFAULT_MECHANISM</log_entry> <log_entry>[DEBUG] UGI loginUser: {}</log_entry>"
  },
  "614c82b6_2": {
    "exec_flow": "ENTRY→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<AbstractFileSystem>→CALL:get→RETURN→EXIT",
    "log": "<log_entry>[ERROR] ex.toString()</log_entry>"
  },
  "614c82b6_3": {
    "exec_flow": "ENTRY→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<AbstractFileSystem>→CALL:get→EXCEPTION:RuntimeException→CALL:error→EXIT",
    "log": "<log_entry>[ERROR] Failed to get the AbstractFileSystem for path: + uri</log_entry>"
  },
  "614c82b6_4": {
    "exec_flow": "ENTRY→TRY→CALL:doAs→NEW:PrivilegedExceptionAction<AbstractFileSystem>→EXCEPTION:InterruptedException→CALL:error→EXIT",
    "log": "<log_entry>[ERROR] ex.toString()</log_entry>"
  },
  "614c82b6_5": {
    "exec_flow": "ENTRY → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log>[WARN] Exception while notifying listeners of {}</log>"
  },
  "602037e3_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED → TRY → CALL: serviceStop → FINALLY → terminationNotification.set(true) → SYNC: terminationNotification → terminationNotification.notifyAll() → CALL: notifyListeners → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [WARN] Exception while notifying listeners of {}"
  },
  "602037e3_2": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: LOG.DEBUG: Ignoring re-entrant call to stop() → CALL: notifyListeners → TRY → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "5d2ac0f6_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → SYNC:YarnAuthorizationProvider.class → IF_TRUE:authorizer==null → CALL:getClass → ENTRY → IF_TRUE:props!=null → CALL:loadResources → IF_FALSE:overlay!=null → EXIT → CALL:newInstance → CALL:init → LOG:info → RETURN → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration [INFO] authorizerClass.getName() + \" is instantiated.\""
  },
  "5d2ac0f6_2": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.util.resource.Resources:componentwiseMax(org.apache.hadoop.yarn.api.records.Resource, org.apache.hadoop.yarn.api.records.Resource)→TRY→LOG:LOG.WARN:Resource is missing:+ye.getMessage()→CONTINUE→RETURN→EXIT",
    "log": "[WARN] Resource is missing: + ye.getMessage()"
  },
  "13cf80eb_1": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <virtual_call>org.apache.hadoop.fs.FileSystem:setDefaultUri</virtual_call> <call>setConf</call> <call>getTrimmed</call> <call>handleDeprecation</call> <foreach>names</foreach> <call>getProps</call> <call>substituteVars</call> <foreach_exit/> <call>LOG_DEPRECATION.info</call> <exit/>",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[INFO] message</log_entry>"
  },
  "13cf80eb_2": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <if_true>props != null</if_true> <call>loadResources</call> <if_true>loadDefaults && fullReload</if_true> <foreach>defaultResources</foreach> <call>loadResource</call> <for_init/> <for_cond>i < resources.size()</for_cond> <call>loadResource</call> <call>addTags</call> <if_true>overlay != null</if_true> <foreach>overlay.entrySet()</foreach> <if_true>source != null</if_true> <put>updatingResource</put> <log>[DEBUG] Handling deprecation for all properties in config...</log> <foreach>log>[DEBUG] Handling deprecation for (String)item</foreach>",
    "log": "<log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>FOREACH: [DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "13cf80eb_3": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <new>HdfsConfiguration</new> <call>get</call> <if_true>LOG.isDebugEnabled()</if_true> <log>LOG.DEBUG</log> <call>set</call> <return/> <exit/>",
    "log": "<log_entry>[DEBUG] Using NN principal: + nameNodePrincipal</log_entry>"
  },
  "13cf80eb_4": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <if_true>props != null</if_true> <call>loadResources</call> <if_true>overlay != null</if_true> <foreach>overlay.entrySet()</foreach> <if_true>source != null</if_true> <put>updatingResource</put> <log>LOG.info(\"The workload will start at \" + startTimestampMs + \" ms (\" + startTimeString + \")\")</log> <foreach>log.debug(\"Handling deprecation for \" + (String) item)</foreach> <foreach>overlay.entrySet()</foreach> <entry>CALL:getTrimmed</entry> <log>Handling deprecation for all properties in config...</log> <call>handleDeprecation</call> <foreach>names</foreach> <call>getProps</call> <call>substituteVars</call> <log>Handling deprecation for (String)item</log> <foreach_exit/> <return/> <exit/>",
    "log": "<log_entry>LOG.debug(\"Handling deprecation for all properties in config...\")</log_entry> <log_entry>FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item)</log_entry> <log_entry>LOG.info(\"The workload will start at \" + startTimestampMs + \" ms (\" + startTimeString + \")\")</log_entry>"
  },
  "22333ca2_1": {
    "exec_flow": "ENTRY→IF_TRUE: val instanceof Configurable→CALL: ((Configurable) val).setConf→CALL: seekToCurrentValue→IF_FALSE: !blockCompressed→CALL: deserializeValue→IF_TRUE: (valLength < 0) AND LOG.isDebugEnabled()→LOG.DEBUG: val + is a zero-length value→RETURN→EXIT",
    "log": "[DEBUG] val + is a zero-length value"
  },
  "22333ca2_2": {
    "exec_flow": "ENTRY→IF_TRUE: val instanceof Configurable→CALL: ((Configurable) val).setConf→CALL: seekToCurrentValue→IF_TRUE: !blockCompressed→CALL: deserializeValue→IF_TRUE: valIn.read() > 0→LOG.INFO: available bytes: + valIn.available()→THROW: new IOException(val + \" read \" + (valBuffer.getPosition() - keyLength) + \" bytes, should read \" + (valBuffer.getLength() - keyLength))→EXIT",
    "log": "[INFO] available bytes: + valIn.available()"
  },
  "22333ca2_3": {
    "exec_flow": "ENTRY→IF_FALSE: val instanceof Configurable→CALL: seekToCurrentValue→IF_TRUE: !blockCompressed→CALL: deserializeValue→IF_TRUE: valIn.read() > 0→LOG.INFO: available bytes: + valIn.available()→THROW: new IOException(val + \" read \" + (valBuffer.getPosition() - keyLength) + \" bytes, should read \" + (valBuffer.getLength() - keyLength))→EXIT",
    "log": "[INFO] available bytes: + valIn.available()"
  },
  "22333ca2_4": {
    "exec_flow": "ENTRY→IF_FALSE: val instanceof Configurable→CALL: seekToCurrentValue→IF_FALSE: !blockCompressed→CALL: deserializeValue→IF_TRUE: (valLength < 0) AND LOG.isDebugEnabled()→LOG.DEBUG: val + is a zero-length value→RETURN→EXIT",
    "log": "[DEBUG] val + is a zero-length value"
  },
  "f8e7599b_1": {
    "exec_flow": "ENTRY→IF_TRUE: queueName.startsWith(\"root.\")→WHILE: queue == null→WHILE_COND: queue == null→CALL: org.slf4j.Logger:isDebugEnabled()→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→WHILE_EXIT→IF_FALSE: queue == null→CALL: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:hasAccess→RETURN:true→EXIT",
    "log": "[DEBUG] Queue {} does not exist, checking parent {}"
  },
  "f8e7599b_2": {
    "exec_flow": "ENTRY→IF_TRUE: queueName.startsWith(\"root.\")→WHILE: queue == null→WHILE_COND: queue == null→CALL: org.slf4j.Logger:isDebugEnabled()→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→WHILE_EXIT→IF_TRUE: queue == null→LOG: LOG.DEBUG: ACL not found for queue access-type {} for queue {}, acl, queueName→RETURN:false→EXIT",
    "log": "[DEBUG] Queue {} does not exist, checking parent {} [DEBUG] ACL not found for queue access-type {} for queue {}"
  },
  "f8e7599b_3": {
    "exec_flow": "ENTRY→IF_FALSE: queueName.startsWith(\"root.\")→IF_TRUE: queue == null→LOG: LOG.DEBUG: ACL not found for queue access-type {} for queue {}, acl, queueName→RETURN:false→EXIT",
    "log": "[DEBUG] ACL not found for queue access-type {} for queue {}"
  },
  "a1cbf5ba_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:Preconditions.checkNotNull → NEW:FSDataOutputStream → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating a new file: [{}] in COS. [ERROR] Failed to create file: {fileToCreate} at fallback : {linkedFallbackFs.getUri()} [DEBUG] Overwriting file {}"
  },
  "a1cbf5ba_2": {
    "exec_flow": "ENTRY → TRY → CALL:resolve → IF_FALSE: path.length <= 1 → IF_FALSE: root.isLink() → CALL: Preconditions.checkState → CALL: tryResolveInRegexMountpoint → IF_TRUE: resolveResult != null → RETURN → EXIT → CALL:trackDurationAndSpan",
    "log": "[DEBUG] Creating a new file: [{}] in COS. [DEBUG] Path: [{}] is a file. COS key: [{}] [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Overwriting file {} [INFO] message"
  },
  "a1cbf5ba_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Creating file:{}, f.toString() → IF_FALSE:containsColon(f) → CALL:getAncestor → CALL:performAuthCheck → CALL:createInternal → RETURN → EXIT",
    "log": "[DEBUG] Creating file: {} [DEBUG] Overwriting file {} [WARN] oss: {} capped to ~2.14GB(maximum allowed size with current output mechanism)"
  },
  "a1cbf5ba_4": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:getRaw → ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → FOREACH:names → CALL:getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "a1cbf5ba_5": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:status.isDirectory() → IF_FALSE:!overwrite → LOG:debug → EXCEPTION:debug → CATCH:FileNotFoundException → CALL:getMultipartSizeProperty → NEW:FSDataOutputStream → NEW:AliyunOSSBlockOutputStream → CALL:getConf → NEW:SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "[DEBUG] Overwriting file {}"
  },
  "4a097a4c_1": {
    "exec_flow": "ENTRY→IF_TRUE:stream != null→CALL:IOUtils.cleanupWithLogger→CALL:IOUtils.cleanupWithLogger→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "4a097a4c_2": {
    "exec_flow": "ENTRY→IF_TRUE:stream != null→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→CALL:IOUtils.cleanupWithLogger→CALL:IOUtils.cleanupWithLogger→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "4a097a4c_3": {
    "exec_flow": "ENTRY→IF_TRUE:stream != null→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→CALL:IOUtils.cleanupWithLogger→CALL:IOUtils.cleanupWithLogger→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "c4a107e5_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → CALL: recordLifecycleEvent → CALL: setConfig → TRY → CALL: org.slf4j.Logger:debug(java.lang.String) → CALL: setConfig → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: LOG.WARN: Exception while notifying listeners of {}, this, e → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Service: {} entered state {}</message> </log> <log> <level>DEBUG</level> <message>Config has been overridden during init</message> </log> <log> <level>WARN</level> <message>Exception while notifying listeners of {}</message> </log>"
  },
  "d3398ac7_1": {
    "exec_flow": "ENTRY→WHILE:true→WHILE_COND:true→TRY→CALL:run→EXCEPTION→CALL:info(java.lang.String,java.lang.Throwable)→CONDITIONAL:false→CALL:info(java.lang.String)→EXIT",
    "log": "[INFO] Exception while executing a FS operation. [INFO] Maxed out FS retries. Giving up!"
  },
  "d6dd65f6_1": {
    "exec_flow": "ENTRY -> CALL: org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:findCounter(java.lang.String) -> IF: {TRUE_BRANCH: map == null -> NEW: ConcurrentSkipListMap<>} OR {FALSE_BRANCH: map != null} -> IF_TRUE: counters == null -> CALL: values -> CALL: put -> CALL: newCounter -> CALL: org.slf4j.Logger:warn(java.lang.String) -> IF_TRUE: findCounter succeeded -> CALL: setValue -> RETURN -> EXIT",
    "log": "org.slf4j.Logger:warn(java.lang.String)"
  },
  "3bde1123_1": {
    "exec_flow": "ENTRY→IF_TRUE:info==null→CALL:getRealUser→LOG:LOG.WARN:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→RETURN→EXIT",
    "log": "[WARN] {}, Token={}"
  },
  "3bde1123_2": {
    "exec_flow": "ENTRY→IF_FALSE:info==null→IF_TRUE:info.getRenewDate()<now→CALL:getRealUser→CALL:formatTime→CALL:formatTime→CALL:getRenewDate→CALL:getRenewDate→LOG:LOG.INFO:{},Token={},err,formatTokenId(identifier)→THROW:newInvalidToken(err)→RETURN→EXIT",
    "log": "[INFO] {}, Token={}"
  },
  "4801ab5e_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → CALL:PerformanceAdvisory.LOG.debug → CALL:doEncode → FOR_INIT → FOR_COND:i<encodingState.outputs.length → CALL:ByteBuffer.get → FOR_EXIT → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs"
  },
  "1390761e_1": {
    "exec_flow": "ENTRY→CALL:setName→WHILE:!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock→WHILE_COND:!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock→TRY→CALL:readFields→IF_FALSE:ack.getSeqno() != DFSPacket.HEART_BEAT_SEQNO→LOG:LOG.INFO: \"Slow ReadProcessor read fields for block \" + block + \" took \" + duration + \"ms\"→LOG:LOG.DEBUG: \"DFSClient {}\", ack→FOR_INIT→FOR_COND:i >= 0 && dfsClient.clientRunning→IF_FALSE:PipelineAck.getECNFromHeader(ack.getHeaderFlag(i)) == PipelineAck.ECN.CONGESTED→IF_TRUE:PipelineAck.isRestartOOBStatus(reply)→CALL:errorState.initRestartingNode→THROW:new IOException(message)→EXIT",
    "log": "[INFO] Slow ReadProcessor read fields for block XXX took YYYms [DEBUG] DFSClient {}"
  },
  "1390761e_2": {
    "exec_flow": "ENTRY→CALL:setName→WHILE:!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock→WHILE_COND:!responderClosed && dfsClient.clientRunning && !isLastPacketInBlock→TRY→CALL:readFields→IF_FALSE:ack.getSeqno() != DFSPacket.HEART_BEAT_SEQNO→LOG:LOG.INFO: \"Slow ReadProcessor read fields for block \" + block + \" took \" + duration + \"ms\"→LOG:LOG.DEBUG: \"DFSClient {}\", ack→FOR_INIT→FOR_COND:i >= 0 && dfsClient.clientRunning→IF_FALSE:PipelineAck.getECNFromHeader(ack.getHeaderFlag(i)) == PipelineAck.ECN.CONGESTED→IF_FALSE:PipelineAck.isRestartOOBStatus(reply)→IF_TRUE:reply != SUCCESS→CALL:errorState.setBadNodeIndex→THROW:new IOException(\"Bad response \" + reply + \" for \" + block + \" from datanode \" + targets[i])→EXIT",
    "log": "[INFO] Slow ReadProcessor read fields for block XXX took YYYms [DEBUG] DFSClient {}"
  },
  "1390761e_3": {
    "exec_flow": "ENTRY→IF_FALSE:shouldWait→LOG:LOG.INFO:message→CALL:org.slf4j.Logger:info→EXIT",
    "log": "[INFO] {message}"
  },
  "3444eee9_1": {
    "exec_flow": "ENTRY→IF_FALSE: ch != null→TRY→CALL: toByteArray→IF_FALSE: data != null && data.length != 0→IF_FALSE: data.length != Longs.BYTES→CALL: fromByteArray→TRY→CALL: getChannel→IF_TRUE: ch == null→CALL: IOUtils.closeStream→ENTRY → IF_TRUE: stream != null → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "<log> [DEBUG] Exception in closing {} </log>"
  },
  "3a8c60e6_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.yarn.logaggregation.LogAggregationUtils:isOlderPathEnabled → CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:getRemoteAppLogDir → CALL:org.apache.hadoop.fs.FileContext:getFileContext → TRY → CALL:org.apache.hadoop.fs.FileContext$Util:exists → IF_FALSE:fc.util().exists → CALL:org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:getOlderRemoteAppLogDir → CALL:org.apache.hadoop.fs.FileContext$Util:exists → IF_FALSE:fc.util().exists → TRY → CALL:org.apache.hadoop.fs.FileContext$Util:globStatus → LOG:LOG.INFO:glob path pattern → LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString → LOG:LOG.DEBUG:Pattern: {} → IF_TRUE:matching != null && matching.length == 1 → CALL:org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:logDirNoAccessPermission → CALL:getName → RETURN → EXIT",
    "log": "<log>[INFO] glob path pattern</log> <log>[DEBUG] Filesystem glob {}, pathPatternString</log> <log>[DEBUG] Pattern: {}</log> <log>[INFO] Resolving file system path</log> <log>[DEBUG] Attempting symlink resolution</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "3a8c60e6_2": {
    "exec_flow": "ENTRY → CALL:addKVAnnotation → TRY → CALL:schemeFromPath → CALL:authorityFromPath → LOG:LOG.INFO:glob path pattern → CALL:DurationInfo:<init> → CALL:doGlob → FOREACH:flattenedPatterns → CALL:fixRelativePart → LOG:LOG.INFO:glob path pattern → LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString → LOG:LOG.DEBUG:Pattern: {} → IF_FALSE: fs != null → CALL:org.apache.hadoop.util.DurationInfo:close() → CALL:finished → IF_TRUE:logAtInfo → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) → LOG:[INFO] {} → RETURN → EXIT",
    "log": "<log>[INFO] glob path pattern</log> <log>[DEBUG] Filesystem glob {}</log> <log>[DEBUG] Pattern: {}</log> <log>[INFO] {}</log>"
  },
  "3a8c60e6_3": {
    "exec_flow": "ENTRY → CALL:addKVAnnotation → TRY → CALL:schemeFromPath → CALL:authorityFromPath → LOG:LOG.INFO:glob path pattern → CALL:DurationInfo:<init> → CALL:doGlob → FOREACH:flattenedPatterns → CALL:fixRelativePart → LOG:LOG.INFO:glob path pattern → LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString → LOG:LOG.DEBUG:Pattern: {} → CALL:listStatus → CALL:FileSystem:listStatus → EXCEPTION:FileNotFoundException → CALL:Logger:debug → LOG:[DEBUG] listStatus({}) failed; returning empty array → CALL:org.apache.hadoop.util.DurationInfo:close() → CALL:finished → IF_TRUE:logAtInfo → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) → LOG:[INFO] {} → RETURN → EXIT",
    "log": "<log>[INFO] glob path pattern</log> <log>[DEBUG] Filesystem glob {}</log> <log>[DEBUG] Pattern: {}</log> <log>[DEBUG] listStatus({}) failed; returning empty array</log> <log>[INFO] {}</log>"
  },
  "3a8c60e6_4": {
    "exec_flow": "ENTRY → CALL:addKVAnnotation → TRY → CALL:schemeFromPath → CALL:authorityFromPath → LOG:LOG.INFO:glob path pattern → CALL:DurationInfo:<init> → CALL:doGlob → FOREACH:flattenedPatterns → CALL:fixRelativePart → LOG:LOG.INFO:glob path pattern → LOG:LOG.DEBUG:Filesystem glob {}, pathPatternString → LOG:LOG.DEBUG:Pattern: {} → CALL:getPathComponents → CALL:listStatus → CALL:FileContext$Util:listStatus → EXCEPTION:FileNotFoundException → CALL:Logger:debug → LOG:[DEBUG] listStatus({}) failed; returning empty array → CALL:org.apache.hadoop.util.DurationInfo:close() → CALL:finished → IF_TRUE:logAtInfo → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object) → LOG:[INFO] {} → RETURN → EXIT",
    "log": "<log>[INFO] glob path pattern</log> <log>[DEBUG] Filesystem glob {}</log> <log>[DEBUG] Pattern: {}</log> <log>[DEBUG] listStatus({}) failed; returning empty array</log> <log>[INFO] {}</log>"
  },
  "40552482_1": {
    "exec_flow": "ENTRY → TRY → LOG: [INFO] Failing over to the ResourceManager for SubClusterId: {} → EXCEPTION: info → CATCH: Exception e → LOG: [ERROR] Exception while trying to create proxy to the ResourceManager for SubClusterId: {} → IF_TRUE: proxy == null → THROW: new YarnRuntimeException(String.format(\"Create initial proxy to the ResourceManager for SubClusterId %s failed\", subClusterId), e) → EXIT",
    "log": "[INFO] Failing over to the ResourceManager for SubClusterId: ... [ERROR] Exception while trying to create proxy to the ResourceManager for SubClusterId: ..."
  },
  "86ba2b8d_1": {
    "exec_flow": "ENTRY → IF_FALSE: getProvider() == null → IF_TRUE: action == ReencryptAction.START → CALL: FSDirEncryptionZoneOp:getCurrentKeyVersion → IF_FALSE: keyVersionName == null → CALL: Logger:info → CALL: writeLock → TRY → CALL: checkOperation → CALL: checkNameNodeSafeMode → CALL: writeLock → TRY → IF_FALSE: iip.getLastINode() == null → SWITCH: action → CASE: [START] → CALL: reencryptEncryptionZone → BREAK → CALL: FSEditLog:logSetXAttrs → CALL: writeUnlock → CALL: FSEditLog:logSync → EXIT → ENTRY: FSDirEncryptionZoneOp:reencryptEncryptionZone → IF_FALSE → CALL: EncryptionZoneManager:reencryptEncryptionZone → IF_FALSE → LOG: [INFO] Zone {}({}) is submitted for re-encryption., zoneName, inode.getId() → CALL: org.slf4j.Logger:info → CALL: xAttrs.add → CALL: ReencryptionHandler:notifyNewSubmission → LOG: [DEBUG] Notifying handler for new re-encryption command. → CALL: org.slf4j.Logger:debug → EXIT: notify → RETURN: List<XAttr> → EXIT: reencryptEncryptionZone",
    "log": "[INFO] Re-encryption using key version [INFO] Zone {}({}) is submitted for re-encryption., zoneName, inode.getId() [DEBUG] Notifying handler for new re-encryption command."
  },
  "86ba2b8d_2": {
    "exec_flow": "ENTRY→IF_FALSE:reencryptionHandler==null→CALL:checkEncryptionZoneRoot→CALL:reencryptionHandler.cancelZone→LOG:[INFO]:Cancelled zone {}({}) for re-encryption.,zoneName,zoneId→CALL:updateReencryptionFinish→CALL:fsd.ezManager.getReencryptionStatus().markZoneCompleted→CALL:setState→CALL:add→CALL:FSDirXAttrOp.unprotectedSetXAttrs→RETURN→EXIT",
    "log": "[INFO] Cancelled zone {}({}) for re-encryption. [INFO] Zone {} completed re-encryption."
  },
  "1e9e65ce_1": {
    "exec_flow": "Parent.ENTRY → LOG: LOG.DEBUG: Call the getFileStatus to obtain the metadata for the file: [{f}]. → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE: meta != null → IF_TRUE: meta.isFile() → LOG: LOG.DEBUG: Path: [{f}] is a file. COS key: [{key}]. → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{f}].</log> <log>[DEBUG] Path: [{f}] is a file. COS key: [{key}].</log>"
  },
  "bba81360_1": {
    "exec_flow": "ENTRY -> IF_FALSE: acls == null || hosts == null -> IF_TRUE: UserGroupInformation.isSecurityEnabled() -> CALL: getClientPrincipal -> TRY -> IF_TRUE: clientPrincipal != null -> CALL: getServerPrincipal -> IF_TRUE: (clientPrincipal != null && !clientPrincipal.equals(user.getUserName())) || acls.length != 2 || !acls[0].isUserAllowed(user) || acls[1].isUserAllowed(user) -> CALL: AUDITLOG.warn -> THROW: new AuthorizationException(\"User \" + user + \" is not authorized for protocol \" + protocol + cause) -> RETURN -> EXIT",
    "log": "[WARN] AUTHZ_FAILED_FOR ..."
  },
  "bba81360_2": {
    "exec_flow": "ENTRY -> IF_FALSE: acls == null || hosts == null -> IF_TRUE: UserGroupInformation.isSecurityEnabled() -> CALL: getClientPrincipal -> TRY -> IF_TRUE: clientPrincipal != null -> CALL: getServerPrincipal -> IF_FALSE: (clientPrincipal != null && !clientPrincipal.equals(user.getUserName())) || acls.length != 2 || !acls[0].isUserAllowed(user) -> IF_TRUE: addr != null -> IF_TRUE: hosts.length != 2 || !hosts[0].includes(hostAddress) || hosts[1].includes(hostAddress) -> CALL: AUDITLOG.warn -> THROW: new AuthorizationException(\"Host \" + hostAddress + \" is not authorized for protocol \" + protocol) -> RETURN -> EXIT",
    "log": "[WARN] AUTHZ_FAILED_FOR ..."
  },
  "bba81360_3": {
    "exec_flow": "ENTRY -> IF_FALSE: acls == null || hosts == null -> IF_TRUE: UserGroupInformation.isSecurityEnabled() -> CALL: getClientPrincipal -> TRY -> IF_FALSE: clientPrincipal != null -> IF_FALSE: (clientPrincipal != null && !clientPrincipal.equals(user.getUserName())) || acls.length != 2 || !acls[0].isUserAllowed(user) || acls[1].isUserAllowed(user) -> IF_FALSE: addr != null -> CALL: AUDITLOG.info -> RETURN -> EXIT",
    "log": "[INFO] AUTHZ_SUCCESSFUL_FOR ..."
  },
  "bba81360_4": {
    "exec_flow": "<!-- Optimized execution flow structure from UserGroupInformation -->",
    "log": "<!-- Merged log sequence -->"
  },
  "8b372c10_1": {
    "exec_flow": "ENTRY→CALL:verifyAdminAccess→ENTRY→TRY→CALL:getCurrentUser→IF_TRUE:!authorizer.isAdmin(user)→LOG:LOG.WARN:User {user.getShortUserName()} doesn't have permission to call '{method}'→CALL:RMAuditLogger.logFailure→THROW:new AccessControlException(\"User \" + user.getShortUserName() + \" doesn't have permission to call '\" + method + \"'\")→EXIT→IF_TRUE:LOG.isTraceEnabled()→LOG:LOG.TRACE:{method} invoked by user {user.getShortUserName()}→RETURN→EXIT",
    "log": "[WARN] User {user.getShortUserName()} doesn't have permission to call '{method}' [TRACE] {method} invoked by user {user.getShortUserName()}"
  },
  "51228f87_1": {
    "exec_flow": "<!-- Optimized execution flow structure for P2-C1 --> ENTRY→FOREACH: connections→IF_FALSE: !scanAll && size() < idleScanThreshold→ IF_TRUE: connection.isIdle() && connection.getLastContact() < minLastContact && CALL:org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection) && !scanAll && (++closed == maxIdleToClose)→ CALL:org.slf4j.Logger:isDebugEnabled()→ LOG:Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()→ CALL:access$7→CALL:close→IF_TRUE:connection.user!=null&&connection.connectionContextRead→CALL:decrUserConnections→RETURN→EXIT→BREAK→EXIT",
    "log": "<!-- Merged log sequence for P2-C1 --> [DEBUG] Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()"
  },
  "51228f87_2": {
    "exec_flow": "<!-- Optimized execution flow structure for P2-C2 --> ENTRY→FOREACH: connections→IF_FALSE: !scanAll && size() < idleScanThreshold→ IF_TRUE: connection.isIdle() && connection.getLastContact() < minLastContact && CALL:org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection) && !scanAll && (++closed == maxIdleToClose)→ CALL:org.slf4j.Logger:isDebugEnabled()→ LOG:Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()→ CALL:access$7→CALL:close→IF_FALSE:connection.user!=null&&connection.connectionContextRead→RETURN→EXIT→BREAK→EXIT",
    "log": "<!-- Merged log sequence for P2-C2 --> [DEBUG] Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()"
  },
  "2b55ac39_1": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: DEBUG Ignoring re-entrant call to stop() → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: WARN Exception while notifying listeners of {} → EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "2b55ac39_2": {
    "exec_flow": "ENTRY → IF_FALSE: isInState(STATE.STOPPED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED → LOG: DEBUG Service: {} entered state {} → CALL: recordLifecycleEvent → TRY → CALL: serviceStop → EXCEPTION: serviceStop → CATCH: Exception e → LOG: INFO Service {} failed in state {} (exception details) → CALL: noteFailure → THROW: ServiceStateException.convert(e) → EXIT",
    "log": "[DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {} (exception details)"
  },
  "7b1f7d7e_1": {
    "exec_flow": "ENTRY → NEW:SimpleUdpServer → CALL:getPort → CALL:rpcProgram.startDaemons → TRY → CALL:udpServer.run → LOG:LOG.INFO:Started listening to UDP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount → EXCEPTION:run → CATCH:Throwable e → LOG:LOG.ERROR:Failed to start the UDP server., e → IF_TRUE:boundPort != port → LOG:LOG.INFO:The bound port is X, different with configured port Y → FOR_INIT → FOR_COND:vers <= highProgVersion → CALL:register → FOR_EXIT → CALL:udpServer.shutdown → CALL:terminate → CALL:getBoundPort → EXIT",
    "log": "LOG.INFO: Started listening to UDP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount LOG.ERROR: Failed to start the UDP server., e LOG.INFO: The bound port is X, different with configured port Y"
  },
  "7b1f7d7e_2": {
    "exec_flow": "ENTRY → NEW:NioEventLoopGroup → NEW:NioEventLoopGroup → CALL:newCachedThreadPool → NEW:ServerBootstrap → CALL:option → CALL:channel → CALL:getPort → CALL:org.slf4j.Logger:info(java.lang.String) → LOG:LOG.INFO:Started listening to TCP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount → EXIT",
    "log": "LOG.INFO: Started listening to TCP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount"
  },
  "7b1f7d7e_3": {
    "exec_flow": "ENTRY → NEW: SimpleTcpServer → CALL: getPort → CALL: rpcProgram.startDaemons → TRY → CALL: tcpServer.run → EXCEPTION: run → CATCH: Throwable e → LOG: LOG.ERROR: Failed to start the TCP server., e → IF_TRUE: tcpServer.getBoundPort() > 0 → CALL: rpcProgram.unregister → IF_TRUE:boundPort != port → LOG:LOG.INFO:The bound port is + boundPort + , different with configured port + port → FOR_INIT → FOR_COND:vers <= highProgVersion → CALL:register → FOR_EXIT → CALL: tcpServer.shutdown → CALL: terminate → CALL: getBoundPort → EXIT",
    "log": "LOG.ERROR: Failed to start the TCP server., e LOG.INFO: The bound port is X, different with configured port Y"
  },
  "7b1f7d7e_4": {
    "exec_flow": "ENTRY → NEW: SimpleTcpServer → CALL: getPort → CALL: rpcProgram.startDaemons → TRY → CALL: tcpServer.run → EXCEPTION: run → CATCH: Throwable e → LOG: LOG.ERROR: Failed to start the TCP server., e → IF_FALSE: tcpServer.getBoundPort() > 0 → CALL: tcpServer.shutdown → CALL: terminate → CALL: getBoundPort → EXIT",
    "log": "LOG.ERROR: Failed to start the TCP server., e"
  },
  "37197dfe_1": {
    "exec_flow": "<step>org.apache.hadoop.yarn.api.ApplicationClientProtocol.getNodesToAttributes</step>",
    "log": "<!-- Inherited from child node due to parent having no logs -->"
  },
  "37197dfe_2": {
    "exec_flow": "<step>org.apache.hadoop.yarn.server.router.clientrm.AbstractClientRequestInterceptor.ENTRY</step> <step>[VIRTUAL_CALL] org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor</step>",
    "log": "<!-- No logs present, log inheritance from child -->"
  },
  "de42989e_1": {
    "exec_flow": "ENTRY → CALL:getOrCreateJournal → IF_FALSE:!fjm.getStorageDirectory().getCurrentDir().exists() → LOG:LOG.INFO:Scanning storage + fjm → WHILE:!files.isEmpty() → WHILE_COND:!files.isEmpty() → CALL:latestLog.scanLog → LOG:LOG.INFO:Latest log is + latestLog + ; journal id: + journalId → IF_TRUE:latestLog.getLastTxId() == HdfsServerConstants.INVALID_TXID → LOG:LOG.WARN:Latest log + latestLog + has no transactions. moving it aside and looking for previous log ; journal id: + journalId → CALL:latestLog.moveAsideEmptyFile → WHILE_COND:!files.isEmpty() → WHILE_EXIT → LOG:LOG.INFO:No files in + fjm → RETURN → EXIT",
    "log": "[INFO] Scanning storage [INFO] Latest log is ; journal id: [WARN] Latest log has no transactions. moving it aside and looking for previous log ; journal id: [INFO] No files in"
  },
  "de42989e_2": {
    "exec_flow": "ENTRY→CALL:IOUtils.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→CALL:storage.getJournalManager().doRollback→CALL:refreshStorage→EXIT",
    "log": "[DEBUG] IOUtils.cleanupWithLogger called [DEBUG] Exception in closing {} [INFO] storage.getJournalManager().doRollback called [DEBUG] storage.refreshStorage called"
  },
  "e49a2794_1": {
    "exec_flow": "ENTRY → IF_TRUE: lister.hasNext() → CALL:org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:next() → TRY → CALL:org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object) → CALL:org.apache.hadoop.fs.s3a.Invoker:retry → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]) → CALL:getMultipartUploads → CALL:listIterator → CALL:hasNext → RETURN → EXIT",
    "log": "<log>[DEBUG] [{}], Requesting next {} uploads prefix {}, next key {}, next upload id {}</log> <log>[DEBUG] Listing found {} upload(s)</log> <log>[DEBUG] New listing state: {}</log> <log>[DEBUG] Next element retrieved from RemoteIterator</log>"
  },
  "e49a2794_2": {
    "exec_flow": "ENTRY → CALL:sourceHasNext → CALL:requestNextBatch → WHILE:source.hasNext() → WHILE_COND:source.hasNext() → IF_TRUE:buildNextStatusBatch(source.next()) → CALL:org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:next() → CALL:org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult) → RETURN → EXIT",
    "log": "<log>[DEBUG] All entries in batch were filtered...continuing</log> <log>[DEBUG] Next element retrieved from RemoteIterator</log>"
  },
  "e49a2794_3": {
    "exec_flow": "ENTRY → IF_FALSE:currIterator.hasNext() → CALL:getNextIterator → CALL:fetchBatchesAsync → TRY → WHILE:listResult == null && (!isIterationComplete || !listResultQueue.isEmpty()) → WHILE_EXIT → IF_FALSE:listResult == null → IF_TRUE:listResult.isFailedListing() → THROW:listResult.getListingException() → CATCH:InterruptedException → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → THROW:IOException → EXIT",
    "log": "<log>[ERROR] Thread got interrupted: {exception}</log>"
  },
  "eb96f1dc_1": {
    "exec_flow": "ENTRY→CALL:verifyLayoutVersion→CALL:FSNamesystem:getNamespaceInfo→CALL:readLock→TRY→CALL:unprotectedGetNamespaceInfo→CALL:readUnlock→CALL:this.fsLock.readUnlock→CALL:org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:readUnlock→IF_TRUE:journalInfo.getNamespaceId()!=expectedNamespaceID→CALL:getNamespaceId→LOG:LOG.WARN:errorMsg→THROW:newUnregisteredNodeException(journalInfo)→IF_FALSE:journalInfo.getNamespaceId()!=expectedNamespaceID→IF_TRUE:!journalInfo.getClusterId().equals(namesystem.getClusterId())→CALL:getClusterId→CALL:getClusterId→LOG:LOG.WARN:errorMsg→THROW:newUnregisteredNodeException(journalInfo)→IF_TRUE:needReport→CALL:addMetric→CALL:readLockHeldTimeStampNanos.remove→IF_TRUE:needReport && readLockIntervalMs >= this.readLockReportingThresholdMs→CALL:timeStampOfLastReadLockReportMs.compareAndSet→CALL:numReadLockLongHold.increment→CALL:longestReadLockHeldInfo.get→CALL:longestReadLockHeldInfo.compareAndSet→CALL:timer.monotonicNow→CALL:numReadLockWarningsSuppressed.incrementAndGet→RETURN→EXIT",
    "log": "[WARN] Invalid namespaceID in journal request - expected [expectedNamespaceID] actual [journalInfo.getNamespaceId()] [WARN] Invalid clusterId in journal request - expected [journalInfo.getClusterId()] actual [namesystem.getClusterId()] [INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}"
  },
  "9620b9f5_1": {
    "exec_flow": "ENTRY→IF_TRUE: groupSize == 0→WHILE: itemsIterator.hasNext()→CALL:processPaths→CALL:processPathInternal→WHILE_COND: itemsIterator.hasNext()→WHILE_EXIT→CALL:cleanupRemoteIterator→EXIT",
    "log": "<!-- Logs specific to processPaths method with groupSize == 0 -->"
  },
  "9620b9f5_2": {
    "exec_flow": "ENTRY→IF_FALSE: groupSize == 0→WHILE: itemsIterator.hasNext()→CALL:processPaths→CALL:processPathInternal→WHILE_COND: itemsIterator.hasNext()→WHILE_EXIT→CALL:cleanupRemoteIterator→EXIT",
    "log": "<!-- Logs specific to processPaths method with groupSize != 0 -->"
  },
  "9620b9f5_3": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "9620b9f5_4": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "bf5ee6d3_1": {
    "exec_flow": "ENTRY→CALL:getTrimmed→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOGGER.DEBUG:Duplication FS created for {}; discarding {}→EXIT→CALL:LOGGER.DEBUG:Filesystem {} created while awaiting semaphore",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Duplicate FS created for {}; discarding {} [DEBUG] Filesystem {} created while awaiting semaphore </log>"
  },
  "bf5ee6d3_2": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>Child Method Execution Flow</step>",
    "log": "<log>Child Log Sequence</log>"
  },
  "bf5ee6d3_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → C1 paths",
    "log": "<log>[WARN] \"local is a deprecated filesystem name. Use file:/// instead.\"</log> <log>[WARN] \"<name> is a deprecated filesystem name. Use hdfs://<name>/ instead.\"</log>"
  },
  "2edc2194_1": {
    "exec_flow": "<entry>ENTRY</entry> <call>org.apache.hadoop.fs.FileSystem:getFileStatus</call> <sequence> <log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <if condition=\"key.length() != 0\"> <try> <call>retrieveMetadata</call> <if condition=\"meta != null\"> <if condition=\"meta.isDirectory()\"> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <call>conditionalRedoFolderRename</call> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </if> <else> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log> <call>updateFileStatusPath</call> <return>EXIT</return> </else> </if> </try> </if> <else> <throw>FileNotFoundException: absolutePath + \": No such file or directory.\"</throw> </else> </sequence>",
    "log": "<log>LOG.debug(\"Getting the file status for {}\", f.toString())</log> <log>LOG.debug(\"Path {} is a folder.\", f.toString())</log> <log>LOG.debug(\"Found the path: {} as a file.\", f.toString())</log>"
  },
  "2edc2194_2": {
    "exec_flow": "<entry>ENTRY</entry> <if condition=\"key.length() == 0\"> <call>org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</call> <if condition=\"meta != null\"> <log>LOG.DEBUG: List COS key: [{}] to check the existence of the path.</log> <call>org.apache.hadoop.fs.cosn.NativeFileSystemStore:list</call> <if condition=\"listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0\"> <if condition=\"LOG.isDebugEnabled()\"> <log>LOG.DEBUG: Path: [{}] is a directory. COS key: [{}]</log> </if> <call>org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory</call> <return>EXIT</return> </if> </if> </if>",
    "log": "<log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "2edc2194_3": {
    "exec_flow": "<entry>ENTRY</entry> <log>LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path</log> <call>statIncrement</call> <try> <call>makeQualified</call> <call>abfsStore.getFileStatus</call> </try> <exit>EXIT</exit>",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "2edc2194_4": {
    "exec_flow": "<entry>ENTRY</entry> <log>LOG.DEBUG: AzureBlobFileSystem.getFileStatus path: {}, path</log> <call>statIncrement</call> <try> <call>makeQualified</call> <call>abfsStore.getFileStatus</call> </try> <catch>AzureBlobFileSystemException</catch> <call>checkException</call> <exit>EXIT</exit>",
    "log": "<log>[DEBUG] AzureBlobFileSystem.getFileStatus path: {}</log>"
  },
  "71d6e14b_1": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: {}: {} '{}' to '{}'), getName(), operation, source, dest→CALL: requireNonNull→CALL: requireNonNull→TRY→CALL: org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:apply→CALL: invokeTrackingDuration→CALL: drainOrAbortHttpStream→CALL: close→IF_FALSE: !success→EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest'"
  },
  "71d6e14b_2": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: {}: {} '{}' to '{}'), getName(), operation, source, dest→CALL: requireNonNull→CALL: requireNonNull→TRY→CALL: apply→IF_TRUE: !success→CALL: failed→LOG: LOG.INFO: {}: {} raised an exception: {}, getName(), operation, e.toString()→LOG: LOG.DEBUG: {}: {} stack trace, getName(), operation, e→CALL: failed→THROW: escalateRenameFailure(operation, source, dest)→EXIT→ENTRY→CALL: getFileStatus→CALL: getFileStatusOrNull→LOG: LOG.ERROR: {}: failure to {} {} to {} with + source status {} + and destination status {}, getName(), operation, source, dest, sourceStatus, destStatus→NEW: PathIOException→CALL: toString→RETURN→EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest' [INFO] getName(): operation raised an exception: exceptionMessage [DEBUG] getName(): operation stack trace [ERROR] getName(): failure to operation source to dest with source status sourceStatus and destination status destStatus"
  },
  "71d6e14b_3": {
    "exec_flow": "ENTRY→LOG: LOG.DEBUG: {}: {} '{}' to '{}'), getName(), operation, source, dest→CALL: requireNonNull→CALL: requireNonNull→TRY→CALL: org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:apply→EXCEPTION: Exception→CALL: failed→CALL: close→THROW: escalateRenameFailure(operation, source, dest)→EXIT→ENTRY→CALL: getFileStatus→CALL: getFileStatusOrNull→LOG: LOG.ERROR: {}: failure to {} {} to {} with + source status {} + and destination status {}, getName(), operation, source, dest, sourceStatus, destStatus→NEW: PathIOException→CALL: toString→RETURN→EXIT",
    "log": "[DEBUG] getName(): operation: 'source' to 'dest' [ERROR] getName(): failure to operation source to dest with source status sourceStatus and destination status destStatus"
  },
  "226f8ae7_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → org.apache.hadoop.fs.FileSystem:open.ENTRY → LOG[DEBUG]Opening file: {f.toString()} → CALL:performAuthCheck → TRY → CALL:retrieveMetadata → IF_FALSE:meta==null → IF_FALSE:meta.isDirectory() → TRY → CALL:retrieve → NEW:FSDataInputStream → NEW:BufferedFSInputStream → NEW:NativeAzureFsInputStream → CALL:getLen → RETURN → open.RETURN",
    "log": "<log_entry level=\"DEBUG\" template=\"Opening file: {f.toString()}\"/>"
  },
  "226f8ae7_2": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE→NEW:FSDataInputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→CALL:pathToKey→CALL:getLen→CALL:AliyunOSSInputStream:<init>→RETURN→EXIT ENTRY→IF_FALSE: key.length() == 0→IF_TRUE: meta == null && !key.endsWith(\"/\")→CALL:getObjectMetadata→EXCEPTION:getObjectMetadata→CATCH:OSSException osse→CALL:LOG.debug→RETURN→EXIT",
    "log": "<log_entry level=\"DEBUG\" template=\"Exception thrown when get object meta: + key + , exception: + osse\"/>"
  },
  "226f8ae7_3": {
    "exec_flow": "<sequence> ENTRY→CALL:getTrimmed→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT </sequence>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "226f8ae7_4": {
    "exec_flow": "<sequence> ENTRY→VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)→LOG:Unexpected SecurityException in Configuration→EXIT </sequence>",
    "log": "<log>[WARN] Unexpected SecurityException in Configuration</log>"
  },
  "796b6428_1": {
    "exec_flow": "ENTRY → IF_TRUE: curDataSlice == null || (curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) → CALL: readNextPacket → CALL: packetReceiver.receiveNextPacket → CALL: getDataSlice → CALL: getHeader → LOG: LOG.TRACE: DFSClient readNextPacket got header {} → IF_FALSE: !curHeader.sanityCheck(lastSeqNo) → IF_TRUE: curHeader.getDataLen() > 0 → CALL: getSeqno → IF_TRUE: verifyChecksum && curDataSlice.remaining() > 0 → CALL: verifyChunkedSums → CALL: getDataLen → IF_TRUE: curHeader.getOffsetInBlock() < startOffset → CALL: curDataSlice.position → IF_TRUE: bytesNeededToFinish <= 0 → CALL: readTrailingEmptyPacket → IF_TRUE: verifyChecksum → CALL: sendReadResult → EXIT",
    "log": "[TRACE] DFSClient readNextPacket got header {}"
  },
  "796b6428_2": {
    "exec_flow": "ENTRY → IF_TRUE: curDataSlice == null || (curDataSlice.remaining() == 0 && bytesNeededToFinish > 0) → CALL: readNextPacket → CALL: packetReceiver.receiveNextPacket → CALL: getDataSlice → CALL: getHeader → LOG: LOG.TRACE: DFSClient readNextPacket got header {} → IF_FALSE: !curHeader.sanityCheck(lastSeqNo) → IF_TRUE: curHeader.getDataLen() > 0 → CALL: getSeqno → IF_FALSE: verifyChecksum && curDataSlice.remaining() > 0 → CALL: getDataLen → IF_FALSE: curHeader.getOffsetInBlock() < startOffset → IF_TRUE: bytesNeededToFinish <= 0 → CALL: readTrailingEmptyPacket → IF_FALSE: verifyChecksum → CALL: sendReadResult → EXIT",
    "log": "[TRACE] DFSClient readNextPacket got header {}"
  },
  "cfdaeb67_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapred.BackupStore$FileCache:createSpillFile → LOG:[INFO] Created file: {} → CALL:org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream:wrapIfNecessary → CALL:SpillCallBackInjector.get().getSpillFileCB → CALL:CryptoUtils.wrapIfNecessary → CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String) → CALL:org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite) → CALL:getCodecClasses → CALL:newInstance → RETURN → CALL:org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration) → IF_TRUE:isEncryptedSpillEnabled → CALL:getTrimmed → LOG:[DEBUG] Getting configuration value for cipher suite → LOG:[INFO] Calling getInstance with CipherSuite → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:[DEBUG] Handling deprecation for all properties in config... → LOG:[DEBUG] Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:cryptoCodec.generateSecureRandom → CALL:cryptoCodec.close → RETURN → EXIT",
    "log": "[INFO] Created file: {} [DEBUG] Getting configuration value for cipher suite [INFO] Calling getInstance with CipherSuite [DEBUG] Codec classes obtained [DEBUG] New instance created [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] IV read from Stream [<Base64-Encoded-IV>]"
  },
  "63f25da6_1": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: namesystem != null→CALL: FSNamesystem:stopStandbyServices→LOG: LOG.INFO: Stopping services started for {} state, curState→EXIT→VIRTUAL_CALL→ENTRY→CALL:tailerThread.setShouldRun→CALL:tailerThread.interrupt→TRY→CALL:tailerThread.join→EXCEPTION:join→CATCH:InterruptedException e→LOG:LOG.WARN: Edit log tailer thread exited with an exception→CALL:rollEditsRpcExecutor.shutdown→EXIT",
    "log": "[INFO] Stopping services started for {} state [WARN] Edit log tailer thread exited with an exception"
  },
  "63f25da6_2": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: namesystem != null→CALL: FSNamesystem:stopStandbyServices→LOG: LOG.ERROR: Error encountered requiring NN shutdown. Shutting down immediately., t→CALL: org.slf4j.Logger:error→EXCEPTION: org.apache.hadoop.hdfs.server.namenode.NameNode:doImmediateShutdown(java.lang.Throwable)→EXIT",
    "log": "[ERROR] Error encountered requiring NN shutdown. Shutting down immediately."
  },
  "63f25da6_3": {
    "exec_flow": "ENTRY→TRY→IF_TRUE: namesystem != null→CALL: FSNamesystem:stopStandbyServices→EXCEPTION: org.apache.hadoop.hdfs.server.namenode.NameNode:doImmediateShutdown(java.lang.Throwable)→ENTRY→TRY→LOG: LOG.ERROR: Error encountered requiring NN shutdown. Shutting down immediately., t→CALL: org.slf4j.Logger:error→CALL: ExitUtil:terminate→ENTRY→CALL:terminate→CALL:org.slf4j.Logger:info→CALL:org.slf4j.Logger:debug→CALL:org.slf4j.Logger:error→EXIT→EXIT",
    "log": "[ERROR] Error encountered requiring NN shutdown. Shutting down immediately. [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "b93f1df2_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY -> IF_TRUE: counter == null -> LOG: LOG.DEBUG: Ignoring counter increment for unknown counter {}, key -> CALL: return -> RETURN -> EXIT",
    "log": "[DEBUG] Ignoring counter increment for unknown counter {}"
  },
  "b93f1df2_2": {
    "exec_flow": "ENTRY -> IF_FALSE: counter == null -> IF_TRUE: value < 0 -> LOG: LOG.DEBUG: Ignoring negative increment value {} for counter {}, value, key -> CALL: get -> RETURN -> EXIT",
    "log": "[DEBUG] Ignoring negative increment value {} for counter {}"
  },
  "b93f1df2_3": {
    "exec_flow": "ENTRY -> IF_FALSE: counter == null -> IF_FALSE: value < 0 -> CALL: incAtomicLong -> LOG: LOG.TRACE: Incrementing counter {} by {} with final value {}, key, value, l -> RETURN -> EXIT",
    "log": "[TRACE] Incrementing counter {} by {} with final value {}"
  },
  "d74139f8_1": {
    "exec_flow": "<entry_point>Parent.ENTRY</entry_point> <virtual_call>true</virtual_call> <child_paths> <child_path> ENTRY→IF_FALSE: reduceResourceRequest.equals(Resources.none())→IF_FALSE: assignedRequests.maps.size() > 0→IF_FALSE: scheduledRequests.maps.size() <= 0→IF_TRUE: reducerUnconditionalPreemptionDelayMs >= 0→CALL: preemptReducersForHangingMapRequests→IF_TRUE: hangingMapRequests > 0→CALL: preemptReducer→CALL: clearAllPendingReduceRequests→CALL: getResourceLimit→CALL: Resources.multiply→CALL: Resources.multiply→CALL: Resources.multiply→LOG: [INFO] Going to preempt \" + toPreempt + \" due to lack of space for maps→CALL: assignedRequests.preemptReduce→CALL: Collections.sort→FOR_INIT→FOR_COND: i < toPreempt && reduceList.size() > 0→CALL: org.slf4j.Logger:info(java.lang.String)→CALL: org.apache.hadoop.yarn.event.EventHandler:handle(org.apache.hadoop.yarn.event.Event)→FOR_EXIT→RETURN→EXIT </child_path> </child_paths>",
    "log": "[INFO] Going to preempt \" + toPreempt + \" due to lack of space for maps [INFO] Preempting {id}"
  },
  "d74139f8_2": {
    "exec_flow": "<entry_point>Parent.ENTRY</entry_point> <virtual_call>true</virtual_call> <child_paths> ENTRY→CALL:clone→CALL:subtractFrom→ENTRY→CALL:org.apache.hadoop.yarn.util.resource.ResourceUtils:getNumberOfCountableResourceTypes→FOR_INIT→FOR_COND:i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT→RETURN→EXIT </child_paths>",
    "log": "[WARN] Resource is missing: {exception_message}"
  },
  "8aa7c4a7_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Handling deprecation for all properties in config...</message> </log> <log> <level>DEBUG</level> <message>Handling deprecation for (String)item</message> </log> <log> <level>INFO</level> <message>message</message> </log>"
  },
  "8aa7c4a7_2": {
    "exec_flow": "ENTRY → IF_TRUE: srcPath.isRoot() → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:debug → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Cannot rename the root of a filesystem</message> </log>"
  },
  "8aa7c4a7_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Could not flush Keystore.. + attempting to reset to previous state !! → CALL: clear → TRY → CALL: loadFromPath → EXCEPTION: loadFromPath → CATCH: Exception e → LOG: LOG.DEBUG: Could not reset Keystore to previous state, e → EXIT",
    "log": "<log> <level>DEBUG</level> <message>Could not flush Keystore.. + attempting to reset to previous state !!</message> </log> <log> <level>DEBUG</level> <message>Could not reset Keystore to previous state</message> </log>"
  },
  "8aa7c4a7_4": {
    "exec_flow": "ENTRY → TRY → CALL: trackDurationAndSpan → EXCEPTION: RenameFailedException → CATCH: RenameFailedException e → LOG: INFO:{} , e.getMessage() → LOG: DEBUG: rename failure → CALL: getExitCode → RETURN → EXIT",
    "log": "<log> <level>INFO</level> <message>{}</message> </log> <log> <level>DEBUG</level> <message>rename failure</message> </log>"
  },
  "8aa7c4a7_5": {
    "exec_flow": "ENTRY → FOREACH: providers → TRY → CALL: provider.flush → CATCH: IOException → CALL: org.slf4j.Logger:error(java.lang.String) → FOREACH_EXIT → EXIT",
    "log": "<log> <level>ERROR</level> <message>Error flushing provider with url[provider.getKMSUrl()]</message> </log>"
  },
  "f26b2b07_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: LOG.DEBUG: Looking for committer factory for path {}, outputPath</step> <step>CALL: org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>IF_TRUE: StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null</step> <step>CALL: org.apache.hadoop.fs.Path:toUri</step> <step>CALL: org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>IF_FALSE: StringUtils.isNotEmpty(conf.getTrimmed(schemeKey))</step> <step>LOG: LOG.DEBUG: No scheme-specific factory defined in {}, schemeKey</step> <step>LOG: LOG.INFO: No output committer factory defined, defaulting to FileOutputCommitterFactory</step> <step>CALL: newInstance</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log> <level>DEBUG</level> <template>Looking for committer factory for path {}</template> </log> <log> <level>DEBUG</level> <template>No scheme-specific factory defined in {}</template> </log> <log> <level>INFO</level> <template>No output committer factory defined, defaulting to FileOutputCommitterFactory</template> </log>"
  },
  "f26b2b07_2": {
    "exec_flow": "<step>ENTRY</step> <step>IF_TRUE: committer == null</step> <step>CALL:getOutputPath</step> <step>CALL:getCommitterFactory</step> <step>CALL:createOutputCommitter</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log> <level>DEBUG</level> <template>Creating FileOutputCommitter for path {outputPath} and context {context}</template> </log>"
  },
  "0e887dfa_1": {
    "exec_flow": "ENTRY → CALL:sort → CALL:sortPass → CALL:mergePass → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.debug(\"running merge pass\") → CALL:cloneFileAttributes → CALL:merge → IF_TRUE:(SegmentContainer initialization succeeded) AND (MergeQueue merge success) → LOG:LOG.debug(\"SegmentContainer initialized\") → LOG:LOG.info(\"MergeQueue started merging\") → CALL:writeFile → CALL:close → EXIT",
    "log": "<log_entry>[DEBUG] running merge pass</log_entry> <log_entry>[DEBUG] SegmentContainer initialized</log_entry> <log_entry>[INFO] MergeQueue started merging</log_entry>"
  },
  "0e887dfa_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] getFileStatus → CALL:qualify → LOG: Stripping trailing '/' from {path} → LOG: Getting path status for {path} ({key}); needEmptyDirectory={needEmptyDirectoryFlag} → CALL:s3GetFileStatus → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG: Path: [{}] is a file. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT → IF_FALSE:meta.isFile() → LOG: Path: [{}] is a dir. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT → IF_FALSE:meta!=null → LOG: List COS key: [{}] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0 → IF_TRUE:LOG.isDebugEnabled() → LOG: Path: [{}] is a directory. COS key: [{}] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT → IF_FALSE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0 → CATCH (FileNotFoundException) → THROW:FileNotFoundException → RETURN(false) → EXIT",
    "log": "<log_entry>[DEBUG] Stripping trailing '/' from {path}</log_entry> <log_entry>[DEBUG] Getting path status for {path} ({key}); needEmptyDirectory={needEmptyDirectoryFlag}</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] List COS key: [{}] to check the existence of the path.</log_entry> <log_entry>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log_entry>"
  },
  "55a70bed_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputCommitter → IF_TRUE: committer == null → LOG: DEBUG: Looking for committer factory for path {}, outputPath → CALL: getOutputPath → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null → CALL: org.apache.hadoop.fs.Path:toUri → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: StringUtils.isNotEmpty(conf.getTrimmed(schemeKey)) → LOG: INFO: Using schema-specific factory for {}, outputPath → LOG: INFO: Using OutputCommitter factory class {} from key {}, factory, key → CALL: getCommitterFactory → CALL: createOutputCommitter → LOG: DEBUG: Creating FileOutputCommitter for path {outputPath} and context {context} → CALL:Preconditions.checkState → CALL:Preconditions.checkNotNull → CALL:org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputName → LOG:DEBUG: Work file for {} extension '{}' is {}, context, extension, workFile → RETURN → EXIT",
    "log": "[DEBUG] Looking for committer factory for path {} [DEBUG] Work file for {} extension '{}' is {}"
  },
  "55a70bed_2": {
    "exec_flow": "ENTRY → IF_TRUE: committer == null → LOG: DEBUG: Looking for committer factory for path {}, outputPath → CALL: getOutputPath → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null → CALL: org.apache.hadoop.fs.Path:toUri → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: StringUtils.isNotEmpty(conf.getTrimmed(schemeKey)) → LOG: INFO: Using schema-specific factory for {}, outputPath → LOG: INFO: Using OutputCommitter factory class {} from key {}, factory, key → CALL: getCommitterFactory → CALL: createOutputCommitter → LOG: DEBUG: Creating FileOutputCommitter for path {outputPath} and context {context} → RETURN → EXIT",
    "log": "[DEBUG] Looking for committer factory for path {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Using schema-specific factory for {} [INFO] Using OutputCommitter factory class {} from key {} [DEBUG] Creating FileOutputCommitter for path {outputPath} and context {context}"
  },
  "55a70bed_3": {
    "exec_flow": "ENTRY → IF_TRUE: committer == null → LOG: DEBUG: Looking for committer factory for path {}, outputPath → CALL: getOutputPath → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: StringUtils.isEmpty(conf.getTrimmed(key)) && outputPath != null → CALL: org.apache.hadoop.fs.Path:toUri → CALL: org.apache.hadoop.conf.Configuration:getTrimmed → IF_FALSE: StringUtils.isNotEmpty(conf.getTrimmed(schemeKey)) → LOG: DEBUG: No scheme-specific factory defined in {}, schemeKey → LOG: INFO: No output committer factory defined, defaulting to FileOutputCommitterFactory → CALL: getCommitterFactory → CALL: createOutputCommitter → LOG: DEBUG: Creating FileOutputCommitter for path {outputPath} and context {context} → RETURN → EXIT",
    "log": "[DEBUG] Looking for committer factory for path {} [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] No scheme-specific factory defined in {} [INFO] No output committer factory defined, defaulting to FileOutputCommitterFactory [DEBUG] Creating FileOutputCommitter for path {outputPath} and context {context}"
  },
  "2af3d969_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: rpcMonitor != null → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT → CALL: invokeConcurrent → ENTRY → IF_FALSE: locations.isEmpty() → IF_FALSE: locations.size() == 1 && timeOutMs <= 0 → FOREACH: locations → FOREACH_EXIT → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.proxyOp → TRY → IF_TRUE: timeOutMs > 0 → CALL: invokeAll → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "2af3d969_2": {
    "exec_flow": "<!-- Virtual Call Chain --> ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: rpcMonitor != null → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT → CALL: invokeConcurrent → ENTRY → IF_FALSE: locations.isEmpty() → IF_FALSE: locations.size() == 1 && timeOutMs <= 0 → FOREACH: locations → FOREACH_EXIT → IF_FALSE: rpcMonitor != null → TRY → IF_TRUE: timeOutMs > 0 → CALL: invokeAll → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "8a827ea9_1": {
    "exec_flow": "ENTRY → IF_TRUE: this.fs == null → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CALL: Subject.doAs → CALL:getProps → CALL:addAll → FOREACH:keys → LOG: Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:get → CALL:toUri → CALL:get(java.net.URI, org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL:getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] Handling deprecation for (String)item [DEBUG] Bypassing cache to create filesystem {}"
  },
  "8a827ea9_2": {
    "exec_flow": "ENTRY → IF_TRUE: this.fs == null → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CALL: Subject.doAs → CALL:get → CALL:toUri → CALL:get(java.net.URI, org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL:createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] Bypassing cache to create filesystem {}"
  },
  "b3a066dc_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → EXIT → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for {item} → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry>"
  },
  "bb6c15d2_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → TRY → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → LOG:Handling deprecation for all properties in config... → CATCH:IOException → THROW:new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe) → EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → EXIT",
    "log": "<log>Handling deprecation for all properties in config...</log> <log>Unexpected SecurityException in Configuration</log>"
  },
  "a7f684ea_1": {
    "exec_flow": "ENTRY → CALL:checkAcls → CALL:checkRMStatus → TRY → CALL:rm.getRMContext().getNodeLabelManager().removeFromClusterNodeLabels → CALL:RMAuditLogger.logSuccess → RETURN → EXIT ENTRY → LOG:LOG.WARN:Exception + msg, exception → CALL:org.slf4j.Logger:warn → CALL:RMAuditLogger.logFailure → IF_TRUE:LOG.isWarnEnabled() → CALL:org.slf4j.Logger:isWarnEnabled() → LOG:LOG.WARN:createFailureLog → CALL:org.slf4j.Logger:warn → CALL:RPCUtil.getRemoteException → RETURN → EXIT",
    "log": "[INFO] RMAuditLogger: User [username] successfully executed removeFromClusterNodeLabels [WARN] Exception + msg [WARN] createFailureLog"
  },
  "a7f684ea_2": {
    "exec_flow": "ENTRY → IF_TRUE:!nodeLabelsEnabled → LOG:LOG.ERROR:NODE_LABELS_NOT_ENABLED_ERR → THROW:new IOException(NODE_LABELS_NOT_ENABLED_ERR) → EXIT",
    "log": "[ERROR] NODE_LABELS_NOT_ENABLED_ERR"
  },
  "15f80fd0_1": {
    "exec_flow": "<step>ENTRY</step> <step>TRY</step> <step>LOG:DEBUG:deleteFilesystem for filesystem: {}</step> <step>CALL:client.deleteFilesystem</step> <step>CALL:IOStatisticsBinding.trackDurationOfInvocation</step> <step>EXCEPTION:trackDurationOfInvocation</step> <step>CATCH:IOException e</step> <step>THROW:new UncheckedIOException(\"Error while tracking Duration of an AbfsRestOperation call\", e)</step> <step>LOG:DEBUG:First execution of REST operation - {}, operationType</step> <step>WHILE:!executeHttpOperation(retryCount, tracingContext)</step> <step>LOG:DEBUG:Retrying REST operation {}. RetryCount = {}</step> <step>WHILE_EXIT</step> <step>IF_FALSE:status < HTTP_CONTINUE</step> <step>IF_FALSE:status >= HttpURLConnection.HTTP_BAD_REQUEST</step> <step>LOG:TRACE:{} REST operation complete</step> <step>CALL:perfInfo.registerResult</step> <step>CALL:perfInfo.registerSuccess</step> <step>CALL:perfInfo:close</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] deleteFilesystem for filesystem: {}</log> <log>[DEBUG] First execution of REST operation - {}</log> <log>[DEBUG] Retrying REST operation {}. RetryCount = {}</log> <log>[TRACE] {} REST operation complete</log> <log>[ERROR] UncheckedIOException thrown</log>"
  },
  "8983067c_1": {
    "exec_flow": "ENTRY → LOG: [DEBUG] Renewing the delegation token → CALL: getInstance → RETURN → CALL: renewDelegationToken → RETURN → EXIT",
    "log": "[DEBUG] Renewing the delegation token"
  },
  "8983067c_2": {
    "exec_flow": "ENTRY → IF_FALSE: rmClient != null → CALL: getRenewer → ENTRY → IF_FALSE: renewer != null → SYNC: renewers → WHILE: it.hasNext() → TRY → LOG: [DEBUG] Failed to load token renewer implementation → WHILE: it.hasNext() → LOG: [WARN] No TokenRenewer defined for token kind → RETURN → EXIT → CALL: renewToken → ENTRY → CALL: createIdentifier → CALL: readFields → LOG: [INFO] Token renewal for identifier: ... → IF_TRUE: id.getMaxDate() < now → THROW: new InvalidToken(renewer + \" tried to renew an expired token \" + formatTokenId(id) + \" max expiration date: \" + Time.formatTime(id.getMaxDate()) + \" currentTime: \" + Time.formatTime(now)) → EXIT → RETURN → EXIT",
    "log": "[INFO] Token renewal for identifier: ... [DEBUG] Failed to load token renewer implementation [WARN] No TokenRenewer defined for token kind {}"
  },
  "8983067c_3": {
    "exec_flow": "ENTRY → IF_FALSE: proxy == null → TRY → IF_FALSE: proxy instanceof Closeable → IF_FALSE: handler instanceof Closeable → CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → THROW: new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass()) → EXIT",
    "log": "[ERROR] Closing proxy or invocation handler caused exception"
  },
  "2246e5f0_1": {
    "exec_flow": "ENTRY -> TRY -> IF_TRUE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> LOG: LOG.INFO: Bad conf file: top-level element not <queues> -> THROW: new RuntimeException(\"No queues defined \") -> EXIT",
    "log": "[INFO] Bad conf file: top-level element not <queues>"
  },
  "2246e5f0_2": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> IF_TRUE: acls != null -> LOG: LOG.WARN: Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. -> LOG: LOG.INFO: Bad configuration no queues defined -> THROW: new RuntimeException(\"No queues defined \") -> EXIT",
    "log": "[WARN] Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. [INFO] Bad configuration no queues defined"
  },
  "2246e5f0_3": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> IF_TRUE: acls != null -> LOG: LOG.WARN: Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. -> IF_FALSE: props == null || props.getLength() <= 0 -> FOR_INIT -> FOR_COND: i < props.getLength() -> CALL:org.apache.hadoop.mapred.QueueConfigurationParser:createHierarchy -> IF_FALSE: !(propNode instanceof Element) -> IF_TRUE: !propNode.getNodeName().equals(QUEUE_TAG) -> LOG: LOG.INFO: At root level only \"queue\" tags are allowed -> THROW: new RuntimeException(\"Malformed xml document no queue defined \") -> EXIT",
    "log": "[WARN] Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. [INFO] At root level only \"queue\" tags are allowed"
  },
  "2246e5f0_4": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> IF_TRUE: acls != null -> LOG: LOG.WARN: Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. -> FOR_INIT -> FOR_COND: i < props.getLength() -> CALL:org.apache.hadoop.mapred.QueueConfigurationParser:createHierarchy -> FOR_EXIT -> RETURN -> EXIT",
    "log": "[WARN] Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid."
  },
  "2246e5f0_5": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> IF_FALSE: acls != null -> IF_TRUE: props == null || props.getLength() <= 0 -> LOG: LOG.INFO: Bad configuration no queues defined -> THROW: new RuntimeException(\"No queues defined \") -> EXIT",
    "log": "[INFO] Bad configuration no queues defined"
  },
  "2246e5f0_6": {
    "exec_flow": "ENTRY -> TRY -> IF_FALSE: !QUEUES_TAG.equals(queuesNode.getTagName()) -> IF_FALSE: acls != null -> IF_FALSE: props == null || props.getLength() <= 0 -> FOR_INIT -> FOR_COND: i < props.getLength() -> CALL:org.apache.hadoop.mapred.QueueConfigurationParser:createHierarchy -> IF_FALSE: !(propNode instanceof Element) -> IF_TRUE: !propNode.getNodeName().equals(QUEUE_TAG) -> LOG: LOG.INFO: At root level only \"queue\" tags are allowed -> THROW: new RuntimeException(\"Malformed xml document no queue defined \") -> EXIT",
    "log": "[INFO] At root level only \"queue\" tags are allowed"
  },
  "37e4584f_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→RETURN→EXIT",
    "log": "<log>[INFO] message</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "37e4584f_2": {
    "exec_flow": "ENTRY→LOG:info→TRY→CALL:getFileSystem→FOREACH:listStatus→FOREACH:listStatus→IF:suffixMatch→FOREACH:listStatus→CALL:deleteOldLogDirsFrom→FOREACH_EXIT→CATCH→CALL:access$100→LOG:info→EXIT",
    "log": "<log>[INFO] aggregated log deletion started.</log> <log>[INFO] aggregated log deletion finished.</log>"
  },
  "37e4584f_3": {
    "exec_flow": "ENTRY→IF_TRUE:task != null && task.getRMClient() != null→CALL:RPC.stopProxy→ENTRY→IF_FALSE:proxy==null→TRY→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→EXIT→EXIT",
    "log": "<log>[ERROR] Closing proxy or invocation handler caused exception</log>"
  },
  "37e4584f_4": {
    "exec_flow": "ENTRY→IF_TRUE:task != null && task.getRMClient() != null→CALL:RPC.stopProxy→ENTRY→IF_FALSE:proxy==null→TRY→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→EXIT→EXIT",
    "log": "<log>[ERROR] RPC.stopProxy called on non proxy: class=</log>"
  },
  "0a7963c7_1": {
    "exec_flow": "<sequence> ENTRY TRY CALL:org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext:stopActiveServices() RETURN EXIT </sequence>",
    "log": "[INFO] Stopping services started for active state [WARN] Interrupted while waiting for reconstructionQueueInitializer. Returning.. [WARN] Encountered exception [INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()}"
  },
  "0a7963c7_2": {
    "exec_flow": "ENTRY→TRY→IF_FALSE: namesystem != null→CALL:stopTrashEmptier→EXCEPTION:stopTrashEmptier→CATCH:Throwable t→CALL:doImmediateShutdown→ENTRY→TRY→LOG: LOG.ERROR: Error encountered requiring NN shutdown. + Shutting down immediately., t→CALL: org.slf4j.Logger:error→EXCEPTION: error→CATCH: Throwable ignored→CALL: terminate→CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[ERROR] Error encountered requiring NN shutdown. Shutting down immediately. [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "0a7963c7_3": {
    "exec_flow": "ENTRY→TRY→IF_FALSE: namesystem != null→CALL:stopTrashEmptier→EXCEPTION:stopTrashEmptier→CATCH:Throwable t→ENTRY→TRY→LOG: LOG.ERROR: Error encountered requiring NN shutdown. + Shutting down immediately., t→CALL: org.slf4j.Logger:error→CALL: terminate→CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object)→CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object[])→CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→EXIT",
    "log": "[ERROR] Error encountered requiring NN shutdown. Shutting down immediately. [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "e582e4cf_1": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata→ENTRY→CALL:doOp→NEW:ProviderCallable<Metadata>→CALL:getMetadata→CALL:nextIdx→ENTRY→IF_FALSE:providers.length==0→FOR_INIT→FOR_COND:TRUE→TRY→CALL:call→CATCH:IOException→CALL:warn→CALL:shouldRetry→IF_TRUE:RetryDecision.FAIL→FOR_EXIT→EXIT→RETURN→EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException:"
  },
  "e582e4cf_2": {
    "exec_flow": "ENTRY→TRY→CALL:org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata→ENTRY→CALL:doOp→NEW:ProviderCallable<Metadata>→CALL:getMetadata→CALL:nextIdx→ENTRY→IF_FALSE:providers.length==0→FOR_INIT→FOR_COND:TRUE→TRY→CALL:call→CATCH:IOException→CALL:warn→CALL:shouldRetry→IF_TRUE:numFailovers>=providers.length-1→CALL:error→THROW:ioe→EXIT→RETURN→EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException: [ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable"
  },
  "e582e4cf_3": {
    "exec_flow": "ENTRY→CALL:checkNotEmpty→CALL:createURL→TRY→CALL:getDoAsUser→CALL:getActualUgi→CALL:UserGroupInformation.getCurrentUser→CALL:UserGroupInformation.logAllUserInfo→IF_TRUE:currentUgi.getRealUser()!=null→CALL:getRealUser→IF_TRUE:UserGroupInformation.isSecurityEnabled()&&!containsKmsDt(actualUgi)&&!actualUgi.shouldRelogin()→LOG:DEBUG:Using loginUser when Kerberos is enabled but the actual user does not have either KMS Delegation Token or Kerberos Credentials→CALL:getLoginUser→RETURN→EXIT→CALL:doAs→NEW:PrivilegedExceptionAction<HttpURLConnection>→CALL:createAuthenticatedURL→CALL:openConnection→CATCH:SocketTimeoutException→CALL:warn→THROW:SocketTimeoutException→EXIT→CALL:call→CALL:parseJSONMetadata→RETURN→EXIT",
    "log": "[DEBUG] Using loginUser when Kerberos is enabled but the actual user does not have either KMS Delegation Token or Kerberos Credentials [WARN] Failed to connect to {}: {}"
  },
  "d24d0e84_1": {
    "exec_flow": "ENTRY→CALL:get→LOG: LOG.INFO: Using Scheduler: + schedulerClassName→TRY→IF_TRUE: ResourceScheduler.class.isAssignableFrom(schedulerClazz)→CALL: newInstance→RETURN→EXIT",
    "log": "[INFO] Using Scheduler: schedulerClassName"
  },
  "d24d0e84_2": {
    "exec_flow": "ENTRY→CALL:get→LOG: LOG.INFO: Using Scheduler: + schedulerClassName→TRY→IF_FALSE: ResourceScheduler.class.isAssignableFrom(schedulerClazz)→THROW: new YarnRuntimeException(\"Class: \" + schedulerClassName + \" not instance of \" + ResourceScheduler.class.getCanonicalName())→EXIT",
    "log": "[INFO] Using Scheduler: schedulerClassName"
  },
  "d24d0e84_3": {
    "exec_flow": "<seq> ENTRY → CALL:getLongBytes → CALL:getFloat → CALL:org.slf4j.Logger:info → LOG.INFO: Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} → IF_TRUE: balancedPreferencePercent > 1.0 → CALL:org.slf4j.Logger:warn → LOG.WARN: The value of {preference_fraction_key} is greater than 1.0 but should be in the range 0.0 - 1.0 → IF_FALSE: balancedPreferencePercent < 0.5 → EXIT </seq>",
    "log": "<log> [INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} [WARN] The value of {preference_fraction_key} is greater than 1.0 but should be in the range 0.0 - 1.0 </log>"
  },
  "d24d0e84_4": {
    "exec_flow": "<seq> ENTRY → CALL:getLongBytes → CALL:getFloat → CALL:org.slf4j.Logger:info → LOG.INFO: Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} → IF_FALSE: balancedPreferencePercent > 1.0 → IF_TRUE: balancedPreferencePercent < 0.5 → CALL:org.slf4j.Logger:warn → LOG.WARN: The value of {preference_fraction_key} is less than 0.5 so volumes with less available disk space will receive more block allocations → EXIT </seq>",
    "log": "<log> [INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} [WARN] The value of {preference_fraction_key} is less than 0.5 so volumes with less available disk space will receive more block allocations </log>"
  },
  "d24d0e84_5": {
    "exec_flow": "<seq> ENTRY → CALL:getLongBytes → CALL:getFloat → CALL:org.slf4j.Logger:info → LOG.INFO: Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} → IF_FALSE: balancedPreferencePercent > 1.0 → IF_FALSE: balancedPreferencePercent < 0.5 → EXIT </seq>",
    "log": "<log> [INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent} </log>"
  },
  "d24d0e84_6": {
    "exec_flow": "<seq> ENTRY → CALL:org.slf4j.Logger:info → LOG.INFO: TextFileRegionAliasMap: read path {tmpfile} </seq>",
    "log": "<log> [INFO] TextFileRegionAliasMap: read path {tmpfile} </log>"
  },
  "d24d0e84_7": {
    "exec_flow": "<sequence> ENTRY → TRY → LOG:Unexpected SecurityException in Configuration → CATCH:SecurityException → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT </sequence>",
    "log": "<log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "d24d0e84_8": {
    "exec_flow": "<sequence> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → EXIT </sequence>",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>INFO</level> <template>message</template> </log_entry>"
  },
  "5a513bc4_1": {
    "exec_flow": "<!-- Preserved and optimized execution flow structure --> ENTRY → FOREACH:args → TRY → CALL:expandArgument → CATCH:IOException → CALL:displayError → ENTRY → CALL:displayWarning → CALL:org.slf4j.Logger:debug(java.lang.String) → EXIT → FOREACH_EXIT → RETURN → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Displaying error: message"
  },
  "e856d8e5_1": {
    "exec_flow": "ENTRY ➔ IF_TRUE: f != null ➔ CALL: setAclEntries ➔ IF_FALSE: x != null ➔ CALL: logEdit ➔ LOG:LOG.DEBUG:doEditTx() op={} txid={} ➔ CALL:org.slf4j.Logger:debug ➔ TRY ➔ CALL:editLogStream.write ➔ EXCEPTION:write ➔ CATCH:IOException ex ➔ CALL:reset ➔ CALL:endTransaction ➔ CALL:shouldForceSync ➔ RETURN ➔ EXIT",
    "log": "[DEBUG] logEdit called [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "e856d8e5_2": {
    "exec_flow": "ENTRY ➔ IF_FALSE: f != null ➔ IF_TRUE: x != null ➔ CALL: setXAttrs ➔ CALL: logEdit ➔ LOG:LOG.DEBUG:doEditTx() op={} txid={} ➔ CALL:org.slf4j.Logger:debug ➔ TRY ➔ CALL:editLogStream.write ➔ CALL:reset ➔ CALL:endTransaction ➔ CALL:shouldForceSync ➔ RETURN ➔ EXIT",
    "log": "[DEBUG] logEdit called [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed"
  },
  "e856d8e5_3": {
    "exec_flow": "ENTRY ➔ IF_TRUE: f != null ➔ CALL: setAclEntries ➔ IF_TRUE: x != null ➔ CALL: setXAttrs ➔ CALL: logEdit ➔ CALL:beginTransaction ➔ TRY ➔ CALL:editLogStream.writeRaw ➔ CALL:endTransaction ➔ EXIT ➔ CALL:logSync() ➔ TRY ➔ SYNC: this ➔ TRY ➔ CALL: printStatistics ➔ WHILE: mytxid > synctxid && isSyncRunning ➔ WHILE_COND: mytxid > synctxid && isSyncRunning ➔ WHILE_EXIT ➔ IF_FALSE: mytxid <= synctxid ➔ CALL: getLastJournalledTxId ➔ LOG: LOG.DEBUG: logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} ➔ IF_FALSE: lastJournalledTxId <= synctxid ➔ TRY ➔ IF_TRUE: journalSet.isEmpty() ➔ THROW: new IOException(\"No journals available to flush\") ➔ EXIT",
    "log": "[DEBUG] logEdit called [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}"
  },
  "4a33e534_1": {
    "exec_flow": "ENTRY→IF_TRUE: lister.hasNext()→CALL:org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:next()→CALL:getMultipartUploads→CALL:listIterator→CALL:hasNext→RETURN→EXIT",
    "log": "<log>[DEBUG] Next element retrieved from RemoteIterator</log> <log>[DEBUG] New listing status: {}</log> <log>[DEBUG] [{}], Requesting next {} objects under {}</log>"
  },
  "4a33e534_2": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→TRY→FOR_EXIT→RETURN→EXIT",
    "log": "<log>[INFO] Resolving file system path</log> <log>[DEBUG] Attempting symlink resolution</log>"
  },
  "4a33e534_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.RemoteIterator:next→RETURN→EXIT",
    "log": "<log><![CDATA[[DEBUG] Next element retrieved from RemoteIterator]]></log>"
  },
  "4a33e534_4": {
    "exec_flow": "ENTRY->IF_TRUE: debugDelay != -1->LOG: LOG.DEBUG: Scheduling DeletionTask (delay {}) : {}, debugDelay, deletionTask->CALL: recordDeletionTaskInStateStore->IF_FALSE:!stateStore.canRecover()->IF_FALSE:task.getTaskId()!=DeletionTask.INVALID_TASK_ID->CALL:setTaskId->FOREACH:successors->FOREACH_EXIT->TRY->CALL:stateStore.storeDeletionTask->EXCEPTION:storeDeletionTask->CATCH:IOException e->CALL:org.slf4j.Logger:error->EXIT->CALL: schedule->EXIT",
    "log": "<log>[DEBUG] Scheduling DeletionTask (delay {}) : {}</log> <log>[ERROR] Unable to store deletion task</log>"
  },
  "a84bfc20_1": {
    "exec_flow": "<entry>Parent.ENTRY</entry> <call>[VIRTUAL_CALL]</call> <child_paths> <path> <id>P1-C3-Merged</id> <eval>true</eval> <exec_flow> ENTRY → IF_TRUE:clientService!=null → CALL:getBindAddress → TRY → IF_FALSE:serviceAddr!=null → CALL:authorizeAndGetInterceptorChain → LOG:LOG.INFO:Registering application master. Host: request.getHost() Port: request.getRpcPort() Tracking Url: request.getTrackingUrl() for application pipeline.getApplicationAttemptId() → CALL:registerApplicationMaster → CALL:succeededRegisterAMRequests → CALL:getMaximumResourceCapability → CALL:this.context.getClusterInfo().setMaxContainerCapability → IF_TRUE:UserGroupInformation.isSecurityEnabled() → CALL:setClientToAMToken → CALL:getApplicationACLs → LOG:LOG.INFO:maxContainerCapability:+maxContainerCapability → EXCEPTION:info → CATCH:Exception are → LOG:LOG.ERROR:Exception while registering, are → THROW:new YarnRuntimeException(are) → EXIT",
    "log": "[INFO] Registering application master. Host: ... Port: ... Tracking Url: ... for application ... [INFO] maxContainerCapability: + maxContainerCapability [INFO] RegisterAM processing finished in ... ms for application ... [ERROR] Exception while registering"
  },
  "a84bfc20_2": {
    "exec_flow": "ENTRY → IF_FALSE:clientService!=null → TRY → CALL:getAMWebappScheme → CALL:setHost → CALL:setRpcPort → CALL:setTrackingUrl → CALL:registerApplicationMaster → CALL:getMaximumResourceCapability → CALL:this.context.getClusterInfo().setMaxContainerCapability → IF_FALSE:UserGroupInformation.isSecurityEnabled() → CALL:getApplicationACLs → LOG:LOG.INFO:maxContainerCapability:+maxContainerCapability → LOG:LOG.INFO:queue:+queue → CALL:setQueueName → CALL:this.schedulerResourceTypes.addAll → EXIT",
    "log": "[INFO] maxContainerCapability: + maxContainerCapability [INFO] queue: + queue"
  },
  "a84bfc20_3": {
    "exec_flow": "ENTRY → TRY → CALL:authorizeAndGetInterceptorChain → LOG:LOG.INFO:Registering application master. Host: request.getHost() Port: request.getRpcPort() Tracking Url: request.getTrackingUrl() for application pipeline.getApplicationAttemptId() → CALL:registerApplicationMaster → EXCEPTION:IOException → CATCH:Throwable t → CALL:incrFailedRegisterAMRequests → THROW:t → EXIT",
    "log": "[INFO] Registering application master. Host: ... Port: ... Tracking Url: ... for application ..."
  },
  "81c8c234_1": {
    "exec_flow": "ENTRY→IF_FALSE: getServiceStopped()→IF_FALSE: entityIds == null→FOREACH: entityIds→CALL: setEntityId→CALL: setEntityType→FOREACH: entity.getEvents()→CALL: org.apache.hadoop.yarn.server.timeline.TimelineStoreMapAdapter:get(java.lang.Object)→IF_FALSE: events.getEvents().size() >= limit→IF_FALSE: event.getTimestamp() <= windowStart→IF_FALSE: event.getTimestamp() > windowEnd→RETURN→EXIT",
    "log": "[INFO] Service stopped, return null for the storage"
  },
  "81c8c234_2": {
    "exec_flow": "ENTRY→IF_TRUE: getServiceStopped()→LOG: LOG.INFO: Service stopped, return null for the storage→RETURN→EXIT",
    "log": "[INFO] Service stopped, return null for the storage"
  },
  "81c8c234_3": {
    "exec_flow": "ENTRY→[VIRTUAL_CALL]→IF_FALSE: startTimeBytes == null→TRY→CALL: getEntityKey→CALL: getEntityForKey→RETURN→EXIT",
    "log": "[ERROR] GenericObjectMapper cannot read key from key {entityId.toString()} into an object. Read aborted! [ERROR] {e.getMessage()}"
  },
  "1c230bb2_1": {
    "exec_flow": "<![CDATA[ ENTRY → IF_TRUE: props != null → ENTRY: loadProps → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource LOG: Handling deprecation for all properties in config... FOREACH: LOG: Handling deprecation for (String)item ]]>",
    "log": "<![CDATA[ LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String)item); ]]>"
  },
  "14441ebe_1": {
    "exec_flow": "ENTRY→CALL:Preconditions.checkNotNull→CALL:Preconditions.checkArgument→CALL:org.apache.hadoop.crypto.key.KeyProvider:getKeyVersion→TRY→CALL:org.apache.hadoop.crypto.CryptoCodec:getInstance→CALL:decryptEncryptedKey→RETURN→EXIT",
    "log": "[WARN] KMS provider at [{}] threw an IOException: [WARN] Failed to connect to {}: {} [ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable [DEBUG] Getting configuration value for cipher suite [INFO] Calling getInstance with CipherSuite [DEBUG] Codec classes obtained [DEBUG] New instance created"
  },
  "abbea41f_1": {
    "exec_flow": "ENTRY → CALL:computeJobAllocation → IF_TRUE: contract.getRecurrenceExpression() != null → IF_TRUE: period > 0 → CALL:PeriodicRLESparseResourceAllocation:<init> → IF_FALSE: allocation == null → IF_FALSE: oldReservation != null → CALL: addReservation → ENTRY → IF_FALSE: inMemReservation.getUser() == null → CALL: writeLock.lock → TRY → IF_FALSE: reservationTable.containsKey(inMemReservation.getReservationId()) → IF_TRUE: !isRecovering → CALL: validate → CALL: setAcceptanceTimestamp → IF_TRUE: rmStateStore != null → CALL: rmStateStore.storeNewReservation → CALL: incrementAllocation → IF_FALSE: reservations == null → IF_TRUE: !reservations.add(inMemReservation) → LOG: LOG.ERROR: Unable to add reservation: ... to plan. → RETURN → EXIT",
    "log": "[INFO] Added new reservation for periodic allocation [ERROR] Unable to add reservation: ... to plan."
  },
  "abbea41f_2": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → IF_FALSE: currReservation == null → CALL: validate → CALL: org.apache.hadoop.yarn.server.resourcemanager.reservation.SharingPolicy:validate(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan, org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation) → IF_FALSE: !removeReservation(currReservation) → TRY → CALL: addReservation → IF_TRUE: result → LOG: LOG.INFO: Successfully updated reservation: {} in plan. → CALL: org.slf4j.Logger:info(java.lang.String, java.lang.Object) → RETURN → EXIT",
    "log": "[INFO] Successfully updated reservation: {} in plan."
  },
  "abbea41f_3": {
    "exec_flow": "ENTRY → CALL: writeLock.lock → TRY → IF_FALSE: currReservation == null → CALL: validate → CALL: org.apache.hadoop.yarn.server.resourcemanager.reservation.SharingPolicy:validate(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan, org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation) → IF_FALSE: !removeReservation(currReservation) → TRY → CALL: addReservation → IF_FALSE: result → CALL: addReservation → LOG: LOG.INFO: Rollbacked update reservation: {} from plan. → CALL: org.slf4j.Logger:info(java.lang.String, java.lang.Object) → RETURN → EXIT",
    "log": "[INFO] Rollbacked update reservation: {} from plan."
  },
  "abbea41f_4": {
    "exec_flow": "ENTRY→IF_FALSE:startTime >= 0 && endTime > startTime && endTime <= timePeriod→LOG:LOG.INFO:Cannot set capacity beyond end time: + timePeriod + was ( + interval.toString() + )→RETURN→EXIT",
    "log": "[INFO] Cannot set capacity beyond end time: {timePeriod} was ({interval.toString()})"
  },
  "252a8d6d_1": {
    "exec_flow": "ENTRY→IF_TRUE: configName != null→IF_TRUE: target == null→THROW: new IllegalArgumentException(\"Target address cannot be null.\" + helpText)→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "252a8d6d_2": {
    "exec_flow": "ENTRY→CALL: substituteVars→CALL: getRaw→ENTRY→LOG: Handling deprecation for all properties in config...→CALL: getProps→CALL: addAll→FOREACH: keys→LOG: Handling deprecation for (String)item→CALL: handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration, se"
  },
  "252a8d6d_3": {
    "exec_flow": "ENTRY→IF_TRUE: configName != null→IF_FALSE: target == null→CALL: trim→CALL: createURI→IF_TRUE: port == -1→IF_FALSE: (host == null) || (port < 0) || (!hasScheme && path != null && !path.isEmpty())→CALL: createSocketAddrForHost→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "252a8d6d_4": {
    "exec_flow": "ENTRY→IF_TRUE: configName != null→IF_FALSE: target == null→CALL: trim→CALL: createURI→IF_FALSE: port == -1→IF_FALSE: (host == null) || (port < 0) || (!hasScheme and path != null and !path.isEmpty())→CALL: createSocketAddrForHost→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "252a8d6d_5": {
    "exec_flow": "ENTRY→IF_FALSE: configName != null→IF_FALSE: target == null→CALL: trim→CALL: createURI→IF_TRUE: port == -1→IF_FALSE: (host == null) || (port < 0) || (!hasScheme and path != null and !path.isEmpty())→CALL: createSocketAddrForHost→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "252a8d6d_6": {
    "exec_flow": "ENTRY→IF_FALSE: configName != null→IF_FALSE: target == null→CALL: trim→CALL: createURI→IF_FALSE: port == -1→IF_FALSE: (host == null) || (port < 0) || (!hasScheme and path != null and !path.isEmpty())→CALL: createSocketAddrForHost→RETURN→EXIT",
    "log": "[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "e025df2f_1": {
    "exec_flow": "ENTRY→CALL: FSImageSerialization.writeString→CALL: org.slf4j.Logger:warn→EXIT",
    "log": "WARN: org.slf4j.Logger:warn"
  },
  "7267602e_1": {
    "exec_flow": "ENTRY → CALL: lock → TRY → IF_TRUE: !replica.purged → IF_TRUE: !replica.getDataStream().getChannel().isOpen() → IF_TRUE: purgeReason != null → LOG: LOG.DEBUG: {}: {}, this, purgeReason → CALL: purge → IF_FALSE: newRefCount == 0 → IF_FALSE: newRefCount == 1 → CALL: Preconditions.checkArgument → IF_FALSE: LOG.isTraceEnabled() → IF_FALSE: shouldTrimEvictionMaps → CALL: unlock → EXIT",
    "log": "[DEBUG]: {}: {}, this, purgeReason"
  },
  "7267602e_2": {
    "exec_flow": "ENTRY → CALL: lock → TRY → IF_TRUE: !replica.purged → IF_TRUE: !replica.getDataStream().getChannel().isOpen() → IF_FALSE: purgeReason != null → IF_TRUE: newRefCount == 0 → CALL: Preconditions.checkArgument → CALL: close → IF_TRUE: LOG.isTraceEnabled() → LOG: LOG.TRACE: this + \": unref replica \" + replica + \": \" + addedString + \" refCount \" + (newRefCount + 1) + \" -> \" + newRefCount + StringUtils.getStackTrace(Thread.currentThread()) → IF_FALSE: shouldTrimEvictionMaps → CALL: unlock → EXIT",
    "log": "[TRACE]: this + \": unref replica \" + replica + \": \" + addedString + \" refCount \" + (newRefCount + 1) + \" -> \" + newRefCount"
  },
  "7267602e_3": {
    "exec_flow": "ENTRY→CALL:IOUtilsClient.cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close→CATCH→IF(log != null AND log.isDebugEnabled())→CALL:log.isDebugEnabled→CALL:log.debug→FOREACH_EXIT→IF_TRUE:slowReadBuff != null→CALL:bufferPool.returnBuffer→IF_TRUE:checksumBuff != null→CALL:bufferPool.returnBuffer→EXIT",
    "log": "[DEBUG] Exception in closing closeable"
  },
  "7267602e_4": {
    "exec_flow": "<step>EncryptedPeer:close() -> in.close()</step> <step>EncryptedPeer:close() -> out.close()</step> <step>EncryptedPeer:close() -> enclosedPeer.close()</step> <step>Peer:close()</step> <step>Org.apache.hadoop.hdfs.net.DomainPeer:close()</step> <step>Org.apache.hadoop.net.unix.DomainSocket:close()</step> <step>IOUtilsClient.cleanupWithLogger -> FOREACH:closeables -> IF(c != null) -> TRY -> CALL:c.close</step> <step>CATCH -> IF(log != null AND log.isDebugEnabled()) -> CALL:log.isDebugEnabled -> CALL:log.debug</step> <step>RETURN -> EXIT -> CALL:startExpiryDaemon</step> <step>IF_TRUE:capacity==multimap.size() -> IF_FALSE:!iter.hasNext() -> CALL:remove</step> <step>CALL:peerCache.put -> EXIT</step>",
    "log": "<log>LOG.error \"shutdown error: \", e</log> <log>[DEBUG] Exception in closing closeable</log>"
  },
  "922a16ae_1": {
    "exec_flow": "ENTRY → IF_TRUE:!isInitialized() → SYNC:UserGroupInformation.class → IF_TRUE:!isInitialized() → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → CALL:debug → LOG:Unexpected SecurityException in Configuration → RETURN → EXIT",
    "log": "<log> [WARN] Unexpected SecurityException in Configuration [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] Loaded {} base64 tokens [DEBUG] UGI loginUser: {} [WARN] Null token ignored for {} [WARN] Unexpected SecurityException in Configuration </log>"
  },
  "2ea2187a_1": {
    "exec_flow": "QuorumJournalManager.LOG.warn(\"Aborting \" + this)→buf = null→close()",
    "log": "[WARN] Aborting [QuorumOutputStream instance]"
  },
  "a7b25b82_1": {
    "exec_flow": "ENTRY → IF_TRUE: conf != null → LOG:Handling deprecation for all properties in config... → CALL:Configuration:get → TRY → IF_TRUE: confUmask != null → LOG:Handling deprecation for (String)item → CALL:getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log>"
  },
  "a7b25b82_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG → CALL: statIncrement → CALL: trailingPeriodCheck → CALL: makeQualified → TRY → CALL: createFile → CALL: statIncrement → NEW: FSDataOutputStream → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</log>"
  },
  "a7b25b82_3": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG → CALL: statIncrement → CALL: trailingPeriodCheck → CALL: makeQualified → TRY → EXCEPTION: createFile → CATCH: AzureBlobFileSystemException → CALL: checkException → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</log>"
  },
  "a7b25b82_4": {
    "exec_flow": "ENTRY → TRY → CALL: getFileStatus → IF_FALSE: status.isDirectory() → IF_FALSE: !overwrite → LOG: debug → EXCEPTION: debug → CATCH: FileNotFoundException → CALL: getMultipartSizeProperty → NEW: FSDataOutputStream → NEW: AliyunOSSBlockOutputStream → CALL: getConf → NEW: SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "<log>[DEBUG] Overwriting file {}</log>"
  },
  "a7b25b82_5": {
    "exec_flow": "ENTRY → TRY → CALL: getFileStatus → IF_FALSE: status.isDirectory() → IF_FALSE: !overwrite → LOG: debug → CALL: getMultipartSizeProperty → NEW: FSDataOutputStream → NEW: AliyunOSSBlockOutputStream → CALL: getConf → NEW: SemaphoredDelegatingExecutor → RETURN → EXIT",
    "log": "<log>[DEBUG] Overwriting file {}</log>"
  },
  "71d86fc0_1": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser == null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → IF_TRUE:readTokenStorageStream succeeded → CALL:addCredentials → CALL:debug → CALL:cleanupWithLogger → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "[DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [INFO] Cleaning up resources [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Name service ID {} will use virtual IP {} for failover"
  },
  "71d86fc0_2": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_FALSE:subject == null || subject.getPrincipals(User.class).isEmpty() → NEW:UserGroupInformation → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → LOG:Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "e23561b2_1": {
    "exec_flow": "ENTRY→CALL:finished→IF_TRUE:logAtInfo→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)→LOG:[INFO] {}→EXIT",
    "log": "<log_entry> <log_level>INFO</log_level> <template>[INFO] {}</template> </log_entry>"
  },
  "e23561b2_2": {
    "exec_flow": "ENTRY→CALL:finished→IF_FALSE:logAtInfo→IF_TRUE:log.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→LOG:[DEBUG] {}→EXIT",
    "log": "<log_entry> <log_level>DEBUG</log_level> <template>[DEBUG] {}</template> </log_entry>"
  },
  "8a9f5972_1": {
    "exec_flow": "ENTRY → TRY → CALL:toBoolean → CATCH:TrileanConversionException → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext) → CATCH:AbfsRestOperationException → CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode() → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close() → ENTRY → VIRTUAL_CALL → LOG: LOG.DEBUG: createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {} → CALL:createDefaultHeaders → IF_TRUE: isFile → CALL: addCustomerProvidedKeyHeaders → IF_TRUE: !overwrite → CALL: requestHeaders.add → IF_FALSE: permission != null && !permission.isEmpty() → IF_FALSE: umask != null && !umask.isEmpty() → IF_FALSE: eTag != null && !eTag.isEmpty() → CALL: abfsUriQueryBuilder.addQuery → IF_FALSE: isAppendBlob → CALL: appendSASTokenToQuery → TRY → CALL: IOStatisticsBinding.trackDurationOfInvocation → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:lambda$execute$0 → CATCH: AzureBlobFileSystemException ex → IF_TRUE: !op.hasResult() → THROW: ex → CALL: perfInfo.registerResult(op.getResult()).registerSuccess → CALL: maybeCreateLease → ENTRY → VIRTUAL_CALL → LOG: LOG.DEBUG: Attempting to acquire lease on {}, retry {}, path, numRetries → IF_FALSE: future != null && !future.isDone() → CALL: schedule → CALL: acquireLease → CALL: addCallback → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsLease:lambda$0(tracingContext) → EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}</log> <log>[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call</log> <log>[DEBUG] Get root ACL status</log> <log>[DEBUG] IOStatisticsBinding track duration started</log> <log>[TRACE] Fetch SAS token for {operation} on {path}</log> <log>[TRACE] SAS token fetch complete for {operation} on {path}</log> <log>[DEBUG] First execution of REST operation - {}</log> <log>[DEBUG] Retrying REST operation {}. RetryCount = {}</log> <log>[DEBUG] Unexpected error.</log> <log>[TRACE] {} REST operation complete</log> <log>[DEBUG] IOStatisticsBinding track duration completed</log> <log>[DEBUG] Attempting to acquire lease on {}, retry {}</log>"
  },
  "8a9f5972_2": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:create → RETURN → EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "8a9f5972_3": {
    "exec_flow": "ENTRY → IF_FALSE:!enabled → IF_TRUE:isValidInstant(perfInfo.getAggregateStart())&&perfInfo.getAggregateCount()>0 → CALL:recordClientLatency → ENTRY → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:isValidInstant → CALL:isValidInstant → CALL:Duration.between → CALL:String.format → CALL:offerToQueue → IF_TRUE:LOG.isDebugEnabled() → CALL:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Queued latency info [{} ms]: {}, elapsed, latencyDetails → RETURN → EXIT → EXIT",
    "log": "<log>[DEBUG] Queued latency info [{} ms]: {}</log>"
  },
  "8a9f5972_4": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: Creating file: {}, f.toString() → IF_FALSE: containsColon(f) → CALL: performAuthCheck → CALL: createInternal → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "8a9f5972_5": {
    "exec_flow": "ENTRY → CALL:connect → TRY → CALL:getFileStatus → IF_TRUE:status != null → IF_TRUE:overwrite && !status.isDirectory() → CALL:delete → IF_FALSE:parent == null || !mkdirs(client, parent, FsPermission.getDirDefault()) → CALL:allocate → CALL:changeWorkingDirectory → IF_TRUE:!FTPReply.isPositivePreliminary(client.getReplyCode()) → IF_TRUE:outputStream != null → CALL:IOUtils.closeStream → CALL:disconnect → THROW:new IOException(\"Unable to create file: \" + file + \", Aborting\") → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "62a3371c_1": {
    "exec_flow": "<step>Parent.ENTRY → [VIRTUAL_CALL] → Child paths</step> <parent_call>VIRTUAL_CALL</parent_call> <child_node> <method>org.apache.hadoop.fs.AbstractFileSystem:open</method> </child_node>",
    "log": "<log>Fetching FsStatus</log> <log>AccessControlException</log> <log>FileNotFoundException</log> <log>IOException</log> <log_from_child>FSDataInputStream open(final Path f, int bufferSize)</log_from_child> <log>INFO: Connect started</log> <log>DEBUG: Connection details: {details}</log> <log>ERROR: Connection failed at step {step}</log>"
  },
  "62a3371c_2": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.FilterFs:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] FilterFs listStatus call invoked</log>"
  },
  "62a3371c_3": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.viewfs.ViewFs:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] ViewFs listStatus call invoked</log>"
  },
  "62a3371c_4": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] ChRootedFs listStatus call invoked</log>"
  },
  "62a3371c_5": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] ViewFs InternalDirOfViewFs listStatus call invoked</log>"
  },
  "62a3371c_6": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.DelegateToFileSystem:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] DelegateToFileSystem listStatus call invoked</log>"
  },
  "62a3371c_7": {
    "exec_flow": "<step>VIRTUAL_PARENT.ENTRY → CALL:org.apache.hadoop.fs.Hdfs:listStatus → RETURN → EXIT</step>",
    "log": "<log>[INFO] Hdfs listStatus call invoked</log>"
  },
  "62a3371c_8": {
    "exec_flow": "<!-- No specific execution flow as the parent has no logs -->",
    "log": "<!-- Inherited child log sequence due to null parent handling -->"
  },
  "e0d8ca81_1": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> EXIT",
    "log": "[DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {}"
  },
  "e0d8ca81_2": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> RETURN -> EXIT",
    "log": "[DEBUG] Service: {} entered state {}"
  },
  "e0d8ca81_3": {
    "exec_flow": "ENTRY -> TRY -> CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners -> SYNC: this -> CALL: toArray -> CALL: size -> FOREACH: callbacks -> CALL: stateChanged -> FOREACH_EXIT -> CATCH: Throwable e -> CALL: LOG.WARN: Exception while notifying listeners of {}, this, e -> EXIT",
    "log": "[WARN] Exception while notifying listeners of {}"
  },
  "c51bc6d0_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → HANDLE IOException [VIRTUAL_CALL] → CALL: ensureInitialized → IF_FALSE: (subject == null || subject.getPrincipals(User.class).isEmpty()) → CALL: getLoginUser → RETURN → EXIT → CALL: org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → FOR_INIT → FOR_COND:i < futures.size() → CALL:org.slf4j.Logger:error → FOR_EXIT → RETURN → EXIT",
    "log": "[ERROR] Cannot get mount point [DEBUG] Proxying operation: {} [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Cannot execute {m.getName()} in {location}: {cause.getMessage()} [DEBUG] User {} NN {} is using connection {} [ERROR] Cannot get available namenode for... [ERROR] Unexpected exception while proxying API"
  },
  "0fa2856b_1": {
    "exec_flow": "<seq>ENTRY→IF_TRUE:isMultiThreadNecessary(args)→CALL:initThreadPoolExecutor→CALL:org.apache.hadoop.fs.shell.CommandWithDestination:processArguments→IF_TRUE:executor != null→CALL:waitForCompletion→executor.shutdown()→executor.awaitTermination(Long.MAX_VALUE, TimeUnit.MINUTES)→catch (InterruptedException e)→executor.shutdownNow()→displayError(e)→Thread.currentThread().interrupt()→EXIT</seq>",
    "log": "<!-- No merged logs from parent; inheriting child log sequence if applicable -->"
  },
  "0fa2856b_2": {
    "exec_flow": "<seq>ENTRY→IF_FALSE:isMultiThreadNecessary(args)→CALL:org.apache.hadoop.fs.shell.CommandWithDestination:processArguments→IF_TRUE:executor != null→CALL:waitForCompletion→executor.shutdown()→executor.awaitTermination(Long.MAX_VALUE, TimeUnit.MINUTES)→catch (InterruptedException e)→executor.shutdownNow()→displayError(e)→Thread.currentThread().interrupt()→EXIT</seq>",
    "log": "<!-- No merged logs from parent; inheriting child log sequence if applicable -->"
  },
  "0fa2856b_3": {
    "exec_flow": "<seq>ENTRY→IF_TRUE:isMultiThreadNecessary(args)→CALL:initThreadPoolExecutor→CALL:org.apache.hadoop.fs.shell.CommandWithDestination:processArguments→IF_FALSE:executor != null→EXIT</seq>",
    "log": "<!-- No merged logs from parent; inheriting child log sequence if applicable -->"
  },
  "0fa2856b_4": {
    "exec_flow": "<seq>ENTRY→IF_FALSE:isMultiThreadNecessary(args)→CALL:org.apache.hadoop.fs.shell.CommandWithDestination:processArguments→IF_FALSE:executor != null→EXIT</seq>",
    "log": "<!-- No merged logs from parent; inheriting child log sequence if applicable -->"
  },
  "0fa2856b_5": {
    "exec_flow": "ENTRY→IF_FALSE: args.size() > 1→IF_TRUE: dst.exists→IF_TRUE: !dst.stat.isDirectory() && !overwrite→CALL:org.slf4j.Logger:debug→THROW:new PathExistsException→EXIT",
    "log": "[DEBUG] Destination file exists: {dst.stat}"
  },
  "b477877f_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> VIRTUAL_CALL: org.apache.hadoop.streaming.StreamJob:init() → ENTRY→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "699b7c3b_1": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop()→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_FALSE: enterState(STATE.STOPPED) != STATE.STOPPED→LOG: LOG.DEBUG: Ignoring re-entrant call to stop()→CALL: notifyListeners→TRY→CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners→CALL: globalListeners.notifyListeners→CATCH: Throwable e→LOG: LOG.WARN: Exception while notifying listeners of {}→FOR_EXIT→IF_TRUE: firstException != null→THROW: ServiceStateException.convert(firstException)→EXIT",
    "log": "[DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "699b7c3b_2": {
    "exec_flow": "ENTRY→FOR_INIT→FOR_COND: i >= 0→CALL:org.apache.hadoop.service.AbstractService:stop()→IF_FALSE: isInState(STATE.STOPPED)→SYNC: stateChangeLock→IF_TRUE: enterState(STATE.STOPPED) != STATE.STOPPED→CALL: org.apache.hadoop.service.AbstractService:enterState→IF_TRUE: oldState != newState→LOG: LOG.DEBUG: Service: {} entered state {}→CALL: recordLifecycleEvent→TRY→CALL: serviceStop→EXCEPTION: serviceStop→CATCH: Exception e→LOG: LOG.INFO: Service {} failed in state {} (exception details)→CALL: noteFailure→THROW: ServiceStateException.convert(e)→FOR_EXIT→IF_TRUE: firstException != null→THROW: ServiceStateException.convert(firstException)→EXIT",
    "log": "[DEBUG] Stopping service #i: {service} [DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {} (exception details)"
  },
  "21bd690d_1": {
    "exec_flow": "ENTRY → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry>"
  },
  "21bd690d_2": {
    "exec_flow": "<seq> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → EXIT </seq>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "21bd690d_3": {
    "exec_flow": "<step>checkArgument ENTRY</step> <step>IF expression is false THEN</step> <step>TRY to get message from msgSupplier</step> <step>CATCH Exception THEN log \"Error formatting message\"</step> <step>THROW IllegalArgumentException</step>",
    "log": "<log>[DEBUG] Error formatting message</log>"
  },
  "21bd690d_4": {
    "exec_flow": "ENTRY → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log_entry>[INFO] message</log_entry>"
  },
  "314ddf78_1": {
    "exec_flow": "ENTRY→CALL:makeQualified→NEW:Path→CALL:getHomeDirectory→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→ENTRY→CALL:getAuthenticationMethod→IF_TRUE:overrideNameRules || !HadoopKerberosName.hasRulesBeenSet()→TRY→CALL:HadoopKerberosName.setConfiguration→EXCEPTION:setConfiguration→CATCH:IOException→THROW:new RuntimeException(\"Problem with Kerberos auth_to_local name configuration\", ioe)→EXIT→IF_TRUE:GROUPS == null→IF_TRUE:LOG.isDebugEnabled()→CALL:isDebugEnabled→LOG:LOG.DEBUG:Creating new Groups object→NEW:Groups→CALL:<init>→RETURN→EXIT→EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "e55280a9_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→EXCEPTION:debug→CATCH:FileNotFoundException→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "<log>[DEBUG] Overwriting file {}</log>"
  },
  "e55280a9_2": {
    "exec_flow": "ENTRY→CALL: Preconditions.checkNotNull→IF_FALSE: InodeTree.SlashPath.equals(f)→IF_TRUE: this.fsState.getRootFallbackLink() != null→IF_FALSE: theInternalDir.getChildren().containsKey(f.getName())→TRY→CALL: org.apache.hadoop.fs.FileSystem:create→RETURN→EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "6d741c83_1": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:args</step> <step>TRY</step> <step>CALL:expandArgument</step> <step>Invoke PathData.expandAsGlob(arg, getConf())</step> <step>Check if items.length == 0</step> <step>Throw PathNotFoundException if true</step> <step>CATCH:IOException</step> <step>CALL:displayError</step> <step>CALL:displayWarning</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object)</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "6d741c83_2": {
    "exec_flow": "<step>ENTRY</step> <step>FOREACH:args</step> <step>TRY</step> <step>CALL:expandArgument</step> <step>Invoke PathData.expandAsGlob(arg, getConf())</step> <step>Check if items.length == 0</step> <step>Throw PathNotFoundException if true</step> <step>CATCH:IOException</step> <step>CALL:displayError</step> <step>CALL:displayWarning</step> <step>CALL:org.slf4j.Logger:debug(java.lang.String)</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Displaying error: message"
  },
  "1286dc32_1": {
    "exec_flow": "ENTRY→CALL:getNameNodeIds→FOREACH:emptyAsSingletonNull→CALL:checkKeysAndProcess→CALL:createSocketAddr→IF_TRUE:configName != null→IF_FALSE:target == null→CALL:trim→CALL:createURI→IF_TRUE:port == -1→IF_FALSE:(host == null) || (port < 0) || (!hasScheme && path != null && !path.isEmpty())→CALL:createSocketAddrForHost→RETURN→EXIT",
    "log": "<log>[WARN] Namenode for {} remains unresolved for ID {}. Check your hdfs-site.xml file to ensure namenodes are configured properly.</log>"
  },
  "1286dc32_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → [VIRTUAL_CALL] → substituteVars → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log> <log>[WARN] Namenode for {} remains unresolved for ID {}. Check your hdfs-site.xml file to ensure namenodes are configured properly.</log>"
  },
  "1286dc32_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "1286dc32_4": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "1286dc32_5": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "1286dc32_6": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.conf.Configuration:getTrimmed → IF_TRUE: namenodeId != null → RETURN → EXIT",
    "log": "<log>[INFO] Retrieved trimmed value for namenode id</log>"
  },
  "1286dc32_7": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.hdfs.DFSUtil:getSuffixIDs → IF_TRUE: suffixes == null → THROW: new HadoopIllegalArgumentException → EXIT",
    "log": "<log>[ERROR] Configuration must be suffixed with nameservice and namenode ID for HA configuration</log>"
  },
  "1286dc32_8": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.hdfs.DFSUtil:getSuffixIDs → IF_FALSE: suffixes == null → RETURN → EXIT",
    "log": "<log>[INFO] Suffix IDs retrieved successfully</log>"
  },
  "aa220892_1": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CALL:getJobUsingCluster → IF_FALSE:j==null → CALL:getTaskReports → CALL:downgradeArray → RETURN → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}]"
  },
  "aa220892_2": {
    "exec_flow": "ENTRY → TRY → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: PrivilegedAction [as: {}][action: {}], this, action, new Exception() → CATCH: PrivilegedActionException → LOG: LOG.DEBUG: PrivilegedActionException as: {}, this, cause → CALL:getJobUsingCluster → IF_TRUE:j==null → RETURN:EMPTY_TASK_REPORTS → EXIT",
    "log": "[DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {}"
  },
  "eb00b0c4_1": {
    "exec_flow": "ENTRY→CALL:ensureInitialized→IF_TRUE:!isInitialized()→SYNC:UserGroupInformation.class→IF_TRUE:!isInitialized()→CALL:initialize→LOG:Handling deprecation for all properties in config...→RETURN→EXIT→IF_TRUE:loginUser==null→DO_WHILE→IF_TRUE:loginUserRef.compareAndSet(null,newLoginUser)→CALL:createLoginUser→TRY→CALL:doSubjectLogin→IF_TRUE:proxyUser==null→CALL:getProperty→CALL:createProxyUser→CALL:tokenFileLocations.addAll→CALL:getTrimmedStringCollection→CALL:get→CALL:getTrimmedStringCollection→CALL:getTokenFileLocation→CALL:exists→CALL:isFile→CALL:readTokenStorageFile→CALL:addCredentials→CALL:debug→EXCEPTION:readTokenStorageStream→CATCH:IOException ioe→CALL:IOUtils.cleanupWithLogger→THROW:new IOException(\"Exception reading \" + filename, ioe)→EXIT→CALL:debug→CALL:loginUser.spawnAutoRenewalThreadForUserCreds(false)→DO_COND:loginUser==null→DO_EXIT→RETURN→EXIT→IF_TRUE: props != null → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → CALL: addTags → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Creating new Groups object [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [INFO] Cleaning up resources [DEBUG] Failure to load login credentials [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for {item}"
  },
  "b04b9055_1": {
    "exec_flow": "<entry>ENTRY</entry> <if condition=\"LOG.isDebugEnabled()\"> <log>[DEBUG] Looking for Job + jobId</log> </if> <call>getFileInfo</call> <if condition=\"fileInfo == null\"> <exception type=\"HSFileRuntimeException\">Unable to find job + jobId</exception> </if> <virtual_call> <entry>Parent.ENTRY</entry> <call>VIRTUAL_CALL</call> <seq>ENTRY→LOG: LOG.INFO: Loading history file: [historyFileAbsolute]→IF_FALSE: this.jobInfo != null→IF_TRUE: historyFileAbsolute != null→TRY→CALL: createJobHistoryParser→CALL: parse→IF_FALSE: parseException != null→IF_TRUE: loadTasks→CALL: loadAllTasks→LOG: LOG.INFO: TaskInfo loaded→EXIT</seq> </virtual_call> <if condition=\"fileInfo.isDeleted()\"> <exception type=\"HSFileRuntimeException\">Cannot load deleted job + jobId</exception> </if> <return>loadJob</return> <exit>EXIT</exit>",
    "log": "<log> <fingerprint>org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:loadJob()[DEBUG]Looking for Job + jobId</fingerprint> </log> <log> <fingerprint>org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:waitUntilMoved()[WARN]Waiting has been interrupted</fingerprint> </log> <log> <fingerprint>org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:loadJob()[INFO]Loading history file: [historyFileAbsolute]</fingerprint> </log> <log> <fingerprint>org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:loadJob()[INFO]TaskInfo loaded</fingerprint> </log>"
  },
  "e70ed96f_1": {
    "exec_flow": "ENTRY→CALL:readProcCpuInfoFile→IF_FALSE:readCpuInfoFile→TRY→CALL:Files.newInputStream→CALL:Charset.forName→CALL:BufferedReader→TRY→CALL:in.readLine→CALL:PROCESSOR_FORMAT.matcher→CALL:FREQUENCY_FORMAT.matcher→CALL:PHYSICAL_ID_FORMAT.matcher→CALL:CORE_ID_FORMAT.matcher→CALL:in.readLine→CATCH:IOException→CALL:Logger.warn→FINALLY→CALL:Reader.close→CALL:Reader.close→CALL:Logger.warn→RETURN→EXIT→RETURN→EXIT",
    "log": "[WARN] Error reading the stream [IOException] [WARN] Error closing the stream [IOException]"
  },
  "e70ed96f_2": {
    "exec_flow": "ENTRY → CALL: getNumProcessors → TRY → CALL: shellExecutor.execute → EXCEPTION: execute → CATCH: IOException e → LOG: LOG.ERROR: StringUtils.stringifyException(e) → CALL: org.slf4j.Logger:error → RETURN → EXIT",
    "log": "[ERROR] {StringUtils.stringifyException(e)}"
  },
  "58a478de_1": {
    "exec_flow": "ENTRY → TRY → LOG: LOG.TRACE: Entering decryptEncryptedKey method. → CALL:checkNotEmpty → CALL:checkNotNull → LOG: LOG.DEBUG: Decrypting key for {}, the edek Operation is {}, versionName, eekOp → CALL:checkNotNull → CALL:checkNotNull → IF_TRUE: eekOp.equals(KMSRESTConstants.EEK_DECRYPT) → CALL:KMSWebApp.getDecryptEEKCallsMeter().mark → CALL:assertAccess → CALL:user.doAs (PrivilegedExceptionAction<KeyVersion>) → LOG: [VIRTUAL_CALL] → CALL:op → CALL:setEndTime → RETURN → EXIT",
    "log": "[TRACE] Entering decryptEncryptedKey method. [DEBUG] Decrypting key for {}, the edek Operation is {}. [DEBUG] Log an audit event"
  },
  "52361c4f_1": {
    "exec_flow": "ENTRY→CALL:create→NEW:FCDataOutputStreamBuilder→CALL:getUMask→CALL:resolve→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→TRY→FOR_EXIT→RETURN→EXIT",
    "log": "[INFO] Resolving file system path [DEBUG] Attempting symlink resolution"
  },
  "52361c4f_2": {
    "exec_flow": "ENTRY→CALL:create→NEW:FCDataOutputStreamBuilder→CALL:getUMask→CALL:resolve→FOR_INIT→FOR_COND: isLink→CALL:org.apache.hadoop.fs.FileContext:getFSofPath→CALL:org.apache.hadoop.fs.FSLinkResolver:next→CATCH:UnresolvedLinkException→CALL:qualifySymlinkTarget→TRY→FOR_COND: isLink→FOR_EXIT→RETURN→EXIT",
    "log": "[INFO] Resolving file system path [WARN] Unresolved link encountered"
  },
  "ceee5e92_1": {
    "exec_flow": "ENTRY→IF_TRUE→CALL:resolveSymLinks→FOREACH→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→CALL:deleteAsUser→ENTRY→IF_TRUE: baseDirs == null || baseDirs.size() == 0 → LOG: LOG.INFO: Deleting absolute path : {}, subDir → CALL: org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean) → IF_TRUE: !lfs.delete(subDir, true) → LOG: LOG.WARN: delete returned false for path: [{}], subDir → RETURN → EXIT→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] {} deleting {} [INFO] Deleting absolute path : {} [WARN] delete returned false for path: [{}]"
  },
  "1590f853_1": {
    "exec_flow": "ENTRY→CALL:readFully→CALL:validatePositionedReadArgs→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→RETURN→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "1590f853_2": {
    "exec_flow": "ENTRY→CALL:validatePositionedReadArgs→IF_FALSE:length==0→SYNC:this→TRY→CALL:seek→EXCEPTION:seek→CATCH:EOFException e→CALL:org.slf4j.Logger:debug→CALL:seek→RETURN→EXIT",
    "log": "[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}"
  },
  "0906fb62_1": {
    "exec_flow": "ENTRY→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message"
  },
  "0906fb62_2": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "[WARN] Unexpected SecurityException in Configuration"
  },
  "0906fb62_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register→CALL: namedCallbacks.put→CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder→CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource→CALL:checkNotNull→CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>→CALL:put→CALL:start→IF_TRUE:startMBeans→CALL:startMBeans→LOG:LOG.DEBUG:Registered source + name→EXIT",
    "log": "<log>[DEBUG] Registering the metrics source</log> <log>[DEBUG] Registered source + name</log>"
  },
  "303bcc8b_1": {
    "exec_flow": "<seq> ENTRY → LOG: LOG.DEBUG: Creating SAS key from account instance {}, accountName → TRY → CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getAccountKeyFromConfiguration → NEW: CloudStorageAccount → NEW: StorageCredentialsSharedAccessSignature → CALL: generateSharedAccessSignature → CALL: getDefaultAccountAccessPolicy → CALL: getDefaultAccountAccessPolicy → CALL: getEndpointSuffix → RETURN → EXIT </seq>",
    "log": "<log> [DEBUG] Creating SAS key from account instance {} </log>"
  },
  "303bcc8b_2": {
    "exec_flow": "<seq> ENTRY → LOG: LOG.DEBUG: Creating SAS key from account instance {}, accountName → TRY → CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getAccountKeyFromConfiguration → NEW: CloudStorageAccount → NEW: StorageCredentialsSharedAccessSignature → CALL: generateSharedAccessSignature → CALL: getDefaultAccountAccessPolicy → CALL: getDefaultAccountAccessPolicy → CALL: getEndpointSuffix → CATCH: KeyProviderException → EXIT </seq>",
    "log": "<log> [DEBUG] Creating SAS key from account instance {} </log>"
  },
  "303bcc8b_3": {
    "exec_flow": "<seq> ENTRY → LOG: LOG.DEBUG: Creating SAS key from account instance {}, accountName → TRY → CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getAccountKeyFromConfiguration → NEW: CloudStorageAccount → NEW: StorageCredentialsSharedAccessSignature → CALL: generateSharedAccessSignature → CALL: getDefaultAccountAccessPolicy → CALL: getDefaultAccountAccessPolicy → CALL: getEndpointSuffix → CATCH: InvalidKeyException → EXIT </seq>",
    "log": "<log> [DEBUG] Creating SAS key from account instance {} </log>"
  },
  "303bcc8b_4": {
    "exec_flow": "<seq> ENTRY → LOG: LOG.DEBUG: Creating SAS key from account instance {}, accountName → TRY → CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getAccountKeyFromConfiguration → NEW: CloudStorageAccount → NEW: StorageCredentialsSharedAccessSignature → CALL: generateSharedAccessSignature → CALL: getDefaultAccountAccessPolicy → CALL: getDefaultAccountAccessPolicy → CALL: getEndpointSuffix → CATCH: StorageException → EXIT </seq>",
    "log": "<log> [DEBUG] Creating SAS key from account instance {} </log>"
  },
  "303bcc8b_5": {
    "exec_flow": "<seq> ENTRY → LOG: LOG.DEBUG: Creating SAS key from account instance {}, accountName → TRY → CALL: org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getAccountKeyFromConfiguration → NEW: CloudStorageAccount → NEW: StorageCredentialsSharedAccessSignature → CALL: generateSharedAccessSignature → CALL: getDefaultAccountAccessPolicy → CALL: getDefaultAccountAccessPolicy → CALL: getEndpointSuffix → CATCH: URISyntaxException → EXIT </seq>",
    "log": "<log> [DEBUG] Creating SAS key from account instance {} </log>"
  },
  "6713ad0b_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY -> CALL: rpcServer.checkOperation -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: LOG.DEBUG: Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_TRUE: op == OperationCategory.UNCHECKED OR op == OperationCategory.READ -> RETURN -> EXIT -> IF_TRUE: rpcServer.isInvokeConcurrent(src) -> CALL: rpcClient.invokeConcurrent <entry> <call_site>org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getLocationsForPath</call_site> <conditions>failIfLocked = true</conditions> <virtual_call> <callee>org.apache.hadoop.hdfs.server.federation.resolver.FileSubclusterResolver:getDestinationForPath</callee> <fall_through>false</fall_through> </virtual_call> </entry>",
    "log": "<log>[DEBUG] Proxying operation: {}</log> <log>Invocation to \"{}\" for \"{}\" timed out</log> <log>Cannot execute {} in {}: {}</log> <log>Unexpected error while invoking API: {}</log> <log>org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.getLocationsForPath - AccessControlException thrown: {reasons}</log>"
  },
  "6713ad0b_2": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL: org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → FOREACH: locations → TRY → CALL: org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → IF_FALSE: namenodes == null || namenodes.isEmpty() → CALL: addClientInfoToCallerContext → IF_FALSE: rpcMonitor != null → FOREACH: namenodes → TRY → CALL: getConnection → LOG: LOG.DEBUG: User {} NN {} is using connection {}, ugi.getUserName(), rpcAddress, connection → CALL: invoke → IF_FALSE: failover → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT",
    "log": "<log>[DEBUG] User {} NN {} is using connection {}</log> <log>[ERROR] Unexpected exception {} proxying {} to {}</log> <log>[ERROR] No namenode available to invoke...</log> <log>[ERROR] Get connection for {} {} error: {}</log> <log>[ERROR] Cannot get available namenode for {} {} error: {}</log> <log>[ERROR] {} at {} is in Standby: {}</log> <log>[ERROR] {} at {} cannot be reached: {}</log> <log>[ERROR] {} at {} error: \"{}\"</log>"
  },
  "6713ad0b_3": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL: org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → FOREACH: locations → TRY → CALL: org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → TRY → CALL: invoke → CONDITIONAL → CALL: shouldRetry → CALL: invoke → RETURN → EXIT",
    "log": "<log>[ERROR] Unexpected exception while proxying API</log>"
  },
  "6713ad0b_4": {
    "exec_flow": "ENTRY → IF_TRUE: subclusterResolver instanceof MountTableResolver → CALL: org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:getMountPoints → IF_FALSE: mountPoints != null → CALL: isPathAll → TRY → CALL: org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → RETURN → EXIT",
    "log": "<log>[ERROR] Cannot get mount point</log>"
  },
  "68814ab1_1": {
    "exec_flow": "<sequence> ENTRY -> IF_TRUE: rpcMonitor != null -> CALL: rpcMonitor.startOp -> IF_TRUE: LOG.isDebugEnabled() -> CALL: org.slf4j.Logger:isDebugEnabled() -> LOG: [DEBUG] Proxying operation: {}, methodName -> CALL: opCategory.set -> IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ -> CALL: checkSafeMode -> LOG: [DEBUG] Renew delegation token -> TRY -> IF_FALSE: !isAllowedDelegationTokenOp() -> CALL: getRemoteUser -> CALL: renewToken -> CALL: DFSUtil.decodeDelegationToken -> CALL: toStringStable -> IF_TRUE: AccessControlException -> CALL: DFSUtil.decodeDelegationToken -> CALL: toStringStable -> CATCH: AccessControlException -> LOG: [DEBUG] Operation: renewDelegationToken Status: false TokenId: tokenId -> CALL: writeLock -> EXCEPTION: writeLock -> CATCH: AccessControlException ace -> LOG: [ERROR] Access control exception during renew delegation token -> CALL: decodeDelegationToken -> CALL: toStringStable -> CALL: logAuditEvent -> LOG: [INFO] Audit log created for failed operation -> THROW: ace -> EXIT </sequence>",
    "log": "[DEBUG] Proxying operation: {} [DEBUG] Renew delegation token [DEBUG] Operation: renewDelegationToken Status: false TokenId: tokenId [ERROR] Access control exception during renew delegation token [INFO] Audit log created for failed operation"
  },
  "b40ad988_1": {
    "exec_flow": "ENTRY → LOG: [DEBUG] Handling event → FOREACH: listofHandlers → CALL: org.apache.hadoop.yarn.event.EventHandler:handle → LOG: [ERROR] Received + event → IF_FALSE: HAUtil.isHAEnabled(getConfig()) → SWITCH: event.getType() → CASE: [STATE_STORE_OP_FAILED] → IF_TRUE: YarnConfiguration.shouldRMFailFast(getConfig()) → LOG: [ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties. → CALL: ExitUtil.terminate → FOREACH_EXIT → EXIT",
    "log": "<log>[DEBUG] Handling event</log> <log>[ERROR] Received + event</log> <log>[ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.</log>"
  },
  "b40ad988_2": {
    "exec_flow": "ENTRY → IF_TRUE: node != null → TRY → CALL: ((EventHandler<RMNodeEvent>)node).handle → EXCEPTION: handle → CATCH: Throwable t → LOG: [ERROR] Error in handling event type + event.getType() + for node + nodeId, t → EXIT",
    "log": "<log>[ERROR] Error in handling event type + event.getType() + for node + nodeId, t</log>"
  },
  "b40ad988_3": {
    "exec_flow": "ENTRY → SWITCH: event.getType() → CASE: [REQUEST_RESOURCE_LOCALIZATION] → SWITCH: req.getVisibility() → CASE: [PRIVATE] → SYNC: privLocalizers → IF_FALSE: localizer != null && localizer.killContainerLocalizer.get() → LOG: [INFO] New REQUEST_RESOURCE_LOCALIZATION localize request for locId, remove old private localizer. → CALL: privLocalizers.remove → CALL: localizer.interrupt → SYNC: privLocalizers → IF_TRUE: null == localizer → IF_FALSE: recentlyCleanedLocalizers.containsKey(locId) → LOG: [INFO] Created localizer for locId → CALL: new LocalizerRunner → CALL: LocalizerRunner.start → CALL: privLocalizers.put → CALL: localizer.addResource → BREAK → EXIT",
    "log": "<log>[INFO] New REQUEST_RESOURCE_LOCALIZATION localize request for locId, remove old private localizer.</log> <log>[INFO] Created localizer for locId</log>"
  },
  "b40ad988_4": {
    "exec_flow": "ENTRY → SWITCH: event.getType() → CASE: [REQUEST_RESOURCE_LOCALIZATION] → SWITCH: req.getVisibility() → CASE: [APPLICATION] → SYNC: privLocalizers → IF_FALSE: localizer != null && localizer.killContainerLocalizer.get() → IF_TRUE: null == localizer → IF_TRUE: recentlyCleanedLocalizers.containsKey(locId) → LOG: [INFO] Skipping localization request for recently cleaned localizer locId resource: req.getResource() → BREAK → EXIT",
    "log": "<log>[INFO] Skipping localization request for recently cleaned localizer locId resource: req.getResource()</log>"
  },
  "adffda68_1": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → CALL:org.apache.hadoop.fs.azure.NativeAzureFileSystem:open(org.apache.hadoop.fs.Path,int) → LOG: Opening file: {}, f.toString() → CALL: performAuthCheck → TRY → CALL: retrieveMetadata → IF_FALSE: retrieveMetadata successful and meta != null → IF_FALSE: meta.isDirectory() → TRY → CALL: retrieve → NEW: FSDataInputStream → NEW: BufferedFSInputStream → NEW: NativeAzureFsInputStream → CALL: getLen → RETURN → EXIT",
    "log": "[DEBUG] Opening file: {f.toString()}"
  },
  "adffda68_2": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE:fs.isDirectory()→CALL:NativeFileSystemStore:getFileLength(key)→LOG:LOG.INFO:Open the file: [{f}] for reading.→NEW:FSDataInputStream→CALL:getConf→NEW:BufferedFSInputStream→NEW:CosNInputStream→RETURN→EXIT",
    "log": "[INFO] Open the file: [{f}] for reading. [DEBUG] Path: [{}] is a dir. COS key: [{}]"
  },
  "adffda68_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.fs.FileSystem:open→TRY→CALL:org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat→IF_FALSE:!Shell.WINDOWS→TRY→CALL:fstat→CATCH:NativeIOException→IF_FALSE:nioe.getErrorCode==6→CALL:Logger:warn→THROW:NativeIOException→CALL:checkStat→THROW: new IOException(\"Owner '\" + owner + \"' for path \" + f + \" did not match \" + \"expected owner '\" + expectedOwner + \"'\")→EXIT",
    "log": "[DEBUG] Failed to get groups for user {} [WARN] NativeIO.getFstat error (%d): %s"
  },
  "2e2f3a9f_1": {
    "exec_flow": "ENTRY → WHILE: !Thread.currentThread().isInterrupted() → LOG: DEBUG: Schedule probe datanode for probe type: {}. → IF_TRUE: type == ProbeType.CHECK_DEAD → WHILE: (datanodeInfo = deadNodesProbeQueue.poll()) != null → IF_TRUE: probeInProg.containsKey(datanodeInfo.getDatanodeUuid()) → LOG: DEBUG: The datanode {} is already contained in probe queue, skip to add it. → CALL: org.apache.hadoop.hdfs.DeadNodeDetector$Probe:<init>(org.apache.hadoop.hdfs.DeadNodeDetector,org.apache.hadoop.hdfs.DeadNodeDetector,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.DeadNodeDetector$ProbeType) → CALL: probeDeadNodesThreadPool.execute → WHILE_EXIT → TRY: Sleep thread for specified time → CATCH: InterruptedException → LOG: DEBUG: Got interrupted while probe is scheduling. → EXIT",
    "log": "[DEBUG] Schedule probe datanode for probe type: {}. → [DEBUG] The datanode {} is already contained in probe queue, skip to add it. → [DEBUG] Got interrupted while probe is scheduling."
  },
  "7f49f8e3_1": {
    "exec_flow": "ENTRY → IF_FALSE: conf == null → IF_FALSE: isInState(STATE.INITED) → SYNC: stateChangeLock → IF_TRUE: enterState(STATE.INITED) != STATE.INITED → CALL: setConfig → TRY → CALL: serviceInit → IF_TRUE: isInState(STATE.INITED) → CALL: notifyListeners → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → SYNC: this → CALL: toArray → CALL: size → FOREACH: callbacks → CALL: stateChanged → FOREACH_EXIT → EXIT → CATCH: Throwable e → LOG.warn(\"Exception while notifying listeners of {}\", this, e) → EXCEPTION: serviceInit → CATCH: Exception e → CALL: noteFailure → SYNC: this → LOG.debug(\"noteFailure\", exception) → IF_TRUE: failureCause == null → ASSIGN: failureCause = exception → ASSIGN: failureState = getServiceState() → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → EXIT_SYNC → LOG.debug(\"noteFailure\", exception) → LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception) → CALL: ServiceOperations.stopQuietly → IF_TRUE: service != null → CALL: stop → CATCH_EXCEPTION → CALL: log.warn → EXIT",
    "log": "<log>[DEBUG] Service: {getName()} entered state {getServiceState()}</log> <log>[DEBUG] Config has been overridden during init</log> <log>LOG.debug(\"noteFailure\", exception);</log> <log>LOG.info(\"Service {} failed in state {}\", getName(), failureState, exception);</log> <log>[WARN] Exception while notifying listeners of {}</log> <log>org.apache.hadoop.service.ServiceOperations:stopQuietly - WARN When stopping the service {service_name}</log>"
  },
  "f4564e10_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}].</step> <step>TRY</step> <step>CALL: connect</step> <step>CALL: deleteFile</step> <step>RETURN</step> <step>CALL: disconnect</step> <step>IF_TRUE: client != null</step> <step>IF_TRUE: !client.isConnected()</step> <step>THROW: new FTPException(\"Client not connected\")</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log_entry> <log_entry>[DEBUG] Path: [{}] is a file. COS key: [{}]</log_entry> <log_entry>[DEBUG] Create parent key: {parentKey}</log_entry>"
  },
  "f4564e10_2": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}].</step> <step>TRY</step> <step>CALL: getFileStatus</step> <step>ENTRY</step> <step>IF_FALSE: key.length()==0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_FALSE: meta.isFile()</step> <step>LOG: [DEBUG] Path: [{}] is a dir. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:delete</step> <step>RETURN</step> <step>IF_FALSE: key.compareToIgnoreCase(\"/\") == 0</step> <step>IF_FALSE: status.isDirectory()</step> <step>LOG: [DEBUG] Delete the file: {}</step> <step>CALL: createParent</step> <step>IF_TRUE: parent != null</step> <step>LOG: [DEBUG] Create parent key: {parentKey}</step> <step>IF_TRUE: !parentKey.equals(PATH_DELIMITER)</step> <step>IF_TRUE: key.length() > 0</step> <step>TRY</step> <step>CALL: storeEmptyFile</step> <step>EXIT</step>",
    "log": "<log_entry>[DEBUG] Ready to delete path: [{}]. recursive: [{}].</log_entry> <log_entry>[DEBUG] Path: [{}] is a dir. COS key: [{}]</log_entry> <log_entry>[DEBUG] Delete the file: {}</log_entry> <log_entry>[DEBUG] Create parent key: {parentKey}</log_entry>"
  },
  "87f7d37e_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "87f7d37e_2": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → EXIT",
    "log": "[INFO] message"
  },
  "87f7d37e_3": {
    "exec_flow": "ENTRY → IF_TRUE → CALL:mountCGroupController → LOG:LOG.INFO:Mounting controller controller.getName() at requestedMountPath → EXIT",
    "log": "[INFO] Mounting controller controller.getName() at requestedMountPath"
  },
  "87f7d37e_4": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:initializePreMountedCGroupController → IF_FALSE: controllerPath == null → LOG: LOG.INFO: Initializing mounted controller ... at ... → IF_TRUE: !rootHierarchy.exists() → THROW: new ResourceHandlerException(...) → EXIT",
    "log": "[INFO] Initializing mounted controller ... at ..."
  },
  "87f7d37e_5": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:initializePreMountedCGroupController → IF_FALSE: controllerPath == null → LOG: LOG.INFO: Initializing mounted controller ... at ... → IF_FALSE: !rootHierarchy.exists() → IF_TRUE: !yarnHierarchy.exists() → LOG: LOG.INFO: Yarn control group does not exist. Creating ... → TRY → IF_TRUE: !yarnHierarchy.mkdir() → THROW: new ResourceHandlerException(...) → EXIT",
    "log": "[INFO] Initializing mounted controller ... at ... [INFO] Yarn control group does not exist. Creating ..."
  },
  "87f7d37e_6": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:initializePreMountedCGroupController → IF_FALSE: controllerPath == null → LOG: LOG.INFO: Initializing mounted controller ... at ... → IF_FALSE: !rootHierarchy.exists() → IF_TRUE: !yarnHierarchy.exists() → LOG: LOG.INFO: Yarn control group does not exist. Creating ... → TRY → IF_FALSE: !yarnHierarchy.mkdir() → EXIT",
    "log": "[INFO] Initializing mounted controller ... at ... [INFO] Yarn control group does not exist. Creating ..."
  },
  "87f7d37e_7": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:initializePreMountedCGroupController → IF_FALSE: controllerPath == null → LOG: LOG.INFO: Initializing mounted controller ... at ... → IF_FALSE: !rootHierarchy.exists() → IF_FALSE: !yarnHierarchy.exists() → IF_TRUE: !FileUtil.canWrite(yarnHierarchy) → THROW: new ResourceHandlerException(...) → EXIT",
    "log": "[INFO] Initializing mounted controller ... at ..."
  },
  "87f7d37e_8": {
    "exec_flow": "ENTRY → IF_FALSE → CALL:initializePreMountedCGroupController → IF_FALSE: controllerPath == null → LOG: LOG.INFO: Initializing mounted controller ... at ... → IF_FALSE: !rootHierarchy.exists() → IF_FALSE: !yarnHierarchy.exists() → IF_FALSE: !FileUtil.canWrite(yarnHierarchy) → EXIT",
    "log": "[INFO] Initializing mounted controller ... at ..."
  },
  "20dd9e04_1": {
    "exec_flow": "ENTRY → IF_FALSE:isInState(STATE.STARTED) → SYNC:stateChangeLock → IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED → TRY → CALL:currentTimeMillis → CALL:serviceStart → IF_TRUE:isInState(STATE.STARTED) → CALL:debug → CALL:notifyListeners → ENTRY → TRY → CALL: org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners → CALL: globalListeners.notifyListeners → CATCH: Throwable e → LOG: WARN: Exception while notifying listeners of {}, this, e → CALL:RETURN → EXIT",
    "log": "[DEBUG] Service {} is started [WARN] Exception while notifying listeners of {} [WARN] When stopping the service {service_name}"
  },
  "20dd9e04_2": {
    "exec_flow": "<step>noteFailure</step> <step>info</step>",
    "log": "<log>[DEBUG] noteFailure, exception</log> <log>[INFO] Service {getName()} failed in state {getServiceState()}, exception</log>"
  },
  "3741b682_1": {
    "exec_flow": "<entry_point>DFSClient:getBlockSize</entry_point> <details> <step>ENTRY→CALL:checkOpen→TRY→CALL:getPreferredBlockSize</step> <step>ENTRY→CALL:rpcServer.checkOperation→CALL:rpcServer.getLocationsForPath→CALL:invokeSequential→RETURN→EXIT</step> <step>If failIfLocked, verify mount points under the path</step> <exception>AccessControlException - if mount points are found</exception> <step>Resolve destination for path</step> <exception>IOException - if location cannot be determined</exception> <step>If WRITE operation, check for read-only and quota conditions</step> <exception>IOException - if path in read-only or quota violates</exception> <step>Filter out disabled subclusters</step> <step>If rpcMonitor != null, start operation and log debugging info</step> </details>",
    "log": "<log>[WARN] Problem getting block size</log> <log>LOG.error(\"Cannot get mount point\", e) - RouterRpcServer:isPathReadOnly</log> <log>[DEBUG] Proxying operation: {} - RouterRpcServer:checkOperation</log> <log>[ERROR] Unexpected exception {} proxying {} to {} - RouterRpcClient:invokeSequential</log>"
  },
  "fa489c66_1": {
    "exec_flow": "ENTRY → IF_FALSE: locations.isEmpty() → IF_FALSE: locations.size() == 1 && timeOutMs <= 0 → FOREACH: locations → FOREACH_EXIT → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.proxyOp → TRY → IF_TRUE: timeOutMs > 0 → CALL: invokeAll → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → IF_FALSE: rpcMonitor != null → TRY → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT → IF_FALSE: index>0 → IF_FALSE: ioe instanceof RemoteException → TRY → CALL: newInstance → CALL: org.slf4j.Logger:error → IF_FALSE: ret!=null → RETURN → EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out [DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] No namenode available to invoke... [ERROR] Could not create exception {ioeClass.getSimpleName()}, {ReflectiveOperationException}"
  },
  "fa489c66_2": {
    "exec_flow": "ENTRY → TRY → CALL: invoke → CALL: org.slf4j.Logger:error(java.lang.String, java.lang.Throwable) → RETURN → EXIT",
    "log": "[ERROR] Unexpected exception while proxying API"
  },
  "fa489c66_3": {
    "exec_flow": "ENTRY → TRY → CALL: invoke → CALL: shouldRetry → CONDITIONAL → CALL: invoke → RETURN → EXIT",
    "log": "[ERROR] Unexpected exception while proxying API"
  },
  "fa489c66_4": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "fa489c66_5": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "41c4797c_1": {
    "exec_flow": "ENTRY → IF_TRUE:hsr!=null → CALL:getCallerUserGroupInformation → IF_TRUE:callerUGI==null → LOG:ERROR:Unable to obtain user name, user not authenticated → RETURN → EXIT TRY → IF_TRUE:LOG.isDebugEnabled() → LOG:DEBUG:PrivilegedAction [as: {}][action: {}] → CATCH:PrivilegedActionException → LOG:DEBUG:PrivilegedActionException as: {} → THROW:IOException|Error|RuntimeException|InterruptedException|UndeclaredThrowableException → CALL:org.slf4j.Logger:error → IF_FALSE:chain!=null && chain.getRootInterceptor()!=null → CALL:initializePipeline → IF_TRUE:this.userPipelineMap.containsKey(user) → LOG.INFO:Request to start an already existing user: {} was received, so ignoring., user → EXIT",
    "log": "[ERROR] Unable to obtain user name, user not authenticated [DEBUG] PrivilegedAction [as: {}][action: {}] [DEBUG] PrivilegedActionException as: {} [ERROR] Cannot get user: {} [INFO] Request to start an already existing user: {} was received, so ignoring."
  },
  "41c4797c_2": {
    "exec_flow": "ENTRY → CALL:initForWritableEndpoints → CALL:getCallerUserGroupInformation → TRY → CALL:createReservationSubmissionRequest → CALL:callerUGI.doAs → EXCEPTION:doAs → CATCH:UndeclaredThrowableException ue → IF_FALSE:ue.getCause() instanceof YarnException → LOG:INFO:Submit reservation request failed, ue → THROW:ue → EXIT",
    "log": "[INFO] Submit reservation request failed, ue"
  },
  "0ed84883_1": {
    "exec_flow": "ENTRY→CALL: addCustomerProvidedKeyHeaders→CALL: requestHeaders.add→IF_TRUE: leaseId != null→CALL: requestHeaders.add→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: appendSASTokenToQuery→TRY→LOG: TRACE Fetch SAS token for {operation} on {path}→IF_TRUE: cachedSasToken == null→CALL: getSASToken→IF_TRUE: (sasToken == null) || sasToken.isEmpty()→THROW: new UnsupportedOperationException(\"SASToken received is empty or null\")→EXIT→CREATE URL→TRY→LOG.DEBUG IOStatisticsBinding track duration started→IF_TRUE: IOException e→CATCH: IOException e→THROW new UncheckedIOException(\"Error while tracking Duration of an AbfsRestOperation call\", e)→EXIT",
    "log": "[TRACE] Fetch SAS token for {operation} on {path} [DEBUG] IOStatisticsBinding track duration started [ERROR] UncheckedIOException thrown"
  },
  "0ed84883_2": {
    "exec_flow": "ENTRY→CALL: addCustomerProvidedKeyHeaders→CALL: requestHeaders.add→IF_TRUE: leaseId != null→CALL: requestHeaders.add→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: appendSASTokenToQuery→TRY→LOG: TRACE Fetch SAS token for {operation} on {path}→IF_TRUE: cachedSasToken == null→CALL: getSASToken→IF_FALSE: (sasToken == null) || sasToken.isEmpty()→IF_TRUE: sasToken.charAt(0) == '?'→CALL: substring→CALL: queryBuilder.setSASToken→LOG: TRACE SAS token fetch complete for {operation} on {path}→RETURN→TRY→LOG: DEBUG IOStatisticsBinding track duration started→LOG: DEBUG First execution of REST operation - {}→LOG: DEBUG Retrying REST operation {}. RetryCount = {}→LOG: TRACE {} REST operation complete→RETURN→EXIT",
    "log": "[TRACE] Fetch SAS token for {operation} on {path} [TRACE] SAS token fetch complete for {operation} on {path} [DEBUG] IOStatisticsBinding track duration started [DEBUG] First execution of REST operation - {} [DEBUG] Retrying REST operation {}. RetryCount = {} [TRACE] {} REST operation complete"
  },
  "0ed84883_3": {
    "exec_flow": "ENTRY→CALL: addCustomerProvidedKeyHeaders→CALL: requestHeaders.add→IF_TRUE: leaseId != null→CALL: requestHeaders.add→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: abfsUriQueryBuilder.addQuery→CALL: appendSASTokenToQuery→TRY→LOG: TRACE Fetch SAS token for {operation} on {path}→IF_TRUE: cachedSasToken == null→CALL: getSASToken→IF_FALSE: (sasToken == null) || sasToken.isEmpty()→IF_FALSE: sasToken.charAt(0) == '?'→CALL: queryBuilder.setSASToken→LOG: TRACE SAS token fetch complete for {operation} on {path}→RETURN→TRY→LOG: DEBUG IOStatisticsBinding track duration started→EXCEPTION: trackDurationOfInvocation→CATCH: AzureBlobFileSystemException aze→THROW: aze→LOG: ERROR AzureBlobFileSystemException thrown→EXIT",
    "log": "[TRACE] Fetch SAS token for {operation} on {path} [TRACE] SAS token fetch complete for {operation} on {path} [DEBUG] IOStatisticsBinding track duration started [ERROR] AzureBlobFileSystemException thrown"
  },
  "b0452350_1": {
    "exec_flow": "ENTRY → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {}, property → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_FALSE: clazz == null → LOG: LOGGER.DEBUG: FS for {} is {}, scheme, clazz → RETURN → EXIT",
    "log": "<log> <template>Looking for FS supporting {}</template> <level>debug</level> </log> <log> <template>looking for configuration option {}</template> <level>debug</level> </log> <log> <template>Looking in service filesystems for implementation class</template> <level>debug</level> </log> <log> <template>FS for {} is {}</template> <level>debug</level> </log>"
  },
  "b0452350_2": {
    "exec_flow": "ENTRY → LOG: LOG.DEBUG: requested seek to position {}, n → IF_FALSE: closed → IF_FALSE: n < 0 → IF_TRUE: n > contentLength → THROW: new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF) → LOG: LOG.DEBUG: Seek to position {}. Bytes skipped {}, pos, this.pos → EXCEPTION: debug → CATCH: IOException e → IF_TRUE: innerException instanceof StorageException && NativeAzureFileSystemHelper.isFileNotFoundException((StorageException) innerException) → THROW: new FileNotFoundException(String.format(\"%s is not found\", key)) → EXIT",
    "log": "<log> <template>requested seek to position {}</template> <level>debug</level> </log> <log> <template>Seek to position {}. Bytes skipped {}, pos, this.pos</template> <level>debug</level> </log>"
  },
  "b0452350_3": {
    "exec_flow": "ENTRY → CALL: borrow → IF_TRUE: decompressor == null → CALL: createDecompressor → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → CALL: getDefaultExtension → LOG: LOG.INFO → IF_TRUE: decompressor != null && !decompressor.getClass().isAnnotationPresent(DoNotPool.class) → CALL: updateLeaseCount → RETURN → EXIT",
    "log": "<log> <template>Handling deprecation for all properties in config...</template> <level>debug</level> </log> <log> <template>Handling deprecation for (String)item</template> <level>debug</level> </log> <log> <template>Got brand-new decompressor [<default extension>]</template> <level>info</level> </log> <log> <template>Registering fake codec for extension {extension}</template> <level>info</level> </log>"
  },
  "b0452350_4": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → EXIT",
    "log": "<log> <template>Handling deprecation for all properties in config...</template> <level>debug</level> </log> <log> <template>Handling deprecation for (String)item</template> <level>debug</level> </log>"
  },
  "b0452350_5": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log> <template>message</template> <level>info</level> </log>"
  },
  "eabaabe1_1": {
    "exec_flow": "<seq> ENTRY → LOG:Handling deprecation for all properties in config... → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:Preconditions.checkNotNull → NEW:FSDataOutputStream → RETURN → EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [DEBUG] Creating a new file: [{}] in COS. [ERROR] Failed to create file: {fileToCreate} at fallback : {linkedFallbackFs.getUri()} [DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {} [DEBUG] Overwriting file {} </log>"
  },
  "eabaabe1_2": {
    "exec_flow": "<seq> ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for + (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:LOG_DEPRECATION.info → EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "0bd91447_1": {
    "exec_flow": "ENTRY -> CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:getDestFS() -> CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:createSuccessData -> IF_TRUE: createJobMarker -> CALL: commitOperations.createSuccessMarker -> IF_TRUE: addMetrics -> CALL:addFileSystemStatistics -> LOG:[DEBUG] Touching success marker for job {}: {}, markerPath, successData -> CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[]) -> TRY -> CALL:successData.save -> CALL:org.apache.hadoop.util.DurationInfo:close() -> CALL:finished -> IF_TRUE: logAtInfo -> CALL:org.slf4j.Logger:info -> LOG:[INFO] {} -> RETURN -> EXIT",
    "log": "[DEBUG] Touching success marker for job {}: {}, markerPath, successData [INFO] {}"
  },
  "0bd91447_2": {
    "exec_flow": "ENTRY -> CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:getDestFS() -> CALL:org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:createSuccessData -> IF_TRUE: createJobMarker -> CALL: commitOperations.createSuccessMarker -> IF_FALSE: addMetrics -> IF_TRUE:log.isDebugEnabled() -> CALL:org.slf4j.Logger:debug -> LOG:[DEBUG] {} -> RETURN -> EXIT",
    "log": "[DEBUG] {}"
  },
  "6663de48_1": {
    "exec_flow": "ENTRY → CALL: Preconditions.checkNotNull → CALL: org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:updateReplicaUnderRecovery → CALL: newBlock.setGenerationStamp → CALL: newBlock.setBlockId → CALL: newBlock.setNumBytes → ENTRY→IF_FALSE: bpos != null → LOG:LOG.ERROR:Cannot find BPOfferService for reporting block received + for bpid={} → EXIT → RETURN → EXIT",
    "log": "[ERROR] Cannot find BPOfferService for reporting block received + for bpid={}"
  },
  "303a5d36_1": {
    "exec_flow": "ENTRY → TRY → CALL: getFileStatus → IF_TRUE: meta != null → IF_FALSE: meta.isDirectory() → LOG: LOG.DEBUG: Found the path: {} as a file., f.toString() → CALL: updateFileStatusPath → RETURN → EXIT",
    "log": "<log> <template>org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatus[DEBUG] \"Getting the file status for {path}\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log> <log> <template>LOG.DEBUG: \"Found the path: {} as a file.\"</template> <parameters> <parameter name=\"path\" source=\"org.apache.hadoop.fs.Path\"/> </parameters> </log>"
  },
  "303a5d36_2": {
    "exec_flow": "ENTRY → IF_FALSE: key.length() == 0 → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE: meta != null → LOG: DEBUG: List COS key: [{}] to check the existence of the path. → CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_FALSE: listing.getFiles().length > 0 || listing.getCommonPrefixes().length > 0 → THROW: FileNotFoundException → EXIT",
    "log": "<log> <template>DEBUG: List COS key: [{}] to check the existence of the path.</template> </log>"
  },
  "665ad31a_1": {
    "exec_flow": "P1.ENTRY → IF_TRUE: instance == null → NEW: NullInstance → IF_FALSE: allowCompactArrays && declaredClass.isArray() && instance.getClass().getName().equals(declaredClass.getName()) && instance.getClass().getComponentType().isPrimitive() → CALL: UTF8.writeString → IF_FALSE: declaredClass.isArray() → IF_TRUE: declaredClass == String.class → CALL: UTF8.writeString → UTF8.ENTRY → IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "<log>[WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)</log>"
  },
  "665ad31a_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.io.Writable:write → IF_TRUE: s.length() > 0xffff / 3 → LOG.WARN: truncating long string: + s.length() + chars, starting with + s.substring(0, 20) → CALL:substring → CALL:LOG.warn → IF_FALSE: len > 0xffff → CALL:writeShort → CALL:writeChars → RETURN → EXIT",
    "log": "<log>[DEBUG] Starting write operation</log> <log>[WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)</log>"
  },
  "53d7bad3_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→EXIT ENTRY→CALL: LOG_DEPRECATION.info→CALL:org.slf4j.Logger:info(java.lang.String)→EXIT→FOREACH:names→CALL:substituteVars→RETURN→EXIT ENTRY → CALL:getUriPath → CALL:resolve → CALL:open → LOG:INFO:Open the file: [{f}] for reading. → CALL:Credentials:readTokenStorageStream → CALL:cleanupWithLogger → FOREACH:closeables → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] Open the file: [{f}] for reading."
  },
  "53d7bad3_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL:getInternal → TRY → CALL:creatorPermits.acquireUninterruptibly → CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC:this → CALL:get → IF_TRUE:fs != null → RETURN → EXIT",
    "log": "[WARN] Null token ignored for {alias} [DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "53d7bad3_3": {
    "exec_flow": "ENTRY → CALL:FileSystem:get → IF_TRUE:scheme != null && authority == null → NOT (scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null) → IF_TRUE:conf.getBoolean(disableCacheName, false) → LOG:DEBUG: Bypassing cache to create filesystem {uri} → CALL:FileSystem:createFileSystem → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {uri}"
  },
  "e5d17165_1": {
    "exec_flow": "ENTRY → CALL:getBlockPoolSlice → ENTRY → IF_FALSE: !deleteDuplicateReplicas → CALL:selectReplicaToDelete → CALL:volumeMap.add → IF_TRUE: replicaToDelete != null → CALL:deleteReplica → ENTRY → CALL:org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:deleteBlockData() → IF_TRUE: !replicaToDelete.deleteBlockData() → CALL:org.slf4j.Logger:warn(java.lang.String) → CALL:org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:deleteMetadata() → IF_TRUE: !replicaToDelete.deleteMetadata() → CALL:org.slf4j.Logger:warn(java.lang.String) → EXIT → RETURN → EXIT",
    "log": "[DEBUG] resolveDuplicateReplicas decide to keep + replicaToKeep + . Will try to delete + replicaToDelete [WARN] Failed to delete block file for replica + replicaToDelete [WARN] Failed to delete meta file for replica + replicaToDelete"
  },
  "b6450feb_1": {
    "exec_flow": "ENTRY→CALL:addKVAnnotation→TRY→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→CALL:doGlob→ENTRY→TRY→CALL:FileSystem:listStatus→EXCEPTION:FileNotFoundException→CALL:Logger:debug→RETURN→EXIT→RETURN→EXIT→NEW:Globber→CALL:org.apache.hadoop.fs.Globber:glob()→RETURN→EXIT",
    "log": "[INFO] glob path pattern [DEBUG] listStatus({}) failed; returning empty array"
  },
  "b6450feb_2": {
    "exec_flow": "ENTRY→CALL:finished→IF_TRUE:logAtInfo→CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object)→LOG:[INFO] {}→EXIT→NEW:Globber→CALL:org.apache.hadoop.fs.Globber:glob()→RETURN→EXIT",
    "log": "[INFO] {}"
  },
  "b6450feb_3": {
    "exec_flow": "ENTRY→CALL:finished→IF_FALSE:logAtInfo→IF_TRUE:log.isDebugEnabled()→CALL:org.slf4j.Logger:isDebugEnabled()→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object)→LOG:[DEBUG] {}→NEW:Globber→CALL:org.apache.hadoop.fs.Globber:glob()→RETURN→EXIT",
    "log": "[DEBUG] {}"
  },
  "613d16c4_1": {
    "exec_flow": "<step>org.apache.hadoop.lib.service.security.GroupsService:init()</step> <step>ENTRY</step> <step>LOG: LOG.INFO: Using FileSystemAccess JARs version [{}]</step> <step>IF_TRUE: security.equals(\"kerberos\")</step> <step>CALL: getServiceConfig</step> <step>CALL: get</step> <step>CALL: trim</step> <step>CALL: UserGroupInformation.setConfiguration</step> <step>IF_FALSE: !isSecurityEnabled</step> <step>CALL: loginUserFromKeytabAndReturnUGI</step> <step>IF_TRUE: isKerberosKeyTabLoginRenewalEnabled</step> <step>CALL: spawnAutoRenewalThreadForKeytab</step> <step>CALL: setLoginUser</step> <step>LOG: LOG.INFO: Login successful for user {} using keytab file {}. Keytab auto renewal enabled : {}</step> <step>LOG: LOG.INFO: Using FileSystemAccess Kerberos authentication, principal {} keytab {}</step> <step>CALL: loadHadoopConf</step> <step>CALL: getNewFileSystemConfiguration</step> <step>CALL: setRequiredServiceHadoopConf</step> <step>CALL: getTrimmedStringCollection</step> <step>CALL: iterator</step> <step>LOG: LOG.DEBUG: FileSystemAccess FileSystem configuration:</step> <step>EXIT</step> <step>org.apache.hadoop.lib.util.ConfigurationUtils:copy()</step>",
    "log": "<log_entry>[INFO] Using FileSystemAccess JARs version {}</log_entry> <log_entry>[INFO] Login successful for user {} using keytab file {}. Keytab auto renewal enabled : {}</log_entry> <log_entry>[INFO] Using FileSystemAccess Kerberos authentication, principal {} keytab {}</log_entry> <log_entry>[DEBUG] FileSystemAccess FileSystem configuration:</log_entry>"
  },
  "613d16c4_2": {
    "exec_flow": "<step>org.apache.hadoop.lib.service.security.GroupsService:init()</step> <step>ENTRY</step> <step>LOG: LOG.INFO: Using FileSystemAccess JARs version [{}]</step> <step>IF_TRUE: security.equals(\"simple\")</step> <step>CALL: getServer</step> <step>CALL: getConfigDir</step> <step>CALL: set</step> <step>CALL: UserGroupInformation.setConfiguration</step> <step>LOG: LOG.INFO: Using FileSystemAccess simple/pseudo authentication, principal {}</step> <step>CALL: loadHadoopConf</step> <step>CALL: getNewFileSystemConfiguration</step> <step>CALL: setRequiredServiceHadoopConf</step> <step>CALL: getTrimmedStringCollection</step> <step>CALL: iterator</step> <step>LOG: LOG.DEBUG: FileSystemAccess FileSystem configuration:</step> <step>EXIT</step> <step>org.apache.hadoop.lib.util.ConfigurationUtils:copy()</step>",
    "log": "<log_entry>[INFO] Using FileSystemAccess JARs version {}</log_entry> <log_entry>[INFO] Using FileSystemAccess simple/pseudo authentication, principal {}</log_entry> <log_entry>[DEBUG] FileSystemAccess FileSystem configuration:</log_entry>"
  },
  "b07040a2_1": {
    "exec_flow": "ENTRY → LOG: LOG.INFO: Killing container + container → CALL: completedContainer → IF_TRUE: rmContainer == null → LOG: LOG.INFO: Container + containerStatus.getContainerId() + completed with event + event + , but corresponding RMContainer doesn't exist. → RETURN → EXIT",
    "log": "[INFO] Killing container ${container} [INFO] Container {} completed with event {}, but corresponding RMContainer doesn't exist."
  },
  "b07040a2_2": {
    "exec_flow": "ENTRY → LOG: LOG.INFO: Killing container + container → CALL: completedContainer → IF_FALSE: rmContainer == null → IF_FALSE: rmContainer.getExecutionType() == ExecutionType.GUARANTEED → CALL: rmContainer.handle → IF_FALSE: schedulerAttempt != null → LOG: LOG.DEBUG: Completed container: {} in state: {} event:{} → IF_TRUE: node != null → CALL: releaseContainer → CALL: OpportunisticSchedulerMetrics.getMetrics().incrReleasedOppContainers → CALL: recoverResourceRequestForContainer → IF_FALSE: info==null → IF_FALSE: !releasedByNode && info.launchedOnNode → CALL: launchedContainers.remove → IF_TRUE: rmContext != null && rmContext.getAllocationTagsManager() != null → CALL: rmContext.getAllocationTagsManager().removeContainer → CALL: updateResourceForReleasedContainer → IF_TRUE: LOG.isDebugEnabled → LOG: LOG.DEBUG: Released container [containerId] of capacity [containerResource] on host [rmNodeAddress], which currently has [numContainers] containers, [allocatedResource] used and [unallocatedResource] available, release resources=[true] → EXIT",
    "log": "[INFO] Killing container ${container} [DEBUG] Completed container: {} in state: {} event:{} [DEBUG] Released container [containerId] of capacity [containerResource] on host [rmNodeAddress], which currently has [numContainers] containers, [allocatedResource] used and [unallocatedResource] available, release resources=[true]"
  },
  "a36da51e_1": {
    "exec_flow": "ENTRY→TRY→CALL:getFileStatus→IF_FALSE:status.isDirectory()→IF_FALSE:!overwrite→EXCEPTION:debug→CATCH:FileNotFoundException→CALL:getMultipartSizeProperty→NEW:FSDataOutputStream→NEW:AliyunOSSBlockOutputStream→CALL:getConf→NEW:SemaphoredDelegatingExecutor→RETURN→EXIT",
    "log": "<log>[DEBUG] Overwriting file {}</log>"
  },
  "a36da51e_2": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG→CALL:statIncrement→CALL:trailingPeriodCheck→CALL:makeQualified→TRY→CALL:createFile→CALL:statIncrement→NEW:FSDataOutputStream→RETURN→EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</log>"
  },
  "a36da51e_3": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG→IF_TRUE:containsColon(f)→THROW:new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\")→EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "a36da51e_4": {
    "exec_flow": "ENTRY→LOG:LOG.DEBUG→IF_FALSE:containsColon(f)→CALL:performAuthCheck→CALL:createInternal→RETURN→EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "a36da51e_5": {
    "exec_flow": "ENTRY→CALL: Preconditions.checkNotNull→IF_FALSE: InodeTree.SlashPath.equals(f)→IF_TRUE: this.fsState.getRootFallbackLink() != null→IF_FALSE: theInternalDir.getChildren().containsKey(f.getName())→TRY→CALL: org.apache.hadoop.fs.FileSystem:create→RETURN→EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "6f094c39_1": {
    "exec_flow": "ENTRY→IF_TRUE:isInState(STATE.STARTED)→RETURN→EXIT ENTRY→IF_FALSE:isInState(STATE.STARTED)→SYNC:stateChangeLock→IF_TRUE:stateModel.enterState(STATE.STARTED) != STATE.STARTED→TRY→CALL:currentTimeMillis→CALL:serviceStart→IF_TRUE:isInState(STATE.STARTED)→CALL:debug→CALL:notifyListeners→EXIT",
    "log": "[DEBUG] Service {} is started"
  },
  "daf2e0fb_1": {
    "exec_flow": "ENTRY → TRY → CALL:toBoolean → CATCH:TrileanConversionException → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Throwable) → TRY → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext) → CATCH:AbfsRestOperationException → CONDITIONAL:HttpURLConnection.HTTP_BAD_REQUEST==ex.getStatusCode() → CALL:org.slf4j.Logger:debug(java.lang.String) → CALL:org.apache.hadoop.fs.azurebfs.services.AbfsPerfInfo:close() → ENTRY → LOG: LOG.DEBUG: createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {} → CALL:createDefaultHeaders → IF_TRUE: isFile → CALL: addCustomerProvidedKeyHeaders → IF_TRUE: !overwrite → CALL: requestHeaders.add → IF_FALSE: permission != null && !permission.isEmpty() → IF_FALSE: umask != null && !umask.isEmpty() → IF_FALSE: eTag != null && !eTag.isEmpty() → CALL: abfsUriQueryBuilder.addQuery → IF_FALSE: isAppendBlob → CALL: appendSASTokenToQuery → TRY → CALL: IOStatisticsBinding.trackDurationOfInvocation → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:lambda$execute$0 → CATCH: AzureBlobFileSystemException ex → IF_TRUE: !op.hasResult() → THROW: ex → CALL: perfInfo.registerResult(op.getResult()).registerSuccess → CALL: maybeCreateLease → ENTRY → LOG: LOG.DEBUG: Attempting to acquire lease on {}, retry {}, path, numRetries → IF_FALSE: future != null && !future.isDone() → CALL: schedule → CALL: acquireLease → CALL: addCallback → CALL: org.apache.hadoop.fs.azurebfs.services.AbfsLease:lambda$0(tracingContext) → EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}</log> <log>[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call</log> <log>[DEBUG] Get root ACL status</log> <log>[DEBUG] IOStatisticsBinding track duration started</log> <log>[TRACE] Fetch SAS token for {operation} on {path}</log> <log>[TRACE] SAS token fetch complete for {operation} on {path}</log> <log>[DEBUG] First execution of REST operation - {}</log> <log>[DEBUG] Retrying REST operation {}. RetryCount = {}</log> <log>[DEBUG] Unexpected error.</log> <log>[TRACE] {} REST operation complete</log> <log>[DEBUG] IOStatisticsBinding track duration completed</log> <log>[DEBUG] Attempting to acquire lease on {}, retry {}</log>"
  },
  "daf2e0fb_2": {
    "exec_flow": "create.ENTRY → qualify.ENTRY → QUALIFY_CALL → makeQualified.ENTRY → makeQualified.LOG[DEBUG]Stripping trailing '/' from {q} → makeQualified.RETURN → qualify.RETURN → create.LOG[INFO]Create path in S3AFileSystem → create.RETURN",
    "log": "<log>[DEBUG] Stripping trailing '/' from {q}</log> <log>[INFO] Create path in S3AFileSystem</log>"
  },
  "daf2e0fb_3": {
    "exec_flow": "ENTRY → CALL:connect → TRY → CALL:getFileStatus → IF_TRUE:status != null → IF_TRUE:overwrite && !status.isDirectory() → CALL:delete → IF_FALSE:parent == null || !mkdirs(client, parent, FsPermission.getDirDefault()) → CALL:allocate → CALL:changeWorkingDirectory → IF_TRUE:!FTPReply.isPositivePreliminary(client.getReplyCode()) → IF_TRUE:outputStream != null → CALL:IOUtils.closeStream → CALL:disconnect → THROW:new IOException(\"Unable to create file: \" + file + \", Aborting\") → EXIT",
    "log": "<log>[WARN] Logout failed while disconnecting, error code - </log>"
  },
  "daf2e0fb_4": {
    "exec_flow": "ENTRY → TRY → CALL:getFileStatus → IF_FALSE:fileStatus.isDirectory() → IF_FALSE:!overwrite → CALL:org.slf4j.Logger:debug → NEW:FSDataOutputStream → NEW:CosNOutputStream → CALL:getConf → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating a new file: [{}] in COS.</log> <log>[DEBUG] Call the getFileStatus to obtain the metadata for the file: [{}].</log> <log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log> <log>[DEBUG] List COS key: [{}] to check the existence of the path.</log> <log>[DEBUG] Path: [{}] is a directory. COS key: [{}]</log>"
  },
  "daf2e0fb_5": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.apache.hadoop.fs.http.client.HttpFSFileSystem:create → RETURN → EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "daf2e0fb_6": {
    "exec_flow": "ENTRY → CALL:Preconditions.checkNotNull → IF_FALSE:InodeTree.SlashPath.equals(f) → IF_TRUE:this.fsState.getRootFallbackLink() != null → IF_FALSE:theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL:org.apache.hadoop.fs.sftp.SFTPFileSystem:create → RETURN → EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "e8344f7e_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "LOG.info(\"Set {} to '{}'{}\", name, redactor.redact(name, value), source == null ? \"\" : \" from \" + source); LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item);"
  },
  "e8344f7e_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "LOG.info(\"Set {} to '{}'{}\", name, redactor.redact(name, value), source == null ? \"\" : \" from \" + source); LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item);"
  },
  "e8344f7e_3": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "LOG.info(\"Set {} to '{}'{}\", name, redactor.redact(name, value), source == null ? \"\" : \" from \" + source); LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item);"
  },
  "e8344f7e_4": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → ENTRY: loadProps → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → CALL: addTags → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource",
    "log": "LOG.info(\"Set {} to '{}'{}\", name, redactor.redact(name, value), source == null ? \"\" : \" from \" + source); <!-- Log sequence accounting for possible logs under 'loadProps' -->"
  },
  "606b994c_1": {
    "exec_flow": "ENTRY → IF_TRUE: bindUserAliases != null && bindUserAliases.length > 0 → CALL: org.apache.hadoop.conf.Configuration:getStrings → FOREACH: bindUserAliases → CALL: org.apache.hadoop.conf.Configuration:get → CALL: org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser → IF_TRUE: bindUsername == null || bindPassword == null → THROW: new RuntimeException(\"Bind username or password not \" + \"configured for user: \" + bindUserAlias) → EXIT",
    "log": "<log_entry> <log_statement>[INFO] message</log_statement> <variables> <variable>---(initialization information)</variable> </variables> </log_entry> <log_entry> <log_statement>[DEBUG] Handling deprecation for all properties in config...</log_statement> <variables> <variable>---(initialization information)</variable> </variables> </log_entry> <log_entry> <log_statement>[DEBUG] Handling deprecation for + (String)item</log_statement> <variables> <variable>item</variable> </variables> </log_entry>"
  },
  "8726821b_1": {
    "exec_flow": "close() → ENTRY → [VIRTUAL_CALL] → cleanup() → LOG: [DEBUG] Ready to delete path: [{}]. recursive: [{}]. → TRY → CALL:getFileStatus → IF_FALSE:key.compareToIgnoreCase(\"/\") == 0 → IF_FALSE:status.isDirectory() → LOG: [DEBUG] Delete the file: {}. → CALL:createParent → IF_TRUE: parent != null → LOG: [DEBUG] Create parent key: {parentKey} → CALL:store.delete → TRY → CALL:getFileStatus → IF_FALSE:key.length() == 0 → IF_FALSE:meta == null && !key.endsWith(\"/\") → IF_TRUE: meta == null → DO_WHILE → IF_FALSE: CollectionUtils.isNotEmpty(listing.getObjectSummaries()) || CollectionUtils.isNotEmpty(listing.getCommonPrefixes()) → IF_FALSE: listing.isTruncated() → THROW:new FileNotFoundException(path + \": No such file or directory!\") → CATCH: IOException e → LOG: [DEBUG] Store a empty file in COS failed., e → EXCEPTION: maybeCreateFakeParentDirectory → CATCH: FileNotFoundException e → LOG: [DEBUG] Couldn't delete {} - does not exist, path → RETURN → EXIT",
    "log": "[DEBUG] Ready to delete path: [{}]. recursive: [{}]. [DEBUG] Delete the file: {}. [DEBUG] Create parent key: {parentKey} [DEBUG] Couldn't delete {} - does not exist [DEBUG] Store a empty file in COS failed., e"
  },
  "99b8e673_1": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → CALL: getInternal → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → ENTRY → TRY → CALL: org.apache.hadoop.fs.FileSystem:intialize(java.net.URI, org.apache.hadoop.conf.Configuration) → CATCH: IOException|RuntimeException → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String, boolean) → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → RETURN → EXIT",
    "log": "[DEBUG] Duplicate FS created for {}; discarding {}"
  },
  "99b8e673_2": {
    "exec_flow": "ENTRY → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → CALL: getInternal → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC: this → CALL: get → IF_TRUE: fs != null → LOG: LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "99b8e673_3": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → CALL: createFileSystem(java.net.URI, org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "99b8e673_4": {
    "exec_flow": "ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → ENTRY → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG(\"Looking for FS supporting {}, scheme\") → IF_TRUE: conf != null → LOG: LOGGER.DEBUG(\"looking for configuration option {}, property\") → CALL: getClass → IF_FALSE: clazz != null → LOG: LOGGER.DEBUG(\"Filesystem {} defined in configuration option, scheme\") → IF_FALSE: clazz == null → LOG: LOGGER.DEBUG(\"FS for {} is {}, scheme, clazz\") → RETURN → EXIT → TRY → CALL: initialize → EXCEPTION:initialize → CATCH:IOException | RuntimeException e → LOG: LOGGER.WARN(\"Failed to initialize filesystem {}: {}, uri, e.toString()\") → LOG: LOGGER.DEBUG(\"Failed to initialize filesystem, e\") → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → CALL: org.slf4j.Logger:debug(java.lang.String, java.lang.Object, java.lang.Object) → FOREACH_EXIT → EXIT → THROW: e → EXIT",
    "log": "[DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Filesystem {} defined in configuration option [DEBUG] FS for {} is {} [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem, e"
  },
  "99b8e673_5": {
    "exec_flow": "ENTRY → CALL: borrow → IF_TRUE: decompressor == null → CALL: createDecompressor → CALL: getDefaultExtension → LOG: LOGGER.INFO(\"Got brand-new decompressor {}\", <default extension>) → RETURN → EXIT",
    "log": "[INFO] Got brand-new decompressor {}"
  },
  "99b8e673_6": {
    "exec_flow": "ENTRY → CALL: borrow → IF_FALSE: decompressor == null → CALL: isDebugEnabled → IF_TRUE → LOG: LOGGER.DEBUG(\"Got recycled decompressor\") → RETURN → EXIT",
    "log": "[DEBUG] Got recycled decompressor"
  },
  "fa83bc12_1": {
    "exec_flow": "ENTRY→CALL:handleDeprecation→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for (String)item→CALL:handleDeprecation→FOREACH_EXIT→RETURN→EXIT",
    "log": "<log_entry level=\"DEBUG\" template=\"Handling deprecation for all properties in config...\"/> <log_entry level=\"DEBUG\" template=\"Handling deprecation for (String)item\"/>"
  },
  "fa83bc12_2": {
    "exec_flow": "ENTRY→VIRTUAL_CALL:org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)→LOG:Unexpected SecurityException in Configuration→EXIT",
    "log": "<log_entry level=\"WARN\" template=\"Unexpected SecurityException in Configuration\"/>"
  },
  "fa83bc12_3": {
    "exec_flow": "ENTRY→CALL: LOG.debug(\"Handling deprecation for all properties in config...\")→CALL: org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)→CALL: LOG_DEPRECATION.info→CALL: org.slf4j.Logger:info(java.lang.String)→EXIT",
    "log": "<log_entry level=\"DEBUG\" template=\"Handling deprecation for all properties in config...\"/> <log_entry level=\"INFO\" template=\"message\"/>"
  },
  "94d82abb_1": {
    "exec_flow": "ENTRY→TRY→CALL:getConnection→CALL:prepareCall→CALL:executeQuery→CALL:FederationStateStoreClientMetrics:failedStateStoreCall→CATCH:SQLException→LOG:ERROR:Unable to obtain the information for all the SubClusters→CALL:FederationStateStoreUtils.logAndThrowRetriableException→CALL:FederationStateStoreUtils.returnToPool→RETURN→EXIT",
    "log": "[ERROR] Unable to obtain the information for all the SubClusters"
  },
  "94d82abb_2": {
    "exec_flow": "ENTRY→IF_FALSE:t != null→LOG:ERROR:SubCluster {subClusterId} is not valid→THROW:new FederationStateStoreRetriableException(errMsg)→EXIT",
    "log": "[ERROR] SubCluster {subClusterId} is not valid"
  },
  "ca843567_1": {
    "exec_flow": "ENTRY → IF_TRUE: needReport && writeLockIntervalMs >= this.writeLockReportingThresholdMs → CALL: numWriteLockLongHold.increment → IF_TRUE: longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs → NEW: LockHeldInfo → CALL: getStackTrace → CALL: currentThread → CALL: currentThread → CALL: record → IF_TRUE: logAction.shouldLog() → NEW: LockHeldInfo → CALL: coarseLock.writeLock().unlock → IF_TRUE: needReport → CALL: addMetric → IF_TRUE: logAction.shouldLog() → CALL: FSNamesystem.LOG.info → EXIT ENTRY → CALL:SetQuotaOp.getInstance → CALL:SetQuotaOp.setSource → CALL:SetQuotaOp.setNSQuota → CALL:SetQuotaOp.setDSQuota → CALL:logEdit → TRY → SYNC: this → TRY → CALL:printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid ≤ synctxid → CALL:getLastJournalledTxId → LOG:LOG.DEBUG:logSync(tx) → IF_FALSE: lastJournalledTxId ≤ synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW:new IOException(\"No journals available to flush\") → EXIT → CALL:logSync → FOREACH:resources → IF_TRUE:!resource.isRequired() → CALL:isResourceAvailable → CALL:buf.append(\"Number of transactions:\").append(numTransactions).append(\"Total time for transactions(ms):\").append(totalTimeTransactions).append(\"Number of transactions batched in Syncs:\").append(numTransactionsBatchedInSync.longValue()).append(\"Number of syncs:\").append(editLogStream.getNumSync()).append(\"SyncTimes(ms):\").append(journalSet.getSyncTimes()) → LOG:LOG.INFO:buf.toString() → CALL:terminate → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]) → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → FOREACH_EXIT → IF_TRUE:redundantResourceCount == 0 → RETURN → EXIT",
    "log": "FSNamesystem.LOG.info(\"\\\\tNumber of suppressed write-lock reports: {}\" + \"\\\\n\\\\tLongest write-lock held at {} for {}ms via {}\" + \"\\\\n\\\\tTotal suppressed write-lock held time: {}\", logAction.getCount() - 1, Time.formatTime(lockHeldInfo.getStartTimeMs()), lockHeldInfo.getIntervalMs(), lockHeldInfo.getStackTrace(), logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()) [INFO] Namespace quota set [INFO] Diskspace quota set [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()>"
  },
  "ca843567_2": {
    "exec_flow": "ENTRY → TRY → SYNC: this → TRY → CALL: printStatistics → WHILE: mytxid > synctxid && isSyncRunning → WHILE_EXIT → IF_FALSE: mytxid ≤ synctxid → CALL: getLastJournalledTxId → LOG: LOG.DEBUG: logSync(tx) → IF_FALSE: lastJournalledTxId ≤ synctxid → TRY → IF_TRUE: journalSet.isEmpty() → THROW: new IOException(\"No journals available to flush\") → EXIT → CALL:logSync → IF_FALSE: (resources.isEmpty() && lastPrintTime + 60000 > now && !force) → FOREACH:resources → IF_TRUE: !resource.isRequired() → CALL:isResourceAvailable → CALL:buf.append(\"Number of transactions:\").append(numTransactions).append(\"Total time for transactions(ms):\").append(totalTimeTransactions).append(\"Number of transactions batched in Syncs:\").append(numTransactionsBatchedInSync.longValue()).append(\"Number of syncs:\").append(editLogStream.getNumSync()).append(\"SyncTimes(ms):\").append(journalSet.getSyncTimes()) → LOG:LOG.INFO:buf.toString() → CALL:terminate → CALL:org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object[]) → CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable) → FOREACH_EXIT → IF_TRUE: redundantResourceCount == 0 → RETURN → EXIT",
    "log": "[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()>"
  },
  "ca843567_3": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_TRUE:subject == null || subject.getPrincipals(User.class).isEmpty() → CALL:getLoginUser → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser == null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → IF_TRUE:readTokenStorageStream succeeded → CALL:addCredentials → CALL:debug → CALL:cleanupWithLogger → RETURN → EXIT",
    "log": "[LOG] getLoginUser [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [DEBUG] Reading credentials from location {} [DEBUG] Loaded {} tokens from {} [INFO] Token file {} does not exist [DEBUG] Failure to load login credentials [DEBUG] UGI loginUser: {} [INFO] Cleaning up resources"
  },
  "ca843567_4": {
    "exec_flow": "ENTRY → CALL:ensureInitialized → IF_FALSE:subject == null || subject.getPrincipals(User.class).isEmpty() → NEW:UserGroupInformation → IF_TRUE:GROUPS == null → IF_TRUE:LOG.isDebugEnabled() → LOG:LOG.DEBUG:Creating new Groups object → NEW:Groups → CALL:<init> → RETURN → EXIT",
    "log": "[DEBUG] Creating new Groups object"
  },
  "c2f09659_1": {
    "exec_flow": "ENTRY → LOG: Handling deprecation for all properties in config... → CALL: handleDeprecation → FOREACH: names → CALL: getProps → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: overlay != null → CALL:getCompressor → CALL:org.slf4j.Logger:isDebugEnabled() → CALL:org.slf4j.Logger:debug → CALL:org.apache.hadoop.io.compress.CompressionCodec:createCompressor → CALL:org.apache.hadoop.io.compress.Compressor:reinit → CALL:org.apache.hadoop.io.compress.CompressionCodec:getDefaultExtension → CALL:org.slf4j.Logger:info → RETURN → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String) item [WARN] Unexpected SecurityException in Configuration [DEBUG] Compression codec created [INFO] Compressor initialized"
  },
  "e7352ee8_1": {
    "exec_flow": "<entry_point>Parent.ENTRY</entry_point> <virtual_call>true</virtual_call> <child_paths> ENTRY→IF_TRUE: !initializedResources → SYNC: ResourceUtils.class → IF_TRUE: !initializedResources → IF_TRUE: resConf == null → NEW: YarnConfiguration → CALL: addResourcesFileToConf → CALL: initializeResourcesMap → CALL: size → CALL: size → FOR_INIT→FOR_COND:i < maxLength→TRY→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:getResourceInformation→CALL:org.apache.hadoop.yarn.api.records.Resource:setResourceValue→CATCH:ResourceNotFoundException→CALL:org.slf4j.Logger:warn→FOR_EXIT→RETURN→EXIT </child_paths>",
    "log": "<entry> <log_level>ERROR</log_level> <log_template>A problem was encountered while calculating resource availability that should not occur under normal circumstances. Please report this error to the Hadoop community by opening a JIRA ticket at http://issues.apache.org/jira and including the following information: * Exception encountered: <stack_trace> * Cluster resources: <cluster_resources> * LHS resource: <lhs_resources> * RHS resource: <rhs_resources> </log_template> </entry> <entry> <log_level>ERROR</log_level> <log_template>The resource manager is in an inconsistent state. It is safe for the resource manager to be restarted as the error encountered should be transitive. If high availability is enabled, failing over to a standby resource manager is also safe. </log_template> </entry> <entry> <log_level>WARN</log_level> <log_template>Resource is missing: {exception_message}</log_template> </entry>"
  },
  "e7352ee8_2": {
    "exec_flow": "<entry_point>Parent.ENTRY</entry_point> <virtual_call>true</virtual_call> <child_paths> <child_path>Resource:newInstance</child_path> </child_paths>",
    "log": "<entry> <log_level>INFO</log_level> <log_template>Initializing Resource instance</log_template> </entry>"
  },
  "fdecba9d_1": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→CALL:rpcServer.getLocationsForPath→IF_TRUE:rpcServer.isInvokeConcurrent(snapshotRoot)→IF_FALSE:locations.isEmpty()→IF_FALSE:locations.size() == 1 && timeOutMs ≤ 0→FOREACH:locations→FOREACH_EXIT→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.proxyOp→TRY→IF_TRUE:timeOutMs > 0→CALL:invokeAll→FOR_INIT→FOR_COND:i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out <log>AccessControlException: \"The operation is not allowed because...\"</log> <log>IOException: \"Cannot find locations for...\"</log> <log>IOException: \"is in a read only mount point\"</log>"
  },
  "fdecba9d_2": {
    "exec_flow": "ENTRY→CALL:rpcServer.checkOperation→CALL:rpcServer.getLocationsForPath→IF_TRUE:rpcServer.isInvokeConcurrent(snapshotRoot)→IF_FALSE:locations.isEmpty()→IF_FALSE:locations.size() == 1 && timeOutMs ≤ 0→FOREACH:locations→FOREACH_EXIT→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.proxyOp→TRY→IF_FALSE:timeOutMs > 0→CALL:invokeAll→FOR_INIT→FOR_COND:i < futures.size()→FOR_EXIT→RETURN→EXIT",
    "log": "[DEBUG] Proxying operation: {} [DEBUG] Cannot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out <log>AccessControlException: \"The operation is not allowed because...\"</log> <log>IOException: \"Cannot find locations for...\"</log> <log>IOException: \"is in a read only mount point\"</log>"
  },
  "c9ac8679_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.mapred.YARNRunner:killTask → ENTRY → CALL:getClient → ENTRY → IF_FALSE:StringUtils.isEmpty(serviceAddr) → CALL:get(java.lang.String) → LOG:debug(Connecting to HistoryServer at: + serviceAddr) → CALL:create(org.apache.hadoop.conf.Configuration) → LOG:debug(Connected to HistoryServer at: + serviceAddr) → CALL:getCurrentUser() → CALL:doAs(java.security.PrivilegedAction) → NEW:PrivilegedAction<MRClientProtocol> → CALL:getProxy → CALL:createSocketAddr → RETURN → EXIT → CATCH:IOException → CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable) → THROW:YarnRuntimeException → EXIT → CALL:getJobID → CALL:killTask → ENTRY → IF_TRUE:fail → CALL:failRequest.setTaskAttemptId → CALL:invoke → RETURN → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Connecting to HistoryServer at: serviceAddr [DEBUG] Connected to HistoryServer at: serviceAddr [WARN] Could not connect to History server. [DEBUG] Failed to contact AM/History for job retrying.. [WARN] ClientServiceDelegate invoke call interrupted"
  },
  "e50a0422_1": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → EXIT → LOG: Handling deprecation for all properties in config... → CALL: getProps → CALL: addAll → FOREACH: keys → LOG: Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → LOG: [INFO] message → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log> <log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[INFO] message</log>"
  },
  "e50a0422_2": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "e50a0422_3": {
    "exec_flow": "Parent.ENTRY → [VIRTUAL_CALL] → ENTRY → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "e50a0422_4": {
    "exec_flow": "ENTRY → CALL: LOG_DEPRECATION.info → CALL: org.slf4j.Logger:info(java.lang.String) → EXIT",
    "log": "<log>[INFO] message</log>"
  },
  "22fe1176_1": {
    "exec_flow": "ENTRY → CALL:getFileStatus → VIRTUAL_CALL → ENTRY → CALL:qualify → LOG:DEBUG [Getting path status for {f}; needEmptyDirectory={}] → CALL:makeQualified → LOG:DEBUG [Stripping trailing '/' from {q}] → CALL:s3GetFileStatus → RETURN → EXIT → ENTRY → VIRTUAL_CALL → CALL:getObjectMetadata → EXCEPTION:getObjectMetadata → CATCH:OSSException osse → CALL:LOG.debug → RETURN → EXIT → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:access$6 → VIRTUAL_CALL → org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus → LOG:DEBUG [AzureBlobFileSystem.getFileStatus path: {}] → CALL:statIncrement → CALL:makeQualified → CALL:AzureBlobFileSystemStore.getFileStatus → RETURN → EXIT → CALL:openConnection → CALL:setRequestMethod → IF_TRUE:method.equals(HTTP_POST) || method.equals(HTTP_PUT) → CALL:setDoOutput → RETURN → EXIT → VIRTUAL_CALL:run() → GetOpParam.Op.GETFILESTATUS → FsPathResponseRunner<HdfsFileStatus> → decodeResponse → return status → RETURN → EXIT",
    "log": "<log_entry> <class_method>org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Exception thrown when get object meta: + key + , exception: + osse</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:getFileStatus</class_method> <level>DEBUG</level> <template>Getting path status for {f}; needEmptyDirectory={}</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.s3a.S3AFileSystem:makeQualified</class_method> <level>DEBUG</level> <template>Stripping trailing '/' from {q}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Getting the file status for {f}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Retrieving metadata for {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Path {f} is a folder.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.azure.NativeAzureFileSystem</class> <method>getFileStatusInternal</method> <level>DEBUG</level> <template>Found the path: {f} as a file.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a file. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a dir. COS key: {key}</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>List COS key: {key} to check the existence of the path.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a directory. COS key: {key}</template> </log_entry> <log_entry> <class_method>org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus</class_method> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry>"
  },
  "8e721ca1_1": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL] → Child paths</step> <step>Callee: ENTRY→TRY→IF_TRUE:types != null→CALL:getDeclaredMethod→CALL:org.slf4j.Logger:error→THROW:IOException→EXIT</step>",
    "log": "[ERROR] Cannot get method {} with types {} from {}"
  },
  "8e721ca1_2": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL] → Child paths</step> <step>Callee: ENTRY→IF_FALSE:namenodes == null || namenodes.isEmpty()→CALL:addClientInfoToCallerContext→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.proxyOp→FOREACH:namenodes→TRY→CALL:getConnection→CALL:invoke→IF_TRUE:failover→CALL:namenodeResolver.updateActiveNamenode→IF_TRUE:this.rpcMonitor != null→CALL:this.rpcMonitor.proxyOpComplete→RETURN→EXIT</step>",
    "log": "[ERROR] Cannot get available namenode for..."
  },
  "8e721ca1_3": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL] → Child paths</step> <step>Callee: ENTRY→IF_FALSE:namenodes == null || namenodes.isEmpty()→CALL:addClientInfoToCallerContext→IF_TRUE:rpcMonitor != null→CALL:rpcMonitor.proxyOp→FOREACH:namenodes→TRY→CALL:getConnection→CALL:invoke→IF_FALSE:failover→IF_TRUE:this.rpcMonitor != null→CALL:this.rpcMonitor.proxyOpComplete→RETURN→EXIT</step>",
    "log": "[ERROR] Get connection for..."
  },
  "8e721ca1_4": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL] → Child paths</step> <step>Callee: ENTRY→IF_FALSE:namenodes == null || namenodes.isEmpty()→CALL:addClientInfoToCallerContext→IF_FALSE:rpcMonitor != null→FOREACH:namenodes→TRY→CALL:getConnection→CALL:invoke→IF_FALSE:failover→IF_FALSE:this.rpcMonitor != null→RETURN→EXIT</step>",
    "log": "[ERROR] No namenode available to invoke..."
  },
  "b441f43d_1": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → CALL: getLoginUser → ENTRY → CALL: ensureInitialized → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds DO_EXIT → RETURN → EXIT → EXIT LOG: [WARN] Unexpected SecurityException in Configuration → CALL: findSubVariable → CALL: getenv → CALL: getProperty → CALL: getRaw → LOG: [DEBUG] Handling deprecation for all properties in config... EXIT → LOG: [DEBUG] Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → LOG: [DEBUG] Handling deprecation for all properties in config... CALL: getProps → CALL: addAll → FOREACH: keys → LOG: [DEBUG] Handling deprecation for (String)item → CALL: handleDeprecation → FOREACH_EXIT → EXIT IF_FALSE: !isInitialized() → EXIT",
    "log": "<log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "c489b512_1": {
    "exec_flow": "ENTRY→IF_FALSE: lhs.equals(rhs)→IF_FALSE: isAllInvalidDivisor(clusterResource)→TRY→EXCEPTION:ArrayIndexOutOfBoundsException→CALL:org.slf4j.Logger:error(java.lang.String)→CALL:org.slf4j.Logger:error(java.lang.String)→THROW:YarnRuntimeException",
    "log": "[ERROR] A problem was encountered while calculating resource availability that should not occur under normal circumstances. Please report this error to the Hadoop community by opening a JIRA ticket at http://issues.apache.org/jira and including the following information: * Exception encountered: <stack_trace> * Cluster resources: <cluster_resources> * LHS resource: <lhs_resources> * RHS resource: <rhs_resources> [ERROR] The resource manager is in an inconsistent state. It is safe for the resource manager to be restarted as the error encountered should be transitive. If high availability is enabled, failing over to a standby resource manager is also safe."
  },
  "c55f2103_1": {
    "exec_flow": "ENTRY→IF_TRUE: stream != null→CALL:cleanupWithLogger→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null)→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT",
    "log": "[DEBUG] Exception in closing {}"
  },
  "d321e2a8_1": {
    "exec_flow": "ENTRY→LOG:Handling deprecation for all properties in config...→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:getProps→FOREACH:names→CALL:substituteVars→FOREACH_EXIT→RETURN→EXIT",
    "log": "[INFO] message [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "a2ed87a2_1": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta!=null→IF_TRUE:meta.isFile()→LOG:LOG.DEBUG:Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "[DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "a2ed87a2_2": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE:key.length()==0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_FALSE:meta!=null→LOG:LOG.DEBUG:List COS key: [{}] to check the existence of the path.→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list→IF_TRUE:listing.getFiles().length>0||listing.getCommonPrefixes().length>0→IF_TRUE:LOG.isDebugEnabled()→LOG:LOG.DEBUG:Path: [{}] is a directory. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory→RETURN→EXIT",
    "log": "[DEBUG] List COS key: [{}] to check the existence of the path. [DEBUG] Path: [{}] is a directory. COS key: [{}]"
  },
  "09a4d8af_1": {
    "exec_flow": "ENTRY → IF_FALSE:locations.isEmpty() → IF_FALSE:locations.size() == 1 && timeOutMs <= 0 → FOREACH:locations → IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.proxyOp → TRY → IF_TRUE:timeOutMs > 0 → CALL:invokeAll → FOR_INIT → FOR_COND:i < futures.size() → RETURN → EXIT",
    "log": "[ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "09a4d8af_2": {
    "exec_flow": "ENTRY → IF_FALSE:locations.isEmpty() → IF_FALSE:locations.size() == 1 && timeOutMs <= 0 → FOREACH:locations → IF_TRUE:rpcMonitor != null → CALL:rpcMonitor.proxyOp → TRY → IF_FALSE:timeOutMs > 0 → CALL:invokeAll → FOR_INIT → FOR_COND:i < futures.size() → RETURN → EXIT",
    "log": "[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()} [ERROR] Invocation to \"{location}\" for \"{method.getMethodName()}\" timed out"
  },
  "09a4d8af_3": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "09a4d8af_4": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "09a4d8af_5": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "09a4d8af_6": {
    "exec_flow": "ENTRY → IF_FALSE: rpcMonitor != null → IF_TRUE: LOG.isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_FALSE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → CALL: checkSafeMode → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "f46a85bd_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\") → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {}, property → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \" + \" \\\"\"+scheme+\"\\\" \") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → CALL: getFileStatus(org.apache.hadoop.fs.Path) → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG: Path: [{f}] is a file. COS key: [{key}]. → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {} [DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Looking in service filesystems for implementation class [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {} [DEBUG] Path: [{f}] is a file. COS key: [{key}]."
  },
  "18238028_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration) → CALL: getInternal → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[]) → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose) → RETURN → EXIT",
    "log": "[DEBUG] Duplicate FS created for {}; discarding {}"
  },
  "18238028_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {}"
  },
  "a8e839bf_1": {
    "exec_flow": "ENTRY→IF_TRUE: !isFirstAttempt() && !recovered()→CALL:getJobContextFromConf→TRY→LOG.INFO: Starting to clean up previous job's temporary files→CALL:abortJob→EXCEPTION: abortJob→CATCH: FileNotFoundException e→LOG.INFO: Previous job temporary files do not exist, no clean up was necessary.→EXIT",
    "log": "[INFO] Starting to clean up previous job's temporary files [INFO] Previous job temporary files do not exist, no clean up was necessary."
  },
  "a8e839bf_2": {
    "exec_flow": "ENTRY→IF_TRUE: !isFirstAttempt() && !recovered()→CALL:getJobContextFromConf→TRY→LOG.INFO: Starting to clean up previous job's temporary files→CALL:abortJob→EXCEPTION: abortJob→CATCH: Exception e→LOG.ERROR: Error while trying to clean up previous job's temporary files, e→EXIT",
    "log": "[INFO] Starting to clean up previous job's temporary files [ERROR] Error while trying to clean up previous job's temporary files"
  },
  "a8e839bf_3": {
    "exec_flow": "ENTRY→IF_TRUE: !isFirstAttempt() && !recovered()→CALL:getJobContextFromConf→TRY→LOG.INFO: Starting to clean up previous job's temporary files→CALL:abortJob→LOG.INFO: Finished cleaning up previous job temporary files→EXIT",
    "log": "[INFO] Starting to clean up previous job's temporary files [INFO] Finished cleaning up previous job temporary files"
  },
  "4907fa33_1": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: JobHistory Init→CALL: newInstance→CALL: newRecordInstance→CALL: getRecordFactory→CALL: getLong→CALL: createHistoryFileManager→CALL: hsManager.init→TRY→CALL: hsManager.initExisting→LOG: LOG.INFO: Initializing Existing Jobs...→CALL: findTimestampedDirectories→CALL: Collections.sort→LOG: LOG.INFO: Found + timestampedDirList.size() + directories to load→FOREACH: timestampedDirList→CALL: addDirectoryToSerialNumberIndex→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled→LOG: LOG.DEBUG: Adding + serialDirPath + to serial index→CALL: org.slf4j.Logger:debug(java.lang.String)→IF_TRUE: timestampPart == null→LOG: LOG.WARN: Could not find timestamp portion from path: + serialDirPath + . Continuing with next→CALL: org.slf4j.Logger:warn(java.lang.String)→RETURN→FOREACH_EXIT→FOR_INIT→FOR_COND: i >= 0 && !jobListCache.isFull()→CALL: addDirectoryToJobListCache→LOG: LOG.INFO: currCacheSize * 100.0 / maxCacheSize + % of cache is loaded.→FOR_EXIT→LOG: LOG.INFO: Existing job initialization finished. + loadedPercent + % of cache is occupied.→EXCEPTION: initExisting→CATCH: IOException e→THROW: new YarnRuntimeException(\"Failed to initialize existing directories\", e)→EXIT",
    "log": "[INFO] JobHistory Init [INFO] Initializing Existing Jobs... [INFO] Found + timestampedDirList.size() + directories to load [DEBUG] Adding [serialDirPath] to serial index [WARN] Could not find timestamp portion from path: [serialDirPath]. Continuing with next [INFO] currCacheSize * 100.0 / maxCacheSize + % of cache is loaded. [INFO] Existing job initialization finished. + loadedPercent + % of cache is occupied."
  },
  "4907fa33_2": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: JobHistory Init→CALL: newInstance→CALL: newRecordInstance→CALL: getRecordFactory→CALL: getLong→CALL: createHistoryFileManager→CALL: hsManager.init→TRY→CALL: hsManager.initExisting→LOG: LOG.INFO: Initializing Existing Jobs...→CALL: findTimestampedDirectories→CALL: Collections.sort→LOG: LOG.INFO: Found + timestampedDirList.size() + directories to load→FOREACH: timestampedDirList→CALL: addDirectoryToSerialNumberIndex→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled→LOG: LOG.DEBUG: Adding + serialDirPath + to serial index→CALL: org.slf4j.Logger:debug(java.lang.String)→IF_FALSE: timestampPart == null→IF_TRUE: serialPart == null→LOG: LOG.WARN: Could not find serial portion from path: + serialDirPath.toString() + . Continuing with next→CALL: org.slf4j.Logger:warn(java.lang.String)→FOREACH_EXIT→FOR_INIT→FOR_COND: i >= 0 && !jobListCache.isFull()→CALL: addDirectoryToJobListCache→LOG: LOG.INFO: currCacheSize * 100.0 / maxCacheSize + % of cache is loaded.→FOR_EXIT→LOG: LOG.INFO: Existing job initialization finished. + loadedPercent + % of cache is occupied.→CALL: createHistoryStorage→IF_TRUE: conf != null→IF_TRUE: theObject instanceof Configurable→CALL: ((Configurable) theObject).setConf→CALL: setJobConf→CALL: handleDeprecation→LOG: Handling deprecation for all properties in config...→CALL: getProps→CALL: addAll→FOREACH: keys→LOG: Handling deprecation for + (String)item→CALL: handleDeprecation→FOREACH_EXIT→FOREACH: names→CALL: getProps→CALL: substituteVars→FOREACH_EXIT→RETURN→IF: storage instance of Service→CALL: init→CALL: setHistoryFileManager→CALL: serviceInit→EXIT",
    "log": "[INFO] JobHistory Init [INFO] Initializing Existing Jobs... [INFO] Found + timestampedDirList.size() + directories to load [DEBUG] Adding [serialDirPath] to serial index [WARN] Could not find serial portion from path: [serialDirPath]. Continuing with next [INFO] currCacheSize * 100.0 / maxCacheSize + % of cache is loaded. [INFO] Existing job initialization finished. + loadedPercent + % of cache is occupied. [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "4907fa33_3": {
    "exec_flow": "ENTRY→LOG: LOG.INFO: JobHistory Init→CALL: newInstance→CALL: newRecordInstance→CALL: getRecordFactory→CALL: getLong→CALL: createHistoryFileManager→CALL: hsManager.init→TRY→CALL: hsManager.initExisting→LOG: LOG.INFO: Initializing Existing Jobs...→CALL: findTimestampedDirectories→CALL: Collections.sort→LOG: LOG.INFO: Found + timestampedDirList.size() + directories to load→FOREACH: timestampedDirList→CALL: addDirectoryToSerialNumberIndex→IF_TRUE: LOG.isDebugEnabled()→CALL: org.slf4j.Logger:isDebugEnabled→LOG: LOG.DEBUG: Adding + serialDirPath + to serial index→CALL: org.slf4j.Logger:debug(java.lang.String)→IF_FALSE: timestampPart == null→IF_FALSE: serialPart == null→CALL: serialNumberIndex.add→CALL: org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$SerialNumberIndex:add(java.lang.String,java.lang.String)→IF_TRUE: !cache.containsKey(serialPart)→CALL: put→IF_TRUE: cache.size() > maxSize→CALL: org.slf4j.Logger:error→LOG: LOG.ERROR: Dropping + key + from the SerialNumberIndex. We will no + longer be able to see jobs that are in that serial index for + cache.get(key)→CALL: remove→CALL: datePartSet.add→FOREACH_EXIT→FOR_INIT→FOR_COND: i >= 0 && !jobListCache.isFull()→CALL: addDirectoryToJobListCache→LOG: LOG.INFO: currCacheSize * 100.0 / maxCacheSize + % of cache is loaded.→FOR_EXIT→LOG: LOG.INFO: Existing job initialization finished. + loadedPercent + % of cache is occupied.→CALL: createHistoryStorage→IF_TRUE: conf != null→IF_FALSE: theObject instanceof Configurable→CALL: setJobConf→CALL: handleDeprecation→LOG: Handling deprecation for all properties in config...→CALL: getProps→CALL: addAll→FOREACH: keys→LOG: Handling deprecation for + (String)item→CALL: handleDeprecation→FOREACH_EXIT→FOREACH: names→CALL: getProps→CALL: substituteVars→FOREACH_EXIT→RETURN→ELSE: storage not instance of Service→CALL: setHistoryFileManager→CALL: serviceInit→EXIT",
    "log": "[INFO] JobHistory Init [INFO] Initializing Existing Jobs... [INFO] Found + timestampedDirList.size() + directories to load [DEBUG] Adding [serialDirPath] to serial index [ERROR] Dropping {key} from the SerialNumberIndex. We will no longer be able to see jobs that are in that serial index for {cache.get(key)} [INFO] currCacheSize * 100.0 / maxCacheSize + % of cache is loaded. [INFO] Existing job initialization finished. + loadedPercent + % of cache is occupied. [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "968ba496_1": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → CALL:statIncrement → CALL:trailingPeriodCheck → CALL:makeQualified → TRY → CALL:createFile → CALL:statIncrement → NEW:FSDataOutputStream → RETURN → EXIT",
    "log": "<log>[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}</log>"
  },
  "968ba496_2": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → IF_TRUE:containsColon(f) → THROW:new IOException(\"Cannot create file \" + f + \" through WASB that has colons in the name\") → EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "968ba496_3": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG → IF_FALSE:containsColon(f) → CALL:performAuthCheck → CALL:createInternal → RETURN → EXIT",
    "log": "<log>[DEBUG] Creating file: {}</log>"
  },
  "968ba496_4": {
    "exec_flow": "ENTRY → CALL: Preconditions.checkNotNull → IF_FALSE: InodeTree.SlashPath.equals(f) → IF_TRUE: this.fsState.getRootFallbackLink() != null → IF_FALSE: theInternalDir.getChildren().containsKey(f.getName()) → TRY → CALL: org.apache.hadoop.fs.FileSystem:create → RETURN → EXIT",
    "log": "<log>[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}</log>"
  },
  "968ba496_5": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for (String)item → CALL:handleDeprecation → FOREACH_EXIT → CALL:getProps → CALL:substituteVars → LOG:Unexpected SecurityException in Configuration → CALL:findSubVariable → CALL:getenv → CALL:getProperty → CALL:getRaw → RETURN → FOREACH_EXIT → CALL: org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration) → ENTRY → IF_TRUE: conf != null → CALL: org.apache.hadoop.conf.Configuration:get → TRY → IF_TRUE: confUmask != null → CALL: getUMask → NEW: UmaskParser → NEW: FsPermission → RETURN → LOG: [WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask. → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[WARN] Unexpected SecurityException in Configuration</log> <log>[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.</log>"
  },
  "bde55b84_1": {
    "exec_flow": "ENTRY → IF_TRUE: splits == null → FOREACH: job.getMapTasks() → IF_TRUE: taskType != Pre21JobHistoryConstants.Values.MAP → LOG: LOG.WARN: TaskType for a MapTask is not Map. task= ... type= ... → CONTINUE → FOREACH_EXIT → IF_TRUE: totalMaps < splitsList.size() → LOG: LOG.WARN: TotalMaps for job ... is less than the total number of map task descriptions (...) → IF_TRUE: splitsList.size() == 0 → FOR_INIT → FOR_COND: i < totalMaps → FOR_EXIT → CALL: toArray → RETURN → EXIT",
    "log": "[WARN] TaskType for a MapTask is not Map. task=... type=... [WARN] TotalMaps for job ... is less than the total number of map task descriptions (...)"
  },
  "cdf17b18_1": {
    "exec_flow": "ENTRY→IF_TRUE:debugMode→LOG:INFO:op.getDebugInfo()→CALL:add→NEW:Operation→NEW:Operation→RETURN→EXIT→FOREACH: bufferPool.getAll()→IF: data.stateEqualsOneOf(...)→CALL: org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData)→FOREACH_EXIT→CALL: org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)→NEW:End→CALL:add→RETURN→EXIT",
    "log": "[INFO] op.getDebugInfo()"
  },
  "cdf17b18_2": {
    "exec_flow": "ENTRY→IF_FALSE: closed→IF_FALSE: cachingDisabled.get()→CALL: Validate.checkNotNull→IF_FALSE: !data.stateEqualsOneOf(EXPECTED_STATE_AT_CACHING)→SYNC: data→IF_FALSE: cache.containsBlock→CALL: ops.requestCaching→CALL: setCaching→CALL: Validate.checkNotNull→CALL: ops.end→CALL:add→IF_TRUE:debugMode→LOG:INFO:op.getDebugInfo()→NEW:End→CALL:add→RETURN→EXIT",
    "log": "[INFO] op.getDebugInfo()"
  },
  "5bba8288_1": {
    "exec_flow": "<sequence> <step>NameNodeRpcServer:addErasureCodingPolicies ENTRY</step> <step>CALL:checkNNStartup</step> <step>CALL:checkSuperuserPrivilege</step> <step>CALL:org.apache.hadoop.ipc.RetryCache:waitForCompletion</step> <step>IF_FALSE:cacheEntry!=null&&cacheEntry.isSuccess()</step> <step>TRY:FSNamesystem:addErasureCodingPolicies</step> <step>CALL:FSNamesystem:addErasureCodingPolicies ENTRY</step> <step>CALL:checkOperation</step> <step>CALL:checkErasureCodingSupported</step> <step>CALL:writeLock</step> <step>TRY</step> <step>CALL:checkOperation</step> <step>CALL:checkNameNodeSafeMode</step> <step>FOR</step> <step>TRY</step> <step>CALL:FSDirErasureCodingOp:addErasureCodingPolicy</step> <step>CATCH:HadoopIllegalArgumentException</step> <step>FINALLY</step> <step>CALL:writeUnlock</step> <step>CALL:getEditLog</step> <step>[VIRTUAL_CALL] to <FSNamesystem:logSync></step> <step>CALL:logAuditEvent</step> <step>IF_TRUE</step> <step>CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[])</step> <step>EXIT</step> <step>RETURN</step> </sequence>",
    "log": "[INFO] Erasure coding policies added [INFO] Audit log for operation addErasureCodingPolicies [WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={} [INFO] Added erasure coding policy [DEBUG] doEditTx() op={} txid={} [INFO] Logger debug executed [DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={} [ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions} [INFO] Logging exit info [DEBUG] Detailed exit debug info [ERROR] An error occurred when terminating"
  },
  "5bba8288_2": {
    "exec_flow": "ENTRY → IF_FALSE:skipRetryCache() → CALL:waitForCompletion → CALL:newEntry → CALL:org.slf4j.Logger:isTraceEnabled() → CALL:org.slf4j.Logger:trace → RETURN → EXIT",
    "log": "[TRACE] Execution trace"
  },
  "c4eff046_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: LOG.WARN: IOException of NvidiaDockerV1CommandPlugin init:, e</step> <step>CALL: org.apache.hadoop.conf.Configuration:get</step> <step>TRY</step> <step>CALL: setRequestProperty</step> <step>EXCEPTION: setRequestProperty</step> <step>CATCH: IOException e</step> <step>LOG: LOG.WARN: IOException of NvidiaDockerV1CommandPlugin init:, e</step> <step>THROW: new ContainerExecutionException(e)</step> <step>EXIT</step>",
    "log": "<log>[WARN] IOException of NvidiaDockerV1CommandPlugin init:, e</log>"
  },
  "c4eff046_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL: org.apache.hadoop.conf.Configuration:get</step> <step>TRY</step> <step>CALL: setRequestProperty</step> <step>CALL: IOUtils.copy</step> <step>CALL: toString</step> <step>LOG: LOG.INFO: Additional docker CLI options from plugin to run GPU containers: cliOptions</step> <step>FOREACH: cliOptions.split(\" \")</step> <step>IF: str.startsWith(VOLUME_DRIVER_OPTION)</step> <step>CALL: addToCommand(VOLUME_DRIVER_OPTION, getValue(str))</step> <step>LOG: LOG.DEBUG: Found volume-driver:volumeDriver</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[INFO] Additional docker CLI options from plugin to run GPU containers: cliOptions</log> <log>[DEBUG] Found volume-driver:volumeDriver</log>"
  },
  "c4eff046_3": {
    "exec_flow": "<step>ENTRY</step> <step>CALL: org.apache.hadoop.conf.Configuration:get</step> <step>TRY</step> <step>CALL: setRequestProperty</step> <step>CALL: IOUtils.copy</step> <step>CALL: toString</step> <step>LOG: LOG.INFO: Additional docker CLI options from plugin to run GPU containers: cliOptions</step> <step>FOREACH: cliOptions.split(\" \")</step> <step>IF: str.startsWith(MOUNT_RO_OPTION)</step> <step>CALL: addToCommand(MOUNT_RO_OPTION, mount.substring(0, mount.lastIndexOf(':')))</step> <step>FOREACH_EXIT</step> <step>EXIT</step>",
    "log": "<log>[INFO] Additional docker CLI options from plugin to run GPU containers: cliOptions</log>"
  },
  "77200d44_1": {
    "exec_flow": "ENTRY → CALL:getFileSystem → CALL:makeQualified → LOG:[DEBUG] Ready to delete path: [{}]. recursive: [{}] → LOG:[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive → CALL:delete → CALL:SorterInit → CALL:sort → EXIT",
    "log": "<log>[DEBUG] Ready to delete path: [{}]. recursive: [{}]</log> <log>[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive</log>"
  },
  "77200d44_2": {
    "exec_flow": "ENTRY → CALL:getFileSystem → CALL:makeQualified → LOG:[DEBUG] Ready to delete path: [{}]. recursive: [{}] → LOG:[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive → CALL:delete → CALL:SorterInit → CALL:sort → EXIT",
    "log": "<log>[DEBUG] Ready to delete path: [{}]. recursive: [{}]</log> <log>[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive</log>"
  },
  "77200d44_3": {
    "exec_flow": "ENTRY → CALL:getFileSystem → CALL:makeQualified → CALL:delete → LOG:[DEBUG] Couldn't delete {} - does not exist, path → CALL:SorterInit → CALL:sort → EXIT",
    "log": "<log>[DEBUG] Couldn't delete {} - does not exist, path</log>"
  },
  "77200d44_4": {
    "exec_flow": "<parent_entry>org.apache.hadoop.fs.FileSystem:get</parent_entry> <step>ENTRY</step> <step>CALL:org.apache.hadoop.conf.Configuration:getBoolean</step> <step>DESCRIPTION:Evaluate boolean property from configuration</step> <step>CALL:org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>DESCRIPTION:Retrieve trimmed property value</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:org.apache.hadoop.util.DurationInfo:<init></step> <step>CALL:addKVAnnotation</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass</step> <step>IF_TRUE:!FILE_SYSTEMS_LOADED</step> <step>CALL:loadFileSystems</step> <step>LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme</step> <step>IF_TRUE:conf != null</step> <step>LOG:LOGGER.DEBUG:looking for configuration option {}, property</step> <step>CALL:getClass</step> <step>IF_TRUE:clazz == null</step> <step>LOG:LOGGER.DEBUG:Looking in service filesystems for implementation class</step> <step>CALL:get</step> <step>IF_TRUE:clazz == null</step> <step>THROW:new UnsupportedFileSystemException(\"No FileSystem for scheme \"+\"\\\\\"\"+scheme+\"\\\\\"\")</step> <step>EXIT</step> <step>CALL:org.apache.hadoop.util.ReflectionUtils:newInstance</step> <step>TRY</step> <step>CALL:initialize</step> <step>EXCEPTION:initialize</step> <step>CATCH:IOException | RuntimeException e</step> <step>LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString()</step> <step>LOG:LOGGER.DEBUG:Failed to initialize filesystem, e</step> <step>CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger</step> <step>THROW:e</step> <step>EXIT</step> <step>CALL:org.apache.hadoop.util.DurationInfo:close</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Looking for FS supporting {}, scheme</log> <log>[DEBUG] looking for configuration option {}, property</log> <log>[DEBUG] Looking in service filesystems for implementation class</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log>"
  },
  "93a1f08b_1": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_TRUE:meta.isFile() → LOG:LOG.DEBUG:Path: [f] is a file. COS key: [key] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a file. COS key: {key}</template> </log_entry>"
  },
  "93a1f08b_2": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_TRUE:meta!=null → IF_FALSE:meta.isFile() → LOG:LOG.DEBUG:Path: [f] is a dir. COS key: [key] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a dir. COS key: {key}</template> </log_entry>"
  },
  "93a1f08b_3": {
    "exec_flow": "ENTRY → IF_FALSE:key.length()==0 → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata → IF_FALSE:meta!=null → LOG:LOG.DEBUG:List COS key: [key] to check the existence of the path. → CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:list → IF_TRUE:listing.getFiles().length>0 || listing.getCommonPrefixes().length>0 → LOG:LOG.DEBUG:Path: [f] is a directory. COS key: [key] → CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newDirectory → RETURN → EXIT",
    "log": "<log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>List COS key: {key} to check the existence of the path.</template> </log_entry> <log_entry> <class>org.apache.hadoop.fs.cosn.CosNFileSystem</class> <method>getFileStatus</method> <level>DEBUG</level> <template>Path: {f} is a directory. COS key: {key}</template> </log_entry>"
  },
  "f7ed6e0a_1": {
    "exec_flow": "ENTRY → IF_TRUE: !isInitialized() → SYNC: UserGroupInformation.class → IF_TRUE: !isInitialized() → CALL: initialize → IF_TRUE: GROUPS == null → IF_TRUE: LOG.isDebugEnabled() → CALL: isDebugEnabled → LOG: Creating new Groups object → NEW: Groups → CALL: <init> → RETURN → CALL: getLoginUser → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → RETURN → EXIT",
    "log": "[LOG] Creating new Groups object [LOG] getLoginUser"
  },
  "f7ed6e0a_2": {
    "exec_flow": "ENTRY → CALL: getConf → NEW: DatanodeHttpServer → CALL: <init> → CALL: httpServer.start → IF_TRUE: httpServer.getHttpAddress() != null → CALL: getHttpAddress → CALL: getPort → IF_FALSE: httpServer.getHttpsAddress() != null → ENTRY → IF_TRUE: httpServer != null → CALL: org.apache.hadoop.hdfs.server.datanode.DataNode:getInfoAddr(org.apache.hadoop.conf.Configuration) → TRY → CALL: syncUninterruptibly → CALL: localAddress → CALL: channel → LOG: LOG.INFO: Listening HTTP traffic on + httpAddress → IF_FALSE: httpsServer != null → EXIT",
    "log": "LOG.INFO: Listening HTTP traffic on + httpAddress"
  },
  "f7ed6e0a_3": {
    "exec_flow": "ENTRY → CALL: getConf → NEW: DatanodeHttpServer → CALL: <init> → CALL: httpServer.start → IF_FALSE: httpServer.getHttpAddress() != null → IF_TRUE: httpServer.getHttpsAddress() != null → CALL: getHttpsAddress → CALL: getPort → ENTRY → IF_FALSE: httpServer != null → IF_TRUE: httpsServer != null → CALL: org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String) → TRY → CALL: syncUninterruptibly → CALL: localAddress → CALL: channel → LOG: LOG.INFO: Listening HTTPS traffic on + httpsAddress → EXIT",
    "log": "LOG.INFO: Listening HTTPS traffic on + httpsAddress"
  },
  "83452b82_1": {
    "exec_flow": "<sequence>ENTRY → CALL:org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputCommitter → CALL:Preconditions.checkState → CALL:Preconditions.checkNotNull → CALL:org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputName → LOG:DEBUG: Work file for {} extension '{}' is {} → RETURN → EXIT</sequence>",
    "log": "<log_entry> <level>DEBUG</level> <template>Work file for {} extension '{}' is {}</template> </log_entry>"
  },
  "f387b30d_1": {
    "exec_flow": "ENTRY → IF_TRUE: wrappedStream instanceof Abortable → CALL: org.apache.hadoop.fs.Abortable:abort() → IF_FALSE: closed.getAndSet(true) → TRY → NEW: AbortableResultImpl → IF_TRUE: multiPartUpload != null → CALL: org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:access$1500(org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload) → RETURN → EXIT → CALL: org.apache.hadoop.fs.statistics.DurationTracker:close() → CALL: cleanupOnClose → LOG: LOG.DEBUG: Statistics: {statistics values} → CALL: clearActiveBlock → RETURN → EXIT",
    "log": "[DEBUG] Statistics: {statistics values}"
  },
  "f387b30d_2": {
    "exec_flow": "ENTRY → IF_FALSE: wrappedStream instanceof Abortable → THROW: new UnsupportedOperationException(FSExceptionMessages.ABORTABLE_UNSUPPORTED) → EXIT",
    "log": ""
  },
  "f387b30d_3": {
    "exec_flow": "ENTRY → IF_TRUE: wrappedStream instanceof Abortable → CALL: org.apache.hadoop.fs.Abortable:abort() → IF_TRUE: closed.getAndSet(true) → LOG: LOG.DEBUG: Ignoring abort() as stream is already closed → NEW: AbortableResultImpl → RETURN → EXIT",
    "log": "[DEBUG] Ignoring abort() as stream is already closed"
  },
  "43c611ae_1": {
    "exec_flow": "ENTRY→IF_FALSE:sourcePaths.size()!=1→CALL:org.apache.hadoop.fs.Path:getFileSystem→IF_FALSE:!(srcFs instanceof DistributedFileSystem)||(tgtFs instanceof DistributedFileSystem)→CALL:checkNoChange→IF_TRUE:!checkNoChange→TRY→CALL:getSnapshotName→CALL:getSnapshotDiffReport→IF_TRUE:!targetDiff.getDiffList().isEmpty()→CALL:DistCp.LOG.warn→CALL:setSourcePaths→RETURN→EXIT",
    "log": "<log>[WARN] The target has been modified since snapshot ...</log>"
  },
  "43c611ae_2": {
    "exec_flow": "ENTRY→CALL:getFileStatus→IF_FALSE:key.length() == 0→CALL:org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata→IF_TRUE:meta != null→IF_TRUE:meta.isFile()→LOG:[DEBUG] Path: [{}] is a file. COS key: [{}]→CALL:org.apache.hadoop.fs.cosn.CosNFileSystem:newFile→RETURN→EXIT",
    "log": "<log>[DEBUG] Path: [{}] is a file. COS key: [{}]</log>"
  },
  "43c611ae_3": {
    "exec_flow": "ENTRY→IF_FALSE:sourcePaths.size()!=1→CALL:org.apache.hadoop.fs.Path:getFileSystem→IF_FALSE:!(srcFs instanceof DistributedFileSystem)||(tgtFs instanceof DistributedFileSystem)→CALL:checkNoChange→IF_FALSE:!checkNoChange→TRY→CALL:getSnapshotName→CALL:getSnapshotDiffReport→EXCEPTION:IOException→CALL:DistCp.LOG.warn→RETURN→EXIT",
    "log": "<log>[WARN] Failed to compute snapshot diff on ...</log>"
  },
  "708e44a5_1": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:loadAWSProviderClasses</step> <step>IF_FALSE:awsClasses.isEmpty()</step> <step>FOREACH:awsClasses</step> <step>IF_TRUE:!Arrays.asList(...)</step> <step>CALL:V2Migration.v1ProviderReferenced</step> <step>CALL:org.apache.hadoop.fs.store.LogExactlyOnce:warn</step> <step>IF_TRUE:!logged.getAndSet(true)</step> <step>CALL:org.slf4j.Logger:warn(java.lang.String,java.lang.Object[])</step> <step>LOG:Directly referencing AWS SDK V1 credential provider {}. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2</step> <step>CALL:createAWSCredentialProvider</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Validating AWS credential provider classes</log> <log>[WARN] Directly referencing AWS SDK V1 credential provider {}. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2</log> <log>[DEBUG] Credential provider class is {}</log>"
  },
  "708e44a5_2": {
    "exec_flow": "<step>ENTRY</step> <step>CALL:loadAWSProviderClasses</step> <step>IF_FALSE:awsClasses.isEmpty()</step> <step>FOREACH:awsClasses</step> <step>IF_FALSE:!Arrays.asList(...)</step> <step>CALL:createAWSCredentialProvider</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "<log>[DEBUG] Validating AWS credential provider classes</log> <log>[DEBUG] Credential provider class is {}</log>"
  },
  "e2a38270_1": {
    "exec_flow": "ENTRY → IF_FALSE: namenodes == null OR namenodes.isEmpty() → CALL: addClientInfoToCallerContext → IF_FALSE: rpcMonitor != null → FOREACH: namenodes → TRY → CALL: getConnection → LOG: LOG.DEBUG: User {} NN {} is using connection {}, ugi.getUserName(), rpcAddress, connection → CALL: invoke → IF_FALSE: failover → IF_FALSE: this.rpcMonitor != null → RETURN → EXIT → TRY → CALL: invoke → CONDITIONAL → CALL: shouldRetry → CALL: invoke → RETURN → EXIT",
    "log": "[DEBUG] User {} NN {} is using connection {} [ERROR] No namenode available to invoke... [ERROR] Get connection for {} {} error: {} [ERROR] Cannot get available namenode for {} {} error: {} [ERROR] {} at {} is in Standby: {} [ERROR] {} at {} cannot be reached: {} [ERROR] {} at {} error: \"{}\" [ERROR] Unexpected exception while proxying API [ERROR] Unexpected exception {} proxying {} to {} [ERROR] Cannot get method {} with types {} from {} [ERROR] Cannot access method {} with types {} from {}"
  },
  "eec69fcc_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL: getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {}, property → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \"+\" \\\"\"+scheme+\"\\\" \") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {} [DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Looking in service filesystems for implementation class [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {}"
  },
  "eec69fcc_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL: createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → ENTRY → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → IF_TRUE: !FILE_SYSTEMS_LOADED → CALL: loadFileSystems → LOG: LOGGER.DEBUG: Looking for FS supporting {}, scheme → IF_TRUE: conf != null → LOG: LOGGER.DEBUG: looking for configuration option {}, property → CALL: getClass → IF_TRUE: clazz == null → LOG: LOGGER.DEBUG: Looking in service filesystems for implementation class → CALL: get → IF_TRUE: clazz == null → THROW: new UnsupportedFileSystemException(\"No FileSystem for scheme \"+\" \\\"\"+scheme+\"\\\" \") → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN: Failed to initialize filesystem {}: {}, uri, e.toString() → LOG: LOGGER.DEBUG: Failed to initialize filesystem, e → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → FOREACH: closeables → IF(c != null) → TRY → CALL: c.close() → CATCH: Throwable → IF(logger != null) → LOG: org.slf4j.Logger:debug: Exception in closing {} → FOREACH_EXIT → THROW: e → EXIT → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {} [DEBUG] Looking for FS supporting {} [DEBUG] looking for configuration option {} [DEBUG] Looking in service filesystems for implementation class [WARN] Failed to initialize filesystem {}: {}, uri, e.toString() [DEBUG] Failed to initialize filesystem [DEBUG] Exception in closing {}"
  },
  "eec69fcc_3": {
    "exec_flow": "ENTRY → CALL: fixName → [VIRTUAL_CALL] → (CALL: handleDeprecation → CALL: getProps → FOREACH: names → CALL: substituteVars → FOREACH_EXIT → RETURN) → EXIT",
    "log": "[WARN] \"local\" is a deprecated filesystem name. Use \"file:///\" instead. [WARN] \"{name}\" is a deprecated filesystem name. Use \"hdfs://{name}/\" instead."
  },
  "d8595e3b_1": {
    "exec_flow": "ENTRY → CALL:checkAcls → CHECK:checkRMStatus → FOR_EACH:nodeIds → IF_TRUE:node==null → LOG:[ERROR] Resource update get failed on all nodes due to change resource on an unrecognized node: {nodeId} → CALL:RPCUtil.getRemoteException → EXIT",
    "log": "[ERROR] Resource update get failed on all nodes due to change resource on an unrecognized node: {nodeId}"
  },
  "d8595e3b_2": {
    "exec_flow": "ENTRY → TRY → CALL:getCurrentUser → CATCH → LOG:[WARN] Couldn't get current user → CALL:RMAuditLogger.logFailure → THROW:IOException → EXIT",
    "log": "[WARN] Couldn't get current user"
  },
  "d8595e3b_3": {
    "exec_flow": "ENTRY → TRY → CALL:doSubjectLogin → IF_TRUE:proxyUser==null → CALL:getProperty → CALL:createProxyUser → CALL:tokenFileLocations.addAll → CALL:getTrimmedStringCollection → CALL:get → CALL:getTrimmedStringCollection → CALL:getTokenFileLocation → CALL:exists → CALL:isFile → CALL:readTokenStorageFile → CALL:addCredentials → LOG:Reading credentials from location {} → CALL:getProps → CALL:addAll → FOREACH:keys → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → CALL:LOG_DEPRECATION.info → CALL:org.slf4j.Logger:info(java.lang.String) → FOREACH_EXIT → TRY → LOG:Unexpected SecurityException in Configuration → CALL:org.apache.hadoop.security.UserGroupInformation.addCredentials → CALL:org.apache.hadoop.security.Credentials.addAll → CATCH:SecurityException → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Reading credentials from location {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Loaded {} tokens from {}</template> </log_entry> <log_entry> <level>INFO</level> <template>Token file {} does not exist</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Failure to load login credentials</template> </log_entry> <log_entry> <level>DEBUG</level> <template>UGI loginUser: {}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for {item}</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>WARN</level> <template>Null token ignored for {alias}</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Creating new Groups object</template> </log_entry> <log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "d8595e3b_4": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → EXCEPTION:readTokenStorageStream → CATCH:IOException ioe → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → THROW:new IOException(\"Exception reading \" + filename, ioe) → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "d8595e3b_5": {
    "exec_flow": "ENTRY → TRY → NEW:DataInputStream → NEW:BufferedInputStream → CALL:newInputStream → CALL:toPath → CALL:toPath → CALL:readTokenStorageStream → RETURN → CALL:IOUtils.cleanupWithLogger → IF(c != null) → TRY → CALL:c.close() → CATCH:Throwable → IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) → FOREACH_EXIT → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Cleaning up resources</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "d8595e3b_6": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_TRUE: this.userPipelineMap.containsKey(user) → LOG: LOG.INFO: Request to start an already existing user: {} was received, so ignoring., user → CALL: get → RETURN → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Request to start an already existing user: {} was received, so ignoring.</template> </log_entry>"
  },
  "d8595e3b_7": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → CALL: interceptorChain.init → CALL: chainWrapper.init → CALL: this.userPipelineMap.put → RETURN → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Initializing request processing pipeline for user: {}</template> </log_entry>"
  },
  "d8595e3b_8": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → EXCEPTION: createRequestInterceptorChain → CATCH: Exception e → LOG: LOG.ERROR: Init RMAdminRequestInterceptor error for user: + user, e → THROW: e → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Initializing request processing pipeline for user: {}</template> </log_entry> <log_entry> <level>ERROR</level> <template>Init RMAdminRequestInterceptor error for user: {}</template> </log_entry>"
  },
  "d8595e3b_9": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → CALL: interceptorChain.init → EXCEPTION: init → CATCH: Exception e → LOG: LOG.ERROR: Init RMAdminRequestInterceptor error for user: + user, e → THROW: e → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Initializing request processing pipeline for user: {}</template> </log_entry> <log_entry> <level>ERROR</level> <template>Init RMAdminRequestInterceptor error for user: {}</template> </log_entry>"
  },
  "d8595e3b_10": {
    "exec_flow": "ENTRY → SYNC: this.userPipelineMap → IF_FALSE: this.userPipelineMap.containsKey(user) → TRY → LOG: LOG.INFO: Initializing request processing pipeline for user: {}, user → CALL: org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:createRequestInterceptorChain() → CALL: interceptorChain.init → CALL: chainWrapper.init → EXCEPTION: init → CATCH: Exception e → LOG: LOG.ERROR: Init RMAdminRequestInterceptor error for user: + user, e → THROW: e → EXIT",
    "log": "<log_entry> <level>INFO</level> <template>Initializing request processing pipeline for user: {}</template> </log_entry> <log_entry> <level>ERROR</level> <template>Init RMAdminRequestInterceptor error for user: {}</template> </log_entry>"
  },
  "3fc65820_1": {
    "exec_flow": "<!-- Optimized execution flow structure --> ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: loadDefaults && fullReload → FOREACH: defaultResources → CALL: loadResource → FOREACH_EXIT → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "<!-- Merged log sequence --> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message [WARN] Unexpected SecurityException in Configuration"
  },
  "3fc65820_2": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_FALSE: loadDefaults && fullReload → FOR_INIT → FOR_COND: i < resources.size() → CALL: loadResource → FOR_EXIT → CALL: addTags → IF_TRUE: overlay != null → CALL: putAll → IF_TRUE: backup != null → FOREACH: overlay.entrySet() → FOREACH_EXIT → EXIT",
    "log": "[INFO] message"
  },
  "bf339fe3_1": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> RETURN -> EXIT",
    "log": "[INFO] Service {} failed in state {}"
  },
  "bf339fe3_2": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> CALL: noteFailure -> CALL: org.slf4j.Logger:info(java.lang.String,java.lang.Object,java.lang.Object) -> RETURN -> EXIT",
    "log": "[DEBUG] Service: {} entered state {} [INFO] Service {} failed in state {} [WARN] Exception while notifying listeners of {}"
  },
  "bf339fe3_3": {
    "exec_flow": "ENTRY -> IF_TRUE: oldState != newState -> CALL: org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object) -> CALL: recordLifecycleEvent -> RETURN -> EXIT",
    "log": "[DEBUG] Service: {} entered state {} [DEBUG] Ignoring re-entrant call to stop() [WARN] Exception while notifying listeners of {}"
  },
  "045a4243_1": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_FALSE: scheme != null && authority == null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.DEBUG(\"Bypassing cache to create filesystem {}\", uri) → TRY → CALL: org.apache.hadoop.util.DurationInfo:<init> → CALL: addKVAnnotation → CALL: org.apache.hadoop.fs.FileSystem:getFileSystemClass → CALL: org.apache.hadoop.util.ReflectionUtils:newInstance → TRY → CALL: initialize → EXCEPTION: initialize → CATCH: IOException | RuntimeException e → LOG: LOGGER.WARN(\"Failed to initialize filesystem {}: {}\", uri, e.toString()) → LOG: LOGGER.DEBUG(\"Failed to initialize filesystem\", e) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger → THROW: e → RETURN → EXIT → CALL: org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI, org.apache.hadoop.conf.Configuration) → ENTRY → SYNC: this → CALL: get → IF_FALSE: fs != null → TRY → CALL: creatorPermits.acquireUninterruptibly → CALL: org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger, boolean, java.lang.String, java.lang.Object[]) → SYNC: this → CALL: get → IF_TRUE: fs != null → LOG: LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri) → RETURN → EXIT → SYNC: this → CALL: get → IF_FALSE: fs != null → CALL: createFileSystem → CALL: org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit) → SYNC: this → IF_TRUE: map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress() → CALL: ShutdownHookManager.get().addShutdownHook → CALL: org.apache.hadoop.conf.Configuration:getBoolean → LOG: LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs) → CALL: org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger, fsToClose) → RETURN → EXIT",
    "log": "<log> <level>DEBUG</level> <template>Bypassing cache to create filesystem {}</template> </log> <log> <level>WARN</level> <template>Failed to initialize filesystem {}: {}</template> </log> <log> <level>DEBUG</level> <template>Failed to initialize filesystem</template> </log> <log> <level>DEBUG</level> <template>Filesystem {} created while awaiting semaphore</template> </log> <log> <level>DEBUG</level> <template>Duplicate FS created for {}; discarding {}</template> </log>"
  },
  "af58b0e7_1": {
    "exec_flow": "ENTRY → TRY → CALL:org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String) → CALL:org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long) → CALL:org.apache.hadoop.yarn.server.sharedcache.SharedCacheUtil:getCacheDepth(org.apache.hadoop.conf.Configuration) → ENTRY → CALL:org.apache.hadoop.conf.Configuration:getInt → IF_TRUE:cacheDepth<=0 → CALL:org.slf4j.Logger:warn → LOG: WARN: Specified cache depth was less than or equal to zero. Using default value instead. Default: {}, Specified: {} → RETURN → EXIT → CALL:org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration) → NEW: CleanerTask → ENTRY → IF_FALSE: !this.cleanerTaskLock.tryLock() → TRY → IF_FALSE: !fs.exists(root) → CALL: process → ENTRY → CALL: reportCleaningStart → TRY → LOG: INFO: Processing + numResources + resources in the shared cache → CALL: FileSystem.globStatus → FOREACH: resources → IF_TRUE: Thread.currentThread().isInterrupted() → LOG: WARN: The cleaner task was interrupted. Aborting. → BREAK → EXIT → CATCH: Throwable e → LOG: ERROR: Unexpected exception while initializing the cleaner task. This task will do nothing,, e → CALL: this.cleanerTaskLock.unlock → EXIT → RETURN → EXIT",
    "log": "[ERROR] Unable to obtain the filesystem for the cleaner service [INFO] Processing + numResources + resources in the shared cache [WARN] The cleaner task was interrupted. Aborting. [ERROR] Unexpected exception while initializing the cleaner task. This task will do nothing,, e [WARN] Specified cache depth was less than or equal to zero. Using default value instead. Default: {}, Specified: {}"
  },
  "af58b0e7_2": {
    "exec_flow": "ENTRY → IF_FALSE: scheme == null && authority == null → IF_TRUE: scheme != null && authority == null → CALL:getDefaultUri(org.apache.hadoop.conf.Configuration) → IF_FALSE: scheme.equals(defaultUri.getScheme()) && defaultUri.getAuthority() != null → IF_TRUE: conf.getBoolean(disableCacheName, false) → LOG: LOGGER.debug(\"Bypassing cache to create filesystem {}\", uri) → CALL:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration) → RETURN → EXIT",
    "log": "[DEBUG] Bypassing cache to create filesystem {} [DEBUG] Filesystem {} created while awaiting semaphore [DEBUG] Duplicate FS created for {}; discarding {}"
  },
  "015448c6_1": {
    "exec_flow": "ENTRY → LOG:[DEBUG] Creating password for {} for user {} to be run on NM {} → CALL:OpportunisticContainerAllocator.normalizeCapability → CALL:Resources.normalize → CALL:org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:normalize → RETURN → CALL:createContainer → RETURN → CALL:this.readLock.lock → TRY → CALL:createPassword → CALL:org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getBytes → CALL:this.currentMasterKey.getSecretKey → RETURN → finally → CALL:this.readLock.unlock → CALL:org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE:logSlowLookups || LOG.isTraceEnabled() → CALL:org.slf4j.Logger:isTraceEnabled() → IF_TRUE:staticHost != null → CALL:getByAddress → CALL:getAddress → CALL:getAddress → IF_TRUE:elapsedMs >= slowLookupThresholdMs → CALL:org.slf4j.Logger:warn(java.lang.String) → NEW:InetSocketAddress → RETURN → EXIT",
    "log": "[DEBUG] Creating password for {} for user {} to be run on NM {} [WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "015448c6_2": {
    "exec_flow": "ENTRY → CALL:OpportunisticContainerAllocator.normalizeCapability → CALL:Resources.normalize → CALL:org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:normalize → RETURN → CALL:createContainer → RETURN → CALL:this.readLock.lock → TRY → CALL:createPassword → CALL:org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getBytes → CALL:this.currentMasterKey.getSecretKey → RETURN → finally → CALL:this.readLock.unlock → CALL:org.apache.hadoop.security.SecurityUtil:getByName → IF_TRUE:logSlowLookups || LOG.isTraceEnabled() → CALL:org.slf4j.Logger:isTraceEnabled() → IF_TRUE:staticHost != null → CALL:getByAddress → CALL:getAddress → CALL:getAddress → IF_TRUE:elapsedMs >= slowLookupThresholdMs → CALL:org.slf4j.Logger:warn(java.lang.String) → NEW:InetSocketAddress → RETURN → EXIT",
    "log": "[DEBUG] Creating password for {} for user {} to be run on NM {} [WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms."
  },
  "015448c6_3": {
    "exec_flow": "<step>Parent.ENTRY</step> <step>[VIRTUAL_CALL]</step> <step>Child.initResources()</step>",
    "log": "<log>[WARN] Got unknown resource type: {entry.getKey()}; skipping</log>"
  },
  "2363ee6f_1": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→EXIT",
    "log": "[DEBUG] Displaying error: message with object1, object2"
  },
  "2363ee6f_2": {
    "exec_flow": "ENTRY→CALL:displayWarning→CALL:org.slf4j.Logger:debug(java.lang.String)→EXIT",
    "log": "[DEBUG] Displaying error: message"
  },
  "7ecd85e3_1": {
    "exec_flow": "ENTRY → CALL: ensureInitialized → IF_TRUE: subject == null || subject.getPrincipals(User.class).isEmpty() → VIRTUAL_CALL → CALL: getLoginUser → DO_WHILE → IF_TRUE: loginUserRef.compareAndSet(null, newLoginUser) → CALL: createLoginUser → TRY → CALL: doSubjectLogin → IF_TRUE: proxyUser == null → CALL: getProperty → CALL: createProxyUser → CALL: tokenFileLocations.addAll → CALL: getTrimmedStringCollection → CALL: get → CALL: getTrimmedStringCollection → CALL: getTokenFileLocation → CALL: exists → CALL: isFile → CALL: readTokenStorageFile → CALL: addCredentials → CALL: debug → CALL: loginUser.spawnAutoRenewalThreadForUserCreds → DO_COND: loginUser == null → DO_EXIT → EXIT → CALL: getAuthenticationMethod → IF_TRUE: overrideNameRules || !HadoopKerberosName.hasRulesBeenSet() → TRY → CALL: HadoopKerberosName.setConfiguration → TRY → CALL: getLong → CALL: getBoolean → IF_TRUE: !(groups instanceof TestingGroups) → CALL: getUserToGroupsMappingService → IF_FALSE: metrics.getGroupsQuantiles == null → EXIT → VIRTUAL_CALL → ENTRY → LOG: [DEBUG] Reading credentials from location {file_path} → LOG: [DEBUG] Loaded {} tokens from {file_path} → LOG: [INFO] Token file {file_path} does not exist → LOG: [DEBUG] Loaded {} base64 tokens → LOG: [DEBUG] Failure to load login credentials → LOG: [DEBUG] UGI loginUser: {loginUser} → LOG: [WARN] Unexpected SecurityException in Configuration → RETURN → EXIT →",
    "log": "<log_entry>[DEBUG] Reading credentials from location {file_path}</log_entry> <log_entry>[DEBUG] Loaded {} tokens from {file_path}</log_entry> <log_entry>[INFO] Token file {file_path} does not exist</log_entry> <log_entry>[DEBUG] Loaded {} base64 tokens</log_entry> <log_entry>[DEBUG] Failure to load login credentials</log_entry> <log_entry>[DEBUG] UGI loginUser: {loginUser}</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "7ecd85e3_2": {
    "exec_flow": "ENTRY → CALL: resolveComponents → IF_FALSE: nComponents < 3 || !isReservedName(pathComponents) → IF_FALSE: Arrays.equals(DOT_INODES, pathComponents[2]) → IF_TRUE: Arrays.equals(RAW, pathComponents[2]) → IF_FALSE: nComponents == 3 → IF_FALSE: nComponents == 4 && Arrays.equals(DOT_RESERVED, pathComponents[3]) → CALL: constructRemainingPath → ENTRY → IF_TRUE: remainder > 0 → CALL: copyOf → CALL: System.arraycopy → IF_TRUE: NameNode.LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → CALL: NameNode.LOG.debug → CALL: org.slf4j.Logger:debug → RETURN → EXIT → RETURN → CALL: byteArray2PathString → RETURN → EXIT",
    "log": "<log_entry>[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)]</log_entry>"
  },
  "3aaf443a_1": {
    "exec_flow": "<seq> ENTRY→LOG:Handling deprecation for all properties in config...→CALL:getProps→CALL:addAll→FOREACH:keys→LOG:Handling deprecation for + (String)item→CALL:handleDeprecation→FOREACH_EXIT→CALL:LOG_DEPRECATION.info→FOREACH:names→CALL:getProps→FOREACH_EXIT→RETURN→EXIT </seq>",
    "log": "<log> [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [INFO] message </log>"
  },
  "dc606d50_1": {
    "exec_flow": "ENTRY → CALL:createResourceCommitRequest → IF_FALSE:null == request → IF_FALSE:scheduleAsynchronously → CALL:tryCommit → IF_FALSE:request.anythingAllocatedOrReserved() → CALL:getApplicationAttemptId → CALL:getSchedulerApplicationAttempt → CALL:getContainersToRelease → LOG:[DEBUG] Try to commit allocation proposal={}, request → IF_TRUE:attemptId != null → IF_TRUE:app != null && attemptId.equals(app.getApplicationAttemptId()) → IF_TRUE:app.accept(cluster, request, updatePending) && app.apply(cluster, request, updatePending) → CALL:CapacitySchedulerMetrics.getMetrics().addCommitSuccess → LOG:[DEBUG] Allocation proposal accepted={}, proposal={}, isSuccess, request → IF_FALSE:updateUnconfirmedAllocatedResource → RETURN → EXIT",
    "log": "[WARN] Resource is missing: [DEBUG] Try to commit allocation proposal={} [DEBUG] Allocation proposal accepted={}"
  },
  "cfb5e0ca_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → LOG:WARN: Environment variable {} is deprecated and overriding property {}, please set the property in {} instead., varName, propName, confFile → PUT: updatingResource → CALL:set → EXIT",
    "log": "[WARN] Environment variable {} is deprecated and overriding property {}, please set the property in {} instead., varName, propName, confFile LOG.debug(\"Handling deprecation for all properties in config...\") FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item)"
  },
  "8c7e1d34_1": {
    "exec_flow": "ENTRY → CALL:init:super → CALL:DefaultMetricsSystem:instance → CALL:QueueMetrics:forQueue → CALL:QueueMetrics:getMetrics → CALL:JvmMetrics:getMetrics → CALL:getConf → CALL:Files:readLines → CATCH:IOException → CATCH:ScriptException → EXIT",
    "log": "<log>[DEBUG] Configuring metrics system</log> <log>[INFO] Queue metrics initialized</log> <log>[INFO] JVM metrics initialized</log>"
  },
  "8c7e1d34_2": {
    "exec_flow": "ENTRY → [VIRTUAL_CALL] → handleDeprecation → [CHECK] → FOREACH: names → [CALL]: getProps → FOREACH_EXIT → RETURN → EXIT",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log>"
  },
  "8c7e1d34_3": {
    "exec_flow": "ENTRY→CALL:org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics:forQueue→CALL:org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register→CALL: namedCallbacks.put →CALL: org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder →CALL: org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource →CALL:checkNotNull→CALL:org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>→CALL:put→CALL:start→LOG:LOG.DEBUG:Registered source + name→EXIT",
    "log": "<log>[DEBUG] Registered source + name</log>"
  },
  "02b29b34_1": {
    "exec_flow": "ENTRY→CALL:RedundantEditLogInputStream:close→EXCEPTION:IOException→EXIT",
    "log": "[ERROR] IOException occurred in close"
  },
  "02b29b34_2": {
    "exec_flow": "ENTRY→IF_FALSE: syncTxid > 0 && txid > syncTxid→TRY→CALL: selectInputStreams→FOREACH: streams→TRY→WHILE: (op = readOp(elis)) != null→IF_TRUE: breakOuter→LOG:info→EXIT",
    "log": "[INFO] NN is transitioning from active to standby and FSEditLog is closed -- could not read edits"
  },
  "94c1e247_1": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: Handling deprecation for all properties in config...</step> <step>CALL: handleDeprecation</step> <step>LOG: Handling deprecation for (String)item</step> <step>FOREACH: names</step> <step>CALL: getProps</step> <step>CALL: substituteVars</step> <step>FOREACH_EXIT</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "94c1e247_2": {
    "exec_flow": "<step>ENTRY</step> <step>LOG: LOG.DEBUG: Getting the file status for {}, f.toString()</step> <step>IF_FALSE: key.length() == 0</step> <step>CALL: org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieveMetadata</step> <step>IF_TRUE: meta != null</step> <step>IF_TRUE: meta.isFile()</step> <step>LOG: LOG.DEBUG: Path: [{}] is a file. COS key: [{}]</step> <step>CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile</step> <step>RETURN</step> <step>EXIT</step>",
    "log": "[DEBUG] Getting the file status for {} [DEBUG] Path: [{}] is a file. COS key: [{}]"
  },
  "94c1e247_3": {
    "exec_flow": "<step>ENTRY</step> <step>CALL: statIncrement</step> <step>LOG: AzureBlobFileSystem.mkdirs path: {path} permissions: {permissions}</step> <step>CALL: trailingPeriodCheck</step> <step>IF_FALSE: parentFolder == null</step> <step>CALL: makeQualified</step> <step>TRY</step> <step>CALL: getUMask</step> <step>CALL: abfsStore.createDirectory</step> <step>TRY</step> <step>CALL: getIsNamespaceEnabled</step> <step>LOG: createDirectory filesystem: {filesystem} path: {path} permission: {permission} umask: {umask} isNamespaceEnabled: {isNamespaceEnabled}</step> <step>CALL: createPath</step> <step>CALL: registerResult</step> <step>CALL: close</step> <step>EXIT</step> <step>EXCEPTION: createDirectory</step> <step>CATCH: AzureBlobFileSystemException ex</step> <step>CALL: checkException</step> <step>RETURN</step> <step>EXIT</step> <step>LOG: isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call</step> <step>LOG: Get root ACL status</step>",
    "log": "[DEBUG] AzureBlobFileSystem.mkdirs path: {path} permissions: {permissions} [DEBUG] createDirectory filesystem: {filesystem} path: {path} permission: {permission} umask: {umask} isNamespaceEnabled: {isNamespaceEnabled} [DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call [DEBUG] Get root ACL status"
  },
  "847783b0_1": {
    "exec_flow": "ENTRY → CALL:getPermissionChecker → CALL:FSPermissionChecker.setOperationType → CALL:checkOperation → TRY → CALL:readLock → CALL:FSDirEncryptionZoneOp.getEZForPath → CALL:readUnlock → LOG:logAuditEvent(operation success) → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:getRemoteUser → CALL:org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod → FOREACH:locations → TRY → CALL:org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getNamenodesForNameservice → IF_FALSE:namenodes == null || namenodes.isEmpty() → CALL:addClientInfoToCallerContext → IF_FALSE:rpcMonitor != null → FOREACH:namenodes → TRY → CALL:getConnection → LOG:LOG.DEBUG:User {} NN {} is using connection {}, ugi.getUserName(), rpcAddress, connection → CALL:invoke → IF_FALSE:failover → IF_FALSE:this.rpcMonitor != null → RETURN → EXIT",
    "log": "<log>[LOG] logAuditEvent(operation success)</log> <log>[DEBUG] User {} NN {} is using connection {}</log> <log>[ERROR] Unexpected exception {} proxying {} to {}</log> <log>[ERROR] Cannot get method {} with types {} from {}</log> <log>[ERROR] Cannot access method {} with types {} from {}</log>"
  },
  "847783b0_2": {
    "exec_flow": "ENTRY → CALL:getPermissionChecker → CALL:FSPermissionChecker.setOperationType → CALL:checkOperation → TRY → CALL:readLock → EXCEPTION:readLock → CATCH:AccessControlException → LOG:logAuditEvent(operation failed) → THROW:AccessControlException → EXIT",
    "log": "<log>[LOG] logAuditEvent(operation failed)</log>"
  },
  "f97132f5_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL: loadResources → IF_TRUE: overlay != null → FOREACH: overlay.entrySet() → IF_TRUE: source != null → PUT: updatingResource LOG: Handling deprecation for all properties in config... → FOREACH: LOG: Handling deprecation for (String)item → GetHomeDirectory → LOG: Params configuration for operation: GETHOMEDIRECTORY → LOG: Connection established and response validated for home directory retrieval → LOG: JSON parsed for home directory path",
    "log": "LOG.debug(\"Handling deprecation for all properties in config...\"); FOREACH: LOG.debug(\"Handling deprecation for \" + (String) item); LOG: Params configuration for operation: GETHOMEDIRECTORY; LOG: Connection established and response validated for home directory retrieval; LOG: JSON parsed for home directory path;"
  },
  "f97132f5_2": {
    "exec_flow": "ENTRY → IF_TRUE: name.equals(\"local\") → LOG.warn(\"\\\"local\\\" is a deprecated filesystem name. Use \\\"file:///\\\" instead.\") → ELSE_IF_TRUE: name.indexOf('/') == -1 → LOG.warn(\"\\\"\" + name + \"\\\" is a deprecated filesystem name. Use \\\"hdfs://\"+ name +\"/\\\" instead.\") → EXIT",
    "log": "LOG.warn(\"\\\"local\\\" is a deprecated filesystem name. Use \\\"file:///\\\" instead.\"); LOG.warn(\"\\\"\" + name + \"\\\" is a deprecated filesystem name. Use \\\"hdfs://\" + name + \"/\\\" instead.\");"
  },
  "c88f26c0_1": {
    "exec_flow": "<parent_entry>org.apache.hadoop.fs.FileSystem:get</parent_entry> <step>ENTRY</step> <step>CALL:org.apache.hadoop.conf.Configuration:getBoolean</step> <step>DESCRIPTION:Evaluate boolean property from configuration</step> <step>CALL:org.apache.hadoop.conf.Configuration:getTrimmed</step> <step>DESCRIPTION:Retrieve trimmed property value</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:org.apache.hadoop.util.DurationInfo:<init></step> <step>CALL:addKVAnnotation</step> <step>CALL:org.apache.hadoop.fs.FileSystem:getFileSystemClass</step> <step>IF_TRUE:!FILE_SYSTEMS_LOADED</step> <step>CALL:loadFileSystems</step> <step>LOG:LOGGER.DEBUG:Looking for FS supporting {}, scheme</step> <step>IF_TRUE:conf != null</step> <step>LOG:LOGGER.DEBUG:looking for configuration option {}, property</step> <step>CALL:getClass</step> <step>IF_TRUE:clazz == null</step> <step>LOG:LOGGER.DEBUG:Looking in service filesystems for implementation class</step> <step>CALL:get</step> <step>IF_TRUE:clazz == null</step> <step>THROW:new UnsupportedFileSystemException(\"No FileSystem for scheme \\\" + scheme + \\\"\")</step> <step>EXIT</step> <step>CALL:org.apache.hadoop.util.ReflectionUtils:newInstance</step> <step>TRY</step> <step>CALL:initialize</step> <step>EXCEPTION:initialize</step> <step>CATCH:IOException | RuntimeException e</step> <step>LOG:LOGGER.WARN:Failed to initialize filesystem {}: {}, uri, e.toString()</step> <step>LOG:LOGGER.DEBUG:Failed to initialize filesystem, e</step> <step>CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger</step> <step>THROW:e</step> <step>EXIT</step> <step>CALL:org.apache.hadoop.util.DurationInfo:close</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>[DEBUG] Looking for FS supporting {}, scheme</log> <log>[DEBUG] looking for configuration option {}, property</log> <log>[DEBUG] Looking in service filesystems for implementation class</log> <log>[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()</log> <log>[DEBUG] Failed to initialize filesystem</log>"
  },
  "c88f26c0_2": {
    "exec_flow": "<parent_entry>org.apache.hadoop.fs.FileSystem:getDefaultUri</parent_entry> <virtual_call>org.apache.hadoop.fs.FileSystem:get</virtual_call> <step>ENTRY</step> <step>CALL:handleDeprecation</step> <step>LOG:Handling deprecation for all properties in config...</step> <step>CALL:getProps</step> <step>CALL:addAll</step> <step>FOREACH:keys</step> <step>LOG:Handling deprecation for (String)item</step> <step>CALL:handleDeprecation</step> <step>FOREACH_EXIT</step> <step>CALL:getProps</step> <step>FOREACH:names</step> <step>CALL:substituteVars</step> <step>CALL:getRaw</step> <step>RETURN</step> <step>EXIT</step> <step>LOG:LOGGER.DEBUG:Bypassing cache to create filesystem {}, uri</step>",
    "log": "<log>[DEBUG] Handling deprecation for all properties in config...</log> <log>[DEBUG] Handling deprecation for (String)item</log> <log>LOG.warn(\"Unexpected SecurityException in Configuration\", se)</log> <log>[DEBUG] Bypassing cache to create filesystem {uri}</log>"
  },
  "c88f26c0_3": {
    "exec_flow": "ENTRY→SYNC:this→CALL:get→IF_FALSE:fs != null→TRY→CALL:creatorPermits.acquireUninterruptibly→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→SYNC:this→CALL:get→IF_TRUE:fs != null→LOG:LOGGER.DEBUG(\"Filesystem {} created while awaiting semaphore\", uri)→RETURN→EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore"
  },
  "c88f26c0_4": {
    "exec_flow": "ENTRY→SYNC:this→CALL:get→IF_FALSE:fs != null→TRY→CALL:creatorPermits.acquireUninterruptibly→CALL:org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])→SYNC:this→CALL:get→IF_FALSE:fs != null→CALL:createFileSystem→CALL:org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)→SYNC:this→IF_TRUE:map.isEmpty() AND !ShutdownHookManager.get().isShutdownInProgress()→CALL:ShutdownHookManager.get().addShutdownHook→CALL:org.apache.hadoop.conf.Configuration:getBoolean→LOG:LOGGER.DEBUG(\"Duplicate FS created for {}; discarding {}\", uri, fs)→CALL:org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,fsToClose)→RETURN→EXIT",
    "log": "[DEBUG] Filesystem {} created while awaiting semaphore [DEBUG] Duplicate FS created for {}; discarding {}"
  },
  "47bfa6d0_1": {
    "exec_flow": "ENTRY → IF: targetStream == null → CALL: org.apache.hadoop.fs.Path:getFileSystem → CALL: org.apache.log4j.Logger:info → IF: fileSystem instance of DistributedFileSystem → CALL: org.apache.hadoop.hdfs.DistributedFileSystem:createFile → CALL: org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:build → [VIRTUAL_CALL] → IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → CALL: org.apache.log4j.Logger:warn → CALL: org.apache.hadoop.fs.FileSystem:create → CALL: org.apache.hadoop.fs.permission.FsPermission:getUMask → CALL: org.apache.log4j.Logger:info → IF: FRAMEWORK_PERMISSION not equals → CALL: org.apache.hadoop.fs.FileSystem:setPermission → CALL: org.apache.hadoop.fs.FileSystem:getFileStatus → LOG:LOG.DEBUG: Call the getFileStatus to obtain the metadata for the file: [{f}]. CALL: org.apache.log4j.Logger:warn → CALL: org.apache.hadoop.fs.FileSystem:getFileStatus → LOG:LOG.DEBUG: Path: [{f}] is a file. COS key: [{key}]. CALL: org.apache.log4j.Logger:warn → CALL: org.apache.hadoop.fs.cosn.CosNFileSystem:newFile → CALL: org.apache.log4j.Logger:debug → EXIT",
    "log": "[INFO] Target ... [INFO] Set replication to ... for path: ... [INFO] Disabling Erasure Coding for path: ... [WARN] Cannot set replication to ... for path: ... on a non-distributed filesystem ... [INFO] Modifying permissions to ... [DEBUG] Call the getFileStatus to obtain the metadata for the file: [{f}]. [DEBUG] Path: [{f}] is a file. COS key: [{key}]. [WARN] Path ... is not accessible for all users. Current permissions are: ... [WARN] Please set EXECUTE permissions on this directory [DEBUG] Stack trace [DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item [WARN] Unexpected SecurityException in Configuration"
  },
  "79fd7caa_1": {
    "exec_flow": "ENTRY → IF_TRUE: props != null → CALL:loadResources → IF_FALSE: overlay != null → EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item WARN: Unexpected SecurityException in Configuration"
  },
  "79fd7caa_2": {
    "exec_flow": "ENTRY→FOR_LOOP:newNames→GET:deprecatedKey→IF_TRUE:deprecatedKey != null && !getProps().containsKey(newName)→IF_TRUE:deprecatedValue != null→SET_PROPERTY:newName→EXIT",
    "log": "[DEBUG] Handling deprecation for all properties in config... [DEBUG] Handling deprecation for (String)item"
  },
  "ff0467f7_1": {
    "exec_flow": "ENTRY → IF_TRUE: rpcMonitor != null → CALL: rpcMonitor.startOp → IF_TRUE: LOG.isDebugEnabled() → CALL: org.slf4j.Logger:isDebugEnabled() → LOG: LOG.DEBUG: Proxying operation: {}, methodName → CALL: opCategory.set → IF_TRUE: op == OperationCategory.UNCHECKED || op == OperationCategory.READ → RETURN → EXIT → CALL: MembershipNamenodeResolver-getNamespaces → IF_FALSE: locations.isEmpty() → FOR_INIT → FOR_COND: i < futures.size() → FOR_EXIT → CALL: merge → RETURN → EXIT",
    "log": "[DEBUG] Proxying operation: {}"
  },
  "31c82ab5_1": {
    "exec_flow": "ENTRY → LOG:Handling deprecation for all properties in config... → CALL:handleDeprecation → FOREACH:names → CALL:getProps → CALL:substituteVars → FOREACH_EXIT → RETURN → ENTRY→FOREACH:closeables→IF(c != null)→TRY→CALL:c.close()→CATCH:Throwable→IF(logger != null) → CALL:org.slf4j.Logger:debug(java.lang.String,java.lang.Object,java.lang.Object)→FOREACH_EXIT→EXIT → CALL:readFile → IF(fsIn != null) → CALL:fsIn.close() → RETURN → EXIT",
    "log": "<log_entry> <level>DEBUG</level> <template>Handling deprecation for all properties in config...</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Handling deprecation for (String)item</template> </log_entry> <log_entry> <level>WARN</level> <template>Unexpected SecurityException in Configuration</template> </log_entry> <log_entry> <level>DEBUG</level> <template>Exception in closing {}</template> </log_entry>"
  },
  "b9a15014_1": {
    "exec_flow": "ENTRY → LOG:LOG.DEBUG:Checking state store connection → IF_TRUE: !stateStore.isDriverReady() → LOG:LOG.INFO:Attempting to open state store driver. → SYNC:this.driver → IF_TRUE:!isDriverReady() → CALL:org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init → LOG:LOG.INFO:Connection to the State Store driver {} is open and ready → CALL:org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches → EXIT",
    "log": "[DEBUG] Checking state store connection [INFO] Attempting to open state store driver. [INFO] Connection to the State Store driver {} is open and ready"
  },
  "b9a15014_2": {
    "exec_flow": "ENTRY → IF_TRUE: SAFE_MODE_ACTIVE → LOG:LOG.INFO:Entering safe mode → EXIT",
    "log": "[INFO] Entering safe mode"
  },
  "b9a15014_3": {
    "exec_flow": "ENTRY → IF_FALSE: delta < startupInterval → IF_FALSE: isCacheStale → IF_TRUE: safeMode && !isSafeModeSetManually → CALL: leave() → LOG: LOG.INFO: Leaving safe mode after {} milliseconds, timeInSafemode → IF_TRUE: routerMetrics == null → LOG: LOG.ERROR: The Router metrics are not enabled → EXIT",
    "log": "[INFO] Leaving safe mode after {} milliseconds [ERROR] The Router metrics are not enabled"
  },
  "cb89f3ec_1": {
    "exec_flow": "<!-- Optimized execution flow structure -->",
    "log": "<!-- Merged log sequence --> <log_entry>[DEBUG] Creating new Groups object</log_entry> <log_entry>[DEBUG] Handling deprecation for all properties in config...</log_entry> <log_entry>[DEBUG] Handling deprecation for (String)item</log_entry> <log_entry>[WARN] Unexpected SecurityException in Configuration</log_entry>"
  },
  "cb89f3ec_2": {
    "exec_flow": "ENTRY→IF_FALSE: xface == ClientProtocol.class→IF_FALSE: xface == JournalProtocol.class→IF_FALSE: xface == NamenodeProtocol.class→IF_FALSE: xface == GetUserMappingsProtocol.class→IF_FALSE: xface == RefreshUserMappingsProtocol.class→IF_FALSE: xface == RefreshAuthorizationPolicyProtocol.class→IF_FALSE: xface == RefreshCallQueueProtocol.class→IF_FALSE: xface == InMemoryAliasMapProtocol.class→IF_FALSE: xface == BalancerProtocols.class→CALL: LOG.error→THROW: new IllegalStateException→EXIT",
    "log": "<log_entry>[ERROR] Unsupported protocol found when creating the proxy connection to NameNode</log_entry>"
  },
  "b36e8ef2_1": {
    "exec_flow": "ENTRY→TRY→CALL:serviceManager.handle→ENTRY→IF_FALSE:blockNewEvents→IF_TRUE:qSize!=0 && qSize % 1000==0 && lastEventQueueSizeLogged!=qSize→LOG:INFO:Size of event-queue is + qSize→IF_TRUE:qSize!=0 && qSize % detailsInterval==0 && lastEventDetailsQueueSizeLogged!=qSize→CALL:printEventQueueDetails→WHILE:iterator.hasNext()→CALL:iterator.next→CALL:iterator.next.getType→CALL:counterMap.containsKey→CALL:counterMap.put→CALL:counterMap.get→WHILE_EXIT→CALL:counterMap.put→FOREACH:counterMap.entrySet()→CALL:Map.Entry.getValue→CALL:Map.Entry.getKey→CALL:org.slf4j.Logger.info→FOREACH_EXIT→CALL:writeLock.lock→TRY→TRY→CALL:stateMachine.doTransition→IF_TRUE:oldState!=getState()→LOG:LOG.INFO:[SERVICE] Transitioned from {} to {} on {} event., oldState, getState(), event.getType()→CALL:writeLock.unlock→EXIT→EXIT→IF_TRUE:remCapacity<1000→LOG:WARN:Very low remaining capacity in event-queue:+remCapacity→TRY→CALL:eventQueue.put→EXIT",
    "log": "[INFO] [SERVICE] Transitioned from {} to {} on {} event. [INFO] Size of event-queue is + qSize [INFO] Event type: {entry.getKey()}, Event record counter: {num} [WARN] Very low remaining capacity in event-queue: + remCapacity"
  },
  "b36e8ef2_2": {
    "exec_flow": "ENTRY→IF_TRUE:node != null→TRY→CALL:((EventHandler<RMNodeEvent>) node).handle→EXCEPTION:handle→CATCH:Throwable t→LOG:LOG.ERROR:Error in handling event type + event.getType() + for node + nodeId, t→EXIT",
    "log": "[ERROR] Error in handling event type + event.getType() + for node + nodeId, t"
  },
  "b36e8ef2_3": {
    "exec_flow": "ENTRY→IF_TRUE:!set.contains(c.getContainerState())→LOG:warn:[INFO] Initialized container resources→RETURN→EXIT",
    "log": "[INFO] Initialized container resources"
  },
  "b36e8ef2_4": {
    "exec_flow": "ENTRY→CALL:handleStoreEvent→CALL:org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler:handle→EXIT",
    "log": "[INFO] Handling NodeAttributesStoreEvent"
  },
  "b36e8ef2_5": {
    "exec_flow": "ENTRY→CALL:nodeToAttr.forEach→TRY→SWITCH:event.getOperation()→CASE:[]→CALL:org.slf4j.Logger:warn→EXIT",
    "log": "[WARN] Unsupported operation"
  },
  "b36e8ef2_6": {
    "exec_flow": "ENTRY→CALL:nodeToAttr.forEach→TRY→SWITCH:event.getOperation()→CASE:[REPLACE|ADD|REMOVE|]→CATCH:IOException→CALL:org.slf4j.Logger:error→THROW:YarnRuntimeException→EXIT",
    "log": "[ERROR] Failed to store attribute modification to storage"
  },
  "9e13243f_1": {
    "exec_flow": "<sequence> <step>INodesInPath.resolve(rootDir, components)</step> <step>checkTraverse(null, iip, dirOp)</step> <step>ENTRY → IF_FALSE → EXIT</step> <step>unprotectedRenameTo(fsd, srcIIP, dstIIP, mtime)</step> <step>fsd.getEditLog().logRename(srcIIP.getPath(), dstIIP.getPath(), mtime, logRetryCache)</step> <step>ENTRY → LOG:LOG.DEBUG:doEditTx() op={} txid={}</step> <step>TRY → CALL:org.slf4j.Logger:debug</step> <step>TRY → CALL:editLogStream.write</step> <step>EXCEPTION:write → CATCH:IOException ex → CALL:reset</step> <step>CALL:endTransaction → CALL:shouldForceSync → RETURN → EXIT</step> </sequence>",
    "log": "<log_entry level=\"ERROR\">Permission denied while accessing pool {poolName}: user {userName} does not have {access} permissions.</log_entry> <log_entry level=\"DEBUG\">DIR* FSDirectory.renameTo: {srcIIP.getPath()} to {dstIIP.getPath()}</log_entry> <log_entry level=\"DEBUG\">doEditTx() op={} txid={}</log_entry> <log_entry level=\"INFO\">Logger debug executed</log_entry>"
  },
  "249a74da_1": {
    "exec_flow": "ENTRY→IF_TRUE: aliasMaps != null→FOREACH: aliasMaps→CALL:RPC.stopProxy→TRY→IF_FALSE:proxy==null→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→FOREACH_EXIT→EXIT",
    "log": "[ERROR] Closing proxy or invocation handler caused exception"
  },
  "249a74da_2": {
    "exec_flow": "ENTRY→IF_TRUE: aliasMaps != null→FOREACH: aliasMaps→CALL:RPC.stopProxy→TRY→IF_FALSE:proxy==null→IF_FALSE:proxy instanceof Closeable→IF_FALSE:handler instanceof Closeable→CALL:org.slf4j.Logger:error(java.lang.String,java.lang.Throwable)→THROW:new HadoopIllegalArgumentException(\"Cannot close proxy - is not Closeable or does not provide closeable invocation handler \" + proxy.getClass())→FOREACH_EXIT→EXIT",
    "log": "[ERROR] RPC.stopProxy called on non proxy: class="
  },
  "7c11af00_1": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed → RETURN → EXIT",
    "log": "[INFO] Obtained number of VCores used from Linux [WARN] Error reading the stream {exception details} [WARN] Error closing the stream {stream details}"
  },
  "7c11af00_2": {
    "exec_flow": "ENTRY → CALL:org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed → RETURN → EXIT",
    "log": "[INFO] Obtained number of VCores used from Windows"
  },
  "7c11af00_3": {
    "exec_flow": "ENTRY → CALL:refreshIfNeeded → IF_TRUE: now - lastRefreshTime > REFRESH_INTERVAL_MS → CALL:reset → CALL:getSystemInfoInfoFromShell → IF_TRUE:sysInfoStr != null → IF_TRUE:index >= 0 → IF_FALSE:sysInfo.length == sysInfoSplitCount → LOG:LOG.WARN:Expected split length of sysInfo to be 11. Got ... → RETURN → EXIT",
    "log": "[WARN] Expected split length of sysInfo to be 11. Got ..."
  },
  "7c11af00_4": {
    "exec_flow": "ENTRY → CALL:refreshIfNeeded → IF_TRUE: now - lastRefreshTime > REFRESH_INTERVAL_MS → CALL:reset → CALL:getSystemInfoInfoFromShell → IF_TRUE:sysInfoStr != null → IF_FALSE:index >= 0 → LOG:LOG.WARN:Wrong output from sysInfo: ... → RETURN → EXIT",
    "log": "[WARN] Wrong output from sysInfo: ..."
  },
  "7c11af00_5": {
    "exec_flow": "ENTRY → TRY → CALL: shellExecutor.execute → EXCEPTION: execute → CATCH: IOException e → LOG: LOG.ERROR: StringUtils.stringifyException(e) → CALL: org.slf4j.Logger:error → RETURN → EXIT",
    "log": "[ERROR] {StringUtils.stringifyException(e)}"
  }
}