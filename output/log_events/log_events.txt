org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:initializeLocalDir(org.apache.hadoop.fs.FileContext,java.lang.String):[WARN] Could not initialize local dir {key}, e
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:createNewApplication(javax.servlet.http.HttpServletRequest):[DEBUG] Getting root interceptor
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateDockerClientConfiguration(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.conf.Configuration):[INFO] The supplied Docker client config is + dockerClientConfig
org.apache.hadoop.yarn.service.ServiceManager$StartUpgradeTransition:transition(org.apache.hadoop.yarn.service.ServiceManager,org.apache.hadoop.yarn.service.ServiceEvent):[ERROR] [SERVICE]: Upgrade to version {} failed
org.apache.hadoop.streaming.PipeMapRed$MRErrorThread:run():[WARN] Cannot parse reporter line: {...}
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean):[DEBUG] pipeline = + Arrays.toString(nodes) + , + this
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorApp():[INFO] Jersey resource package added
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int):[INFO] Shuffling to disk since [requestedSize] is greater than maxSingleShuffleLimit ([maxSingleShuffleLimit])
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:aggregate(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineAggregationTrack):[DEBUG] NoOpTimelineWriter is configured. Not aggregating TimelineEntities.
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List):[DEBUG] Disk file: File Length is L
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:resolveOOM(java.util.concurrent.ExecutorService):[DEBUG] OOM handler timed out
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:poll():[DEBUG] timed poll(): timed out
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[INFO] Set replication to ... for path: ...
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:checkVersion():[INFO] Loaded timeline store version info loadedVersion
org.apache.hadoop.yarn.client.cli.ApplicationCLI:failApplicationAttempt(java.lang.String):[DEBUG] Failing attempt with id: attId
org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig):[INFO] Adding cross-site request forgery (CSRF) protection, headerName = {}, methodsToIgnore = {}, browserUserAgents = {}
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:initCsiAdaptorCache(java.util.Map,org.apache.hadoop.conf.Configuration):[INFO] Retrieving info from csi-driver-adaptor on address + addr
org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[INFO] Delegation token set
org.apache.hadoop.fs.azurebfs.commit.ResilientCommitByRename:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Checking if source and destination paths are different
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[ERROR] Interrupted while attempting to shutdown
org.apache.hadoop.yarn.server.federation.policies.RouterPolicyFacade:getHomeSubcluster(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.List):[WARN] There is no policies configured for queue: + queue + we + fallback to default policy for: + YarnConfiguration.DEFAULT_FEDERATION_POLICY_KEY
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Task attempt registered to finishing monitor
org.apache.hadoop.examples.dancing.DistributedPentomino:createInputDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.examples.dancing.Pentomino,int):[DEBUG] File closed
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[DEBUG] BLOCK* addStoredBlock: {} on {} size {} but it does not belong to any file
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:purgeOldLegacyOIVImages(java.lang.String,long):[WARN] Failed to delete image file: {fileName}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAdminAcls(boolean):[INFO] Log success for user
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing + event.getTaskID() + of type + event.getType()
org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long):[DEBUG] Call: Invocation object details
org.apache.hadoop.yarn.service.client.ServiceClient:actionDestroy(java.lang.String):[INFO] Successfully deleted public resource dir for + serviceName + : + publicResourceDir
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:lastStep():[INFO] AM container = {} reported to finish
org.apache.hadoop.yarn.server.webapp.AppBlock:generateApplicationTable(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,org.apache.hadoop.security.UserGroupInformation,java.util.Collection):[ERROR] Failed to read the AM container of the application attempt <applicationAttemptId>
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] No component instance exists for + containerId
org.apache.hadoop.mapred.pipes.Submitter:run(java.lang.String[]):[INFO] Error :
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[INFO] Got Cluster node info from ASM
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Retrieve reduce output field separator
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[INFO] Sequential invocation completed
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeAttributesProvider(org.apache.hadoop.conf.Configuration):[ERROR] Failed to create NodeAttributesProvider based on Configuration
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.mapred.QueueManager:getQueueAcls(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Checking access permissions for queue
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:identifyContainersToPreemptOnNode(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,int):[INFO] Found container + container + on node + node.getNodeName() + without app, skipping preemption
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfigValidator:validateQueueHierarchy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[WARN] Not a valid queue state for queue
org.apache.hadoop.yarn.service.webapp.ApiServer:stopService(java.lang.String,boolean,org.apache.hadoop.security.UserGroupInformation):[INFO] Successfully deleted service {}
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[DEBUG] Counter value found and recorded for non-taskID entries
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[DEBUG] Attempt to allocate on node successful
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:listStatus(org.apache.hadoop.fs.Path,java.lang.String,java.util.List,boolean,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] listStatus filesystem: {...} path: {...}, startFrom: {...}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:scriptBasedInit(java.util.function.Function,java.lang.String[]):[WARN] Script {} does not exist, falling back to /sbin/DevicePluginScript/
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:recoverCreate(boolean):[INFO] Formatting storage directory
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest):[DEBUG] Remote UGI obtained
org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String):[INFO] Computing capacity for map {mapName}
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:recover():[DEBUG] Block recovery: Ignored replica with invalid original state: {info} from DataNode: {id}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:amContainerFinished(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent):[WARN] No ContainerStatus in containerFinishedEvent
org.apache.hadoop.examples.pi.Util:runJob(java.lang.String,org.apache.hadoop.mapreduce.Job,org.apache.hadoop.examples.pi.DistSum$Machine,java.lang.String,org.apache.hadoop.examples.pi.Util$Timer):[DEBUG] Job Submitted
org.apache.hadoop.fs.azure.ClientThrottlingAnalyzer:analyzeMetricsAndUpdateSleepDuration(org.apache.hadoop.fs.azure.ClientThrottlingAnalyzer$BlobOperationMetrics,int):[DEBUG] %5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d
org.apache.hadoop.ha.ActiveStandbyElector:becomeActive():[WARN] Exception handling the winning of election, e
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Perms after creating + fsStatus.getPermission().toShort() + , Expected: + fsp.toShort()
org.apache.hadoop.fs.s3a.S3ListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Summary: {} {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[INFO] Reinitialized Managed Parent Queue: [{}] with capacity [{}] with max capacity [{}]
org.apache.hadoop.yarn.server.webproxy.ProxyUtils:sendRedirect(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String):[DEBUG] Redirecting {} {} to {}
org.apache.hadoop.hdfs.server.federation.router.RouterUserProtocol:refreshSuperUserGroupsConfiguration():[DEBUG] Refresh superuser groups configuration in Router.
org.apache.hadoop.hdfs.server.balancer.Dispatcher:dispatchBlockMoves():[INFO] Allocating {threadsPerTarget} threads per target.
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:analyseBlocksStorageMovementsAndAssignToDN(org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy):[INFO] File: {} is under construction. So, postpone this to the next retry iteration
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] JobId\\tQueue\\tAMType\\tDuration\\t#Tasks
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStart():[ERROR] Failed to load/recover state
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:readClusterInfo(org.apache.commons.cli.CommandLine):[DEBUG] Reading cluster info
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation):[WARN] Unable to initialize HBase root as an atomic rename directory.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:checkVersion():[INFO] Storing RM state version info [getCurrentVersion()]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:run():[INFO] Preemption thread interrupted! Exiting.
org.apache.hadoop.conf.Configuration:handleDeprecation():[DEBUG] Handling deprecation for (item)
org.apache.hadoop.fs.s3a.impl.CallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture):[DEBUG] Ignoring exception raised in task completion:
org.apache.hadoop.hdfs.server.namenode.BackupNode:handshake(org.apache.hadoop.conf.Configuration):[ERROR] Incompatible build versions: active name-node BV = {0}; backup node BV = {1}
org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable):[DEBUG] beforeExecute in thread: + Thread.currentThread().getName() + ", runnable type: " + r.getClass().getName()
org.apache.hadoop.fs.s3a.S3AInputStream:onReadFailure(java.io.IOException,boolean):[INFO] Got exception while trying to read from stream {}, client: {} object: {}, trying to recover: {}
org.apache.hadoop.hdfs.server.namenode.NameNode:copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration):[DEBUG] ending log segment because of end of stream in {}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:cancelPlan(java.lang.String):[ERROR] Disk Balancer - No such plan. Cancel plan failed. PlanID:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler:logAndStdErr(java.lang.Throwable,java.lang.String):[ERROR] msg
org.apache.hadoop.hdfs.server.federation.store.StateStoreCache:loadCache(boolean):[INFO] MountTableResolver cache loaded
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getNode(java.util.Collection,java.lang.String):[ERROR] Subcluster {} failed to return nodeInfo.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[ERROR] Path not found
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:scanBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[INFO] Replica {} was not found in the VolumeMap for volume {}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask:run():[WARN] Exception thrown when retrieve key: ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockPoolSlice:fetchVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,org.apache.hadoop.fs.FileSystem):[WARN] A block with id {blockId} exists locally. Skipping PROVIDED replica
org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession():[WARN] {KeeperException.toString()}
org.apache.hadoop.tools.rumen.Folder:initialize(java.lang.String[]):[ERROR] You must have an input cycle length.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor:run():[INFO] RedundancyMonitor received an exception while shutting down.
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:recover():[WARN] Failed to recover block (block={block}, datanode={id})
org.apache.hadoop.registry.server.dns.RegistryDNSServer:manageRegistryDNS():[WARN] Unable to monitor the registry. DNS support disabled.
org.apache.hadoop.mapred.gridmix.Gridmix:writeDistCacheData(org.apache.hadoop.conf.Configuration):[INFO] Generating distributed cache data of size + conf.getLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getEventBatchList(long,long,org.apache.hadoop.hdfs.server.namenode.FSEditLog,boolean,int):[INFO] NN is transitioning from active to standby and FSEditLog is closed -- could not read edits
org.apache.hadoop.fs.azure.NativeAzureFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] NativeAzureFileSystem. Initializing.
org.apache.hadoop.hdfs.DFSClient:createWrappedInputStream(org.apache.hadoop.hdfs.DFSInputStream):[INFO] Creating a Wrapped Input Stream since feInfo is not null
org.apache.hadoop.mapreduce.task.reduce.Fetcher:verifyConnection(java.net.URL,java.lang.String,java.lang.String):[WARN] Get a negative backoff value from ShuffleHandler. Setting it to the default value 5000
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[WARN] Interrupted while trying to fence via ssh
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:deleteApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[INFO] Delete from the StateStore the application: {request.getApplicationId()}
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.hadoop.util.LogAdapter):[INFO] createStartupShutdownMessage
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the start data of container [containerId]
org.apache.hadoop.mapreduce.task.reduce.Fetcher:copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean):[WARN] Failed to shuffle for fetcher#id
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getCachedStore(org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,java.util.List):[DEBUG] Set applogs {appLogs} for group id {groupId}
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:getAgent(java.lang.String):[INFO] Using Agent: ${agentClassName} for queue: ${queueName}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:initScheduler(org.apache.hadoop.conf.Configuration):[INFO] Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DominantResourceCalculator, minimumAllocation=<1024, 1>, maximumAllocation=<8192, 8>, asynchronousScheduling=false, asyncScheduleInterval=300000ms,multiNodePlacementEnabled=false, assignMultipleEnabled=false, maxAssignPerHeartbeat=1, offswitchPerHeartbeatLimit=4
org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] ns + . + FairCallQueue.IPC_CALLQUEUE_PRIORITY_LEVELS_KEY + is deprecated. Please use + ns + . + CommonConfigurationKeys.IPC_SCHEDULER_PRIORITY_LEVELS_KEY + .
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[WARN] Path ... is not accessible for all users. Current permissions are: ...
org.apache.hadoop.hdfs.server.namenode.FSEditLog:startLogSegment(long,int):[ERROR] Unable to start log segment + segmentTxId: too few journals successfully started.
org.apache.hadoop.fs.azure.BlockBlobAppendStream:close():[DEBUG] close {} , key
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer():[TRACE] Lexicographical comparer selected for byte aligned system architecture
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,long,org.apache.hadoop.security.UserGroupInformation):[WARN] Unable to parse credentials for applicationId
org.apache.hadoop.yarn.nodelabels.NodeLabelsStore:recover():[DEBUG] Calling FileSystemNodeLabelsStore recover
org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor:run():[WARN] Exception during shutdown:
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:getCredentials():[DEBUG] No credentials provided by {provider}: {e.toString()}
org.apache.hadoop.hdfs.qjournal.server.Journal:scanStorageForLatestEdits():[INFO] Latest log is ; journal id:
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:call():[INFO] Processing batched re-encryption for zone {}, batch size {}, start:{}, zoneNodeId, batch.size(), batch.getFirstFilePath()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[ERROR] Unsupported operation attempted
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:initPlugin(org.apache.hadoop.conf.Configuration):[WARN] Failed to find FPGA discoverer executable configured in YarnConfiguration.NM_FPGA_PATH_TO_EXEC, please check! Try default path
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptKilledTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Not generating HistoryFinish event since start event not generated for task: + task.getID()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] Sending finish application request to {} sub-cluster RMs
org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] client isn't using kerberos
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:createAzureStorageSession():[DEBUG] The account access key is not configured for {sessionUri}. Now try anonymous access.
org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]):[ERROR] Must provide a filename to all commands.
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Buffer size and replication are not honored by ADL backend
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:removeAcl(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.removeAcl path: {}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod(org.apache.hadoop.security.UserGroupInformation,java.util.List,java.lang.Class,java.lang.reflect.Method,java.lang.Object[]):[ERROR] Cannot get available namenode for...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:updateRuleSet(java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Placement rule order check
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:startWepApp():[ERROR] No war file or webapps found for ui2 !
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadDirectories(java.io.FileInputStream,java.util.List,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,org.apache.hadoop.conf.Configuration):[INFO] Finished loading directories in {}ms
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Forcefully kill the service: {serviceName}
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer):[DEBUG] Releasing buffer
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$UpdateContainerResourceTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[ERROR] Callback handler does not implement container resource update callback methods
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:parseCheckSumFiles(java.util.List):[WARN] Exception message if any
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:moveFiles():[ERROR] Trash dir for replica ...
org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable):[ERROR] Exception in RestCsrfPreventionFilterHandler
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent):[ERROR] Could not deallocate container for task attemptId + aId
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction):[INFO] rollingUpgrade PREPARE
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:reportNewCollectorInfo(org.apache.hadoop.yarn.server.api.protocolrecords.ReportNewCollectorInfoRequest):[WARN] collectors are added when the registered collectors are initialized
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:run():[INFO] Exception in getting events
org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine):[DEBUG] setting conf tokensFile: [fileName value]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[ERROR] Aocl output: {shexec.getOutput}
org.apache.hadoop.security.Groups:cacheGroupsAdd(java.util.List):[WARN] Error caching groups
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:mkdir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Directory: [ + path + ] already exists.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:sendHeartBeat(boolean):[DEBUG] Sending heartbeat with [number] storage reports from service actor: [actor]
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:getContainerPid():[DEBUG] Got pid {} for container {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:updateSchedulingMonitors(org.apache.hadoop.conf.Configuration,boolean):[INFO] Scheduling Monitor disabled, stopping all services
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[INFO] Added Application Attempt {applicationAttemptId} to scheduler from user {application.getUser()} in queue {queue.getQueuePath()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:enqueueContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not store container [containerId] state. The Container has been queued.
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] Context Path = {Collections.singletonList(apiServer.getWebAppContext().getContextPath())}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[INFO] Convert + b + from Temporary to RBW, visible length= + visible
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] aggregate statistics\n{}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: QUEUE_AUTO_CREATE
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceStart():[INFO] MRAppMaster launching normal, non-uberized, multi-container job ...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:stopMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[TRACE] stopMaintenance: Node {} in {}, nothing to do.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:removeApplication(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Application is deleted from state store
org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator):[INFO] Processing items with groupSize > 0
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Keytab: <ugi>
org.apache.hadoop.hdfs.server.sps.ExternalSPSBlockMoveTaskHandler:initializeBlockMoverThreadPool(int):[DEBUG] Block mover to satisfy storage policy; pool threads={}
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[INFO] Metrics for the application created successfully
org.apache.hadoop.mapred.LocalJobRunner$Job:createOutputCommitter(boolean,org.apache.hadoop.mapred.JobID,org.apache.hadoop.conf.Configuration):[INFO] OutputCommitter is ...
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType):[DEBUG] Decided to move + this
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:toDevice(java.nio.file.Path,org.apache.commons.lang3.mutable.MutableInt):[INFO] Device: major: {major}, minor: {minor}, devNo: {deviceNumber}, type: {devType}
org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Header origins '{originsList}' not allowed. Returning
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:init(org.apache.hadoop.conf.Configuration):[DEBUG] Initializing RemoteWasbAuthorizerImpl instance
org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[ERROR] Caught an exception while processing JMX request
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:retrieve():[ERROR] Failed to retrieve configuration from zookeeper store
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Storing info for app: {} at: {}
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory:requestBuffer(int):[DEBUG] Requesting buffer of size {}
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:setPlanParams(java.util.List):[DEBUG] Setting bandwidth to {}
org.apache.hadoop.hdfs.server.namenode.LeaseManager:removeLease(long):[WARN] Removing non-existent lease! holder={} src={}
org.apache.hadoop.resourceestimator.translator.impl.BaseLogParser:parseStream(java.io.InputStream):[DEBUG] Data field not found
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfSlowPeerParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[DEBUG] Counter value found and recorded
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object):[TRACE] Failed to register MBean "{name}", iaee
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:setPassword(com.zaxxer.hikari.HikariDataSource,java.lang.String):[DEBUG] Setting non NULL Credentials for Store connection
org.apache.hadoop.hdfs.DFSClient:primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean):[DEBUG] {}: masked={}, src, absPermission
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager:recycle(byte[]):[DEBUG] ,
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Completing previous checkpoint for storage directory {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[DEBUG] Storing {} to {}
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortAllSinglePendingCommits(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[INFO] No directory to abort {pendingDir}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] rollingMonitorInterval is set as {}. The logs will be aggregated every {} seconds
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:findCounter(java.lang.String):[WARN] New counter created
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:scanDirectoryTree(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,boolean):[DEBUG] {}: Number of subdirectories under {} found: {}; file count ...
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:removeAcl(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] removeAcl filesystem: {} path: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startActiveServices():[INFO] Catching up to latest edits from old active before taking over writer role in edits logs
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeValue(int,int,java.lang.String,org.apache.hadoop.mapreduce.JobID):[WARN] name + not defined for + id
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:loadPools(java.io.DataInput):[DEBUG] Begin loading cache pools
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest):[ERROR] Trying to signal an absent container
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation):[DEBUG] Attempted to update a non-existing znode + nodeRemovePath
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:loadNodeChildren(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[]):[TRACE] loadNodeChildren(expected= + expected + , terminators=[ + StringUtils.join(,, terminators) + ]): + parent.dump()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler):[INFO] Initialized workflow priority mappings, override: true
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:deactivateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] User {} removed from activeUsers, currently: {}
org.apache.hadoop.mapreduce.Cluster:initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration):[DEBUG] Trying ClientProtocolProvider : ...
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[DEBUG] Upstream service is down, skipping the sps work.
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNamenodes():[ERROR] Enable to fetch json representation of namenodes {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$LaunchContainerRunnable:run():[ERROR] Error while configuring container!
org.apache.hadoop.hdfs.qjournal.server.Journal:completeHalfDoneAcceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData):[INFO] Rolling forward previously half-completed synchronization: + tmp + -> + dst + ; journal id: + journalId
org.apache.hadoop.hdfs.server.namenode.BackupImage:waitUntilNamespaceFrozen():[WARN] Interrupted waiting for namespace to freeze
org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:run():[ERROR] Failed to transfer block {block}
org.apache.hadoop.hdfs.client.impl.DfsClientConf:loadReplicaAccessorBuilderClasses(org.apache.hadoop.conf.Configuration):[WARN] Unable to load className
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:enqueueContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Opportunistic container containerId will be queued at the NM.
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:init(org.apache.hadoop.yarn.util.Clock,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,java.util.Collection):[INFO] Initializing Plan Follower Policy:org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower
org.apache.hadoop.yarn.service.client.ServiceClient:addAMEnv():[DEBUG] Run as user {}
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$LoggingAuditSpan:attachRangeFromRequest(com.amazonaws.AmazonWebServiceRequest):[WARN] WARN_INCORRECT_RANGE.warn("Expected range to contain 0 or 2 elements." + " Got {} elements. Ignoring.", rangeValue.length)
org.apache.hadoop.hdfs.DataStreamer:transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.security.token.Token):[ERROR] InvalidEncryptionKeyException recorded
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$StoppedAfterCancelUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] {} received stopped but cancellation pending, event.getContainerId()
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:convertCredentialsToByteBuffer(org.apache.hadoop.security.Credentials):[ERROR] Failed to convert Credentials to ByteBuffer.
org.apache.hadoop.fs.azurebfs.services.KeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Retrieving storage account key
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printContainerReport(java.lang.String):[INFO] Application for Container with id doesn't exist in RM or Timeline Server.
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Service definition {} doesn't belong to any user. Ignoring..
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:rename(java.lang.String,java.lang.String):[DEBUG] Rename source key: [{}] to dest key: [{}]
org.apache.hadoop.mapred.MapTask:createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter):[INFO] Map output collector class = + collector.getClass().getName()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:checkRemoveParentZnode(java.lang.String,int):[DEBUG] Unable to remove parent node {} as it does not exist.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer:start(java.lang.String):[INFO] Interrupted. Stopping the WebImageViewer.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float):[INFO] Ramping down
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:offerToQueue(java.time.Instant,java.lang.String):[DEBUG] Queued latency info [{} ms]: {}
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [FLEX UP COMPONENT {component.getName()}]: scaling up from {before} to {event.getDesired()}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent):[INFO] Could not create log file: [historyFile] for job [jobName]
org.apache.hadoop.yarn.server.webapp.AppAttemptBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Failed to read the application attempt
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:deleteApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[ERROR] Application {request.getApplicationId()} does not exist
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[INFO] Invalid call marked
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.ReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] Updated reservation using AlignedPlannerWithGreedy
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer:go():[ERROR] Failed to load image file.
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[DEBUG] Attempted to remove a non-existing znode [nodeRemovePath]
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity):[DEBUG] Publishing the entity + entity + , JSON-style content: + TimelineUtils.dumpTimelineRecordtoJSON(entity)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] AzureBlobFileSystem.getXAttr path: {}
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet):[INFO] modifyDirective of {} successfully applied {}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyConstraints(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional):[DEBUG] canSatisfyOrConstraint check
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:createRMNodeLabelsMappingProvider(org.apache.hadoop.conf.Configuration):[ERROR] RMNodeLabelsMappingProvider should be configured when delegated-centralized node label configuration is enabled
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long):[WARN] Exception running logging thread
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:writeAppendBlobCurrentBufferToService():[DEBUG] Upload data block started
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[DEBUG] getEntity type={} id={}
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String):[DEBUG] Renewing token:{} with renewer:{}.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Symlink target should not be null, fileId: {}
org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable):[ERROR] Failed to setup deferred successful response. ThreadName= + Thread.currentThread().getName() + , Call= + this
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Starting Client
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[DEBUG] Scheduling move to done of {{found}}
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:writeUnlock():[INFO] Write lock info after unlocking: ... [additional formatted placeholders]
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:rollSecret():[DEBUG] Rolling secret
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService$FileSystemCachePurger:run():[DEBUG] Purged [{}] filesystem instances
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Application Attempt is done. finalState=
org.apache.hadoop.hdfs.server.datanode.BlockScanner$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] Returned Servlet info {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Adding replicas to map for block pool on volume ...
org.apache.hadoop.lib.server.Server:init():[DEBUG] Initializing services
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[INFO] Failed to satisfy the policy after retries. Removing inode from the queue.
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:initSecurity():[INFO] Registry default system acls: ...
org.apache.hadoop.registry.server.services.RegistryAdminService:purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback):[DEBUG] Failing deletion operation
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceStop():[INFO] Stopping Router ClientRMService
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:getEncryptionKeyInfo(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.crypto.CryptoProtocolVersion[]):[INFO] Start file before generating key
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(java.lang.Object,java.lang.Object):[INFO] Updating RMDelegationToken and SequenceNumber
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest):[ERROR] ${e.getMessage()}
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:createDelegationToken(org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[INFO] Creating New Delegation Token
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow run id: {parts[1]}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:evictOldStartTimes(long):[INFO] Searching for start times to evict earlier than {minStartTime}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[INFO] Version check started
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:getMajorNumber(java.lang.String):[DEBUG] Get major numbers from /dev/{}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDiscoverer:discover():[WARN] We continue although there're mistakes in user's configuration ...
org.apache.hadoop.fs.s3a.tools.MarkerTool:execMarkerTool(org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanArgs):[DEBUG] Verbose set for marker tool
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:wipeState():[WARN] Failed to wipe tc state. This could happen if the interface is already in its default state. Ignoring.
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getAggregatedLogsMeta(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean):[DEBUG] Calling getLogsInfo
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$InitContainerTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[INFO] Adding <containerId> to application <appString>
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {userName}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable):[INFO] could not get ... due to InvalidToken exception.
org.apache.hadoop.fs.s3a.tools.MarkerTool:scanDirectoryTree(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.impl.DirMarkerTracker,int):[DEBUG] Listing summary {}
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean):[DEBUG] DIR* FSDirectory.renameTo: srcIIP.getPath() to dstIIP.getPath()
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationAttemptStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptStartData):[INFO] Start information of application attempt is written
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:init(org.apache.hadoop.conf.Configuration):[ERROR] Cannot initialize the ZK connection
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor$AzureFileSystemThreadRunnable:run():[WARN] Terminating execution of {} operation now as some other thread already got exception or operation failed
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[WARN] Unable to create SSH session
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceStop():[WARN] HistoryCleanerService/move to done shutdown may not have succeeded, Forcing a shutdown
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] Cluster max priority refreshed
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS FSINFO fileHandle: {} client: {}
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[INFO] Current attempt state of + appId + is + (attemptReport == null ? "N/A" : attemptReport.getYarnApplicationAttemptState()) + ", waiting for current attempt to reach " + attemptState
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:addToCluserNodeLabels(java.util.Collection):[INFO] Add labels: [ + StringUtils.join(labels.iterator(), ",") + ]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:readState():[WARN] Failed to bootstrap outbound bandwidth rules
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:createCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String):[DEBUG] createCgroup: {}
org.apache.hadoop.hdfs.HdfsDtFetcher:addDelegationTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,java.lang.String,java.lang.String):[ERROR] FETCH_FAILED
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewBlock:render():[INFO] Shared Cache Manager overview
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ContainerLauncher:call():[ERROR] {}: Failed to launch container., instance.getCompInstanceId()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$SerialNumberIndex:add(java.lang.String,java.lang.String):[ERROR] Dropping {key} from the SerialNumberIndex. We will no longer be able to see jobs that are in that serial index for {cache.get(key)}
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:notLeader():[INFO] rmId + " relinquish leadership"
org.apache.hadoop.fs.s3a.S3AFileSystem:processDeleteOnExit():[INFO] Ignoring failure to deleteOnExit for path {}
org.apache.hadoop.hdfs.NameNodeProxiesClient:createHAProxy(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider):[INFO] buildTokenServiceForLogicalUri
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeConcurrent(java.util.Collection,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,boolean,boolean,java.lang.Class):[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()}
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run():[ERROR] ExpiredTokenRemover received (InterruptedException ie)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:containerFailedOnHost(java.lang.String):[INFO] Blacklisted host <hostName>
org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int):[INFO] Got {} = '{}' (default '{}')
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:readState():[DEBUG] TC state: {} + output
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics):[WARN] The identifier for the State Store connection is not set
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session):[INFO] Making interactive connection to running docker container with ID: cId
org.apache.hadoop.registry.client.impl.zk.CuratorService:createCurator():[DEBUG] securityConnectionDiagnostics
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:run():[DEBUG] Received new AMRMToken
org.apache.hadoop.yarn.service.ClientAMService:upgrade(org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto):[INFO] Upgrade container {containerId}
org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]):[INFO] Unable to fence old active: <exception_message>
org.apache.hadoop.yarn.appcatalog.application.YarnServiceClient:restartApp(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Error in restarting application:
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[ERROR] Can't handle this event at current state
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:run():[ERROR] Failed to download resource
org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String):[WARN] Failed to connect to {}: {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSyncAll():[INFO] logSyncAll toSyncToTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] ImageServlet allowing administrator: + remoteUser
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:logCurrentHadoopUser():[INFO] Real User = {}
org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver:updateSubclusterMapping():[ERROR] Cannot wait for the updater to finish
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Retrieve reduce input field separator
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose():[DEBUG] Block {}: Deleting buffer file as upload did not start
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:enterState(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState,org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState):[DEBUG] {}: entering state {}, this, next
org.apache.hadoop.tools.rumen.TraceBuilder:run(java.lang.String[]):[WARN] File skipped: Invalid file name
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long):[INFO] Setting event type: LAUNCHED_EVENT_TYPE
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectStreamingInputStreams(java.util.Collection,long,boolean,boolean):[DEBUG] selectStreamingInputStream manifests:\n {}
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] Adding directory: {}
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:selectTokenFromFSOwner():[DEBUG] Finding a token for FS user
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:getMajorNumber(java.lang.String):[WARN] Failed to get major number from reading /dev/
org.apache.hadoop.hdfs.server.datanode.DataNode:secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[WARN] Exiting Datanode
org.apache.hadoop.yarn.server.webapp.ContainerBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Container not found: ${containerid}
org.apache.hadoop.mapred.uploader.FrameworkUploader:addJar(java.io.File):[INFO] Whitelisted non-jar /path/to/jar
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadDirectoriesInINodeSection(java.io.InputStream):[INFO] Found {} directories in INode section.
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager:getTopUsersForMetric(long,java.lang.String,org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$RollingWindowMap):[DEBUG] topN users size for command {} is: {}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeProxies():[WARN] Could not add proxy for Journal at addresss <addr>
org.apache.hadoop.mapred.TaskAttemptListenerImpl:coalesceStatusUpdate(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptStatusUpdateEvent$TaskAttemptStatus,java.util.concurrent.atomic.AtomicReference):[INFO] TaskAttempt {yarnAttemptID}: lastStatusRef changed by another thread, retrying...
org.apache.hadoop.mapreduce.lib.partition.InputSampler$SplitSampler:getSample(org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Job):[DEBUG] Samples obtained from splits
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Added node " + node.getNodeAddress() + " cluster capacity: " + clusterResource
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:getPluginsFromConfig(org.apache.hadoop.conf.Configuration):[INFO] Found Resource plugins from configuration: [ ]
org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration):[DEBUG] Added RM HA URLs
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile):[DEBUG] Directive {}: not scanning file {} because bytesNeeded for pool {} is {}, but the pool's limit is {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processPerfectOverWrite(org.apache.hadoop.hdfs.DFSClient,long,int,org.apache.hadoop.nfs.nfs3.Nfs3Constant$WriteStableHow,byte[],java.lang.String,org.apache.hadoop.nfs.nfs3.response.WccData,org.apache.hadoop.security.IdMappingServiceProvider):[INFO] hsync failed when processing possible perfect overwrite, path={} error: {}, path, e.toString()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[DEBUG] checkAccess operation completed
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuResourcePlugin:checkErrorCount():[ERROR] Failed to execute GPU device information detection script for MAX_REPEATED_ERROR_ALLOWED times, skip following executions.
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:postWrite(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerContext):[ERROR] Failed to move temporary log file to final location
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationOnRecovery(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] FATAL: Queue named queueName missing during application recovery. Queue removal during recovery is not presently supported by the capacity scheduler, please restart with all queues configured which were present before shutdown/restart.
org.apache.hadoop.streaming.PipeMapRed:configure(org.apache.hadoop.mapred.JobConf):[ERROR] configuration exception
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFile(java.lang.String,java.lang.String,long,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3AReadOpContext):[DEBUG] getObjectMetadata({srcKey}) failed to find an expected file
org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer:getFilterHandlers(org.apache.hadoop.conf.Configuration):[DEBUG] Loading filter handler {class}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:start(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[ERROR] Can't start StoragePolicySatisfier for the given mode:{}
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:main(java.lang.String[]):[ERROR] Got exception starting
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:expectTagEnd(java.lang.String):[ERROR] Expected tag end event for {expected}, but got: {ev}
org.apache.hadoop.yarn.service.provider.ProviderUtils:addLocalResource(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,java.lang.String,org.apache.hadoop.yarn.api.records.LocalResource,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams):[INFO] Added file for localization: symlink -> localResource.getResource().getFile(), dest mount path: destFile
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW:createTable(org.apache.hadoop.hbase.client.Admin,org.apache.hadoop.conf.Configuration):[INFO] Status of table creation for [table.getNameAsString()]
org.apache.hadoop.hdfs.server.datanode.DataXceiver:copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[INFO] Not able to copy block ... because it's pinned
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[INFO] Disabling Erasure Coding for path: ...
org.apache.hadoop.fs.s3a.WriteOperationHelper:abortMultipartUploadsUnderPath(java.lang.String):[DEBUG] Already aborted: {}
org.apache.hadoop.fs.azure.BlockBlobAppendStream:writeBlockListRequestInternal():[DEBUG] Encountered exception during uploading block for Blob {} Exception : {}
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:init(org.apache.hadoop.yarn.server.nodemanager.Context):[ERROR] Failed to bootstrap configured resource subsystems!
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:writeEvent(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent):[DEBUG] Writing event
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] Modify this write to write only the appended data
org.apache.hadoop.hdfs.server.datanode.BlockScanner:addVolumeScanner(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[DEBUG] Adding scanner for volume {} (StorageID {})
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String):[DEBUG] Moving source to destination
org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger:logAuditEvent(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus):[ERROR] An error occurred while reflecting the event in top service, event: (cmd={},userName={})
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager:getTopUsersForMetric(long,java.lang.String,org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$RollingWindowMap):[DEBUG] offer window of metric: {} userName: {} sum: {}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:loadMmapInternal():[WARN] this + : mmap error, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:getFlowContext(org.apache.hadoop.yarn.api.records.ContainerLaunchContext,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Flow context: {} created for an application {}
org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask():[WARN] The server is stopped.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getIncrementAllocation():[WARN] Configuration overridingKey=get(overridingKey) is overriding the RM_SCHEDULER_INCREMENT_ALLOCATION_VCORES=get(RM_SCHEDULER_INCREMENT_ALLOCATION_VCORES) property
org.apache.hadoop.examples.pi.DistSum$ReduceSide$PartitionMapper:map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.examples.pi.SummationWritable,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] parts[i] = parts[i]
org.apache.hadoop.fs.azurebfs.oauth2.UserPasswordTokenProvider:refreshToken():[DEBUG] AADToken: refreshing user-password based token
org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean):[ERROR] Unable to make {toSvc} active (unable to connect). Failing back.
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Resource handler chain enabled = false
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfigValidator:validateQueueHierarchy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] Deleting Queue
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:unregisterSlot(int):[TRACE] {}: unregisterSlot {}, this, slotIdx
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest):[INFO] Successfully deleted reservation
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:call():[WARN] Could not copy the file to the shared cache at + tempPath
org.apache.hadoop.hdfs.DataStreamer:getPinnings(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]):[WARN] These favored nodes were specified but not chosen: + favoredSet + Specified favored nodes: + Arrays.toString(favoredNodes)
org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] protocol doesn't use kerberos
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult):[ERROR] Unexpected health check result null for volume {}, reference.getVolume()
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close():[DEBUG] Failed to delete cache file {entry.path}
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String):[INFO] Error getting users for netgroup
org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl:getSASKeyBasedStorageAccountInstance(java.lang.String):[DEBUG] Creating SAS key from account instance {}
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:setMasterKey(org.apache.hadoop.yarn.server.api.records.MasterKey):[INFO] Rolling master-key for container-tokens, got key with id {masterKey.getKeyId()}
org.apache.hadoop.security.Groups:refresh():[INFO] clearing userToGroupsMap cache
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:queueReadAhead(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[TRACE] Done q-ing readAhead for file {} offset {} buffer idx {}
org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery():[WARN] Could not get block locations. Source file \" + src + \" - Aborting... + this
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:printBlockDeletionTime():[INFO] {} is set to {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:validateAndFetch(java.util.List,boolean):[ERROR] Following nodes does not exist: invalidNodes
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getTimelineStoresForRead(java.lang.String,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.List):[DEBUG] plugin {} returns a non-null value on query {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:loadPlan(java.lang.String,java.util.Map):[INFO] Recovered reservations for Plan: {}
org.apache.hadoop.yarn.service.ClientAMService:serviceStart():[INFO] Instantiated ClientAMService at + bindAddress
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:main(java.lang.String[]):[ERROR] Error starting ResourceManager
org.apache.hadoop.hdfs.web.TokenAspect:ensureTokenInitialized():[DEBUG] Created new DT for {}
org.apache.hadoop.hdfs.DFSInputStream:refetchLocations(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[INFO] Could not obtain {block.getBlock()} from any node: {errMsg}. Will get new block locations from namenode and retry...
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] New instance created
org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int):[DEBUG] Got UserName/GroupName <name> for ID <id> from the native implementation
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readRemote(long,byte[],int,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[TRACE] Trigger client.read for path={} position={} offset={} length={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:reacquireContainerClasses(java.lang.String):[INFO] Reacquired container classid: [classId]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[INFO] New generation stamp and access token set
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Looking for FS supporting {}
org.apache.hadoop.registry.server.dns.RegistryDNS:serveNIOUDP(java.nio.channels.DatagramChannel,java.net.InetAddress,int):[INFO] {}: received UDP query {}
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:loadService(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.lang.String):[INFO] Loading service definition from + serviceJson
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getApplicationAttemptReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Failed to fetch application attempt report from ATS v2
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:loadCache(boolean):[ERROR] Cannot get "{}" records from the State Store
org.apache.hadoop.hdfs.server.namenode.BackupNode:stop():[ERROR] Failed to report to name-node.
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] UserGroupInformation initialized to {}
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:getDestination(org.apache.hadoop.hdfs.server.federation.store.protocol.GetDestinationRequest):[ERROR] Cannot get location for {}: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:close():[DEBUG] Shutting down writer; entry lists in queue: {}
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Generating {outputName} using {specClass} and {implClass}
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int,int):[INFO] Exits the main loop.
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client skipping handshake on trusted connection for addr = {}, datanodeId = {}
org.apache.hadoop.fs.s3a.WriteOperations:commitUpload(java.lang.String,java.lang.String,java.util.List,long):[DEBUG] Initiating commit for multipart upload
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:readStats():[DEBUG] TC stats output:{}
org.apache.hadoop.hdfs.server.datanode.DataNode:refreshVolumes(java.lang.String):[ERROR] Failed to add volume: {volume}
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:addPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy):[INFO] Added erasure coding policy
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFilesTotal():[DEBUG] Failed to get number of files
org.apache.hadoop.lib.server.Server:destroy():[INFO] ======================================================
org.apache.hadoop.mapred.gridmix.SleepJob:buildSplits(org.apache.hadoop.mapred.gridmix.FilePool):[DEBUG] SPEC(%d) %d -> %d %d/%d
org.apache.hadoop.yarn.service.ServiceManager$CancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.ServiceManager,org.apache.hadoop.yarn.service.ServiceEvent):[INFO] [SERVICE]: Cannot cancel the upgrade in {} state
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor):[INFO] YARN containers restricted to yarnProcessors cores
org.apache.hadoop.hdfs.DFSStripedInputStream:createBlockReader(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,org.apache.hadoop.hdfs.protocol.LocatedBlock[],org.apache.hadoop.hdfs.StripeReader$BlockReaderInfo[],int,long):[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to dnInfo.addr : e
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:periodicInvoke():[ERROR] Unable to get quota usage for + src, ioe
org.apache.hadoop.mapred.pipes.Application$PingSocketCleaner:run():[INFO] PingSocketCleaner started...
org.apache.hadoop.tools.DistCpSync:getAllDiffs():[WARN] Failed to compute snapshot diff on {ssDir}
org.apache.hadoop.crypto.key.kms.server.KMS:reencryptEncryptedKeys(java.lang.String,java.util.List):[TRACE] Entering reencryptEncryptedKeys method.
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stopApp(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Log aggregation is not initialized for {appId}, did it fail to start?
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:checkReducerHealth():[ERROR] Shuffle failed with too many fetch failures and insufficient progress!
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop():[ERROR] Bug in read selector!
org.apache.hadoop.fs.s3a.impl.V2Migration:v1ProviderReferenced(java.lang.String):[WARN] Directly referencing AWS SDK V1 credential provider {}. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:containerCreated(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,long):[INFO] New instance created
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:updateContainerInternal(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.ContainerTokenIdentifier):[INFO] New instance created
org.apache.hadoop.fs.s3a.tools.MarkerTool:scanDirectoryTree(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.impl.DirMarkerTracker,int):[DEBUG] {}", key
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.hadoop.util.LogAdapter):[WARN] failed to register any UNIX signal loggers:
org.apache.hadoop.hdfs.protocol.ClientProtocol:delete(java.lang.String,boolean):[DEBUG] Starting delete operation
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:prepareFileForTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] BLOCK* prepareFileForTruncate: {uc} Scheduling in-place block truncate to new size {uc.getTruncateBlock().getNumBytes()}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleReportedIncreasedContainers(java.util.List):[INFO] Container + containerId + belongs to an application that is already killed, + no further processing
org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run():[ERROR] TGT is destroyed. Aborting renew thread for {}
org.apache.hadoop.crypto.key.kms.server.KMS:createKey(java.util.Map):[DEBUG] Creating key with name {}, cipher being used{}, length of key {}, description of key {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration):[INFO] {} = {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:startTimelineClient(org.apache.hadoop.conf.Configuration):[INFO] Timeline service V2 client is enabled
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processCommits(long):[DEBUG] FileId: {} Service time: {}ns. Sent response for commit: {}
org.apache.hadoop.mapreduce.task.reduce.Fetcher:copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost):[DEBUG] Fetcher ID going to fetch from host for: maps
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:reportMkDirFailure(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry,java.lang.Exception):[DEBUG] {}: Full exception details
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkBlocksComplete(java.lang.String,boolean,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[]):[INFO] BLOCK* + err + (numNodes= + numNodes + (numNodes < min ? < : >= ) + minimum = + min + ) in file + src
org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamContext:build():[DEBUG] fs.azure.read.request.size[={}] is configured for higher size than fs.azure.read.readahead.blocksize[={}]. Auto-align readAhead block size to be same as readRequestSize., readBufferSize, readAheadBlockSize
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Abort task %s
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List):[INFO] Task assigned to chunk
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:getNMContainerStatuses():[INFO] Sending out X NM container statuses: Y
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:getTimedOutSCs(boolean):[WARN] Subcluster {} doesn't have a successful heartbeat for {} seconds for {}
org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform():[ERROR] Platform is not supported:OS. Can't update user map and group map and 'nobody' will be used for any user and group.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:getResources():[ERROR] Could not contact RM after retryInterval milliseconds.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable):[WARN] {}: failed to load {}
org.apache.hadoop.tools.GetUserMappingsProtocol:getGroupsForUser(java.lang.String):[INFO] Getting groups for user via RouterUserProtocol
org.apache.hadoop.yarn.service.ClientAMService:stop(org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto):[INFO] Stop the service by {}
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError():[DEBUG] start process datanode/external error
org.apache.hadoop.streaming.PipeMapRed:mapRedFinished():[WARN] {}
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:getContainerReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainerReportRequest):[ERROR] e.getMessage()
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[DEBUG] Scanned {scannedDirectives} directive(s) and {scannedBlocks} block(s) in {curTimeMs - startTimeMs} millisecond(s).
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:findNextResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Got exception in parsing URL of LocalResource:
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkFaultTolerantRetry(org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.String,java.io.IOException,org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,java.util.List):[DEBUG] {} exception cannot be retried
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:run():[INFO] Starting up re-encrypt thread with interval={} millisecond.
org.apache.hadoop.hdfs.net.TcpPeerServer:close():[ERROR] error closing TcpPeerServer: , e
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[INFO] STATE* Leaving safe mode after {} secs
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:increaseContainersResource(org.apache.hadoop.yarn.api.protocolrecords.IncreaseContainersResourceRequest):[INFO] Container update process started.
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppCompletelyDoneTransition:transition(java.lang.Object,java.lang.Object):[INFO] LogHandlerAppFinishedEvent dispatched
org.apache.hadoop.yarn.service.ServiceManager:finalizeUpgrade(boolean):[INFO] [SERVICE]: delete upgrade dir version {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Returning JSON response with location for file checksum
org.apache.hadoop.hdfs.server.namenode.FSImage:saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreCollectionCreator:createTimelineSchema(java.lang.String[]):[ERROR] Error while creating Timeline Collections
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateDemand():[DEBUG] The updated demand for + getName() + is + demand + ; the max is + getMaxShare()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState):[ERROR] Cannot finish application from non-leaf queue: + queue.getQueuePath()
org.apache.hadoop.conf.ReconfigurationServlet:getReconfigurable(javax.servlet.http.HttpServletRequest):[INFO] servlet path: + req.getServletPath()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[ERROR] Can't handle this event at current state for + this.taskId
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:terminate(java.lang.Throwable):[ERROR] Exception while edit logging: {t.getMessage()}
org.apache.hadoop.yarn.service.ServiceManager:handle(org.apache.hadoop.yarn.event.Event):[INFO] [SERVICE] Transitioned from {} to {} on {} event.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf):[INFO] Persistent memory is used for caching data instead of DRAM. Max locked memory is set to zero to disable DRAM cache
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine):[ERROR] FORCEFENCE and FORCEACTIVE flags not supported with auto-failover enabled.
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:checkVersion():[ERROR] Incompatible version for timeline store
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Link size: {} is larger than max transfer size: {}
org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[WARN] Failed to get server trash configuration
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy:onChangeDetected(java.lang.String,java.lang.String,java.lang.String,long,java.lang.String,long):[WARN] %s change detected on %s %s%s. Expected %s got %s
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:addDirectoryToSerialNumberIndex(org.apache.hadoop.fs.Path):[DEBUG] Adding [serialDirPath] to serial index
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer():[TRACE] Lexicographical comparer selected
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[DEBUG] Found 'userid' '{}' in application tag
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:checkUserAccessToQueue(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest):[INFO] Interceptor chain acquired
org.apache.hadoop.hdfs.DataStreamer:initDataStreaming():[DEBUG] nodes {} storageTypes {} storageIDs {}
org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams):[INFO] Log exception
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:getMultiNodeSortIterator(java.util.Collection,java.lang.String,java.lang.String):[WARN] Multi Node scheduling is enabled, however invalid class is configured. Valid sorting policy has to be configured in yarn.scheduler.capacity.<queue>.multi-node-sorting.policy
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:initS3AFileSystem(java.lang.String):[DEBUG] Initializing S3A FS to {}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:getNextBlock(java.util.List,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] No movable source blocks found. {}, item.toJson()
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:getClientLatency():[DEBUG] Dequeued latency info [{} ms]: {}, elapsed, latencyDetails
org.apache.hadoop.hdfs.util.MD5FileUtils:saveMD5File(java.io.File,java.lang.String):[DEBUG] Saved MD5 ${digestString} to ${md5File}
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:kill(boolean):[INFO] KILLING + taskAttemptID
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:storeEdits(byte[],long,long,int):[ERROR] Unable to save new edits [%d, %d] due to exception when updating to new layout version %d
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[],boolean):[INFO] Erasure coding policies added
org.apache.hadoop.hdfs.server.federation.router.RouterUserProtocol:refreshUserToGroupsMappings():[DEBUG] Refresh user groups mapping in Router.
org.apache.hadoop.hdfs.server.datanode.DataNode:makeInstance(java.util.Collection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[DEBUG] Initializing StorageLocationChecker
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:run(int):[DEBUG] Lease renewer daemon for [clientsString] with renew id [id] expired
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric name {name} was emitted with a null value.
org.apache.hadoop.hdfs.server.balancer.Dispatcher:executePendingMove(org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove):[WARN] No mover threads available: skip moving + p
org.apache.hadoop.fs.s3a.tools.MarkerTool:execute(org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanArgs):[INFO] Swapping -min (...) and -max (...) values
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:rollMasterKey():[INFO] Rolling master-key for amrm-tokens
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:report(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus):[DEBUG] a metric is reported: cmd: {} user: {}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render():[DEBUG] No logs available for container
org.apache.hadoop.examples.terasort.TeraScheduler:pickBestHost():[DEBUG] picking + result
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:connectUsingAnonymousCredentials(java.net.URI):[ERROR] Service returned StorageException when checking existence of container {} in account {}
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:createTracker(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.statistics.PutTrackerStatistics):[DEBUG] Created {}, tracker
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Checking operation unchecked
org.apache.hadoop.fs.s3a.S3AFileSystem:uploadPart(com.amazonaws.services.s3.model.UploadPartRequest,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[INFO] Incrementing put completed statistics
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyToFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,long,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet,org.apache.hadoop.fs.FileChecksum):[DEBUG] Creating file in FileSystem
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:blockReport(long):[INFO] Successfully sent block report 0x...
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:sortAndSpill():[INFO] Finished spill 0
org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer):[TRACE] CleanerUtil.UNMAP_NOT_SUPPORTED_REASON
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEvent):[ERROR] Invalid eventtype eventType. Ignoring!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer):[INFO] Adding block pool [bpid] to volume with id [getStorageID()]
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager:allocate():[DEBUG] , org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance
org.apache.hadoop.mapreduce.task.reduce.Fetcher:run():[INFO] Scheduler host freed
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStop():[INFO] Shutting down the background thread.
org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream):[DEBUG] Failed to decode token identifier
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:start():[INFO] Starting standby checkpoint thread...\nCheckpointing active NN to possible NNs: {}\nServing checkpoints at {}, activeNNAddresses, myNNAddress
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop():[INFO] Stopping + prefix + metrics system...
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + but flowrun not found (Took + (Time.monotonicNow() - startTime) + ms.)
org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread:run():[INFO] Submitted the job ...
org.apache.hadoop.hdfs.util.ByteArrayManager:logDebugMessage():[DEBUG] {b.toString()}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$RMAppStateFileProcessor:processChildNode(java.lang.String,java.lang.String,byte[]):[DEBUG] Loading application attempt from node: {childNodeName}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:updateRuleSet(java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] PlacementManager active with new rule set
org.apache.hadoop.yarn.client.api.impl.TimelineWriter:doPosting(java.lang.Object,java.lang.String):[ERROR] Failed to get the response from the timeline server.
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:convertProtoToDeletionTask(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[DEBUG] Converting recovered DockerContainerDeletionTask
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest):[ERROR] YarnException(msg)
org.apache.hadoop.hdfs.DataStreamer:handleDatanodeReplacement():[WARN] Failed to replace datanode. Continue with the remaining datanodes since BEST_EFFORT_KEY is set to true.
org.apache.hadoop.hdfs.server.namenode.NameNode:startCommonServices(org.apache.hadoop.conf.Configuration):[ERROR] Unable to load NameNode plugins. Specified list of plugins: conf.get(DFS_NAMENODE_PLUGINS_KEY)
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Using fallback authenticator sequence.
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.files.SuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] Summary directory set to {reportDir}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread:run():[DEBUG] Interrupted while waiting to put on response queue
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processCommits(long):[ERROR] Can't sync for fileId: {}. Channel closed with writes pending
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown from onContainerStopped for Container {event.getContainerId()}, thr
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:writeAsync(org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx):[DEBUG] Scheduling write back task for fileId: ...
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeEmptyFile(java.lang.String):[DEBUG] Store empty file successfully. COS key: [{}], ETag: [{}].
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics):[ERROR] Cannot initialize driver for driverName
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:checkLimit(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$ProcessTreeInfo,long,long):[INFO] Removed ProcessTree with root {}, {pId}
org.apache.hadoop.hdfs.server.namenode.NameNode:stopCommonServices():[WARN] ServicePlugin {p} could not be stopped
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForNameserviceId(java.lang.String):[ERROR] Cannot get active NN for {}, State Store unavailable
org.apache.hadoop.yarn.service.component.Component:resetCompFailureCount():[INFO] [COMPONENT {}]: Reset container failure count from {} to 0.
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,boolean,boolean):[DEBUG] Aborting %s uploads
org.apache.hadoop.yarn.applications.distributedshell.Client:init(java.lang.String[]):[WARN] Can not set up custom log4j properties. +
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:startStorage():[INFO] Creating a new database at th path: {dbPath}
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:readNextRpcPacket():[DEBUG] unwrapping token of length: ...
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader:getEntityTypes(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[ERROR] IOException while listing entity types
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:getApplicationAttemptEntity(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Map):[DEBUG] Fields parameter is empty, defaulting to INFO
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler:getTaskReports(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsRequest):[INFO] Getting task report for <taskType> <jobId>. Report-size will be <tasks.size()>
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:onDeleteThrottled(com.amazonaws.services.s3.model.DeleteObjectsRequest):[INFO] Bulk delete {} keys throttled -first key = {}; last = {}
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[ERROR] Response from the timeline server is not successful, HTTP error code: ...
org.apache.hadoop.fs.s3a.S3AUtils:buildAWSProviderList(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,java.util.List,java.util.Set):[DEBUG] Validating AWS credential provider classes
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:createACLForUser(org.apache.hadoop.security.UserGroupInformation,int):[DEBUG] Creating ACL For , new UgiInfo(ugi)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing token master key to {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:assignContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits):[WARN] Node : ... does not have sufficient resource...
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Setting input paths
org.apache.hadoop.fs.s3a.S3AInstrumentation:close():[DEBUG] Unregistering metrics for {name}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:ahsRedirectPath(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp):[DEBUG] Error parsing {} as an ApplicationAttemptId
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer):[WARN] has a full queue and can't consume the given metrics.
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[INFO] The storage policy of [path] is unspecified
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:metaSave(java.io.PrintWriter):[WARN] {} is corrupt but has no associated node.
org.apache.hadoop.util.Shell:runCommand():[ERROR] Caught OutOfMemoryError. One possible reason is that ulimit setting of 'max user processes' is too low. If so, do 'ulimit -u <largerNum>' and try again.
org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC:getProxy(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration):[DEBUG] Creating a HadoopYarnProtoRpc proxy for protocol {}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Emitting job history data to the timeline service is enabled
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:setContainerState(org.apache.hadoop.yarn.service.api.records.ContainerState):[INFO] {} spec state state changed from {} -> {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS MKDIR dirHandle: dirHandle.dumpFileHandle() filename: fileName client: remoteAddress
org.apache.hadoop.hdfs.server.namenode.JournalSet:selectInputStreams(java.util.Collection,long,boolean,boolean):[INFO] Skipping jas + jas + since it's disabled
org.apache.hadoop.hdfs.qjournal.server.Journal:journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[]):[TRACE] Writing txid {firstTxnId}-{lastTxnId} ; journal id: {journalId}
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:updateState():[DEBUG] Received service state: <State> from HA namenode: <NamenodeDesc>
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlocksOnInService(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Invalidated {} extra redundancy blocks on {} after it is in service, numExtraRedundancy, srcNode
org.apache.hadoop.ipc.Server$Handler:run():[DEBUG] Served: [{}]
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:deleteTaskWorkingPathQuietly(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Delete working path
org.apache.hadoop.fs.s3a.S3AInputStream:validateRangeRequest(org.apache.hadoop.fs.FileRange):[WARN] Requested range [%d, %d) is beyond EOF for path %s
org.apache.hadoop.fs.s3a.S3AInstrumentation:close():[DEBUG] Shutting down metrics publisher
org.apache.hadoop.tools.rumen.TraceBuilder:run(java.lang.String[]):[WARN] TraceBuilder got an error while processing the [possibly virtual] file
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:initDefaultNameService(org.apache.hadoop.conf.Configuration):[WARN] Default name service is not set.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:moveBlockFiles(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File):[DEBUG] addFinalizedBlock: Moved replicaInfo.getMetadataURI() to dstmeta and replicaInfo.getBlockURI() to dstfile
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:sendContainerRequest():[DEBUG] Application {} sends out request for {} streams.
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createPersistentNode(java.lang.String):[DEBUG] nodePath + " znode already exists !!"
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:invalidateLocationCache(java.lang.String):[DEBUG] Location cache after invalidation: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:processor():[DEBUG] Write failure
org.apache.hadoop.metrics2.sink.KafkaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[DEBUG] Kafka topic {topic}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[ERROR] GPU is enabled on the NodeManager, but couldn't find any usable GPU devices, please double check configuration!
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[DEBUG] [STRESS] Cluster underloaded in run! Stressing...
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Deleted from target: files: {0} directories: {0}; skipped deletions {0}; deletions already missing {0}; failed deletes {0}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeIgnore(boolean,java.lang.String,java.io.IOException):[DEBUG] action, ex
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand):[DEBUG] no resource restrictions specified. not using docker's cgroup options
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,java.lang.String):[INFO] Container completed without partition
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$StatusUpdaterRunnable:run():[DEBUG] Node's resource is updated
org.apache.hadoop.hdfs.server.namenode.LeaseManager:removeLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,long):[WARN] Removing non-existent lease! holder={} src={}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkAllVolumes(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi):[INFO] Scheduled health check for volume {}
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:aboutToComplete(java.lang.String,java.util.List,long,org.apache.hadoop.fs.statistics.IOStatistics):[INFO] Uncommitted data pending to file {} commit metadata for {} parts in {}. size: {} byte(s)
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:prepareListing(org.apache.hadoop.fs.Path):[DEBUG] Traversing into source dir: {}
org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean):[DEBUG] Server connection from + connection + ; # active connections: + size() + ; # queued calls: + callQueue.size()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:loadConversionRules(java.lang.String):[INFO] Conversion rules file is not defined, + using default conversion config!
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] NameNode safe mode checked
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[DEBUG] Validity of parameters checked
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:removeAclEntries(java.lang.String,java.util.List):[INFO] Operation check performed
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:doRecovery():[DEBUG] Performing recovery in + latestNameSD + and + latestEditsSD
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent):[INFO] Got event [event.getType()] for appId [event.getApplicationID()]
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:isSecurityEnabled():[DEBUG] Failed to get security status.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:getResources():[INFO] ApplicationMaster is out of sync with ResourceManager, hence resync and send outstanding requests.
org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE):[DEBUG] Statistics of the operation evaluated
org.apache.hadoop.yarn.util.FSDownload:changePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] Changing permissions for path {} to perm {}
org.apache.hadoop.hdfs.DataStreamer:waitForAckedSeqno(long):[DEBUG] Closed channel exception
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRename(java.lang.String,java.lang.String,long,boolean):[DEBUG] Logging RPC IDs
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:putAll(java.util.List,boolean,boolean):[ERROR] Cannot write {}
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:run():[INFO] Launching master[APP_ID]
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Output Path is null in commitTask()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:updateContainerAndNMToken(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerUpdateType):[ERROR] Error trying to assign container token and NM token to an updated container
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] Hdfs listStatus call invoked
org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:add(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[DEBUG] BLOCK* ExcessRedundancyMap.add({}, {})
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumResourceCapability(java.lang.String):[ERROR] Ambiguous queue reference: + queueName + please use full queue path instead.
org.apache.hadoop.fs.s3a.Invoker:retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.util.functional.CallableRaisingIOE):[WARN] {}: exception in retry processing
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:execute(org.apache.commons.cli.CommandLine):[ERROR] Errors while recording the output of plan command.
org.apache.hadoop.yarn.server.router.RouterServerUtil:logAndThrowException(java.lang.String,java.lang.Throwable):[ERROR] {errMsg}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean):[DEBUG] Storing ZKDTSMDelegationKey_ + key.getKeyId()
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:disableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.DisableNameserviceRequest):[ERROR] Unable to disable Nameservice {}, nsId
org.apache.hadoop.hdfs.server.sps.ExternalSPSFilePathCollector:processPath(java.lang.Long,java.lang.String):[DEBUG] The scanning start dir/sub dir + childPath + does not have childrens.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:initializeClient(java.net.URI,java.lang.String,java.lang.String,boolean):[TRACE] Initializing AbfsClient for {}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica):[TRACE] this + : replica refCount + (replica.refCount - 1) + -> + replica.refCount + StringUtils.getStackTrace(Thread.currentThread())
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:checkDirs():[WARN] Directory {dir} error {dirsFailedCheck.get(dir).message}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:postDelegationTokenExpiration(javax.servlet.http.HttpServletRequest):[ERROR] Authorization failed due to YarnException
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForAndGetNameNodeProperties(java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.slf4j.Logger):[DEBUG] NameNode host information not yet available
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:validateIntegrityAndSetLength(java.io.File,long):[WARN] Getting exception while validating integrity and setting length for blockFile
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[ERROR] FSError from child ...
org.apache.hadoop.tools.DistCh$ChangeFilesMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[INFO] FAIL: value, StringUtils.stringifyException(e)
org.apache.hadoop.yarn.service.webapp.ApiServer:stopService(java.lang.String,boolean,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {} doesn't exist
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[INFO] Successfully warmed up {} EDEKs.
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(byte[],int,int):[DEBUG] read requested b = null offset = {}, len = {}
org.apache.hadoop.fs.cosn.CosNFileSystem:validatePath(org.apache.hadoop.fs.Path):[DEBUG] The Path: [{}] does not exist.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:processTimelineResponseErrors(org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse):[DEBUG] Timeline entities are successfully put
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:processSummationMajorCompaction(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter,long):[TRACE] reading flow app id sum=
org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean):[WARN] Error reading the stream
org.apache.hadoop.mapred.IndexCache:removeMap(java.lang.String):[WARN] Map ID + mapId + not found in queue!!
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:replaceLabelsOnNode(java.util.Map):[ERROR] NODE_LABELS_NOT_ENABLED_ERR
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:resolveOOM(java.util.concurrent.ExecutorService):[DEBUG] OOM resolved successfully
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:setReadBufferManagerConfigs(int):[DEBUG] ReadBufferManager not initialized yet. Overriding readAheadBlockSize as {}
org.apache.hadoop.hdfs.server.federation.router.ErasureCoding:setErasureCodingPolicy(java.lang.String,java.lang.String):[INFO] Checked operation
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentEventHandler:handle(org.apache.hadoop.yarn.service.component.ComponentEvent):[ERROR] No component exists for {event.getName()}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:retrieveBlock(java.lang.String,long,long):[ERROR] Retrieving key [%s] with byteRangeStart [%d] occurs an CosServiceException: [%s].
org.apache.hadoop.hdfs.DFSInputStream:readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock):[DEBUG] Failed to getReplicaVisibleLength from datanode {} for block {}
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManagerImpl:loadProfiles():[INFO] Added profile 'profileName' with resources: resource
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + but entity not found + (Took + (Time.monotonicNow() - startTime) + ms.)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:addVolume(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] Caught exception when adding fsVolume. Will throw later.
org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI):[TRACE] we can read the local file.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Cannot find reserved container map.
org.apache.hadoop.mapred.LocalJobRunner$Job:createMapExecutor():[DEBUG] Starting mapper thread pool executor.
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using match all for 'host' and READ_ONLY
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getUnderReplicatedBlocks():[DEBUG] Failed to get number of blocks under replicated
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:openDatabase(org.apache.hadoop.conf.Configuration):[INFO] Creating state database at {dbfile}
org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[]):[DEBUG] Properties: {cf}
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:run():[WARN] Received unknown event-type [eventType]. Ignoring.
org.apache.hadoop.fs.s3a.S3AUtils:longOption(org.apache.hadoop.conf.Configuration,java.lang.String,long,long):[DEBUG] Value of {} is {}
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:proxyLink(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.net.URI,javax.servlet.http.Cookie,java.lang.String,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] REQ HEADER: {} : {}
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable):[DEBUG] Proxy for + uri + failed. cause: , cause
org.apache.hadoop.tools.rumen.ZombieJob:getInputSplits():[WARN] TaskType for a MapTask is not Map. task=... type=...
org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean):[INFO] Configuring zookeeper to use {} as the server principal, zkPrincipal
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:parseSummaryLogs():[DEBUG] Try to parse summary log for log {} in {}
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[INFO] File not found, creating parent directories
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute():[ERROR] {keyName} has not been deleted.
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getMaxChunksIdeal(org.apache.hadoop.conf.Configuration):[WARN] DistCpConstants.CONF_LABEL_MAX_CHUNKS_IDEAL + should be positive. Fall back to default value: + DistCpConstants.MAX_CHUNKS_IDEAL_DEFAULT
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot remove records {clazz} query {query}
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:load(java.io.File):[INFO] Loading image file curFile using compression
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyNames():[DEBUG] Exception in getkeyNames., e
org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]):[DEBUG] Native library check initiated
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[ERROR] Exception encountered while processing heart beat for attemptId, ex
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[ERROR] Task final state is not FAILED or KILLED: + finalState
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:refreshCallQueue():[INFO] Refresh call queue successfully for {hostport}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Removed node address cluster capacity: resource
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] Change nextOffset (after trim) to {}
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[INFO] START STRESS @ <timestamp>
org.apache.hadoop.yarn.security.NMTokenIdentifier:readFields(java.io.DataInput):[WARN] Recovering old formatted token
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Setting file size is not supported when setattr, fileId: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:writeCredentials(org.apache.hadoop.fs.Path):[INFO] Writing credentials to the nmPrivate file
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:checkDirs():[INFO] Directory {dir} passed disk check, adding to list of valid directories.
org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger:logAuditEvent(org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$OpStatus,org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$AuditEvent):[DEBUG] Log an audit event
org.apache.hadoop.fs.s3a.impl.ChangeTracker:processNewRevision(java.lang.String,java.lang.String,long):[DEBUG] Setting revision ID for object at {}: {}
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSAppend operation executed
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean):[INFO] Unregistering slot because the requestShortCircuitFdsForRead operation failed.
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration):[WARN] Failed to create [dirStrings[i]]
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,int,java.lang.String,boolean):[ERROR] prefix: [%s], delimiter: [%s], maxListingLength: [%d], priorLastKey: [%s]. List objects occur an exception: [%s].
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:initExisting():[INFO] Existing job initialization finished. + loadedPercent + % of cache is occupied.
org.apache.hadoop.yarn.applications.distributedshell.Client:prepareTimelineDomain():[ERROR] Error when putting the timeline domain
org.apache.hadoop.fs.azurebfs.services.AbfsLease:free():[DEBUG] Freed lease {} on {}
org.apache.hadoop.fs.FileUtil:readLink(java.io.File):[WARN] Can not read a null symLink
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:manageWriterOsCache(long):[WARN] Slow manageWriterOsCache took {duration}ms (threshold={datanodeSlowLogThresholdMs}ms), volume={getVolumeBaseUri()}, blockId={replicaInfo.getBlockId()}
org.apache.hadoop.hdfs.LocatedBlocksRefresher:run():[DEBUG] Running refresh for {} streams
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than 1.0 but should be in the range 0.0 - 1.0
org.apache.hadoop.mapred.nativetask.NativeBatchProcessor:create(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.nativetask.DataChannel):[INFO] NativeHandler: direct buffer size: + bufferSize
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:serviceStart():[INFO] NMCollectorService started at
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:updateSchedulingMonitors(org.apache.hadoop.conf.Configuration,boolean):[WARN] Failed to find class of specified policy=...
org.apache.hadoop.yarn.util.resource.ResourceUtils:addMandatoryResources(java.util.Map):[DEBUG] Adding resource type - name = MEMORY, units = ResourceInformation.MEMORY_MB.getUnits(), type = ResourceTypes.COUNTABLE
org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory:getCluster(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Cluster URI : {}
org.apache.hadoop.tools.mapred.CopyCommitter:listTargetFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Scanning destination directory {} with thread count: {}
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:pullFromZK(boolean):[ERROR] An unexpected exception occurred while pulling data from ZooKeeper
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:reserve(org.apache.hadoop.hdfs.ExtendedBlockId,long):[WARN] {e.getMessage()}
org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper():[DEBUG] waitForZKConnectionEvent completed
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl):[WARN] Unknown method {methodName} called on {connectionProtocolName} protocol.
org.apache.hadoop.fs.s3a.select.SelectBinding:executeSelect(org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.conf.Configuration,com.amazonaws.services.s3.model.SelectObjectContentRequest):[INFO] Issuing SQL request {expression}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveXAttrs(java.lang.String,java.util.List,boolean):[INFO] logEdit executed
org.apache.hadoop.hdfs.server.namenode.NameNode:startCommonServices(org.apache.hadoop.conf.Configuration):[INFO] getRole() + " service RPC up at: " + rpcServer.getServiceRpcAddress()
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] JDK performed authentication on our behalf.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[],boolean):[INFO] Audit log for operation addErasureCodingPolicies
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[DEBUG] setting webapp host class to {}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:init(java.lang.String):[ERROR] FederationPolicyInitializationException error message
org.apache.hadoop.hdfs.server.common.JspHelper:getUGI(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] getUGI is returning: ugi.getShortUserName()
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:updateHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[ERROR] Trying to updateHistory non-existing resource skylines for {recurrenceId}
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:map(org.apache.hadoop.io.NullWritable,org.apache.hadoop.mapred.gridmix.GridmixRecord,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Error in resource usage emulation! Message:
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:httpServerTemplateForRM(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String):[INFO] Starting Web-server for name at: uri
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:startSpill():[INFO] kvstart= + kvstart + ( + (kvstart * 4) + ); kvend= + kvend + ( + (kvend * 4) + ); length= + (distanceTo(kvend, kvstart, kvmeta.capacity()) + 1) + / + maxRec
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:replaceLabelsOnNode(java.util.Map):[INFO] No Modified Node label Mapping to replace
org.apache.hadoop.mapreduce.lib.output.NamedCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Using PathOutputCommitter implementation {}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] attemptId + transitioned from state + oldState + to + getInternalState() + , event type is + event.getType() + and nodeId= + nodeId
org.apache.hadoop.mapred.ReduceTask:runOldReducer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapred.RawKeyValueIterator,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class):[INFO] Processing records with reducer
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:processConf():[ERROR] {} must be greater than zero. Defaulting to {}
org.apache.hadoop.mapred.MapTask:updateJobWithSplit(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.InputSplit):[INFO] Processing split: + inputSplit
org.apache.hadoop.streaming.StreamJob:parseArgv():[WARN] -jobconf option is deprecated, please use -D instead.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Headroom calculation for user {}: userLimit={} queueMaxAvailRes={} consumed={} partition={}
org.apache.hadoop.hdfs.server.namenode.JournalSet:finalizeLogSegment(long,long):[INFO] Log segment finalized
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateNodeAttributesIfNecessary(org.apache.hadoop.yarn.api.records.NodeId,java.util.Set):[DEBUG] Skip updating node attributes since there is no change for {} : {}
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] MinReplicationToBeInMaintenance is set to zero. {node} is put in maintenance state immediately.
org.apache.hadoop.tools.HadoopArchiveLogs:prepareWorkingDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[INFO] Existing Working Dir detected: - FORCE_OPTION not specified -> exiting
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:getRecoveredMappableBlock(java.io.File,java.lang.String,byte):[INFO] Recovering persistent memory cache for block {}, path = {}, address = {}, length = {}
org.apache.hadoop.yarn.service.webapp.ApiServer:createService(javax.servlet.http.HttpServletRequest,org.apache.hadoop.yarn.service.api.records.Service):[INFO] POST: createService = {} user = {}
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:addHealthReporter(org.apache.hadoop.service.Service):[DEBUG] Omitting duplicate service: {}.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: someClass
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread:enqueue(java.lang.Runnable):[INFO] Block report queue is full
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:setConf(org.apache.hadoop.conf.Configuration):[INFO] Available space volume choosing policy initialized: {threshold_key} = {balancedSpaceThreshold}, {preference_fraction_key} = {balancedPreferencePercent}
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:checkAddLabelsToNode(java.util.Map):[ERROR] %d labels specified on host=%s after add labels...
org.apache.hadoop.mapred.JobConf:setMaxPhysicalMemoryForTask(long):[WARN] The API setMaxPhysicalMemoryForTask() is deprecated. The value set is ignored. Refer to setMemoryForMapTask() and setMemoryForReduceTask() for details.
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:fetchLatestDoc(org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType,java.lang.String,java.lang.StringBuilder):[DEBUG] No previous Document found with id : {documentId} for Collection : {collectionType.getCollectionName()} under Database : {databaseName}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:checkRemoveParentZnode(java.lang.String,int):[DEBUG] No leaf znode exists. Removing parent node {}, parentZnode
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean,long):[DEBUG] RenameSnapshotOp created
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:checkSaslComplete(org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant,java.util.Map):[DEBUG] Verifying QOP, requested QOP = {requestedQop}, negotiated QOP = {negotiatedQop}
org.apache.hadoop.fs.s3a.S3AInputStream:seekInStream(long,long):[DEBUG] Now at {pos}: bytes remaining in current request: {remainingInCurrentRequest()}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:processSummationMajorCompaction(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter,long):[INFO] After major compaction for qualifier= with currentColumnCells.size= returning finalCells.size= with zero sum=
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable):[TRACE] {}: found waitable for {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[INFO] Multi-node update recording started
org.apache.hadoop.hdfs.server.namenode.FSImage:format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,boolean):[INFO] Allocated new BlockPoolId: [value from ns.getBlockPoolID()]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaResourcePlugin:createFpgaVendorPlugin(org.apache.hadoop.conf.Configuration):[INFO] Using FPGA vendor plugin: + vendorPluginClass
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getBlockLocations(java.lang.String,java.lang.String,long,long):[INFO] Successfully got block locations
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker:getNodesByResourceName(java.lang.String):[INFO] Could not find a node matching given resourceName
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown in handler for event [ROLLBACK_LAST_REINIT] for Container
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseEvenlyFromRemainingRacks(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,int,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException):[TRACE] Chosen nodes: {}
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocateContainers(org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest,java.util.List,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,long,java.lang.String):[INFO] Not allocating more containers as we have reached max allocations per AM heartbeat
org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl:run():[DEBUG] Checking state of job
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:initialize(org.apache.hadoop.conf.Configuration):[INFO] Initializing Reservation system
org.apache.hadoop.ipc.Server$RpcCall:sendDeferedResponse():[ERROR] Failed to send deferred response. ThreadName= + Thread.currentThread().getName() + , CallId= + callId + , hostname= + getHostAddress()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for dir fileId: {}
org.apache.hadoop.registry.server.dns.RegistryDNS:addAnswer(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int,int,int):[INFO] found local record? false
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error while storing reservation allocation., e
org.apache.hadoop.hdfs.server.namenode.FSImage:doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[ERROR] Failed to move aside pre-upgrade storage in image directory ...
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long):[DEBUG] Ignoring counter increment for unknown counter {}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:build():[TRACE] {}: returning new block reader local.
org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String):[INFO] VM type = {vmBit}-bit
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory:init(int,int,org.slf4j.Logger):[INFO] SkipList is disabled
org.apache.hadoop.hdfs.server.datanode.BPOfferService:updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat):[INFO] Namenode actor taking over ACTIVE state from bpServiceToActive at higher txid= txid
org.apache.hadoop.tools.RegexCopyFilter:initialize():[ERROR] An error occurred while attempting to read from
org.apache.hadoop.tools.util.ProducerConsumer:shutdown():[WARN] Shutdown() is called but there are still unprocessed work!
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumApplicationLifetime(java.lang.String):[ERROR] Unknown queue: + queueName
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read():[ERROR] Encountered Storage Exception for read on Blob : {key} Exception details: {e} Error Code : {((StorageException) innerException).getErrorCode()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_INIT
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:scanForLogs():[DEBUG] scanForLogs on {statAttempt.getPath().getName()}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:reAttachUAMAndMergeRegisterResponse(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Found {} existing UAMs for application {} in Yarn Registry. Reattaching in parallel
org.apache.hadoop.fs.azure.NativeAzureFileSystem:performStickyBitCheckForRenameOperation(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Source {} doesn't exist. Failing rename.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:finish():[INFO] Application completed. Signalling finished to RM
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createReservationSystem():[INFO] Using ReservationSystem: reservationClassName
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:setWorkPath(org.apache.hadoop.fs.Path):[DEBUG] Setting work path to {workPath}
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger:merge(java.util.List):[INFO] OnDiskMerger: We have {inputs.size()} map outputs on disk. Triggering merge...
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationFinishData):[INFO] Finish information of application + appFinish.getApplicationId() + is written
org.apache.hadoop.hdfs.DFSInputStream:readWithStrategy(org.apache.hadoop.hdfs.ReaderStrategy):[WARN] DFS Read
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:savePools(java.io.DataOutputStream,java.lang.String):[INFO] Begin step for saving cache pools
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Region for endpoint {endpoint}, URI {epr} is determined as {region}
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String):[DEBUG] Token not set, looking for delegation token. Creds:{}, size:{}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] List status for path: {path}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:throttle():[DEBUG] Throttling re-encryption, sleeping for {} ms
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] Key type not supported. Cannot find serializer for
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] Loading standard ssl config
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser:run():[DEBUG] Begin parsing summary logs.
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppLogInitFailTransition:transition(java.lang.Object,java.lang.Object):[WARN] "Log Aggregation service failed to initialize, there will be no logs for this application"
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateToken(com.nimbusds.jwt.SignedJWT):[INFO] Expiration validation failed.
org.apache.hadoop.hdfs.DFSClient:addNodeToDeadNodeDetector(org.apache.hadoop.hdfs.DFSInputStream,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] DeadNode detection is not enabled, skip to add node {}., datanodeInfo
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:initReplicaRecoveryImpl(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long):[INFO] initReplicaRecovery: update recovery id for + block + from + oldRecoveryID + to + recoveryId
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[DEBUG] Updating SPS service status, current mode:{}, new mode:{}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_STOP
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:setEquator(int):[INFO] (EQUATOR) {pos} kvi {kvindex} ({kvindex * 4})
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:evictLazyPersistBlocks(long):[INFO] Ignoring exception
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[INFO] assignedContainer queue= getQueuePath() usedCapacity= getUsedCapacity() absoluteUsedCapacity= getAbsoluteUsedCapacity() used= queueUsage.getUsed() cluster= cluster
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:notLeader():[INFO] rmId + " did not transition to standby successfully."
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:run():[TRACE] {}: thread starting.
org.apache.hadoop.fs.s3a.s3guard.S3Guard:checkNoS3Guard(java.net.URI,org.apache.hadoop.conf.Configuration):[WARN] Ignoring S3Guard store option of S3GUARD_METASTORE_LOCAL -no longer needed or supported. Origin {origin}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] SASL server skipping handshake in unsecured configuration for peer = {}, datanodeId = {}
org.apache.hadoop.http.HttpServer2$Builder:build():[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:setConfig(java.lang.Object):[DEBUG] Default rule instantiated with default queue name: {}, and create flag: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:run():[INFO] Public cache exiting
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:shutdown():[INFO] All async data service threads have been shut down
org.apache.hadoop.fs.s3a.S3AFileSystem:operationRetried(java.lang.Exception):[DEBUG] Operation retried debug log
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:rebootNodeStatusUpdaterAndRegisterWithRM():[INFO] NodeStatusUpdater thread is reRegistered and restarted
org.apache.hadoop.mapred.lib.MultipleOutputs:close():[WARN] Closing is Interrupted
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitShm(java.lang.String):[INFO] cliID: ..., src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: n/a, srvID: ..., success: false
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:setAcl(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] setAcl filesystem: {} path: {} aclspec: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getSubdirEntries():[TRACE] getSubdirEntries({}, {}): no entries found in {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getTimelineStoresFromCacheIds(java.util.Set,java.lang.String,java.util.List):[DEBUG] Using summary store for {}
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager$CollectorTokenRenewer:run():[WARN] Unable to renew/regenerate token for appId
org.apache.hadoop.fs.cosn.CosNOutputStream:uploadPart():[DEBUG] Thread.currentThread().getName() is uploading a part.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:parseSummaryLogs():[INFO] {} state is UNKNOWN and logs are stale, assuming COMPLETED
org.apache.hadoop.hdfs.tools.DFSAdmin:shutdownDatanode(java.lang.String[],int):[DEBUG] Print usage for -shutdownDatanode
org.apache.hadoop.fs.s3a.Invoker:ignoreIOExceptions(org.slf4j.Logger,java.lang.String,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[INFO] Operation executed with action and path
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseLists():[INFO] Blacklisted {pattern}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:tryToCreateExternalBlockReader():[TRACE] {}: No ReplicaAccessor created by {}
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[INFO] Attempting to get socket address for target.
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSHomeDir operation executed
org.apache.hadoop.yarn.server.timeline.EntityCacheItem:refreshCache(org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager,org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStoreMetrics):[DEBUG] Cache new enough, skip refreshing
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:readNextPacket():[TRACE] DFSClient readNextPacket got header {}
org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader:getProcessId(org.apache.hadoop.fs.Path):[DEBUG] Got pid {} from path {}
org.apache.hadoop.io.nativeio.NativeIO:getOperatingSystemPageSize():[WARN] Unable to get operating system page size. Guessing 4096.
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopTimer():[WARN] {prefix} metrics system timer already stopped!
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveXAttrs(java.lang.String,java.util.List,boolean):[INFO] logRpcIds executed
org.apache.hadoop.fs.Hdfs:renewDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Renewing delegation token
org.apache.hadoop.hdfs.tools.DFSck:getCurrentNamenodeAddress(org.apache.hadoop.fs.Path):[ERROR] FileSystem is {fs.getUri()}
org.apache.hadoop.hdfs.protocol.ClientProtocol:rollEdits():[INFO] Rolled edit log using RouterRpcServer
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Trying to preempt following containers to make reserved container={} on node={} can be allocated:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Failed to submit application to parent-queue: + getParent().getQueuePath(), ace
org.apache.hadoop.fs.s3a.tools.MarkerTool:execute(org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanArgs):[INFO] Authoritative path list is "..."
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:copyPlacementQueueToSubmissionContext(org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext):[INFO] Placed application with ID <ApplicationId> in queue: <PlacementQueue>, original submission queue was: <OriginalQueue>
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:leaveSafeMode(boolean):[INFO] STATE* Safe mode is already OFF
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker:getNodeIdsByResourceName(java.lang.String):[INFO] Could not find a node matching given resourceName RESOURCE_NAME
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getAvailable():[WARN] Volume {} has less than 0 available space
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getApplicationsHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationsHomeSubClusterRequest):[ERROR] Cannot get apps: {Exception message}
org.apache.hadoop.yarn.service.utils.ServiceUtils:putAllJars(java.util.Map,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path,java.lang.String,java.lang.String):[DEBUG] File does not exist, skipping: [name of the jar file]
org.apache.hadoop.fs.s3a.S3AFileSystem:putObjectDirect(com.amazonaws.services.s3.model.PutObjectRequest,org.apache.hadoop.fs.s3a.impl.PutObjectOptions,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[DEBUG] PUT {} bytes to {}
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionSave(java.lang.String,java.lang.String,java.lang.Long,java.lang.String):[ERROR] Fail to save application: , e
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseArguments(java.lang.String[]):[WARN] Unexpected parameters
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:handleVolumeFailures(java.util.Set):[DEBUG] Caught exception when obtaining reference count on closed volume
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:internalSignalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest,java.lang.String):[INFO] containerId signal request request.getCommand() by sentBy
org.apache.hadoop.hdfs.NameNodeProxiesClient:createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean):[WARN] Currently creating proxy using LossyRetryInvocationHandler requires NN HA setup
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController$BatchBuilder:commitBatchToTempFile():[WARN] Failed to create or write to temporary file in dir: + tmpDirPath
org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean):[INFO] Unexpected error occurred
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionDestroy(java.lang.String):[ERROR] Fail to destroy application:
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] Waiting on availability of NameNode information at %s
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:commit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit,java.lang.String):[WARN] Failed to commit upload against unknown destination: {}
org.apache.hadoop.fs.s3a.S3AFileSystem:removeKeys(java.util.List,boolean):[DEBUG] Deleting %d keys
org.apache.hadoop.yarn.service.client.ServiceClient:addKeytabResourceIfSecure(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Adding service.getName()'s keytab for localization, uri = keytabOnhdfs
org.apache.hadoop.tools.SimpleCopyListing$FileStatusProcessor:processItem(org.apache.hadoop.tools.util.WorkRequest):[DEBUG] Interrupted while sleeping in exponential backoff.
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:check(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result):[INFO] {path} <symlink>
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:startContainerAsync(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.api.records.ContainerLaunchContext):[WARN] Exception when scheduling the event of starting Container + container.getId()
org.apache.hadoop.metrics2.sink.KafkaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[DEBUG] Kafka brokers: {brokerList}
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader:nextKeyValue():[INFO] Trying to initialize the next record reader
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:createAppDir(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.UserGroupInformation):[ERROR] Failed to setup application log directory for [appId]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setOwner(java.lang.String,java.lang.String,java.lang.String):[WARN] Audit Event: setOwner failed for src
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:getRedirectedUrl():[DEBUG] Decoding FileEncryptionInfo
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:parse():[WARN] "Bad element in allocations file: " + tagName
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore$EntityDeletionThread:run():[INFO] Deletion thread received interrupt, exiting
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:trimWrite(int):[DEBUG] Trim write request by delta: " + delta + " " + toString()
org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Thread {} threw an exception: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:parseOutput(java.lang.String):[INFO] Parsing output: {output}
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext):[INFO] Total input files to process : ...
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[ERROR] {}: commit of task {} failed
org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient):[WARN] Logout failed while disconnecting, error code -
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:remove(org.apache.hadoop.net.Node):[DEBUG] removing node {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:fromXml(org.w3c.dom.Element,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Creating new rule: {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] Using remote NameNode with RPC address: %s
org.apache.hadoop.streaming.PipeMapper:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Configuring job jar
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open(org.apache.hadoop.fs.Path,java.util.Optional):[INFO] File opened for read
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:getPlanQueue(java.lang.String):[ERROR] The queue (planQueueName) cannot be found or is not a ParentQueue
org.apache.hadoop.hdfs.DistributedFileSystem:getTrashRoot(org.apache.hadoop.fs.Path):[WARN] Exception in checking the encryption zone for the path
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersion(java.lang.String):[DEBUG] Getting key with version name {}.
org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Drop the response. Current retryCount == {retryCount}
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:checkAndDeleteCgroup(java.io.File):[WARN] Failed attempt to delete cgroup: + cgf
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier):[DEBUG] Check the condition for main loop.
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:printConfiguredHosts(boolean):[DEBUG] exclude:
org.apache.hadoop.hdfs.DFSInputStream:actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to [datanode]
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$PollTimerTask:run():[DEBUG] remove {} {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumResourceCapability(java.lang.String):[ERROR] queue + queueName + is not an leaf queue
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue):[ERROR] In the current state, queue + getName() + has + children.size() + but the new state has none!
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:getBlockPathInfo(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.security.token.Token,boolean,org.apache.hadoop.fs.StorageType):[DEBUG] Cached location of block {} as {}, blk, pathinfo
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String):[WARN] Disk error on + dnName + : + msg
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink):[WARN] some warn message here
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForBlockPoolId(java.lang.String):[ERROR] Cannot get active NN for {}, State Store unavailable
org.apache.hadoop.fs.s3a.tools.MarkerTool:purgeMarkers(org.apache.hadoop.fs.s3a.impl.DirMarkerTracker,int):[INFO] Deleting markers
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$CompactionTimerTask:run():[INFO] Starting full compaction cycle
org.apache.hadoop.fs.azure.BlockBlobAppendStream:writeBlockListRequestInternal():[DEBUG] Upload block list took {} ms for blob {}
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:loadState():[INFO] Loading timeline service state from leveldb
org.apache.hadoop.nfs.nfs3.Nfs3Interface:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] FSSTAT operation initiated
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path):[WARN] Either jobs are running concurrently or failed jobs are not being cleaned up
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:cleanupStagingDirs():[DEBUG] cleanup job directory Path
org.apache.hadoop.yarn.service.client.ServiceClient:buildCommandLine(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean):[DEBUG] AM launch command: {}
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:savePools(java.io.DataOutputStream,java.lang.String):[INFO] Set total cache pools
org.apache.hadoop.examples.terasort.TeraScheduler:main(java.lang.String[]):[INFO] done
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStart():[INFO] Scheduling reloading auxiliary services manifest file at interval {manifestReloadInterval} ms
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:addConstraintToMap(java.util.Map,java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint,boolean):[INFO] Constraint {} will not be added. There is already a constraint associated with tag {}., placementConstraint, sourceTag
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long):[DEBUG] Updating token {}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumNameservices():[ERROR] Cannot fetch number of expired registrations from the store: {e.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] ACLs checked
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Attempting to service {method.getName()} using proxy {current.proxyInfo}
org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress):[DEBUG] IP address returned for FQDN detected: {}
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:tryDatanode(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] write to {}: {}, block={}, datanode, Op.BLOCK_CHECKSUM, block
org.apache.hadoop.hdfs.server.namenode.FSImage:renameImageFileInDir(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,boolean):[DEBUG] renaming [fromFile.getAbsolutePath()] to [toFile.getAbsolutePath()]
org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Thread {} threw an error: {}. Shutting down Halting due to Out Of Memory Error...
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStop():[INFO] Manifest filesystem closed
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:scanActiveLogs(org.apache.hadoop.fs.Path):[WARN] Ignoring unexpected file in active directory {stat.getPath()}
org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext:isSchedulerReadyForAllocatingContainers():[INFO] Scheduler recovery is done. Start allocating new containers.
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:getProxyInternal(boolean):[ERROR] Exception while trying to create proxy to the ResourceManager for SubClusterId: ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Updating RMDelegationToken and SequenceNumber
org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text):[DEBUG] Cannot find class for token kind {}
org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Binding {} to {}
org.apache.hadoop.hdfs.server.federation.router.PeriodicService:serviceStart():[INFO] Starting periodic service {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Queue x already has n applications, cannot accept submission of application: y
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:render():[ERROR] Cannot render ResourceManager
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Sorry, no counters for nonexistent task
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getNodeResources(org.apache.hadoop.conf.Configuration):[DEBUG] Set vcores to {}
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session):[INFO] session.getRemoteAddress().getHostString() connected!
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] HttpRequestFailure: {}, {}
org.apache.hadoop.mapred.lib.MultithreadedMapRunner:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Configuring jobConf name to use numberOfThreads threads
org.apache.hadoop.yarn.service.client.ServiceClient:deleteZKNode(java.lang.String):[INFO] Service ' + serviceName + ' doesn't exist at ZK path: + zkPath
org.apache.hadoop.hdfs.server.namenode.FSImage:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int):[INFO] Start checkpoint at txid X<LastWrittenTxId>
org.apache.hadoop.examples.pi.DistSum$Machine:compute(org.apache.hadoop.examples.pi.math.Summation,org.apache.hadoop.mapreduce.TaskInputOutputContext):[INFO] sigma=...
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[ERROR] Unable to shutdown executor service after timeout {} {}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processCheckpoints(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker):[WARN] Failed to update re-encrypted progress to xattr for zone ..., ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS RMDIR dir fileHandle: {dirHandle.dumpFileHandle()} fileName: {fileName} client: {remoteAddress}
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadBlockPoolSliceStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[WARN] Failed to upgrade storage directory {} for block pool {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Creating ProcfsBasedProcessTree for process <PROCESS_ID>
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromHadoopCommon(java.util.function.Function,java.lang.String):[INFO] Checking script {}:
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction):[INFO] rollingUpgrade UNKNOWN_ACTION
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:refreshHostsReader(org.apache.hadoop.conf.Configuration,boolean,java.lang.Integer):[INFO] hostsReader include:{ + StringUtils.join(,, hostsReader.getHosts()) + } exclude:{ + StringUtils.join(,, hostsReader.getExcludedHosts()) + }
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:serviceStart():[INFO] New instance created
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlows(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ${url} (Took ${latency} ms.)
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[WARN] Unable to warm up EDEKs.
org.apache.hadoop.hdfs.StripeReader:readToBuffer(org.apache.hadoop.hdfs.BlockReader,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.ByteBufferStrategy,org.apache.hadoop.hdfs.protocol.ExtendedBlock):[WARN] Exception while reading from ...
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean):[DEBUG] Removing ZKDTSMDelegationToken_{ident.getSequenceNumber()}
org.apache.hadoop.mapred.TaskAttemptListenerImpl:commitPending(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[INFO] Commit-pending state update from + taskAttemptID.toString()
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] Preparing file for truncate
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:processFinishedContainer(org.apache.hadoop.yarn.api.records.ContainerStatus):[ERROR] Container complete event for unknown container {containerId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Rolling back Container reInitialization for []
org.apache.hadoop.hdfs.server.namenode.NameNode:checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[WARN] Allowing manual HA control from + Server.getRemoteAddress() + even though automatic HA is enabled, because the user + specified the force flag
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute():[INFO] Credential <alias> has NOT been deleted.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processOverWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.security.IdMappingServiceProvider):[DEBUG] Process perfectOverWrite
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:methodAction(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP):[INFO] {} is accessing unchecked {}, remoteUser, toFetch
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[DEBUG] Write lock released
org.apache.hadoop.mapred.lib.MultipleOutputs:close():[ERROR] Thread <X> failed unexpectedly
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[DEBUG] Request for appInfo of unknown attempt {}
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:analyzeFileState(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[]):[DEBUG] BLOCK* NameSystem.allocateBlock: handling block allocation writing to a file with a complete previous block: src= /* src from code */ lastBlock= /* lastBlockInFile from code */
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[INFO] Resource + rsrc.getLocalPath() + is missing, localizing it again
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[DEBUG] Read block from receiver
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendFinishedEvents():[INFO] Scheduler container completed
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Cpu usage <CPU_USAGE>
org.apache.hadoop.fs.azure.NativeAzureFileSystem:performStickyBitCheckForRenameOperation(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Path {} doesn't exist, failing rename.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[ERROR] Initialization failed for this . Exiting.
org.apache.hadoop.fs.FSInputChecker:read(long,byte[],int,int):[DEBUG] Attempted read operation; reached end of file or no bytes read
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:heartbeat():[DEBUG] Before Scheduling:
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Storing reservation allocation. + reservationEvent.getReservationIdName()
org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager):[DEBUG] Created SASL server with mechanism = mechanism_value
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources():[DEBUG] Stopping metrics source {source_key}: class={source_class}
org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing AMS Processing Chain
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool():[WARN] Unable to find stream starting with ...
org.apache.hadoop.crypto.key.kms.server.KMS:rolloverKey(java.lang.String,java.util.Map):[TRACE] Exiting rolloverKey Method.
org.apache.hadoop.yarn.client.cli.RMAdminCLI:handleReplaceLabelsOnNodes(java.lang.String[],java.lang.String,boolean):[ERROR] NO_MAPPING_ERR_MSG
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:redo():[INFO] Unable to delete source folder during folder rename redo. If the source folder is already gone, this is not an error condition. Continuing with redo.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Attempt num: ... is last retry: true because the staging dir doesn't exist.
org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeDescriptorsScriptRunner:run():[WARN] Node Labels script timed out, Caught exception : + e.getMessage(), e
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:loadStateFromRegistry(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Reading amrmToken for subcluster {} for {}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:verifyPlanVersion(long):[ERROR] Disk Balancer - Invalid plan version.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlows(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ${url} from user ${TimelineReaderWebServicesUtils.getUserName(callerUGI)}
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEvents(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[ERROR] Error getting entity timelines
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStaleReplicas(java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[DEBUG] BLOCK* Removing stale replica {} of {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getTimelineStoresFromCacheIds(java.util.Set,java.lang.String,java.util.List):[DEBUG] Adding {} as a store for the query
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Application attempt + application.getApplicationAttemptId() + released container + container.getId() + on node: + node + with event: + event
org.apache.hadoop.mapreduce.task.reduce.Fetcher:connect(java.net.URLConnection,int):[ERROR] Connection retry failed with {attempts} attempts in {retryTimeInSeconds} seconds
org.apache.hadoop.hdfs.server.namenode.FSEditLog:startLogSegment(long,boolean,int):[INFO] Starting log segment at + segmentTxId
org.apache.hadoop.hdfs.server.datanode.DataXceiver:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[TRACE] "{}: Ignoring exception while serving {} to {}", dnR, block, remoteAddress, ignored
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEvent):[ERROR] Unable to record log deleter state
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:enterNeutralMode():[WARN] Lost contact with Zookeeper. Transitioning to standby in {zkSessionTimeout} ms if connection is not reestablished.
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:processResponseQueue():[INFO] Application {} sends out event to clean up its AM container.
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$DeletionStoragePurger:purgeLog(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile):[INFO] Purging old edit log {log}
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:runWithPrincipal(java.lang.String,byte[],org.apache.commons.codec.binary.Base64,javax.servlet.http.HttpServletResponse):[TRACE] SPNEGO initiated with server principal [{}]
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int,int):[DEBUG] No space available. Available: {availableSize} MinSize: {minSize}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:call():[ERROR] Failed to launch container due to configuration error.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] assignContainers: partition= + candidates.getPartition() + #applications= + orderingPolicy.getNumSchedulableEntities()
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[ERROR] Unable to insert the newly generated application {}
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier):[DEBUG] Storing token {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[INFO] Application {} transitioned from {} to {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:activateApplications():[DEBUG] Not activating application {applicationId} as amIfStarted: {amIfStarted} exceeds amLimit: {amLimit}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Invalid READDIR request, with negative cookie: {cookie}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Object):[INFO] {}: Directory count = {}; maximum depth {}, getName(), dirCount, depth
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:getRecord(java.lang.String,java.lang.Class):[ERROR] Cannot parse line {} in file {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String):[WARN] Script envBinaryPath does not exist
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeDownloadJars(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile$TypeEnum,org.apache.hadoop.conf.Configuration):[INFO] delete old aux service jar dir:...
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateReservation(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationUpdateRequestInfo,javax.servlet.http.HttpServletRequest):[INFO] Update reservation request failed
org.apache.hadoop.mapred.JobQueueClient:displayQueueAclsInfoForCurrentUser():[INFO] Queue Operations
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getCumulativeCpuTime():[DEBUG] CPU Comparison: + procfs.getCumulativeCpuTime() + " " + cgroup.getCumulativeCpuTime()
org.apache.hadoop.tools.HadoopArchiveLogs:checkFilesAndSeedApps(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Adding [appName] for user [userName]
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run():[INFO] Starting expired delegation token remover thread, tokenRemoverScanInterval= X min(s)
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSDelete operation executed
org.apache.hadoop.tools.dynamometer.ApplicationMaster:main(java.lang.String[]):[ERROR] Error running ApplicationMaster
org.apache.hadoop.yarn.server.timeline.LogInfo:parsePath(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[DEBUG] Exception in parse path: {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:activate(org.apache.hadoop.conf.Configuration):[DEBUG] Activating DatanodeAdminManager with interval {} seconds, {} max blocks per interval, {} max concurrently tracked nodes.
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:launchAM(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Error reading the out stream
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks:decrement(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo):[DEBUG] Removing pending reconstruction for {}
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner$NodeHealthMonitorExecutor:run():[WARN] Caught exception :
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:call():[WARN] Unable to locate pid file for container [containerIdStr]
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] source nodes = <p.getSourceNodes()>
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl$ContainerLogAggregator:doContainerLogAggregation(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController,boolean,boolean):[INFO] Uploading logs for container + containerId + ". Current good log dirs are " + StringUtils.join(",", dirsHandler.getLogDirsForRead())
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,int,java.lang.String,boolean):[DEBUG] List objects. prefix: [%s], delimiter: [%s], maxListLength: [%d], priorLastKey: [%s].
org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl:initialize(org.apache.hadoop.conf.Configuration):[DEBUG] Initialization of RemoteSASKeyGenerator instance successful
org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils:getDefaultFileContext():[ERROR] Unable to create default file context [ + defaultConf.get(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY) + ], e
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[ERROR] Device programming failed, aocl output is:
org.apache.hadoop.registry.server.dns.RegistryDNS:nioTCPClient(java.nio.channels.SocketChannel):[INFO] received TCP query {}
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorApp():[DEBUG] Adding resource configuration file
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List):[DEBUG] Creating path {} with mode {} and ACL {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set):[DEBUG] getEntityTimeline type={} id={}
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[DEBUG] Storing token {tokenId.getSequenceNumber()}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$UpdateContainerResourceTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Callback handler does not implement container resource update callback methods
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureEndpoint(com.amazonaws.services.s3.AmazonS3Builder,com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration):[WARN] SDK_REGION_CHAIN_IN_USE
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:getPathForLocalization(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[WARN] Resource + req + has been removed + and will no longer be localized
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[ERROR] Can't handle this event at current state for [attemptId]
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:stop():[DEBUG] Storage policy satisfier service is running outside namenode, ignoring
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:innerCommitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[ERROR] At least one commit file could not be read: failing
org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread:run():[DEBUG] Pipe child done
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSRename operation executed
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded$SPSPathIdProcessor:run():[WARN] Exception while scanning file inodes to satisfy the policy
org.apache.hadoop.hdfs.PeerCache:evictExpired(long):[DEBUG] Cleaning up expired peer
org.apache.hadoop.mapred.YarnChild:configureTask(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task,org.apache.hadoop.security.Credentials,org.apache.hadoop.security.token.Token):[WARN] Shuffle secret missing from task credentials. Using job token secret as shuffle secret.
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] Type = {}
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized state store client class
org.apache.hadoop.hdfs.util.CombinedHostsFileReader:readFile(java.lang.String):[WARN] hostsFilePath + has legacy JSON format. + REFER_TO_DOC_MSG
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:deleteAppDirLogs(long,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.yarn.api.ApplicationClientProtocol,org.apache.hadoop.fs.FileStatus):[INFO] Deleting aggregated logs in + appDir.getPath() [EXCEPTION] logException
org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer):[TRACE] Thread.currentThread().getId() + ": Response <- " + remoteId + ": " + method.getName() + " {" + TextFormat.shortDebugString(returnMessage) + "}"
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] Default key bitlength is {}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the flowActivityTable table
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Returning JSON response with location
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:createPassword(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier):[DEBUG] Generating block token for + identifier
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap):[TRACE] storageTypes={}
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream():[WARN] Excluding datanode: + badNode
org.apache.hadoop.hdfs.DFSClient:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[]):[DEBUG] {}: masked={}
org.apache.hadoop.yarn.service.client.ServiceClient:upgradePrecheck(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Service state not STABLE
org.apache.hadoop.fs.azurebfs.services.AbfsClient:setPathProperties(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Adding default headers
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfCacheReportParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}
org.apache.hadoop.security.KDiag:main(java.lang.String[]):[ERROR] e.toString(), e
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:dump():[DEBUG] After dump, nonSequentialWriteInMemory == {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:doFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain):[INFO] There is no active RM right now.
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeReadManifestFile():[WARN] Manifest file + manifest + is not a file
org.apache.hadoop.ipc.Server$Responder:doRunLoop():[DEBUG] Checking for old call responses.
org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable():[INFO] pmdkSupportState.getMessage()
org.apache.hadoop.hdfs.server.balancer.Balancer:runOneIteration():[ERROR] Balancer exiting as upgrade is not finalized, please finalize the HDFS upgrade before running the balancer.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String):[DEBUG] AzureBlobFileSystem.setOwner path: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseDataNode(java.lang.String,java.util.Collection):[DEBUG] Choosing data node
org.apache.hadoop.hdfs.server.datanode.DataXceiver:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String):[INFO] Moved block from StorageType to StorageType
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:getProxyInternal():[ERROR] Unable to create proxy to the ResourceManager
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS ACCESS fileHandle: {} client: {}
org.apache.hadoop.yarn.client.cli.TopCLI:parseOptions(java.lang.String[]):[WARN] Delay set too low, using default
org.apache.hadoop.tools.mapred.CopyCommitter:commitData(org.apache.hadoop.conf.Configuration):[INFO] Atomic commit enabled. Moving workDir to finalDir
org.apache.hadoop.fs.s3a.S3AInputStream:closeStream(java.lang.String,boolean,boolean):[DEBUG] initiating asynchronous drain of {} bytes
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:notifyIsLastAMRetry(boolean):[INFO] Notify RMCommunicator isAMLastRetry: [isLastAMRetry]
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int,int):[DEBUG] Check the condition for main loop.
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Resource Types Info Retrieved
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] AM registration plus applicationAttemptId
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:makeCompositeCrcResult():[DEBUG] Added lastBlockCrc 0x{} for block index {} of size {}, Integer.toString(lastBlockCrc, 16), locatedBlocks.size() - 1, consumedLastBlockLength
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[DEBUG] Getting pid for container y to send signal to from pid file z
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:allocateDevices(java.util.Set,int,java.util.Map):[ERROR] Error in getting GPU topology info. Skip topology aware scheduling
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:startContainerLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext):[INFO] Considering container + logContext.getContainerId() + for log-aggregation
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetReplication(java.lang.String,short):[DEBUG] Adding set replication record to edit log
org.apache.hadoop.yarn.service.component.Component$CheckStableTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[DEBUG] Checking if component is stable
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] {serviceName} is at {state}, forcefully killed by user!
org.apache.hadoop.mapred.TaskAttemptListenerImpl$TaskProgressLogPair:resetLog(boolean,float,double,long):[DEBUG] Progress of TaskAttempt ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:releaseNumaResource(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Releasing the assigned NUMA resources for + containerId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getQueues(java.lang.String):[DEBUG] CSConf - getQueues called for: queuePrefix={}
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:moveBlock(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.Socket,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[INFO] Start moving block
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:failDestinationExists(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] stack trace
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[DEBUG] rpcServer.checkOperation called
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadReservationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Incorrect reservation state key {key}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDeadDataNodes():[DEBUG] Failed to get number of dead nodes
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:initExisting():[INFO] Initializing Existing Jobs...
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit):[WARN] Thread interrupted while trying to acquire WRITE lock
org.apache.hadoop.fs.cosn.CosNOutputStream:waitForFinishPartUploads():[INFO] Wait for all parts to finish their uploading.
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock):[WARN] src={}, datanodes[{}]={}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:executeRenamingOperation(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[DEBUG] getName(): operation: 'source' to 'dest'
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveCacheDirectiveInfo(java.lang.Long,boolean):[INFO] Executing logEdit
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:reportNewCollectorInfoToNM(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.Token):[INFO] Report a new collector for application: + appId + to the NM Collector Service.
org.apache.hadoop.fs.azure.NativeAzureFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Creating directory: {}
org.apache.hadoop.mapred.LocalJobRunner$Job:createOutputCommitter(boolean,org.apache.hadoop.mapred.JobID,org.apache.hadoop.conf.Configuration):[INFO] OutputCommitter set in config ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:stopContainerInternal(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[INFO] Stopping container with container Id: + containerIDStr
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[DEBUG] Writing output to file
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent):[DEBUG] Ignoring closed channel error, cause
org.apache.hadoop.http.HttpServer2:stop():[ERROR] Error while stopping web app context for webapp + webAppContext.getDisplayName(), e
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStop():[DEBUG] Stopping auxiliary service
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[DEBUG] Exception closing executor service
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute():[DEBUG] active commands: {activeBlockCommands.size()} for {key}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean):[WARN] Update but the new block does not have a larger generation stamp
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$LogMonitorThread:run():[INFO] Removed ProcessTree with root PID
org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String):[WARN] Fail to create symbolic links on Windows. The default security settings in Windows disallow non-elevated administrators and all non-administrators from creating symbolic links. This behavior can be changed in the Local Security Policy management console
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishContainerLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent,java.lang.String):[ERROR] Failed to publish Container metrics for container ${container.getContainerId()} with exception {}
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:refreshJobRetentionSettings():[INFO] Success: user action refreshJobRetentionSettings on HISTORY_ADMIN_SERVER
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] Exception encountered , {e}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing master key
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeDefaultAcl(java.lang.String):[DEBUG] Attempting operation: removeDefaultAcl
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[WARN] Missing GetSubClusterInfo Request. Please try again by specifying a Get SubCluster information.
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:reconstruct():[INFO] Reading minimum sources...
org.apache.hadoop.fs.s3a.S3AUtils:deleteWithWarning(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean):[WARN] Failed to delete {}, path, e
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Can't get path for fileId: {}
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean):[DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for {} to add as corrupt on {} by {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:remove(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[INFO] Removed rsrc.getLocalPath() from localized cache
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:writeChannel(io.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int):[INFO] Null channel should only happen in tests. Do nothing.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] Got overwrite with appended data [{start}-{end}), current offset {currentOffset}, drop the overlapped section [{overlapStart}-{overlapEnd}) and append new data [{appendStart}-{appendEnd})
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readInternal(long,byte[],int,int,boolean):[DEBUG] Received data from read ahead, not doing remote read
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Wrote {} bytes to {}.java
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:removeNodeFromStaleList(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[INFO] String.format(isDead ? REPORT_REMOVE_DEAD_NODE_ENTRY : REPORT_REMOVE_STALE_NODE_ENTRY, d)
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[DEBUG] Found {{fileStatusList.size()}} files
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:startThreads():[INFO] Start loading token cache
org.apache.hadoop.conf.ReconfigurationServlet:getReconfigurable(javax.servlet.http.HttpServletRequest):[INFO] getting attribute: + CONF_SERVLET_RECONFIGURABLE_PREFIX + req.getServletPath()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[TRACE] storageTypes={}, storageTypes
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:deleteOldLogDirsFrom(org.apache.hadoop.fs.Path,long,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.yarn.api.ApplicationClientProtocol):[ERROR] Could not read the contents of {dir}, {e}
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the start data of application attempt [applicationAttemptId]
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Unable to remove token {}
org.apache.hadoop.hdfs.DFSOutputStream:addBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String[],java.util.EnumSet):[INFO] Exception while adding a block
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:loadTokenMasterKeys(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState):[DEBUG] Loading master key from + key
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createUserLocalDirs(java.util.List,java.lang.String):[WARN] Unable to create the user directory : {}
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:appFinished(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Removing application attempts NMToken keys for application {appId}
org.apache.hadoop.yarn.webapp.view.JQueryUI:render():[DEBUG] initDialogs
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:loadMap(java.util.HashMap,java.lang.String,int,int):[ERROR] Error while parsing mapping file
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:markSuspectBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] {}: Not scheduling suspect block {} for rescanning, because this volume scanner is stopping.
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:use(org.apache.hadoop.yarn.api.protocolrecords.UseSharedCacheResourceRequest):[INFO] Error getting UGI
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectStreamingInputStreams(java.util.Collection,long,boolean,boolean):[WARN] Found endTxId ...
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun():[ERROR] Unable to fetch namespace information from any remote NN. Possible NameNodes: {remoteNNs}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[INFO] Loaded key cache.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptFailedTransition:transition(java.lang.Object,java.lang.Object):[INFO] Max app attempts is 1 for {app.applicationId}, preventing further attempts.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:precheckNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,java.util.Optional):[DEBUG] [Original log message with parameters]
org.apache.hadoop.util.RunJar:run(java.lang.String[]):[ERROR] Error creating temp dir in java.io.tmpdir
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Unable to remove token {tokenId.getSequenceNumber()}, {e}
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:shouldDeleteLogDir(org.apache.hadoop.fs.FileStatus,long,org.apache.hadoop.fs.FileSystem):[DEBUG] Error reading the contents of {dir.getPath()}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error storing app: + appId, e
org.apache.hadoop.hdfs.web.URLConnectionFactory:openConnection(java.net.URL,boolean):[DEBUG] open AuthenticatedURL connection {}
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:handleJobCommit(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobCommitEvent):[ERROR] could not create failure file.
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean):[WARN] KMS provider at [{}] threw an IOException:
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[DEBUG] [STRESS] Cluster overloaded in run! Sleeping...
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Problem connecting to server: + nnAddr + : + e.getLocalizedMessage()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] % of the mappers will be scheduled using OPPORTUNISTIC containers
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage):[INFO] Downloaded file + tmpFiles.get(0).getName() + size + finalFiles.get(0).length() + bytes.
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecomLiveDataNodes():[DEBUG] Failed to get the number of live decommissioned datanodes
org.apache.hadoop.yarn.util.DockerClientConfigHandler:getCredentialsFromTokensByteBuffer(java.nio.ByteBuffer):[DEBUG] Token read from token storage: {token}
org.apache.hadoop.ha.ActiveStandbyElector:becomeActive():[DEBUG] Becoming active for {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.rename src: {} dst: {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] SASL server skipping handshake in secured configuration with no SASL protection configured for peer = {}, datanodeId = {}
org.apache.hadoop.security.LdapGroupsMapping:failover(int,int):[INFO] Reached {} attempts on {}, failing over to {}
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] unknown GET someUri 200
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing master key {keyId}
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.lang.String):[DEBUG] Could not resolve hostName. Falling back to default-rack
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:initializeQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[ERROR] Failed to initialize queues
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:getCredentials():[DEBUG] Using credentials from {provider}
org.apache.hadoop.contrib.utils.join.JobBase:report():[INFO] log the counters
org.apache.hadoop.fs.azurebfs.services.AbfsListStatusRemoteIterator:asyncOp():[ERROR] Fetching filestatuses failed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:computeUserLimit(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,boolean):[DEBUG] User limit computation for + userName + , in queue: + lQueue.getQueuePath() + , userLimitPercent= + lQueue.getUserLimit() + , userLimitFactor= + lQueue.getUserLimitFactor() + , required= + required + , consumed= + consumed + , user-limit-resource= + userLimitResource + , queueCapacity= + queueCapacity + , qconsumed= + lQueue.getQueueResourceUsage().getUsed() + , currentCapacity= + currentCapacity + , activeUsers= + usersSummedByWeight + , clusterCapacity= + clusterResource + , resourceByLabel= + partitionResource + , usageratio= + getUsageRatio(nodePartition) + , Partition= + nodePartition + , resourceUsed= + resourceUsed + , maxUserLimit= + maxUserLimit + , userWeight= + getUser(userName).getWeight()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:addDirectoryToJobListCache(org.apache.hadoop.fs.Path):[DEBUG] Adding in history for + fs.getPath()
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:nodeUnpublishVolume(org.apache.hadoop.yarn.api.protocolrecords.NodeUnpublishVolumeRequest):[DEBUG] Received nodeUnpublishVolume call, request: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:replaceFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] File doesn't exist. Skip deleting the file + dstPath
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock():[WARN] delete({}) returned false
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:addReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation,boolean):[INFO] Successfully added reservation: ... to plan.
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleExitCode(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Exit code from container {} is : {}
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:handleUploadImageRequest(javax.servlet.http.HttpServletRequest,long,org.apache.hadoop.hdfs.server.common.Storage,java.io.InputStream,long,org.apache.hadoop.hdfs.util.DataTransferThrottler):[INFO] Downloaded file + dstFiles.get(0).getName() + size + dstFiles.get(0).length() + bytes.
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$LoggingAuditSpan:start():[TRACE] {} Start {}, getSpanId(), getDescription()
org.apache.hadoop.fs.s3a.S3AFileSystem:removeKeysS3(java.util.List,boolean):[DEBUG] Initiating delete operation for {} objects
org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object):[WARN] Ignoring stale result from old client with sessionId 0x%08x
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Error launching job , bad input path :
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy):[INFO] New proxy instance created
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[INFO] Bad request: requires Application ID
org.apache.hadoop.mount.MountdBase:startTCPServer():[ERROR] Failed to start the TCP server.
org.apache.hadoop.hdfs.server.federation.router.RouterStoragePolicy:satisfyStoragePolicy(java.lang.String):[DEBUG] Checked operation
org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover():[INFO] Successfully became active. Attempt succeeded
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.MetricsInvariantChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[DEBUG] Configuring metrics system
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:processSummationMajorCompaction(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter,long):[INFO] After major compaction for qualifier= with currentColumnCells.size= returning finalCells.size= with sum=
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy):[INFO] HA enabled check completed
org.apache.hadoop.mapred.JobConf:getTaskJavaOpts(org.apache.hadoop.mapreduce.TaskType):[WARN] Invalid value for + MRJobConfig.HEAP_MEMORY_MB_RATIO + , using the default.
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo:getCmdLine(java.lang.String):[WARN] Error reading the stream
org.apache.hadoop.hdfs.qjournal.server.JNStorage:getOrCreatePaxosDir():[ERROR] Could not create paxos dir: {}
org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode():[INFO] Successfully deleted + znodeWorkingDir + from ZK.
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[ERROR] Error getting entities
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:waitForAllPartUploads():[DEBUG] Waiting for {} uploads to complete
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + but entity not found + (Took + (Time.monotonicNow() - startTime) + ms.)
org.apache.hadoop.conf.Configuration:handleDeprecation():[DEBUG] Handling deprecation for all properties in config...
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadRMDelegationKeyState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loaded delegation key: keyId={}, expirationDate={}
org.apache.hadoop.registry.server.dns.RegistryDNS:serveNIOUDP(java.nio.channels.DatagramChannel,java.net.InetAddress,int):[DEBUG] Error during message receipt
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:scanDirectoryTree(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,boolean):[DEBUG] To rename: ...
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:load(java.io.File):[INFO] Number of files = numFiles
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStop():[INFO] The background thread stopped.
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:failApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Failing application attempt + attemptId
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:deregisterSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterDeregisterRequest):[ERROR] SubCluster {subClusterId} not found
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:deactivateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.api.records.NodeState):[INFO] Deactivating Node + rmNode.nodeId + as it is now + finalState
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:uploadBlockAsync(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock,java.lang.Boolean):[DEBUG] Stream statistics of {}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(java.lang.Object):[WARN] Volume {} detected as being unhealthy
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest):[DEBUG] Forwarding allocateForDistributedScheduling request to the real YARN RM
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadProxyCAManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Couldn't recover Proxy CA data
org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey):[INFO] Thread.currentThread().getName() + ": readAndProcess from client " + c + " threw exception [" + e + "]"
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:getMajorAndMinorNumber(java.lang.String):[DEBUG] Get FPGA major-minor numbers from /dev/{devName}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingReconstructionBlocks():[DEBUG] Failed to get number of blocks pending replica
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW:createTable(org.apache.hadoop.hbase.client.Admin,org.apache.hadoop.conf.Configuration):[INFO] CoprocessorJarPath=[coprocessorJarPath.toString()]
org.apache.hadoop.hdfs.server.namenode.NameNode:createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[INFO] Generated new cluster id: {clusterID}
org.apache.hadoop.hdfs.server.balancer.Balancer:checkKeytabAndInit(org.apache.hadoop.conf.Configuration):[INFO] Keytab is configured, will login using keytab.
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set group to {info.getGroupName()}
org.apache.hadoop.hdfs.server.namenode.FSImage:saveLegacyOIVImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.util.Canceler):[INFO] Purging old legacy OIV images
org.apache.hadoop.ipc.Client:stop():[DEBUG] Stopping client
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:buildAppProto(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$FlowContext):[ERROR] Cannot serialize credentials
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String):[INFO] Storing state for reservation + reservationIdName + from + plan + planName + at path + reservationPath
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:signalProcess(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Sent signal {} to pid {} as user {} for container {}, result={}
org.apache.hadoop.tools.DistCp:configureOutputFormat(org.apache.hadoop.mapreduce.Job):[INFO] DistCp job log path: + logPath
org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean):[DEBUG] Service {name} implements LaunchableService
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:bulkDeleteRetried(com.amazonaws.services.s3.model.DeleteObjectsRequest,java.lang.Exception):[DEBUG] Retrying on error during bulk delete
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:shouldRetry(java.io.IOException,int,java.lang.String):[DEBUG] Not retrying anymore, already retried the urls {} time(s)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] {} files to commit under {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator:calculateResToObtainByPartitionForLeafQueues(java.util.Set,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] skipping from queue={} because it's a non-preemptable queue
org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject):[DEBUG] Failure to load login credentials
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:loadDfsUsed():[INFO] Cached dfsUsed found for
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeAddrs():[ERROR] Could not construct Shared Edits Uri
org.apache.hadoop.hdfs.server.datanode.DataNode:handleVolumeFailures(java.util.Set):[DEBUG] handleVolumeFailures done with empty unhealthyVolumes
org.apache.hadoop.mapreduce.task.reduce.Fetcher:checkTimeoutOrRetry(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.IOException):[WARN] Timeout for copying MapOutput with retry on host host after fetchRetryTimeout milliseconds.
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Hadoop Version:
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromHadoopCommon(java.util.function.Function,java.lang.String):[INFO] Found script: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:validateResourceRequests(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue):[TRACE] Validating resource request: [ResourceRequest details]
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Invalid MKDIR request
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[ERROR] Skip recovering container for unknown application.
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Delegation token request processed
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest):[WARN] Policy for queue: {} does not exist.
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Enter safe mode after {} ms without reaching the State Store
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[INFO] {JobContext.IO_SORT_MB}: {sortmb}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:addResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent):[DEBUG] Skip downloading resource: {key} since it's in state: {rsrc.getState()}
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:leave():[ERROR] The Router metrics are not enabled
org.apache.hadoop.yarn.server.webapp.LogServlet:getLogFile(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String,boolean):[WARN] Could not obtain appInfo object from provider.
org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils:authorizeRequest():[WARN] No AMRMToken found for user
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:initializeFacadeInternal(org.apache.hadoop.conf.Configuration):[ERROR] Failed to initialize the FederationStateStoreFacade object
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:deleteApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[ERROR] Cannot check app: e.getMessage()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateSchedulerConfiguration(org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo,javax.servlet.http.HttpServletRequest):[ERROR] Exception thrown when modifying configuration.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType):[DEBUG] addResourceRequest: applicationId=
org.apache.hadoop.hdfs.server.namenode.FSImage:saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler):[INFO] New instance created
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:recoverAppend(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long):[INFO] Recover failed append to +
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:getRedirectedUrl():[DEBUG] Running URLRunner
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] amRegistrationRequest recovered for {}
org.apache.hadoop.mapred.BackupStore:write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer):[INFO] Activated file cache and written
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Added service {} for the user {}, filename = {}
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] unknown POST /webhdfs/v1/...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] allocate: pre-update applicationAttemptId= ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceAllocator:addFpgaDevices(java.lang.String,java.util.List):[WARN] Duplicate device found:
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[INFO] POST: createService = {} user = {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:getPathForLocalization(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[ERROR] Unable to record localization start for + rsrc, e
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path):[DEBUG] KeyStore resetting to previously flushed state !!
org.apache.hadoop.yarn.client.cli.TopCLI$KeyboardMonitor:run():[ERROR] Caught exception
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorApp():[INFO] HTTP server started
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:waitForAllPartUploads():[DEBUG] While waiting for upload completion
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[DEBUG] Processing event for {} of type {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[INFO] Error storing info for RMDelegationToken: + rmDTIdentifier
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:loadServices(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords,org.apache.hadoop.conf.Configuration,boolean):[ERROR] Failed to load auxiliary service
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:getDirectoryPolicy(org.apache.hadoop.conf.Configuration,java.util.function.Predicate):[DEBUG] Directory markers will be deleted
org.apache.hadoop.hdfs.server.namenode.CacheManager:addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet):[WARN] addDirective of + info + failed: , e
org.apache.hadoop.fs.DelegationTokenRenewer:reset():[WARN] Failed to reset renewer
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntityTypes(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.streaming.PipeReducer:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[DEBUG] maybeLogRecord called
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Directory created
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getOrCreateQueueFromPlacementContext(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext,boolean):[ERROR] Queue named could not be auto-created during application recovery.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNodeHealthyForDecommissionOrMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[WARN] Node {} is dead while in {}. Cannot be safely decommissioned or be in maintenance since there is risk of reduced data durability or data loss. Either restart the failed node or force decommissioning or maintenance by removing, calling refreshNodes, then re-adding to the excludes or host config files.
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi():[DEBUG] Using loginUser when Kerberos is enabled but the actual user does not have either KMS Delegation Token or Kerberos Credentials
org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown():[INFO] Inside shutdown, con2infoMap size= + con2infoMap.size()
org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec:parse(java.lang.String):[INFO] Parsed source tag: {}, number of allocations: {}
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:getJSON():[DEBUG] Connection context details logged
org.apache.hadoop.fs.s3a.impl.OpenFileSupport:prepareToOpenFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters,long):[DEBUG] File was opened with a supplied FileStatus; skipping getFileStatus call in open() operation: {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] OUTCOME: FAILURE, Reservation ID: + reservationId.toString() + , Contract: + contract.toString()
org.apache.hadoop.mapred.IndexCache:readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String):[DEBUG] IndexCache MISS: MapId mapId not found
org.apache.hadoop.nfs.NfsExports$RegexMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] RegexMatcher 'pattern', allowing client 'address', 'hostname'
org.apache.hadoop.yarn.service.utils.JsonSerDeser:fromStream(java.io.InputStream):[ERROR] Exception while parsing json input stream
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache$CleanInActiveFDsTask:run():[WARN] e.toString()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[INFO] Submit app request failed, ue
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] %s: setup task attempt path %s
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Deleted {0} - missing at source
org.apache.hadoop.hdfs.server.namenode.NNStorage:attemptRestoreRemovedStorage():[INFO] currently disabled dir {}; type={} ;canwrite={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$NvidiaCommandExecutor:searchBinary():[INFO] Found binary: pathOfGpuBinary
org.apache.hadoop.fs.cosn.ByteBufferWrapper:close():[ERROR] Close the random access file occurs an exception.
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.ReservationInvariantsChecker:editSchedule():[ERROR] Number of reservations (...) does NOT match the number of reservationQueues (...), while it should.
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] Using user: "{user}" with name: {user.getName()}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:cleanup(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[DEBUG] Cleanup job %s
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[ERROR] Timeout for waiting current attempt of + appId + to reach + attemptState
org.apache.hadoop.yarn.service.component.Component:requestContainers(long):[DEBUG] Resource sizing: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Scheduler Info Initialized
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] Results of dry run:
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.volume.csi.event.VolumeEvent):[INFO] Processing volume event, type= + event.getType().name() + , volumeId= + volumeId.toString()
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.ReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[DEBUG] Configuring job jar
org.apache.hadoop.security.token.DtUtilShell$Get:validate():[ERROR] URL does not contain a service specification:
org.apache.hadoop.yarn.client.api.async.NMClientAsync$AbstractCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onContainerStopped called
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:intOption(org.apache.hadoop.conf.Configuration,java.lang.String,int,int):[DEBUG] Value of {} is {}
org.apache.hadoop.yarn.service.client.ServiceClient:actionStartAndGetId(java.lang.String):[INFO] Persisted service {service.getName()} at {appJson}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:pruneIfExpired(long,org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData):[INFO] Removing expired block report lease 0x{} for DN {}.
org.apache.hadoop.mapred.YARNRunner:generateResourceRequests():[DEBUG] ResourceRequest: resource = {resourceName}, locality = {locality}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Federation Enabled: true/false // conditional depending on isEnabled
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeDefaultAcl(java.lang.String):[INFO] Audit event succeeded: removeDefaultAcl
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleCommit(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,int):[INFO] No opened stream for fileId: + fileHandle.dumpFileHandle() + commitOffset= + commitOffset + . Return success in this case.
org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect():[ERROR] Too many connection failures, would not try to connect again.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[WARN] short-circuit read access for the file {fileName} is disabled for DataNode {datanode}. reason: {resp.getMessage()}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] Got a repeated request, same range, with xid: {} nextOffset {} req offset={}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary:logAppSummary(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp):[INFO] createAppSummary(app).toString()
org.apache.hadoop.mapred.QueueManager:hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation):[INFO] Queue + queueName + is not present
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable):[DEBUG] *DIR* NameNode.append: file {src} for {clientName} at {clientMachine}
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:render():[INFO] Container information displayed
org.apache.hadoop.hdfs.server.balancer.Balancer$Cli:parse(java.lang.String[]):[INFO] Using a threshold of
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:uploadSourceFromFS():[INFO] Upload initiated
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List):[INFO] Minimum records per chunk calculated
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet):[INFO] Invoking method sequentially
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] nodeUpdate: {} cluster capacity: {}<br/>
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable:handleDTRenewerAppSubmitEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerAppSubmitEvent):[WARN] Unable to add the application to the delegation token renewer.
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:initializePipeline(java.lang.String):[INFO] Request to start an already existing user: {} was received, so ignoring.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits):[DEBUG] User {} in queue {} will exceed limit based on reservations - consumed: {} reserved: {} limit: {}
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor:run():[DEBUG] LeaseManager is interrupted
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.LocalAllocationTagsManager:addTempTags(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set):[DEBUG] Added TEMP container with tags=[ + StringUtils.join(allocationTags, ,) + ]
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render():[WARN] Can not load log meta from the log file:
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] taskId + Task Transitioned from + oldState + to + getInternalState()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initDriver():[ERROR] Cannot create State Store root directory {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[ERROR] Wrong behavior during the update of SubCluster {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createScheduler():[INFO] Using Scheduler: schedulerClassName
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:dumpOpCounts(java.util.EnumMap):[DEBUG] Summary of operations loaded from edit log:\n ...
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:saveDelegationToken(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path):[DEBUG] Fetched token {fs.getUri()} for {token.getService()} into {tokenFile}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendFinishedEvents():[INFO] Log handler container finished
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] Found {} existing UAMs for application {} in NMStateStore
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initializing Document Store Writer for : storeType
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:archiveCompletedReservations(long):[DEBUG] Running archival at time: {}
org.apache.hadoop.fs.s3a.S3AFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] rename failure
org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials):[DEBUG] selected by service={} token={}
org.apache.hadoop.util.LogAdapter:error(java.lang.String):[ERROR] LOGGER.error(msg)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:computeIgnoreBlacklisting():[INFO] Ignore blacklisting set to true. Known: <clusterNmCount>, Blacklisted: <blacklistedNodeCount>, <val>%
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Setting up task commit statistics
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:main(java.lang.String[]):[INFO] Initializing ApplicationMaster
org.apache.hadoop.yarn.client.cli.ApplicationCLI:failApplicationAttempt(java.lang.String):[INFO] Attempt failed for application: appId
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] Exception encountered during storage policy setting.
org.apache.hadoop.hdfs.DeadNodeDetector:probeCallBack(org.apache.hadoop.hdfs.DeadNodeDetector$Probe,boolean):[INFO] Remove the node out from dead node list: {}.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest):[DEBUG] Verifying user access
org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig):[INFO] Allowed Origins: + StringUtils.join(allowedOrigins, ',')
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:syncWithJournalAtIndex(int):[DEBUG] Could not sync with Journal at...
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease):[WARN] Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[INFO] File not found, creating node
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[WARN] Exit code from container {} is: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet,boolean):[DEBUG] logAuditEvent success
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean):[DEBUG] Trying to schedule on node: {}, available: {}
org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser:traverseDirInt(long,org.apache.hadoop.hdfs.server.namenode.INode,java.util.List,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo):[DEBUG] Traversing directory {}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:finalizeBlock(long):[INFO] Client trace information logged
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerBecomeReadyTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] Dispatching CHECK_STABLE event
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadBlockPoolSliceStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[WARN] Failed to add storage directory {} for block pool {}
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader:nextJob():[ERROR] Its jobID is + result.getJobID()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Error creating user intermediate history done directory
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppFinishTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing [containerID] from application [app.toString()]
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[ERROR] Mkdirs failed to create ${jobAttemptPath}
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:loadJob(org.apache.hadoop.mapreduce.v2.api.records.JobId):[DEBUG] Looking for Job + jobId
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage):[INFO] Performing upgrade of storage directory + sd.getRoot()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] To-release container={}, for to a new allocated container, is in final state
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:manageWriterOsCache(long):[WARN] Error managing cache for writer of block {block}, {t}
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:writeDocument(java.lang.Object,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType):[ERROR] Unable to perform upsert for Document Id : {} under Collection : {} under Database {}
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:getHost():[DEBUG] Assigning host with numKnownMapOutputs to ThreadName
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:lock():[INFO] Cannot lock storage {}. The directory is already locked
org.apache.hadoop.tools.DistCh$ChangeFilesMapper:map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.DistCh$FileOperation,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[INFO] FAIL: [value], [exception message]
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest):[ERROR] Fail to create a new application.
org.apache.hadoop.yarn.service.client.ApiServiceClient:getStatusString(java.lang.String):[ERROR] Fail to check application status:
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator:calculateResToObtainByPartitionForLeafQueues(java.util.Set,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] {}
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:processBlockInvalidation(org.apache.hadoop.hdfs.ExtendedBlockId):[INFO] Block <blockId> has been invalidated. Marking short-circuit slots as invalid: <slotDetails>
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:mountCGroupController(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController):[ERROR] Failed to mount controller: controller.getName()
org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String):[WARN] Failed to connect to: [URL]
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:handleApplicationAttemptStateOp(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$AppAttemptOp):[DEBUG] Path {} for {} didn't exist. Created a new znode to update the application attempt state., path, appAttemptId
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render():[ERROR] Error getting logs for [logEntity]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:checkPlacementPoliciesPresent(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler,org.apache.hadoop.conf.Configuration):[DEBUG] Queue placement policy is present
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getInitialCachedResources(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[INFO] Found numEntries files: processing for one resource per key
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[WARN] Unable to fetch completion status of workload job. Will proceed to attempt to kill it.
org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker:initializeStripedBlkReconstructionThreadPool(int):[DEBUG] Using striped block reconstruction; pool threads={}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNodesImpl(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[ERROR] Cannot get {} nodes, subclusters timed out responding
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner:run():[DEBUG] {}: cache cleaner running at {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetXAttrs(java.lang.String,java.util.List,boolean):[DEBUG] logEdit
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop():[INFO] Stopping the InMemorySCMStore service.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeBRLeaseIfNeeded(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.BlockReportContext):[DEBUG] Processing RPC with index {} out of total {} RPCs in processReport 0x{}
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:getApplications(long,long,long):[ERROR] Error on generating application report for {entityId}
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] delete({0}) returned false ({1})
org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode():[WARN] Unable to delete our own bread-crumb of being active at {}. . Expecting to be fenced by the next active.
org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean):[WARN] I/O error finding interface {}: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:updateAppsRunnability(java.util.List,int):[ERROR] Can't make app runnable that does not already exist in queue as non-runnable: [appSched]. This should never happen.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed to init state store
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] Using existing subject: {subject.getPrincipals()}
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateExpiration(com.nimbusds.jwt.SignedJWT):[DEBUG] JWT token expiration date has been successfully validated
org.apache.hadoop.registry.client.binding.RegistryUtils:extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String,java.util.Collection):[DEBUG] Extracting service records
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:discardOldEntities(long):[ERROR] Got IOException while deleting entities for type entityType, continuing to next type
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceStop():[INFO] Stopping History Cleaner/Move To Done
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:main(java.lang.String[]):[INFO] KMSWebServer startup/shutdown message
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter():[DEBUG] {}: no block pools are registered.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid argument, data size is less than count in request
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] GETATTR for fileHandle: {} client: {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:load(java.io.File):[INFO] Upgrading to sequential block IDs. Generation stamp for new blocks set to startingGenStamp
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=GETFILESTATUS target=path
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:syncYarnSysFS(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String):[ERROR] Fail to sync yarn sysfs for application ID: {}, reason:
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:start():[INFO] Disabling StoragePolicySatisfier service as {} set to {}.
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:stopApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Stopping the request processing pipeline for application: {applicationId}
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:initializePipeline(java.lang.String):[INFO] Initializing request processing pipeline for user: {}
org.apache.hadoop.lib.server.Server:init():[ERROR] Services initialization failure, destroying initialized services
org.apache.hadoop.service.launcher.AbstractLaunchableService:bindArgs(org.apache.hadoop.conf.Configuration,java.util.List):[DEBUG] arg
org.apache.hadoop.fs.s3a.impl.V2Migration:v1CustomSignerUsed():[WARN] The signer interface has changed in AWS SDK V2, custom signers will need to be updated once S3A is upgraded to SDK V2
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseEvenlyFromRemainingRacks(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,int,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException):[TRACE] Excluded nodes: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] Application {applicationID} transitioned from {oldState} to {newState}
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:writeAppendBlobCurrentBufferToService():[DEBUG] Update SAS token
org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain:addProcessor(org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Adding [ + processor.getClass().getName() + ] tp top of + AMS Processing chain.
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:deleteApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[ERROR] Wrong behavior during deleting the application {request.getApplicationId()}
org.apache.hadoop.oncrpc.SimpleUdpServer:run():[INFO] Started listening to UDP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] return byte[array.length]
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:read(java.nio.ByteBuffer):[TRACE] read(arr.length={}, off={}, len={}, filename={}, block={}, canSkipChecksum={}): I/O error
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration):[ERROR] Could not initialize shared edits dir
org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean):[DEBUG] Found existing {servletName} servlet at path {pathSpec}; will replace mapping with {servletHolderName} servlet
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:receivedNewWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider):[INFO] OpenFileCtx is inactive, fileId: {fileId}
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String):[WARN] Total Nodes in scope : {totalInScopeNodes} are less than Available Nodes : {availableNodes}
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEvent):[ERROR] Unknown event arrived at ContainerScheduler: {event.toString()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[DEBUG] Skipping container release on removed node: {nodeID}
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:openInputStream(org.apache.hadoop.fs.azure.StorageInterface$CloudBlobWrapper,java.util.Optional):[DEBUG] Using stream seek algorithm {inputStreamVersion}
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:createDirAndPersistApp(org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Persisted service service.getName() version service.getVersion() at appJson
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean):[INFO] File committed successfully
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object):[WARN] Failed to register MBean "{name}": Instance already exists.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:calculateAndGetAMResourceLimitPerPartition(java.lang.String):[DEBUG] Queue: {}, node label : {}, queue partition resource : {}, queue current limit : {}, queue partition usable resource : {}, amResourceLimit : {}
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] No principal name specified. Will use AM login identity {} to attempt keytab-based login
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handle(org.apache.hadoop.yarn.event.Event):[INFO] [CompInstanceId] Transitioned from [oldState] to [getState] on [eventType] event
org.apache.hadoop.hdfs.server.namenode.CacheManager:startMonitorThread():[INFO] CacheReplicationMonitor already running
org.apache.hadoop.yarn.service.client.ServiceClient:addKeytabResourceIfSecure(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[WARN] No Kerberos keytab specified for service.getName()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:getWriter(java.lang.String):[ERROR] Cannot open write stream for {}
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:addBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[],org.apache.hadoop.hdfs.protocol.BlockType):[DEBUG] DIR* FSDirectory.addBlock: {path} with {block} block is added to the in-memory file system
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:serviceStart():[INFO] Starting ApplicationHistory
org.apache.hadoop.hdfs.server.balancer.Balancer:matchSourceWithTargetToMove(org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup):[INFO] Decided to move X bytes from sourceName to targetName
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:removeMountTableEntry(org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest):[WARN] Unable to clear quota at the destinations for {}: {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:cleanup():[ERROR] Failed to delete dumpfile: {}
org.apache.hadoop.hdfs.server.federation.resolver.PathLocation:orderedNamespaces(java.util.List,java.lang.String):[DEBUG] Cannot find location with namespace {} in {}
org.apache.hadoop.fs.s3a.WriteOperationHelper:abortMultipartUploadsUnderPath(java.lang.String):[DEBUG] Number of outstanding uploads: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:apply(java.lang.Object):[ERROR] {}: Stage {} failed: after {}: {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis()), e.toString()
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[DEBUG] Initializing job
org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable:run():[INFO] Finishing task: <reduceId>
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext):[INFO] BLOCK* processReport: logged info for {} of {} reported.
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String):[INFO] prefix + metrics system started in standby mode
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] Added {} entries; ignored {}; hasNext={}; hasMoreObjects={}
org.apache.hadoop.tools.HadoopArchiveLogs:filterAppsByAggregatedStatus():[INFO] appId has aggregation status aggStatus
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int):[DEBUG] Generating encrypted key with name {}, the edek Operation is {}.
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor:processDirDiffEntry():[DEBUG] Processing dirDiffEntry
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline service is enabled; version: [...
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[INFO] Starting to load token cache.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:call():[WARN] Exception while uploading the file + localPath.getName()
org.apache.hadoop.fs.s3a.S3AInputStream:readSingleRange(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer):[DEBUG] Finished reading range {} from path {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[DEBUG] Configuration failed, falling back to default
org.apache.hadoop.fs.RemoteIterator:next():[DEBUG] Next element retrieved from RemoteIterator
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:writeGlobalCleanerPidFile():[INFO] Created the global cleaner pid file at pidPath.toString()
org.apache.hadoop.fs.s3a.auth.SignerManager:initCustomSigners():[DEBUG] Creating signer initializer: [{parts[2]}] for signer: [{parts[0]}]
org.apache.hadoop.util.JsonSerialization:fromJson(java.lang.String):[ERROR] Exception while parsing json: {}\\n{}
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge():[DEBUG] writing intermediate results to <outputFile>
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:checkVersion():[INFO] Storing state version info
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:validateConf(org.apache.hadoop.conf.Configuration):[INFO] per directory file limit = ${perDirFileLimit}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}
org.apache.hadoop.fs.azurebfs.services.AbfsListStatusRemoteIterator:asyncOp():[ERROR] Thread got interrupted: {}
org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedAction):[DEBUG] PrivilegedAction [as: {}][action: {}]
org.apache.hadoop.ipc.Client$Connection:close():[WARN] A connection is closed for no cause and calls are not empty
org.apache.hadoop.fs.s3a.Invoker:once(java.lang.String,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE):[DEBUG] Executing operation with no result
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean):[DEBUG] BLOCK* InvalidateBlocks: add Block to DatanodeInfo
org.apache.hadoop.tools.dynamometer.ApplicationMaster:cleanup():[INFO] Application completed. Signalling finish to RM
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:datanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] logAuditEvent
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:balanceVolumeSet(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[DEBUG] Skipping compute move. lowVolume: {}, highVolume: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceStop():[INFO] Stop + getName()
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List):[INFO] finalMerge called with X in-memory map-outputs and Y on-disk map-outputs
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:nextOp():[INFO] Next operation retrieved from redundant input stream
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:handleAppSubmitEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$AbstractDelegationTokenRenewerAppEvent):[DEBUG] Registering tokens for renewal for: appId = {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String):[INFO] Storing reservationallocation for {reservationIdName} for plan {planName}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[INFO] Audit log for success
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:launchAM(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] ShellExecutor: Interrupted while reading the error/out stream
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Setup Task %s
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateFromReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[ERROR] Trying to schedule for a finished app, please double check. nodeId= + node.getNodeID() + container= + reservedContainer.getContainerId()
org.apache.hadoop.yarn.server.webproxy.AppReportFetcher:getApplicationReport(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application report fetched from AHS
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processPerfectOverWrite(org.apache.hadoop.hdfs.DFSClient,long,int,org.apache.hadoop.nfs.nfs3.Nfs3Constant$WriteStableHow,byte[],java.lang.String,org.apache.hadoop.nfs.nfs3.response.WccData,org.apache.hadoop.security.IdMappingServiceProvider):[ERROR] Can't read back {} bytes, partial read size: {}, count, readCount
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:uploadLogsForContainers(boolean):[DEBUG] Cycle #{} of log aggregator
org.apache.hadoop.hdfs.server.namenode.NameNode:copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration):[DEBUG] Beginning to copy stream {} to shared edits
org.apache.hadoop.hdfs.server.mover.Mover$Processor:processFile(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.server.mover.Mover$Result):[WARN] The storage policy + policy.getName() + is not suitable for Striped EC files. + So, Ignoring to move the blocks
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: USER_MAX_RUNNING_APPS
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[INFO] Storing info for attempt: + appAttemptId + at: + nodeCreatePath
org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps():[ERROR] Can't update the maps. Will use the old ones, which can potentially cause problem.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:trackPreemptionsAgainstNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt):[DEBUG] Adding containers for preemption
org.apache.hadoop.yarn.service.ServiceScheduler:recoverComponents(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Handling {} from previous attempt
org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set):[DEBUG] Ldap group query string: + filter.toString()
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeDirectorySection(java.io.InputStream,java.util.List):[INFO] Loading inode directory section
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[INFO] Successfully loaded file fileName
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Source file + src.toString() + not found, but target + file + target.toString() + already exists. Move already + happened.
org.apache.hadoop.mapred.LocalJobRunner$Job:fsError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String):[ERROR] FSError: [message] from task: [taskId]
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:validateAndGetSchedulerConfiguration(org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo,javax.servlet.http.HttpServletRequest):[WARN] Configuration change validation only supported by MutableConfScheduler.
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(java.lang.Object,java.lang.Object):[INFO] [FLEX COMPONENT {}]: Flex deferred because dependencies not satisfied., component.getName
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.TimelineCollectionReader:readDocument(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[DEBUG] Fetching document for entity type {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendContainerMonitorStartEvent():[DEBUG] Handling Container Start Monitoring Event
org.apache.hadoop.yarn.webapp.WebApps$Builder:start():[INFO] Web app {name} started at {httpServer.getConnectorAddress(0).getPort()}
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[INFO] EDEKCacheLoader interrupted during retry.
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] Path: [{}] is a file. COS key: [{}]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Target node's reservation status changed, moving cancelled.
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumInMaintenanceDeadDataNodes():[DEBUG] Failed to get number of dead in maintenance nodes
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[WARN] Request #getBlocks to Standby NameNode but meet exception, will fallback to normal way.
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:getProcessId(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Got exception reading pid from pid-file {}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:printConfiguredHosts(boolean):[DEBUG] hostsReader: in= + conf.get(YarnConfiguration.RM_NODES_INCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_INCLUDE_FILE_PATH) + out= + conf.get(YarnConfiguration.RM_NODES_EXCLUDE_FILE_PATH, YarnConfiguration.DEFAULT_RM_NODES_EXCLUDE_FILE_PATH)
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:initRollingLevelDB(java.lang.Long,org.apache.hadoop.fs.Path):[INFO] Initializing rolling leveldb instance :{rollingInstanceDBPath} for start time: {dbStartTime}
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest):[ERROR] e.getMessage(), e
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[INFO] Successfully cached one replica:{} into persistent memory, [cached path={}, address={}, length={}]
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:isStale():[TRACE] {}: checked shared memory segment. isStale={}
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:revertCommit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit):[INFO] Revert {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getNetworkDependenciesWithDefault(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[ERROR] Unresolved dependency mapping for host + node.getHostName() + . Continuing with an empty dependency list
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStop():[ERROR] Error closing store.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:loadDetailLog(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId):[DEBUG] Refresh logs for cache id {groupId}
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext):[WARN] Output Path is null in cleanupJob()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] The job has a total of [taskCount] tasks.
org.apache.hadoop.util.Shell:runCommand():[DEBUG] Error reading the error stream due to shell command timeout
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:visit(java.io.RandomAccessFile):[INFO] Loading string table
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing event for {} of type {}
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:registerNamenode(org.apache.hadoop.hdfs.server.federation.resolver.NamenodeStatusReport):[WARN] Cannot register namenode, router ID is not known {}
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean):[ERROR] Could not initialize shared edits dir, ioe
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:startInternal():[INFO] Using state database at + storeRoot + for recovery
org.apache.hadoop.mapred.lib.MultipleOutputs:close():[ERROR] Error while closing MultipleOutput file
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:checkForCompletedNodes(java.util.List):[INFO] Node {} has {} blocks yet to process
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.util.Map,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType):[DEBUG] Checking user [{}] for: {}: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getNonLabeledQueueCapacity(java.lang.String):[DEBUG] CSConf - getCapacity: queuePrefix={}, capacity={}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] BEFORE decResourceRequest: applicationId= applicationId.getId() priority= priority.getPriority() resourceName= resourceName numContainers= remoteRequest.getNumContainers() #asks= ask.size()
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:setStoragePolicy(java.lang.String,java.lang.String):[DEBUG] *DIR* NameNode.setStoragePolicy for path: {}, policyName: {}
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectoryMultiThreaded():[DEBUG] Traversing into source dir: {}
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:isTokenAboutToExpire():[DEBUG] MSIToken: token renewing. Time elapsed since last token fetch: {} milli seconds
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[INFO] Initialized Runc runtime
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[DEBUG] Block {}: we only have {} of {} cached replicas. + {} DataNodes have insufficient cache capacity.
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] Processing volume event, type=EVENT_TYPE, volumeId=VOLUME_ID
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator:load(java.io.File,boolean):[DEBUG] Loading using Default Loader
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:updateLeafQueueState():[INFO] managedParentQueue.getQueuePath() + " : Removed queue" + queue + " from leaf queue state from partition " + partition
org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean):[ERROR] error parsing conf UNKNOWN_RESOURCE
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult):[ERROR] Cannot check overrides for record
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Unable to locate any logs for container
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:logAllLiveStackTraces():[DEBUG] Thread <thread_name>
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromTcp():[TRACE] {}: got security exception while constructing a remote block reader from {}
org.apache.hadoop.fs.s3a.InconsistentAmazonS3Client:deleteObject(com.amazonaws.services.s3.model.DeleteObjectRequest):[DEBUG] key {}
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[ERROR] Hadoop HttpServer2 App **failed**, {ex}
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[ERROR] ====== Fencing on target failed, skipping fencing on source ======
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[WARN] ServicePlugin {} could not be stopped
org.apache.hadoop.fs.s3a.WriteOperations:commitUpload(java.lang.String,java.lang.String,java.util.List,long):[INFO] Multipart upload committed successfully
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] Got finalize command for block pool ...
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:verifyRMHeartbeatResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[ERROR] NM node labels { + StringUtils.join(,, getPreviousValue()) + } were not accepted by RM and message from RM : + response.getDiagnosticsMessage()
org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable):[INFO] available bytes: + valIn.available()
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:completeFileInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long):[INFO] DIR* completeFile: request from ... to complete inode ... which is already closed.
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:uploadFileToPendingCommit(java.io.File,org.apache.hadoop.fs.Path,java.lang.String,long,org.apache.hadoop.util.Progressable):[DEBUG] File size is {}, number of parts to upload = {}
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$StatusUpdaterRunnable:updateTimelineCollectorData(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] No collectors to update RM
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationAttemptFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptFinishData):[INFO] Finish information of application attempt + appAttemptFinish.getApplicationAttemptId() + is written
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] Nodes refreshed
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:nextDomainPeer():[TRACE] nextDomainPeer: reusing existing peer {}
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:flush():[DEBUG] Flushing cache
org.apache.hadoop.fs.s3a.S3AFileSystem:toString():[INFO] toString method executed
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is greater than 1.0 but should be in the range 0.0 - 1.0
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerStatusRetriever:run():[INFO] instance.compInstanceId + IP = + status.getIPs() + , host = + status.getHost() + , cancel container status retriever
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] DNS Service added
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Auth method <meth>
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameTo(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.Options$Rename[]):[INFO] logAuditEvent(true, "rename (options=" + Arrays.toString(options) + ")", src, dst, res.auditStat)
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation):[DEBUG] Page blob directories: {}
org.apache.hadoop.hdfs.server.namenode.FSNDNCacheOp:removeCacheDirective(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.CacheManager,long,boolean):[INFO] Cache directive removed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserve(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey):[INFO] Making reservation: node= + node.getNodeName() + app_id= + getApplicationId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuResourcePlugin:validateExecutorConfig(org.apache.hadoop.conf.Configuration):[WARN] Using GPU plugin with disabled LinuxContainerExecutor is considered to be unsafe.
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:execute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] IOStatisticsBinding track duration started
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:requestContainerUpdate(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.api.records.UpdateContainerRequest):[INFO] Requesting Container update : container=<container>, updateType=<updateType>, targetCapability=<targetCapability>, targetExecType=<targetExecType>
org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException):[ERROR] An error occurred when terminating
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[TRACE] Acquiring write lock to replay edit log
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String):[DEBUG] Chosen node {ret} from first random
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[INFO] Failed to process fileInfo for job: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[INFO] Resource requests recovered for container
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocateContainersInternal(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$AllocationParams,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$ContainerIdGenerator,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.Map,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$EnrichedResourceRequest,int):[WARN] Unable to allocate any opportunistic containers.
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:enableBlockPoolId(java.lang.String):[WARN] {}: already enabled scanning on block pool {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSync():[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}
org.apache.hadoop.yarn.server.webapp.LogWebService:getEntity(java.lang.String,javax.ws.rs.core.MultivaluedMap):[ERROR] Response from the timeline reader server is {msg}
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] PrimaryGroup rule: parent rule result: {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:get(java.lang.Class):[ERROR] Cannot get data for {}: {}
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:render():[WARN] Unknown Container. Container might have completed, please go back to the previous page and retry
org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] Get token info proto: + protocol + info: + tokenInfo
org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport:translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException):[INFO] Bulk delete operation failed to delete all objects; failure count = {errors.size()}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent):[DEBUG] Ignoring client socket close, cause
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleInitApplicationResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application):[INFO] Signaled container init for app: {appIdStr}
org.apache.hadoop.lib.server.Server:destroyServices():[DEBUG] Destroying service [{}]
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[INFO] Flushing subClusters from cache and rehydrating from store, most likely on account of RM failover.
org.apache.hadoop.yarn.client.cli.ApplicationCLI:killApplication(java.lang.String):[INFO] Killing application appId
org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String):[WARN] NativeIO.getStat error ({}): {} -- file path: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseEvenlyFromRemainingRacks(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,int,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException):[DEBUG] Best effort placement failed: expecting {} replicas, only chose {}.
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:start(org.apache.hadoop.conf.Configuration):[INFO] Submission Context Preprocessor enabled: file=[{}], interval=[{}]
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[INFO] Job is running in background.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown from onContainerStopped for Container event.getContainerId()
org.apache.hadoop.ipc.Server$Connection:checkDataLength(int):[WARN] Unexpected data length x!! from y
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeLabelsProvider(org.apache.hadoop.conf.Configuration):[DEBUG] Distributed Node Labels is enabled with provider class as : {Class:NodeLabelsProvider}
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$ReplicatedBlockChecksumComputer:computeCompositeCrc(long):[DEBUG] block={}, getBytesPerCRC={}, crcPerBlock={}, compositeCrc={}
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation):[DEBUG] Atomic rename directories: {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:removeFromClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RemoveFromClusterNodeLabelsRequest):[INFO] RMAuditLogger: User [username] successfully executed removeFromClusterNodeLabels
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] KeyStore loaded successfully from '%s'!!
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot get existing records
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:sleepAndLogInterrupts(int,java.lang.String):[INFO] BPOfferService + this + interrupted while + stateString
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Node being looked for scheduling {} availableResource: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:addOrUpdateReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String,org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction,boolean):[DEBUG] Updating reservation: {reservationIdName} in plan:{planName} at: {reservationPath}
org.apache.hadoop.nfs.nfs3.Nfs3Interface:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Creating a directory
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeEmptyFile(java.lang.String):[ERROR] Store empty file failed. COS key: [{}], exception: [{}].
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop():[INFO] Service {} unregistered with RM, with attemptId = {} , diagnostics = {}
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[DEBUG] Setting the title for the application
org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key):[DEBUG] Duplicate FS created for {}; discarding {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:getChildren(java.lang.String):[ERROR] Cannot get children for {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[INFO] Added Application Attempt + appAttemptId + to scheduler from user + application.getUser()
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:logJobHistoryFinishedEvent():[INFO] Calling handler for JobFinishedEvent
org.apache.hadoop.mapred.JobQueueClient:displayQueueAclsInfoForCurrentUser():[INFO] =====================
org.apache.hadoop.yarn.service.ServiceScheduler:terminateServiceIfDominantComponentFinished(org.apache.hadoop.yarn.service.component.Component):[INFO] Dominate component {} finished, exiting Service Master... , final status= {Succeeded/Failed}
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String):[WARN] Invalid value {maxArgs} for {SCRIPT_ARG_COUNT_KEY}; must be >= {MIN_ALLOWABLE_ARGS}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getClusterId():[ERROR] Cannot fetch cluster ID metrics {IOException message}
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfBlockReportParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Setup Job %s
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo):[WARN] Invalid namespaceID in journal request - expected [expectedNamespaceID] actual [journalInfo.getNamespaceId()]
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier):[DEBUG] Trying to retrieve password for {applicationAttemptId}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread:run():[ERROR] Error processing response
org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration):[DEBUG] Usersearch baseDN: {userbaseDN}
org.apache.hadoop.tools.util.DistCpUtils:getFileSize(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Retrieving file size for: {path}
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:printLocalLogFile(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,java.io.File):[ERROR] Exception reading log file [file path]
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelayedTokenRemovalRunnable:run():[INFO] Delayed Deletion Thread Interrupted. Shutting it down
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[WARN] Event + event + sent to absent application + event.getApplicationID()
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getNodeResources(org.apache.hadoop.conf.Configuration):[DEBUG] Set memory to {}
org.apache.hadoop.yarn.service.ServiceScheduler:registerServiceInstance(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Registering + attemptId + , + service.getName() + into registry
org.apache.hadoop.yarn.service.component.Component$ContainerRecoveredTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [COMPONENT {}]: Recovered {} for component instance {} on host {}, num pending component instances reduced to {}
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context):[INFO] GridMix is configured to use a compression ratio of <compressionRatio> for the reduce output data.
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:takeAndProcessTasks():[DEBUG] Skipped a canceled re-encryption task
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:authorizeStartAndResourceIncreaseRequest(org.apache.hadoop.yarn.security.NMTokenIdentifier,org.apache.hadoop.yarn.security.ContainerTokenIdentifier,boolean):[ERROR] Unauthorized request to start container. Attempt to relaunch the same container with id ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:setHostname(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand,java.lang.String,java.lang.String,java.lang.String):[INFO] setting hostname in container to: + name
org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options):[INFO] Key creation with LoadBalancingKMSClientProvider successful
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:createFileSystem(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] AzureBlobFileSystem.createFileSystem uri: {}
org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded():[WARN] Wrong output from sysInfo: ...
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:setDecommissionedNMs():[DEBUG] Creating unknown node ID
org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration):[WARN] ${DEPRECATED_UGI_KEY} should not be used. Instead, use ${HADOOP_HTTP_STATIC_USER}.
org.apache.hadoop.yarn.server.federation.store.metrics.FederationStateStoreClientMetrics:succeededStateStoreCall(long):[ERROR] UNKNOWN_SUCCESS_ERROR_MSG, methodName
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:refreshLoadedJobCache():[WARN] Failed to execute refreshLoadedJobCache: CachedHistoryStorage is not started
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Source path and dest path refer to the same file or directory: [{}]
org.apache.hadoop.yarn.service.ServiceMaster:serviceStop():[INFO] Stopping app master
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[INFO] Failed to cache + key + : failed to find backing files.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:queueReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.lang.String):[DEBUG] Queueing reported block {} in state {} from datanode {} for later processing because {}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:onDevicesAllocated(java.util.Set,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.YarnRuntimeType):[DEBUG] Generating runtime spec for allocated devices: {}, {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[]):[WARN] Can not set up custom log4j properties.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet,boolean):[INFO] Audit log creation successful
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessSmapMemoryInfo:setMemInfo(java.lang.String,java.lang.String):[DEBUG] setMemInfo : memInfo : {info}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,long):[INFO] updating RMDelegation token with sequence number: (actual sequence number retrieved from id.getSequenceNumber())
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$PutEventThread:run():[WARN] SystemMetricsPublisher.class.getName() + " is interrupted. Exiting."
org.apache.hadoop.yarn.service.client.ServiceClient:addKeytabResourceIfSecure(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[WARN] service.getName()'s keytab (principalName = principalName) doesn't exist at: keytabOnhdfs
org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception):[INFO] <log message>
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:cleanupStagingDirs():[INFO] StagingCommitter staging dirs cleaned
org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler:setup(org.apache.hadoop.hdfs.server.datanode.VolumeScanner):[TRACE] Starting VolumeScanner {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[]):[DEBUG] *DIR* NameNode.rename: " + src + " to " + dst
org.apache.hadoop.nfs.NfsExports$CIDRMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] CIDRNMatcher low = + subnetInfo.getLowAddress() + , high = + subnetInfo.getHighAddress() + , allowing client ' + address + ', ' + hostname + '
org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent):[DEBUG] Event received with stale zk
org.apache.hadoop.ipc.Server$Handler:run():[DEBUG] Thread.currentThread().getName(): exiting
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Failed to assign to queue: + getQueuePath() + nodePartition: + nodePartition + , usedResources: + queueUsage.getUsed(nodePartition) + , clusterResources: + clusterResource + , reservedResources: + resourceCouldBeUnreserved + , maxLimitCapacity: + currentLimitResource + , currTotalUsed: + usedExceptKillable
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] Failed creation of <ioPath> directory\n
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:sendLifelineIfDue():[DEBUG] Skipping sending lifeline for + BPServiceActor.this + , because heartbeats are disabled for tests.
org.apache.hadoop.hdfs.server.namenode.BackupImage:recoverCreateRead():[INFO] Storage directory + sd.getRoot() + is not formatted.
org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration):[DEBUG] Groupsearch baseDN: {groupbaseDN}
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getNumUnderConstructionBlocks():[WARN] The file {} is not under construction but has lease.
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[INFO] Initialized readable endpoints
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:executeRenamingOperation(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[DEBUG] getName(): operation stack trace
org.apache.hadoop.fs.s3a.S3AFileSystem:innerCreateFile(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.fs.s3a.impl.CreateFileBuilder$CreateFileOptions):[DEBUG] Overwriting file {}
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeLoggedTask(org.apache.hadoop.tools.rumen.LoggedTask):[WARN] Task + task.getTaskID() + has nulll TaskStatus
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSOpen operation executed
org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker:check(org.apache.hadoop.conf.Configuration,java.util.Collection):[WARN] StorageLocation {location} appears to be degraded.
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean):[INFO] logEdit executed
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:init(org.apache.hadoop.conf.Configuration):[DEBUG] Driver class not found.
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KilledDuringSetupTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Job received kill in SETUP state.
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:addPlacementConstraintHandler(org.apache.hadoop.conf.Configuration):[INFO] YARN Configuration: Processor RM Placement Constraints Handler will be used. Scheduling requests will be handled by the placement constraint processor
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:waitAllPeers(long,java.util.concurrent.TimeUnit):[DEBUG] Interrupted waiting for peers to close
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode):[DEBUG] The property '+ locationProperty +' has not been set, no TrustStore will be loaded
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:retryableRequest(java.lang.String[],java.lang.String,java.util.List,java.lang.String):[DEBUG] {exceptionMessage}
org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC:getServer(java.lang.Class,java.lang.Object,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,int,java.lang.String):[DEBUG] Creating a HadoopYarnProtoRpc server for protocol {} with {} + handlers
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[INFO] Running Client
org.apache.hadoop.crypto.key.kms.server.KMS:handleEncryptedKeyOp(java.lang.String,java.lang.String,java.util.Map):[ERROR] IllegalArgumentException Wrong eekOp value, it must be EEK_GENERATE or EEK_DECRYPT
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[INFO] [COMPONENT {}] Health recovered above threshold at ts = {} ({})
org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration):[DEBUG] Compression codec created
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerPoolTracker:run():[INFO] Exhausted max retry attempts {} in token renewer thread for {}
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics:getOutliers():[TRACE] DataNodePeerMetrics: Got stats: {}
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable):[DEBUG] Proxy for + uri + failed. cause: , cause
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:startWebApp():[INFO] Instantiating AHSWebApp at {getPort()}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for fileId: {}
org.apache.hadoop.hdfs.DFSStripedOutputStream:hsync(java.util.EnumSet):[DEBUG] DFSStripedOutputStream does not support hsync {}. Caller should check StreamCapabilities before calling.
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:shedQueuedOpportunisticContainers():[INFO] Opportunistic container {} will be killed to meet NM queuing limits.
org.apache.hadoop.portmap.RpcProgramPortmap:getport(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR):[DEBUG] Found mapping for key: + key + port: + res
org.apache.hadoop.hdfs.server.namenode.FSImageFormat:renameReservedRootComponentOnUpgrade(byte[],int):[INFO] Renamed root path + FSDirectory.DOT_RESERVED_STRING + to + renameString
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:purgeLogsOlderThan(long):[INFO] Purging remote journals older than txid + minTxIdToKeep
org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Access control headers '{accessControlRequestHeaders}' not allowed. Returning
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getInitialCachedResources(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[ERROR] The shared cache root directory location was not found
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.event.Event):[WARN] Event + event + sent to absent application + event.getApplicationID()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Can't get path for fileId: {}
org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean):[DEBUG] responding to call
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:createNewApplication(javax.servlet.http.HttpServletRequest):[DEBUG] Initializing
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session):[INFO] [s.getRemoteAddress().getHostString() + " connected!"]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuNodeResourceUpdateHandler:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[WARN] Found nUsableGpus usable GPUs, however GPU_URI resource-type is not configured inside resource-types.xml, please configure it to enable GPU feature or remove GPU_URI from YarnConfiguration.NM_RESOURCE_PLUGINS
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler:killTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskAttemptRequest):[INFO] Kill task attempt TASK_ID received from USER at SERVER_ADDRESS
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized HBaseTimelineWriterImpl UGI to ...
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:cleanup():[INFO] There are {} pending writes.
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,boolean,boolean):[DEBUG] getContainerLogsInfo
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:serviceStart():[DEBUG] scheduleLogDeletionTask
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List):[INFO] Storage directory with location {} is not formatted for namespace {}. Formatting...
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getDestinationForPath(java.lang.String):[ERROR] Cannot find resolver for order {}, order
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:addDirectoryToJobListCache(org.apache.hadoop.fs.Path):[DEBUG] Adding + path + to job list cache.
org.apache.hadoop.yarn.service.component.Component:handle(org.apache.hadoop.yarn.event.Event):[INFO] [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType()
org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping():[INFO] Not doing static UID/GID mapping because 'staticMappingFile' does not exist.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.event.Event):[WARN] Got exception while signaling container + containerId + with command + signalEvent.getCommand()
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:alterWriteRequest(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,long):[DEBUG] Got overwrite with appended data
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:updatePipelineInternal(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean):[WARN] Update (size=) to a smaller size block
org.apache.hadoop.mapred.gridmix.Statistics:add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats):[DEBUG] Reached maximum limit of jobs in a polling interval + completedJobsInCurrentInterval
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Lost Nodes: countHere
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId):[WARN] No file for jobconf with jobId found in cache!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getBlockInputStreamWithCheckingPmemCache(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[DEBUG] Get InputStream by cache address.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[DEBUG] Delete {cachePath} due to unsuccessful mapping.
org.apache.hadoop.yarn.client.cli.ApplicationCLI:listApplicationAttempts(java.lang.String):[INFO] Total number of application attempts: X
org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Searching for KMS delegation token in user {}'s credentials
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getApplicationOwner(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Loaded indexed logs meta
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:prepareFileForTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] BLOCK* prepareFileForTruncate: Scheduling copy-on-truncate to new size {truncatedBlockUC.getNumBytes()} new block {newBlock} old block {oldBlock}
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:createSuccessMarker(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.commit.files.SuccessData,boolean):[DEBUG] Touching success marker for job {}: {}, markerPath, successData
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Error when publishing entity
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Publishing the entity {} JSON-style content: {}
org.apache.hadoop.hdfs.server.balancer.Balancer$Cli:run(java.lang.String[]):[INFO] ...(IOException e.Message)
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$StatusUpdateWhenHealthyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[INFO] Node {rmNode.nodeId} reported DECOMMISSIONED
org.apache.hadoop.fs.azure.NativeAzureFileSystem:updateParentFolderLastModifiedTime(java.lang.String):[WARN] Got unexpected exception trying to get lease on parentKey. e.getMessage
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:processHeartbeat(java.util.List):[ERROR] Unknown resource reported: {req}
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long):[DEBUG] Setting counter {} to {}, key, value
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Sorry, [jid] not found.
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] ------------------------------------
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:executeDockerCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor,boolean,org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] Running docker command: {}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:setResponseHeaders(org.jboss.netty.handler.codec.http.HttpResponse,boolean,long):[DEBUG] Setting connection close header...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:cleanupAssignedDevices(java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] Recycle devices: {}, type: {} from {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:getSmapBasedRssMemorySize(int):[DEBUG] total({}, olderThanAge, p.getPid(), info, (total * KB_TO_BYTES))
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] New Resource Instance Created
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token kind is {} and the token's service name is {}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:uploadPart(java.io.File,java.lang.String,java.lang.String,int):[ERROR] Current thread: [%d], COS key: [%s], upload id: [%s], part num: [%d], exception: [%s]
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[ERROR] Unable to update the ApplicationId {appId} into the FederationStateStore
org.apache.hadoop.yarn.service.webapp.ApiServer:getService(javax.servlet.http.HttpServletRequest,java.lang.String):[ERROR] Get service failed: {}
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer):[DEBUG] Pushing record {entry.name}.{record.context}.{record.name} to {name}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handleComponentInstanceRelaunch(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent,boolean,java.lang.String):[INFO] compInstance.getCompInstanceId() + (!hasContainerFailed ? succeeded : failed) + without retry, exitStatus= + event.getStatus()
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getDomains(java.lang.String):[INFO] Service stopped, return null for the storage
org.apache.hadoop.fs.s3a.impl.DeleteOperation:deleteDirectoryTree(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Getting objects for directory prefix {} to delete
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,int,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] got reply from datanode:{}, md5={}
org.apache.hadoop.yarn.security.AMRMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token kind is {} and the token's service name is {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[INFO] Promotion Update requests : + promotionRequests
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] Got response from RM for container ask, completedCnt= [size]
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:verifyOutstandingPathQLimit():[DEBUG] Satisifer Q - outstanding limit:{}, current size:{}
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Redirecting to URI
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:parseGpuDevicesFromUserDefinedValues():[INFO] Allowed GPU devices: + gpuDevices
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS CREATE dir fileHandle: {} filename: {} client: {}
org.apache.hadoop.yarn.service.webapp.ApiServer:deleteService(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] DELETE: deleteService for appName = {} user = {}
org.apache.hadoop.registry.server.dns.RegistryDNS:getReverseZoneName(org.apache.hadoop.conf.Configuration):[WARN] Unable to convert {} to DNS name
org.apache.hadoop.yarn.server.utils.BuilderUtils:newContainerToken(org.apache.hadoop.yarn.api.records.NodeId,byte[],org.apache.hadoop.yarn.security.ContainerTokenIdentifier):[DEBUG] Building token service
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:fenceOldActive(byte[]):[DEBUG] Request to fence old active being ignored, as embedded leader election doesn't support fencing
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean):[ERROR] Failed to create file:
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] {}: pulled the last slot {} out of {}, this, slot.getSlotIdx(), shm
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:cleanLogs(org.apache.hadoop.fs.Path,long):[DEBUG] Cluster timestamp validation check
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:checkLimit(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$ProcessTreeInfo,long,long):[ERROR] Killed container process with PID {} but it is not a process group leader., {pId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:cleanupContainerFiles(org.apache.hadoop.fs.Path):[DEBUG] cleanup container {} files
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:moveBlock(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.Socket,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[WARN] Failed to move block
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest):[DEBUG] Trying to delete reservation
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:updateAMRMTokens(org.apache.hadoop.yarn.security.AMRMTokenIdentifier,org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$RequestInterceptorChainWrapper,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[INFO] RM rolled master-key for amrm-tokens
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[DEBUG] Removing RMDelegationToken_{}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to start namenode container ID {containerId} with error: {Throwable}
org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration):[WARN] Failed to initialize filesystem {}: {}, uri, e.toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Delete ClusterNode:
org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int):[INFO] Asking remote to cede its active state for timeout ms
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] -delete option is enabled. About to remove entries from target that are missing in source
org.apache.hadoop.fs.s3a.S3AInstrumentation:toString():[DEBUG] instanceIOStatistics=[value]
org.apache.hadoop.hdfs.server.datanode.DataNode:notifyNamenodeDeletedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[ERROR] Cannot find BPOfferService for reporting block deleted for bpid= + block.getBlockPoolId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleDestroyApplicationResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application):[ERROR] Unable to remove resource + rsrc + for + appIDStr + from state store
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:startWepApp():[INFO] Using war file at: /path/to/war/file
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String):[TRACE] First execution of REST operation getTokenSingleCall
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processCompletedNodes(java.util.List):[INFO] Node {} completed decommission and maintenance but has been moved back to in service
org.apache.hadoop.fs.cosn.CosNInputStream:read(byte[],int,int):[DEBUG] Statistics incremented
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Application added - appId: <Placeholder for applicationId> user: <Placeholder for user> leaf-queue of parent: <Placeholder for getQueuePath()> #applications: <Placeholder for getNumApplications()>
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:validateReconstructionWork(org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork):[DEBUG] BLOCK* Removing {} from neededReconstruction as it has enough replicas
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:compileReport(java.io.File,java.io.File,java.util.Collection,org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler):[WARN] Exception occurred while compiling report
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getRMWebAppURLWithScheme(org.apache.hadoop.conf.Configuration):[DEBUG] Check if HA is enabled
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:initCostTable():[ERROR] Failed to get devices!, e
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager:sendIBRs(org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol,org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.lang.String):[WARN] Failed to call blockReceivedAndDeleted: {}, nnId: {} + , duration(ms): {}, Arrays.toString(reports), nnRpcLatencySuffix, monotonicNow() - startTime
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Updating AMRMToken
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:updateDfsUsageConfig(java.lang.Long,java.lang.Long,java.lang.Class):[INFO] Preconditions check for interval
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest):[DEBUG] getNewApplication try #{} on SubCluster {}, i, subClusterId
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:cleanUpLocalDir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.yarn.server.nodemanager.DeletionService,java.lang.String):[WARN] Failed to delete localDir: + localDir
org.apache.hadoop.hdfs.server.datanode.BlockSender:close():[WARN] Unable to drop cache on file close
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[DEBUG] From NM Context container {}
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:addPlacementConstraintHandler(org.apache.hadoop.conf.Configuration):[INFO] YARN Configuration: Scheduler RM Placement Constraints Handler + placement handler will be used. Scheduling requests will be handled by the main scheduler.
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.LinkedList,org.apache.hadoop.tools.CopyListingFileStatus):[DEBUG] concat: result: + dstfs.getFileStatus(firstChunkFile)
org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String):[WARN] truncating long string: + s.length() + chars, starting with + s.substring(0, 20)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS READLINK fileHandle: {handle.dumpFileHandle()} client: {remoteAddress},
org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover():[INFO] Local node is already active. No need to failover. Returning success.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid MKDIR request
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentInstanceEventHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] No component instance exists for + event.getContainerId()
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:satisfyStoragePolicy(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,java.lang.String,boolean):[WARN] Cannot request to call satisfy storage policy on path: {}, as this file/dir was already called for satisfying storage policy.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:updatePendingResources(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.PendingAskUpdateResult,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics):[INFO] checking for deactivate of application : + this.applicationId
org.apache.hadoop.net.unix.DomainSocketWatcher:kick():[ERROR] this + : error writing to notificationSockets[0], e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean):[INFO] Accepted application <applicationId> from user: <user>, currently num of applications: <app_size>
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL (Took x ms.)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:executeDockerInspect(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerInspectCommand):[INFO] {} : docker inspect output {}
org.apache.hadoop.hdfs.server.datanode.DataStorage:doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Layout version rolled back to DATANODE_LAYOUT_VERSION for storage sd.getRoot()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:enqueueEdit(org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$Edit):[DEBUG] logEdit {edit}
org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl:getContainerSASUri(java.lang.String,java.lang.String):[DEBUG] Generating Container SAS Key: Storage Account {}, Container {}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$SetupFailedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Job setup failed with diagnostic message
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:run():[INFO] Stopping MarkedDeleteBlockScrubber.
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Starting the cleanup phase.
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectRequest,java.lang.String):[DEBUG] Restricting metadata request to version {}
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:updateCgroup(java.lang.String,java.lang.String,java.lang.String,java.lang.String):[DEBUG] updateCgroup: {}: {}={}, path, param, value
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:loadFilesUnderConstruction(java.io.DataInput,boolean,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress$Counter):[INFO] Number of files under construction = + size
org.apache.hadoop.yarn.service.client.ServiceClient:addJarResource(java.lang.String,java.util.Map):[INFO] Loading lib tar from + dependencyLibTarGzip
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileQueueParser:checkMinAndMaxResource(java.util.Map,java.util.Map,java.lang.String):[WARN] Queue %s has max resources %s less than min resources %s
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:renewTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path):[DEBUG] Renewed token for [TOKEN_SERVICE] until: [DATE]
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:call():[ERROR] Unable to set exit code for container [containerId]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:retrieve():[ERROR] Exception while deserializing scheduler configuration from store
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:compute():[WARN] Exception while reading checksum
org.apache.hadoop.hdfs.server.balancer.Balancer:init(java.util.List):[DEBUG] logUtilizationCollections
org.apache.hadoop.hdfs.util.ECPolicyLoader:loadPolicy(java.lang.String):[WARN] Invalid tagName: [actual_tag_name]
org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker):[DEBUG] rpcKind= + rpcKind + , rpcRequestWrapperClass= + rpcRequestWrapperClass + , rpcInvoker= + rpcInvoker
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler$PingChecker:checkRunning(long):[INFO] Task attempt timed out and removed
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.AbstractPreemptableResourceCalculator:resetCapacity(org.apache.hadoop.yarn.api.records.Resource,java.util.Collection,boolean):[DEBUG] Added to active capacity from queues
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getPoliciesConfigurations(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPoliciesConfigurationsRequest):[WARN] Policy for queue: {} does not exist.
org.apache.hadoop.yarn.sls.SLSRunner:start():[INFO] Node Managers started
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] [attemptId] TaskAttempt Transitioned from [oldState] to [newState]
org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush():[DEBUG] Resetting permissions to ' + permissions + '
org.apache.hadoop.hdfs.DataStreamer:addDatanode2ExistingPipeline():[DEBUG] lastAckedSeqno = {}
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map):[WARN] Exception running local (uberized) 'child'
org.apache.hadoop.hdfs.DFSInputStream:refreshBlockLocations(java.util.Map):[DEBUG] Discarding refreshed blocks for path {} because lastBlockLength was -1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter:evictBlocks(long):[DEBUG] Evicting block ...
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppNewlySavingTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] Storing application with id {app.applicationId}
org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService:storeContainerKilled(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Container kill state stored
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:refreshHostsReader(org.apache.hadoop.conf.Configuration,boolean,java.lang.Integer):[INFO] refreshNodes excludesFile + excludesFile
org.apache.hadoop.hdfs.server.namenode.NameNode:createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[INFO] createNameNode [argv]
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Setting file size is not supported when mkdir: {} in dirHandle {}
org.apache.hadoop.tools.HadoopArchiveLogs:handleOpts(java.lang.String[]):[INFO] Setting maxEligibleApps to 0 accomplishes nothing. Please either set it to a negative value (default, all) or a more reasonable value.
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:addDigestACL(org.apache.zookeeper.data.ACL):[DEBUG] Ignoring added ACL - registry is insecure{}, aclToString(acl)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token of kind {} is found
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:getFileLength(java.lang.String):[ERROR] Getting file length occurs an exception. COS key: %s, exception: %s
org.apache.hadoop.fs.azure.PageBlobOutputStream:close():[DEBUG] ioThreadPool.toString()
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[WARN] Invocation returned exception on {current.proxyInfo}; {failedObserverCount} failure(s) so far
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateOrReserveNewContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,boolean):[DEBUG] Configuring job jar
org.apache.hadoop.fs.s3a.S3AInputStream:drainUnnecessaryData(com.amazonaws.services.s3.model.S3ObjectInputStream,long):[DEBUG] {} bytes drained from stream , drainBytes
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$LoggingAuditSpan:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[DEBUG] [{}] {} Executing {} with {}; {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Failed to load HDFS runC image to hash file. Config not set
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:fixGlobalQuota(org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,org.apache.hadoop.fs.QuotaUsage):[INFO] Fix Quota src={} dst={} type={} oldQuota={} newQuota={}, location.getSrc(), location, t, remoteQuota.getTypeQuota(t), gQuota.getTypeQuota(t)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadTokenState(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState):[INFO] Loaded + numKeys + master keys and + numTokens + tokens from + tokenStatePath
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:writeFile(org.apache.hadoop.fs.azure.NativeAzureFileSystem):[DEBUG] Preparing to write atomic rename state to {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean):[TRACE] Block {} does not need replication.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not update cgroup for container, re
org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce():[TRACE] org.apache.hadoop.io.retry.RetryInvocationHandler$Call@instance
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Unknown application has completed!
org.apache.hadoop.registry.server.dns.RegistryDNS:serveNIOUDP(java.nio.channels.DatagramChannel,java.net.InetAddress,int):[DEBUG] {}: sending response
org.apache.hadoop.mapreduce.JobResourceUploader:uploadFiles(org.apache.hadoop.mapreduce.Job,java.util.Collection,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,short,java.util.Map,java.util.Map):[INFO] Using shared cache
org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run():[WARN] Thread Interrupted waiting to refresh disk information: {e.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute):[DEBUG] Starting to compare Incoming requestAttribute :{} with requestAttribute value= {}, stored nodeAttribute value= {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:remove(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[ERROR] Attempt to remove resource: rsrc with non-zero refcount
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:enableSubSectionsIfRequired():[WARN] {} is set to {}. It must be greater than zero. Setting to default of {}, DFSConfigKeys.DFS_IMAGE_PARALLEL_TARGET_SECTIONS_KEY, targetSections, DFSConfigKeys.DFS_IMAGE_PARALLEL_TARGET_SECTIONS_DEFAULT
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:loadManifest(org.apache.hadoop.fs.FileStatus):[TRACE] {}: loadManifest('{}'), getName(), status
org.apache.hadoop.security.ssl.FileMonitoringTimerTask:run():[ERROR] PROCESS_ERROR_MESSAGE + filePath.toString(), t
org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:run():[DEBUG] Connecting to datanode
org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int):[DEBUG] doGetGroups({}) returned no groups because the user is not found.,user
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner:run():[DEBUG] Active scan complete
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] Adding: rd (not a dir): {path}
org.apache.hadoop.registry.client.impl.zk.CuratorService:dumpRegistryRobustly(boolean):[DEBUG] Ignoring exception: {}
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:serviceStop():[DEBUG] Stopping webapp
org.apache.hadoop.fs.s3a.S3AFileSystem:hasCapability(java.lang.String):[DEBUG] Ignoring exception on hasCapability({})
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:periodicInvoke():[INFO] Delaying safemode exit for {} milliseconds...
org.apache.hadoop.mapreduce.JobResourceUploader:uploadFiles(org.apache.hadoop.mapreduce.Job,java.util.Collection,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,short,java.util.Map,java.util.Map):[INFO] Copied remote files
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] movedContainer container= + rmContainer.getContainer() + containerState= + rmContainer.getState() + resource= + rmContainer.getContainer().getResource() + queueMoveOut= + this + usedCapacity= + getUsedCapacity() + absoluteUsedCapacity= + getAbsoluteUsedCapacity() + used= + queueUsage.getUsed() + cluster= + clusterResource
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[INFO] Initialized Java Sandbox runtime
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService):[WARN] Couldn't remove BPOS BPOfferService from bpByNameserviceId map
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[DEBUG] Updating token + tokenId.getSequenceNumber()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeConcurrent(java.util.Collection,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,boolean,long,java.lang.Class):[DEBUG] Canot execute {m.getName()} in {location}: {cause.getMessage()}
org.apache.hadoop.hdfs.server.common.Util:isDiskStatsEnabled(int):[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Disabling file IO profiling
org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents:setSampleRangeMax(int):[WARN] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY + " value cannot be more than 100. Setting value to 100"
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$AddNodeTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[INFO] Node health check passed
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[INFO] Selecting file with query
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:build():[TRACE] {}: returning new legacy block reader local.
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:loadLogDeleterState():[WARN] Skipping unknown log deleter key fullKey
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor:executeParallel(org.apache.hadoop.fs.azure.FileMetadata[],org.apache.hadoop.fs.azure.AzureFileSystemThreadTask):[ERROR] {} failed as operation on subfolders and files failed., operation
org.apache.hadoop.ipc.Server$Handler:run():[DEBUG] Thread.currentThread().getName(): starting
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:execute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] IOStatisticsBinding track duration completed
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean):[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to node[0]
org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread:run():[WARN] Interrupted deletion of {context.fullPath}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser:run():[WARN] ShortCircuitCache.this: failed to release short-circuit shared memory slot ...
org.apache.hadoop.mapred.LocalJobRunner$Job:createReduceExecutor():[DEBUG] Reduce tasks to process: this.numReduceTasks
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas):[DEBUG] BLOCK* invalidateBlock: {b} on {dn}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] LOG: Successfully finished killed container
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:removeDirectoryFromSerialNumberIndex(org.apache.hadoop.fs.Path):[WARN] Could not find timestamp portion from path: + serialDirPath.toString() + . Continuing with next
org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient:requestSessionCredentials(long,java.util.concurrent.TimeUnit):[INFO] Requesting Amazon STS Session credentials
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:killContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Killing container {container}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:updateDfsUsageConfig(java.lang.Long,java.lang.Long,java.lang.Class):[INFO] Preconditions check for jitter
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: [event.getClass()]
org.apache.hadoop.tools.mapred.CopyMapper:map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Path could not be found: target
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient:getReader(org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Reader$Options,java.lang.String):[INFO] Loading InMemoryAliasMapReader for block pool id {}
org.apache.hadoop.hdfs.server.datanode.DataStorage:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Finalize upgrade for {} is complete
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:run(java.lang.String[]):[DEBUG] Configuring job jar
org.apache.hadoop.crypto.key.kms.server.KMSConfiguration:initLogging():[DEBUG] KMS log starting
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Generating {} using {} and {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[DEBUG] Added attempt req to rack {rack}
org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet):[TRACE] this: adding notificationSocket [fd_value], connected to [fd_value]
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:writeFile(org.apache.hadoop.fs.Path,byte[]):[ERROR] Got an exception while writing file
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:setDecommissioned(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Decommissioning complete for node {}
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager:sendIBRs(org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol,org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.lang.String):[DEBUG] call blockReceivedAndDeleted: + Arrays.toString(reports)
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[INFO] Node already renewed by peer [nodeRemovePath] so this token should not be deleted
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Queue Management Policy monitor: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Could not store token [formatted_identifier] !!
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:scheduleLogDeletionTask():[INFO] Log Aggregation deletion is disabled because retention is too small (retentionSecs)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[INFO] String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState, getAppAttemptState(), event.getType())
org.apache.hadoop.hdfs.server.namenode.FSEditLog:selectInputStreams(java.util.Collection,long,boolean,boolean):[ERROR] Exception while selecting input streams
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS RENAME from: {}/{} to: {}/{} client: {}
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureEndpoint(com.amazonaws.services.s3.AmazonS3Builder,com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration):[DEBUG] fs.s3a.endpoint.region="{}", region
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:refreshToken():[DEBUG] AADToken: refreshing custom based token
org.apache.hadoop.hdfs.client.impl.metrics.BlockReaderIoProvider:addLatency(long):[WARN] The Short Circuit Local Read latency, %d ms, is higher then the threshold (%d ms). Suppressing further warnings for this BlockReaderLocal.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] Number of warnings: {noOfWarnings}
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[ERROR] Group placement rule failed: No groups returned for user {}
org.apache.hadoop.yarn.server.nodemanager.NodeManager:initAndStartNodeManager(org.apache.hadoop.conf.Configuration,boolean):[ERROR] Error starting NodeManager
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:skipVolume(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume):[DEBUG] Skipping volume. Volume : %s Type : %s Target Number of bytes : %f lowVolume dfsUsed : %d. Skipping this volume from all future balancing calls.
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] NameNode can be reached at: %s
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity$UgiInfo:fromCurrentUser():[INFO] Failed to get current user {}
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] {} RPC address: {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onGetContainerStatusError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to query the status of Container
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Setting file size is not supported when creating file: {} + dir fileId: {}
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[INFO] Done.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getBlockReaderLocal():[DEBUG] {}: {} is not usable for short circuit; giving up on BlockReaderLocal., this, pathInfo
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:longOption(org.apache.hadoop.conf.Configuration,java.lang.String,long,long):[DEBUG] Value of {} is {}
org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class):[INFO] Connecting to ResourceManager at {rmAddress}
org.apache.hadoop.mapred.ClientServiceDelegate:getProxy():[INFO] Network ACL closed to AM for job . Not going to try to reach the AM.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor:run():[INFO] Unchecked exception is thrown from onContainerStatusReceived for Container [containerId]
org.apache.hadoop.oncrpc.SimpleTcpServer:run():[INFO] Started listening to TCP requests at port + boundPort + for + rpcProgram + with workerCount + workerCount
org.apache.hadoop.fs.azurebfs.commit.ResilientCommitByRename:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[INFO] Commit operation via AzureBlobFileSystem in progress
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:updateResolvedLaunchParams(java.util.concurrent.Future):[ERROR] {} updating resolved params
org.apache.hadoop.yarn.client.api.async.NMClientAsync$AbstractCallbackHandler:onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus):[INFO] Container status received
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setAcl(java.lang.String,java.util.List):[DEBUG] logAuditEvent(false, operationName, src) (from catch block)
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection):[INFO] Removing expired token ...
org.apache.hadoop.mapreduce.task.reduce.Fetcher:copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean):[INFO] fetcher#id - MergeManager returned status WAIT ...
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[INFO] Not enough replicas was chosen. Reason: {}, reasonMap
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext):[WARN] Error in executing container interactive shell {} exit = {}
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[INFO] Secure cluster detected
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state
org.apache.hadoop.fs.cosn.CosNInputStream:read(byte[],int,int):[INFO] Reopened at position {position}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] rename {} to {} failed, checking etag of destination
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:call():[INFO] File + actualPath.getName() + was uploaded to the shared cache at + finalPath
org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding$TokenSecretManager:createIdentifier():[DEBUG] Creating Delegation Token Identifier
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult):[DEBUG] Volume {} is {}., reference.getVolume(), result
org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute():[INFO] keyName has been successfully rolled.
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:initCosmosDBClient(org.apache.hadoop.conf.Configuration):[INFO] Creating Cosmos DB Reader Async Client...
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[ERROR] Received a container with following resources suited for a DataNode but no NameNode container exists: containerMem= + rsrc.getMemorySize() + , containerVcores= + rsrc.getVirtualCores()
org.apache.hadoop.hdfs.server.namenode.FSImage:saveLegacyOIVImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.util.Canceler):[INFO] Saving image to target directory
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:reencryptEncryptionZone(long):[INFO] Submission completed of zone {} for re-encryption.
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor:pendingReconstructionCheck():[DEBUG] PendingReconstructionMonitor checking Q
org.apache.hadoop.hdfs.server.datanode.DataNode:initDataXceiver():[INFO] Listening on UNIX domain socket: {}
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet):[WARN] modifyDirective of {} failed: {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddErasureCodingPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy,boolean):[DEBUG] logEdit called
org.apache.hadoop.tools.rumen.Folder:run():[ERROR] The job trace is empty
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] Initializing queue path
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$ResilientCommitByRenameImpl:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Rename operation failed.
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String,boolean):[TRACE] First execution of REST operation getTokenSingleCall
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeOrUpdateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[INFO] Updating RMDelegationToken_SEQUENCE_NUMBER
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:put(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo):[DEBUG] Added priority={}
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:internalUpdateLabelsOnNodes(java.util.Map,org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$NodeLabelUpdateOperation):[INFO] " NM=" + entry.getKey() + ", labels=[" + StringUtils.join(entry.getValue().iterator(), ",") + "]"
org.apache.hadoop.tools.rumen.ParsedTask:dumpParsedTask():[INFO] ParsedTask details: ... (detailed data) ...
org.apache.hadoop.security.token.DtUtilShell$Remove:validate():[ERROR] -alias flag is not optional for remove or cancel
org.apache.hadoop.hdfs.DFSInputStream:pread(long,java.nio.ByteBuffer):[DEBUG] Sanity checks completed
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:syncWithJournalAtIndex(int):[ERROR] JournalNode Proxy not found.
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:abort():[DEBUG] Aborting upload
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Not merging the ranges as they are disjoint
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Registering application master. Host: ... Port: ... Tracking Url: ... for application ...
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onError(java.lang.Throwable):[ERROR] Error in RMCallbackHandler: <exception details>
org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[]):[INFO] Handler responds to 'identifier', says: 'message', returns returnCode
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:checkPauseForTesting():[INFO] Continuing re-encryption updater after pausing.
org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String):[INFO] Login successful for user {} using keytab file {}. Keytab auto renewal enabled : {}
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:addPredicates(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,java.lang.String,java.lang.StringBuilder):[DEBUG] CosmosDB Sql Query with predicates : {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattrInternal(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.nfs.nfs3.request.SetAttr3,boolean):[DEBUG] set new mode: {}
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:cleanAllApplications():[WARN] Unexpected exception from removeKeyRegistry, e
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:rollMasterKey():[ERROR] Unable to update next master key in state store
org.apache.hadoop.hdfs.server.namenode.FSImage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[DEBUG] About to load edits
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logDeleteSnapshot(java.lang.String,java.lang.String,boolean,long):[INFO] Edit logged
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:isAvailable():[INFO] CGroupElasticMemoryController requires enabling memory CGroups with YarnConfiguration.NM_MEMORY_RESOURCE_ENABLED
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseLists():[INFO] Whitelisted {pattern}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:fsck():[INFO] FSCK started by UserGroupInformation from remoteAddress for path path at [date]
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.String):[ERROR] Cannot remove records {} query {clazz, query}
org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST:cancelDelegationToken(javax.servlet.http.HttpServletRequest):[INFO] Forwarding request to web application
org.apache.hadoop.fs.s3a.S3AFileSystem:continueListObjects(org.apache.hadoop.fs.s3a.S3ListRequest,org.apache.hadoop.fs.s3a.S3ListResult,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[INFO] LIST (continued)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Unknown event arrived at FairScheduler: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[DEBUG] Failed to accept this proposal because node is in state (not RUNNING)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[WARN] RemoteException in register, e
org.apache.hadoop.hdfs.server.datanode.DataNode:refreshVolumes(java.lang.String):[INFO] Adding new volumes: {locations}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState, getAppAttemptState(), event.getType())
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] allocate: pre-update applicationId=applicationAttemptId application=application
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext):[WARN] {}: using deprecated cleanupJob call for {}, r, id
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceUpdaterImpl:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[INFO] resourceName + plugin update resource
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unable to decrease container resource
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[INFO] Successfully killed process that was listening on port {port}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadTokens(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState):[WARN] Skipping unexpected file in history server token state
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:optimisedRead(byte[],int,int,long,long):[DEBUG] Optimized read failed. Defaulting to readOneBlock {}
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[WARN] {}, Token={}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask:run():[WARN] Exception thrown when retrieve key: ..., exception: ...
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline service v1 batch publishing disabled
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:close():[WARN] scanner close called but scanner is null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] {warning}
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache:createAttemptDir(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[DEBUG] New attempt directory created - {attemptDir}
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[TRACE] Unsuccessful invocation on [{}]
org.apache.hadoop.registry.server.dns.RegistryDNSServer:main(java.lang.String[]):[DEBUG] Launching DNS server
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Storing state for app {appId} at {key}
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:read(byte[],int,int):[TRACE] read(arr.length={}, off={}, len={}, filename={}, block={}, canSkipChecksum={}): starting
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkFaultTolerantRetry(org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.String,java.io.IOException,org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,java.util.List):[INFO] {} allows retrying failed subclusters in {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:executeStage(java.lang.Boolean):[INFO] {}: Creating Job Attempt directory {}
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:compute():[WARN] Failed to get the checksum for block {} at index {} in blockGroup {}
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] User rule: parent rule failed
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:launchJobHistoryServer(java.lang.String[]):[ERROR] Error starting JobHistoryServer
org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int):[WARN] An un-configured port is being requested {ingressPort} using default
org.apache.hadoop.mapred.IndexCache:readIndexFileToCache(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String):[DEBUG] IndexCache HIT: MapId mapId found
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL but app not found (Took x ms.)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] The next sequential write has not arrived yet
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String):[DEBUG] StatNode result: rc for path: path connectionState: zkConnectionState for this
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:handleNodeReport(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder,java.lang.String,java.lang.String):[ERROR] DiskBalancerException message
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:reference():[TRACE] Reference trace incremented
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:handleDecreaseRequests(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,java.util.List):[WARN] Cannot demote/decrease non-existent (or completed) Container [%s]
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadDelegationTokenFromNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String):[DEBUG] Loaded RMDelegationTokenIdentifier: {} renewDate={}, identifier, renewDate
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Fields read
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:logWarningWhenAuxServiceThrowExceptions(org.apache.hadoop.yarn.server.api.AuxiliaryService,org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType,java.lang.Throwable):[WARN] (null == service ? The auxService is null : The auxService name is + service.getName()) + and it got an error at event: + eventType
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:logLineFromTasksFile(java.io.File):[DEBUG] First line in cgroup tasks file: {} {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Heartbeat from attemptId with responseId request.getResponseId() when we are expecting lastAllocateResponse.getResponseId()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeReservationState(java.lang.String,java.lang.String):[DEBUG] Removing state for reservation {reservationIdName} plan {planName} at {reservationKey}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[WARN] couldn't find app + appId + while processing + FINISH_CONTAINERS event
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[DEBUG] Handle called for event
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[WARN] [STRESS] Finished consuming the input trace. Exiting..
org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL server GSSAPI callback: setting canonicalized client ID: {authzid}
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[WARN] Unable to fence - it is running but we cannot kill it
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadReservationSystemState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loading reservation from znode: {reservationNodePath}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],java.util.Set,int,java.lang.String):[WARN] NN safe mode check
org.apache.hadoop.fs.cosn.ByteBufferWrapper:munmap(java.nio.MappedByteBuffer):[WARN] Failed to unmap the buffer
org.apache.hadoop.tools.rumen.ZombieJob:getInputSplits():[WARN] TotalMaps for job ... is less than the total number of map task descriptions (...)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop():[DEBUG] Service stopped
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:delete(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo):[DEBUG] Removing from cache + fileInfo
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ContainerLauncher:call():[INFO] launching container {container.getId()}
org.apache.hadoop.security.token.DtUtilShell$Renew:validate():[ERROR] -alias flag is not optional for renew
org.apache.hadoop.mapred.LocalJobRunner$Job:run():[INFO] Error cleaning up job: [Exception Detail]
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSContentSummary operation executed
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:addPlacementConstraintHandler(org.apache.hadoop.conf.Configuration):[INFO] YARN Configuration: Disabled RM Placement Constraints Handler + placement handler will be used, all scheduling requests will + be rejected.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Reacquired containerId -> classId mapping: + containerIdStr + -> + classId
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[]):[INFO] New instance created
org.apache.hadoop.examples.pi.Util:runJob(java.lang.String,org.apache.hadoop.mapreduce.Job,org.apache.hadoop.examples.pi.DistSum$Machine,java.lang.String,org.apache.hadoop.examples.pi.Util$Timer):[INFO] Completion Successful
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:getPathToDelete(org.apache.hadoop.fs.Path):[WARN] Random directory component did not match. Deleting localized path only
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start():[INFO] prefix + metrics system started
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAcl(java.lang.String):[INFO] removeAcl operation completed
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[INFO] Warming up {} EDEKs... (initialDelay={}, retryInterval={})
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL):[DEBUG] verifying request. enc_str=enc_str; hash=...urlHashStr
org.apache.hadoop.mapred.YARNRunner:generateResourceRequests():[WARN] Configuration MB is overriding the configured value
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:parse():[WARN] error trying to open previous history file. No history data will be copied over.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getMinimumAllocation():[INFO] Minimum allocation = + ret
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:resumeContainer():[INFO] Exception when trying to resume container [containerIdStr]: [exception details]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappedBlock:close():[WARN] IOException occurred for block {}!
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:submitPlan(java.lang.String,java.lang.String,boolean):[ERROR] Submitting plan on {} failed. Result: {}, Message: {}
org.apache.hadoop.yarn.service.monitor.probe.PortProbe:ping(org.apache.hadoop.yarn.service.component.instance.ComponentInstance):[DEBUG] instance.getCompInstanceName() + ": Connecting " + sockAddr.toString() + ", timeout=" + MonitorUtils.millisToHumanTime(timeout)
org.apache.hadoop.mapreduce.lib.db.DBSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[INFO] Splitting using FloatSplitter
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded$SPSPathIdProcessor:run():[INFO] Starting SPSPathIdProcessor!.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getECTopologyResultForPolicies(java.lang.String[]):[INFO] EC Topology Verifier Result for specified policies
org.apache.hadoop.hdfs.server.namenode.CheckpointConf:warnForDeprecatedConfigs(org.apache.hadoop.conf.Configuration):[WARN] Configuration key {key} is deprecated! Ignoring... Instead please specify a value for {DFS_NAMENODE_CHECKPOINT_TXNS_KEY}
org.apache.hadoop.hdfs.DFSClient:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[],java.lang.String,java.lang.String):[DEBUG] {}: masked={}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports):[DEBUG] DataNode Z reported slow peers: {A, B, C}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getBlockReaderLocal():[TRACE] {}: got InvalidToken exception while trying to construct BlockReaderLocal via {}, this, pathInfo.getPath()
org.apache.hadoop.lib.server.Server:setStatus(org.apache.hadoop.lib.server.Server$Status):[ERROR] Service [{}] exception during status change to [{}] -server shutting down-, {}
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readRemote(long,byte[],int,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] HTTP request read bytes = {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Provide <pid of process to monitor>
org.apache.hadoop.fs.s3a.S3AInstrumentation$OutputStreamStatistics:close():[WARN] Closing output stream statistics while data is still marked as pending upload in {}
org.apache.hadoop.fs.shell.Command:run(java.lang.String[]):[WARN] DEPRECATED: Please use 'replacementCommand' instead.
org.apache.hadoop.fs.s3a.S3AUtils:getMultipartSizeProperty(org.apache.hadoop.conf.Configuration,java.lang.String,long):[WARN] {} must be at least 5 MB; configured value is {}
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:run():[ERROR] Interrupted; exiting from thread.
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:checkReplaceLabelsOnNode(java.util.Map):[ERROR] %d labels specified on host=%s, please note that we do not support specifying multiple labels on a single host for now.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:deleteReservation(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationDeleteRequestInfo,javax.servlet.http.HttpServletRequest):[INFO] Update reservation request failed
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Total time to scan all replicas for block pool {bpid}: {totalTimeTaken}ms
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[TRACE] hasDt={}, queryStr={}
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long):[INFO] Setting timestamp for event
org.apache.hadoop.hdfs.DFSInputStream:addToLocalDeadNodes(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] Add {} to local dead nodes, previously was {}.
org.apache.hadoop.mapreduce.InputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] CompositeInputFormat split executed
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager:allocate():[DEBUG] wake up: org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager instance, recycled? true
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[DEBUG] AMRMProxy is ignoring event: {eventType}
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[INFO] Application {appId} found, returning home sub-cluster
org.apache.hadoop.hdfs.DeadNodeDetector:run():[DEBUG] Got interrupted while DeadNodeDetector is error.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:addBlockToBeRecovered(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[INFO] block is already in the recovery queue
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:commit(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid COMMIT request
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServices():[DEBUG] Scanner skips for unknown file {}
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean):[TRACE] Reading receipt verification byte for slotId
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:placeApplication(org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String,boolean):[ERROR] Failed to place application
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:createFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics,boolean,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] createFile filesystem: {} path: {} overwrite: {} permission: {} umask: {} isNamespaceEnabled: {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster:main(java.lang.String[]):[INFO] Initializing ApplicationMaster
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:validateOneFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry):[WARN] Etag of dest file {}: {} does not match that of manifest entry {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:parseStatsString(java.lang.String):[WARN] Matched a 'bytes sent' line outside of a class stats segment : {line}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.SimpleCapacityReplanner:plan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,java.util.List):[INFO] Removing reservation {reservation.getReservationId()} to repair physical-resource constraints in the plan: {plan.getQueueName()}
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Auto increment set to false
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] RegisterUAM returned {} existing running container and {} NM tokens
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:parseSummaryLogs(org.apache.hadoop.yarn.server.timeline.TimelineDataManager):[INFO] {} state is UNKNOWN and logs are stale, assuming COMPLETED
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getNodeResources(org.apache.hadoop.conf.Configuration):[DEBUG] Setting key {} to {}
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getDomain(java.lang.String):[INFO] Service stopped, return null for the storage
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:rollMasterKey():[INFO] Going to activate master-key with key-id keyid in activationDelay ms
org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]):[WARN] -principal and -keytab not both specified! Kerberos login not attempted.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:fromConfiguration(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Creating base placement policy from config
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:run():[INFO] Processing the event JOB_COMMIT
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:uploadImageFromStorage(java.net.URL,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler):[INFO] Uploaded image with txid 123 to namenode at http://example.com in 0.5 seconds
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:tasks():[ERROR] Failed to render tasks page with task type : $(TASK_TYPE) for job id : $(JOB_ID)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics:updatePreemptionInfo(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Non-AM container preempted, current appAttemptId=%s, containerId=%s, resource=%s
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:preCheckForNodeCandidateSet(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey):[DEBUG] Skip allocating AM container to app_attempt={}, don't allow to allocate AM container in non-exclusive mode
org.apache.hadoop.fs.azurebfs.commit.ResilientCommitByRename:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Ensuring destination doesn't exist and its parent is a directory
org.apache.hadoop.fs.s3a.Invoker:ignoreIOExceptions(org.slf4j.Logger,java.lang.String,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE):[DEBUG] Full stack trace on exception
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:collectCells(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.hbase.Cell,java.util.Set,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,org.apache.hadoop.hbase.regionserver.ScannerContext):[TRACE] In collect cells FlowSannerOperation=...
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager:close():[DEBUG] Exception in closing + domainSocketWatcher, e
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:increaseContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container):[ERROR] Callback handler does not implement container resource increase callback methods
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkAddress(java.lang.String):[WARN] The provided SubCluster Endpoint does not contain a valid host:port authority: {address}
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:loadAMRMPolicy(java.lang.String,org.apache.hadoop.yarn.server.federation.policies.amrmproxy.FederationAMRMProxyPolicy,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[INFO] Creating policy manager of type: + configuration.getType()
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:roll(long):[INFO] Rolling new DB instance for + getName()
org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration):[DEBUG] Bypassing cache to create filesystem {uri}
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:createSnapshot(java.lang.String,java.lang.String):[INFO] Concurrent invocation executed
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:saveFileXAttrsForBatch(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.util.List):[INFO] Cannot find inode {}, skip saving xattr for re-encryption
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.SampleContainerLogAggregationPolicy:parseParameters(java.lang.String):[WARN] The format isn't valid. Sample rate falls back to the default value ${DEFAULT_SAMPLE_RATE}
org.apache.hadoop.mapreduce.JobResourceUploader:disableErasureCodingForPath(org.apache.hadoop.fs.Path):[INFO] Disabling Erasure Coding for path: {path}
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:loadFullHistoryData(boolean,org.apache.hadoop.fs.Path):[WARN] History file not found
org.apache.hadoop.mapreduce.lib.partition.InputSampler:run(java.lang.String[]):[ERROR] Wrong number of parameters
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container):[INFO] Found replacement:
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedWriter:endTargetBlocks():[WARN] {exception message from e.getMessage()}
org.apache.hadoop.hdfs.server.mover.Mover:run(java.util.Map,org.apache.hadoop.conf.Configuration):[INFO] namenodes = + namenodes
org.apache.hadoop.yarn.server.resourcemanager.AdminService:removeFromClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RemoveFromClusterNodeLabelsRequest):[ERROR] IOException occurred during removeFromClusterNodeLabels
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:isExclusiveNodeLabel(java.lang.String):[ERROR] Getting is-exclusive-node-label, node-label = {nodeLabel}, is not existed.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shutDownJob():[INFO] Calling stop for all the services
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration):[INFO] Native Bzip2 library not loaded; using default library name
org.apache.hadoop.tools.mapred.CopyCommitter:commitData(org.apache.hadoop.conf.Configuration):[WARN] Rename failed. Perhaps data already moved. Verifying...
org.apache.hadoop.ipc.Client$Connection:run():[DEBUG] : stopped, remaining connections
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getLogs(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean):[DEBUG] Init for readable endpoints
org.apache.hadoop.fs.shell.Command:run(java.lang.String[]):[ERROR] IOException: errorMessage
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getNextSubDir(java.lang.String,java.io.File):[TRACE] getNextSubDir({}, {}): picking next subdirectory {} within {}
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] Configurations = {conf}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isPathAll(java.lang.String):[ERROR] Cannot get mount point
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:getBlockToCopy(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$BlockIterator,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[INFO] Maximum error count exceeded. Error count: {} Max error:{}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:buildAmazonS3EncryptionClient(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters):[INFO] S3 client-side encryption enabled: Ignore S3-CSE Warnings.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:run():[DEBUG] DatanodeAdminMonitor is running.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:activateApplications():[DEBUG] application {applicationId} AMResource {amResource} maxAMResourcePerQueuePercent {maxAMResourcePerQueuePercent} amLimit {amLimit} lastClusterResource {lastClusterResource} amIfStarted {amIfStarted} AM node-partition name {partitionName}
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run():[INFO] New instance created
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(long,long,int):[DEBUG] before vMemCheck [isEnabled={}, current={} + asked={} > allowed={}]
org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState):[ERROR] Local service " + localTarget + " has changed the serviceState to " + changedState + ". Expected was " + serviceState + ". Quitting election marking fencing necessary.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[ERROR] Failed to refresh maximum allocation
org.apache.hadoop.yarn.service.ServiceScheduler:recoverComponents(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[WARN] Could not resolve record for component {}: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:scheduleContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Start Opportunistic Containers
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:reInitializeContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerLaunchContext,boolean):[INFO] Error when parsing local resource URI for upgrade of + Container [ + containerId + ], e
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfBlockReportParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Moving {} to {}, src, dst
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue:write(java.io.DataOutputStream,java.util.Set):[ERROR] logErrorMessage(logFile, e)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed checking for the existence of history intermediate done directory
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getDatanodesSubcluster():[ERROR] Cannot get the datanodes from the RPC server
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:stopGracefully():[DEBUG] Interrupted Exception while waiting to join sps thread, ignoring it
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[INFO] Outputted {} INodes.
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the start data of application [applicationId]
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[DEBUG] {}: commitTaskInternal
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:getPossiblyCompressedOutputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[INFO] Compression codec instance created
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:cacheBlock(java.lang.String,long):[WARN] Failed to cache block with id + blockId + , pool + bpid + : ReplicaInfo not found.
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run():[ERROR] Master key updating failed: (IOException e)
org.apache.hadoop.fs.cosn.CosNOutputStream:close():[ERROR] An exception occurred while returning the buffer to the buffer pool.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device):[WARN] Failed to get device type from stat {devName}
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:loadAMRMProxyState():[INFO] Recovered for AMRMProxy: current master key id ...
org.apache.hadoop.fs.s3a.S3AFileSystem:createRequestFactory():[DEBUG] Unset storage class property STORAGE_CLASS; falling back to default storage class
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:readContainerLog(java.lang.String,org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,org.apache.hadoop.fs.FileStatus,long,long,java.util.List,long,long,boolean,java.lang.String):[ERROR] Error getting logs for logEntity
org.apache.hadoop.hdfs.qjournal.server.Journal:moveTmpSegmentToCurrent(java.io.File,java.io.File,long):[WARN] Unable to move edits file from {tmpFile} to {finalFile} ; journal id: {journalId}
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] SecondaryNameNode principal not considered, ...
org.apache.hadoop.yarn.service.ServiceMaster:main(java.lang.String[]):[ERROR] Error starting service master
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean):[WARN] Unexpected item in trash: ... Ignoring.
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:getCredentialsProvider(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Credential provider class is: + className
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[INFO] Added {event.getAttemptID()} to list of failed maps
org.apache.hadoop.fs.s3a.impl.CallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture):[INFO] Waiting for task completion
org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List):[WARN] Failed to read topology table. DEFAULT_RACK will be used for all nodes.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Elastic memory control enabled: {}
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Cannot render ResourceManager, e
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:purgeOldLegacyOIVImages(java.lang.String,long):[INFO] Deleting {fileName}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LocalizedTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Localized resource {resourceRequest} for container {containerId}
org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration):[DEBUG] Checking TFile Data Index
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetOwner(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Set owner operation created
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:serviceStop():[ERROR] The thread of + eventDispatcherThread.getName() + didn't finish normally., e
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:doPreUpgrade():[INFO] Starting upgrade of edits directory
org.apache.hadoop.fs.s3a.S3AFileSystem:innerGetFileStatus(org.apache.hadoop.fs.Path,boolean,java.util.Set):[DEBUG] Getting path status for {} ({}); needEmptyDirectory={}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadUUIDFromLogFile(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Cleaning up resources
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[INFO] Setting up data streams.
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getUsed():[DEBUG] Failed to get the used capacity
org.apache.hadoop.fs.s3a.S3AFileSystem:innerRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Rename path {} to {}, src, dst
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:updateReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Rollbacked update reservation: {} from plan.
org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode():[INFO] Deleting bread-crumb of active node...
org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder():[WARN] The first kerberos ticket is not TGT(the server principal is {}), remove and destroy it.
org.apache.hadoop.fs.s3a.WriteOperationHelper:abortMultipartUploadsUnderPath(java.lang.String):[DEBUG] Aborting multipart uploads under {}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:getMultipartSizeProperty(org.apache.hadoop.conf.Configuration,java.lang.String,long):[WARN] {} must be at least 100 KB; configured value is {}
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[DEBUG] Retrieved containers
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] BLOCK* block {}: {} is received from {}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:sendMap(org.apache.hadoop.mapred.ShuffleHandler$ReduceContext):[ERROR] Shuffle error :
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[WARN] FinishApplicationMaster already called by attemptId, skip heartbeat processing and return dummy response
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setPermission(java.lang.String,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Audit: setPermission failed due to AccessControlException
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:run():[INFO] aggregated log deletion started.
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeReferenceSection(java.io.InputStream):[INFO] Loading inode references
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:processSummationMajorCompaction(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter,long):[TRACE] MAJOR COMPACTION loop sum= discarding now: qualifier= value= timestamp=
org.apache.hadoop.yarn.client.AHSProxy:createAHSProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.InetSocketAddress):[INFO] Connecting to Application History server at {ahsAddress}
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:maybeInitSTS():[DEBUG] Parent-provided session credentials will be propagated
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:serviceStart():[ERROR] Error when initializing FileSystemHistoryStorage
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentInstanceEventHandler:handle(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[ERROR] No component instance exists for + event.getContainerId()
org.apache.hadoop.fs.DelegationTokenRenewer:removeRenewAction(org.apache.hadoop.fs.FileSystem):[ERROR] Interrupted while canceling token for + fs.getUri() + filesystem
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:init(java.util.Properties):[INFO] Successfully initialized MultiSchemeAuthenticationHandler
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementAttemptedItems:blocksStorageMovementUnReportedItemsCheck():[INFO] TrackID: {file} becomes timed out and moved to needed retries queue for next iteration.
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getSubClusters(org.apache.hadoop.yarn.server.federation.store.records.GetSubClustersInfoRequest):[ERROR] Cannot get subclusters: exception message
org.apache.hadoop.mapreduce.v2.util.MRApps:addToClasspathIfNotJar(org.apache.hadoop.fs.Path[],java.net.URI[],org.apache.hadoop.conf.Configuration,java.util.Map,java.lang.String):[WARN] The same path is included more than once with different links or wildcards
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:run():[WARN] IOException in LifelineSender for + BPServiceActor.this
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler:killJob(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillJobRequest):[INFO] Kill job [jobId] received from [callerUGI] at [Server.getRemoteAddress()]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:waitVolumeRemoved(int,java.util.concurrent.locks.Condition):[INFO] Thread interrupted when waiting for volume reference to be released.
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:internalRemoveFromClusterNodeLabels(java.util.Collection):[INFO] Remove labels: [ + StringUtils.join(labelsToRemove.iterator(), ,) + ]
org.apache.hadoop.fs.azurebfs.services.AbfsLease:free():[WARN] Exception when trying to release lease {} on {}. Lease will need to be broken: {}
org.apache.hadoop.hdfs.HAUtil:getAddressOfActive(org.apache.hadoop.fs.FileSystem):[DEBUG] Error while connecting to namenode
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:kill(boolean):[WARN] cleanup failed for container...
org.apache.hadoop.fs.s3a.S3AInputStream:readCombinedRangeAndUpdateChildren(org.apache.hadoop.fs.impl.CombinedFileRange,java.util.function.IntFunction):[DEBUG] Start reading combined range {} from path {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[WARN] Received unwanted container allocation: + container
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Removed node from inactive nodes list
org.apache.hadoop.ha.ActiveStandbyElector:becomeActive():[DEBUG] Becoming active for {}\n
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:init(int,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.sls.SLSRunner,long,long,java.lang.String,java.lang.String,boolean,java.lang.String,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map,java.util.Map):[INFO] Added new job with {allMaps.size()} mapper and {allReduces.size()} reducers
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:cleanLogs(org.apache.hadoop.fs.Path,long):[INFO] Cleaning log directories
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[INFO] Job completed successfully
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Placement Algorithm [algorithm.getClass().getName()]
org.apache.hadoop.util.SysInfoLinux:readDiskBlockInformation(java.lang.String,int):[WARN] Error reading the stream /sys/block/diskName/queue/hw_sector_size
org.apache.hadoop.yarn.server.resourcemanager.AdminService:updateNodeResource(org.apache.hadoop.yarn.server.api.protocolrecords.UpdateNodeResourceRequest):[ERROR] Resource update get failed on all nodes due to change resource on an unrecognized node: {nodeId}
org.apache.hadoop.tools.util.DistCpUtils:toCopyListingFileStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,int):[DEBUG] add file + clfs
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[INFO] Exiting...
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object):[WARN] Invalid RPC call program + call.getProgram()
org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable):[DEBUG] beforeExecute in thread: + Thread.currentThread().getName() + , runnable type: + r.getClass().getName()
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:run():[ERROR] Unexpected exception while initializing the cleaner task. This task will do nothing,, e
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:getAllPartialJobs():[WARN] Error trying to scan for all FileInfos
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppQueue(javax.servlet.http.HttpServletRequest,java.lang.String):[ERROR] Trying to get queue of an absent application
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:queueReadAhead(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[TRACE] Start Queueing readAhead for {} offset {} length {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:doSaslHandshake(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler,org.apache.hadoop.security.token.Token):[DEBUG] Block token id is null, sending without handshake secret.
org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String):[DEBUG] Error while changing permission : filename Exception: exception details
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Object):[INFO] {}: directory {} contained {} file(s); data size {}, getName(), taskAttemptDir, fileCount, fileDataSize
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue():[ERROR] Got sink exception, retry in X ms
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[WARN] Found unexpected column for entity of type
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Total time to add all replicas to map for block pool : ms
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:assignGpus(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Container : some_container_id is waiting for free GPU devices.
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[WARN] Failed to close the timeline tables as Hbase is down
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationsHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationsHomeSubClusterRequest):[ERROR] FederationStateStoreClientMetrics failed state store call
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:lookupForLocalGroupIdentity(java.lang.String):[ERROR] Error while parsing the line, returning empty string
org.apache.hadoop.hdfs.server.namenode.BackupImage:namenodeStartedLogSegment(long):[INFO] Stopped applying edits to prepare for checkpoint.
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:logOutput(java.lang.String):[INFO] Each line of output string
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:balanceVolumeSet(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[INFO] Disk Volume set {} - Type : {} plan completed.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage$Arguments):[INFO] Executing Manifest Job Commit with manifests in manifestDir
org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String):[INFO] Got {} = '{}' (default '{}')
org.apache.hadoop.ipc.Server$Handler:run():[INFO] Thread.currentThread().getName() unexpectedly interrupted
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:getLocalNNThreadDump():[WARN] Can't get local NN thread dump due to Exception
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleContainerStatus(java.util.List):[INFO] Container {containerId} already scheduled for cleanup, no further processing
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] Stopped JobHistoryEventHandler. super.stop()
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:refresh():[WARN] Host list file path [{}] is empty or does not exist !!
org.apache.hadoop.mapred.LocalJobRunner$Job:run():[WARN] [JobID] [Exception Detail]
org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream(org.apache.hadoop.hdfs.DFSInputStream):[TRACE] Registering {} for {}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Service address not found
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:init(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] This csi-adaptor is configured to contact with the csi-driver {} via gRPC endpoint: {}, driverName, driverEndpoint
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File[],boolean):[ERROR] In-progress stale edits file + f + has improperly formatted transaction ID
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics:initMetricsCSVOutput():[ERROR] Cannot create directory {}
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Max mem capabililty of resources in this cluster {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[INFO] Got response from RM for container ask, allocatedCnt=x
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:setPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.SetSubClusterPolicyConfigurationRequest):[ERROR] The policy {queue} was not insert into the StateStore
org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor):[INFO] Got UserName [user] for UID [uid] from the native implementation
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:refreshUserToGroupsMappings():[INFO] Successfully executed refreshUserToGroupsMappings
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop():[INFO] Timeline client stopped
org.apache.hadoop.hdfs.server.datanode.DataStorage:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[ERROR] Finalize upgrade for {} failed
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:processResponseQueue():[DEBUG] Application {} has one mapper finished ({})
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:shutdown():[INFO] Shutting down all async data service threads...
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:populateNMTokens(java.util.List):[DEBUG] Replacing token for : {nodeId}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:createAndPopulateNewRMApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,long,org.apache.hadoop.security.UserGroupInformation,boolean,long,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState):[WARN] Application with id {applicationId} already present! Cannot add a duplicate!
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$KilledAfterSuccessTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Ignoring killed event for successful map only task attempt + taskAttempt.getID().toString()
org.apache.hadoop.hdfs.server.datanode.DataNode:transferReplicaForPipelineRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],java.lang.String):[DEBUG] Replica is finalized!
org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor:run():[INFO] Exiting, bbye..
org.apache.hadoop.mapred.TaskAttemptListenerImpl:statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[DEBUG] Ping from + taskAttemptID.toString()
org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration):[INFO] Updating Configuration
org.apache.hadoop.mapred.BackupStore$MemoryCache:reinitialize(boolean):[DEBUG] Created a new mem block of
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Error monitoring job :
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:deleteReservation(org.apache.hadoop.yarn.api.records.ReservationId):[ERROR] The specified Reservation with ID {reservationID} does not exist in the plan
org.apache.hadoop.fs.s3a.s3guard.S3Guard:checkNoS3Guard(java.net.URI,org.apache.hadoop.conf.Configuration):[ERROR] S3Guard is no longer needed/supported, yet {fsPath} is configured to use DynamoDB as the S3Guard metadata store. This is no longer needed or supported. Origin of setting is {origin}
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String):[INFO] createSuccessLog(user, operation, target, null, null)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:handle(org.apache.hadoop.yarn.event.Event):[INFO] Container finished
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Start information is missing for container
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext):[INFO] {}: Job Commit statistics {}, committerConfig.getName(), ioStatisticsToPrettyString(iostatistics)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:copyBlockFiles(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration):[DEBUG] Copied srcReplica.getBlockURI() to dstFile
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Output Path is null in recoverTask()
org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run():[WARN] RuntimeException during Trash.Emptier.run(): {exception}
org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute():[INFO] keyName has been successfully created with options options.toString().
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:resumeContainer():[INFO] Container [containerIdStr] not paused. No resume necessary
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getECTopologyResultForPolicies(java.lang.String[]):[INFO] getECTopologyResultForPolicies called
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterConfig,org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] Report already exists: {}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream):[INFO] Could not obtain block from any node: [IOException message]
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:internalSignalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest,java.lang.String):[INFO] Container containerId no longer exists
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:getJWTFromCookie(javax.servlet.http.HttpServletRequest):[INFO] cookieName + " cookie has been found and is being processed"
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:signRequest(org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation,int):[DEBUG] Authenticating request with OAuth2 access token
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage):[DEBUG] Renaming + tmpFile + to + finalizedFile
org.apache.hadoop.yarn.service.webapp.ApiServer:stopService(java.lang.String,boolean,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {} error cleaning up registry
org.apache.hadoop.hdfs.server.namenode.FSDirectory:setINodeAttributeProvider(org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider):[INFO] Use the new authorization provider API
org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedExceptionAction):[DEBUG] PrivilegedAction [as: {}][action: {}]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:handleContainerStop(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[DEBUG] {} status is {}, skipping stop, containerId, containerStatus
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[INFO] EVENTS field filter added
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] isNamespaceEnabled is UNKNOWN; fall back and determine through getAcl server call
org.apache.hadoop.tools.mapred.CopyMapper:map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] DistCpMapper::map(): Received path
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[ERROR] Fencing method + method + failed with an unexpected error.
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceStop():[DEBUG] Stopping delegation tokens
org.apache.hadoop.net.unix.DomainSocketWatcher:close():[DEBUG] this + ": closing"
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:removeReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[ERROR] The specified Reservation with ID {reservation.getReservationId()} does not exist in the plan
org.apache.hadoop.hdfs.qjournal.server.Journal:doFinalize():[INFO] Finalizing upgrade for journal + storage.getRoot() + . + (storage.getLayoutVersion() == 0 ? : \n cur LV = + storage.getLayoutVersion() + ; cur CTime = + storage.getCTime())
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client skipping handshake in secured configuration with privileged port for addr = {}, datanodeId = {}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:resumeForTesting():[INFO] Resuming re-encrypt updater for testing.
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:checkAcls(java.lang.String):[WARN] User + user.getShortUserName() + doesn't have permission to call ' + method + '
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[INFO] Attempting to delete existing output path
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] AzureBlobFileSystem.create path: {} permission: {} overwrite: {} bufferSize: {}
org.apache.hadoop.crypto.key.kms.server.KMS:getKeysMetadata(java.util.List):[TRACE] Entering getKeysMetadata method.
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction:runWithRetries():[INFO] Retrying operation on FS. Retry no. X
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:updateStateStore():[WARN] Cannot heartbeat router {}: State Store unavailable
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:setOutputPath(java.lang.String):[DEBUG] Another Diskbalancer instance is running ? - Target Directory already exists. {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:formatSchedulerConfiguration(javax.servlet.http.HttpServletRequest):[ERROR] Exception thrown when formatting configuration.
org.apache.hadoop.fs.azure.CachingAuthorizer:get(java.lang.Object):[DEBUG] {}: CACHE MISS: {}
org.apache.hadoop.mapred.pipes.BinaryProtocol:authenticate(java.lang.String,java.lang.String):[DEBUG] Sending AUTHENTICATION_REQ, digest=%s, challenge=%s
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:failApplicationAttempt(org.apache.hadoop.yarn.api.protocolrecords.FailApplicationAttemptRequest):[INFO] Success log for user-induced failure
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] MOUNT MNT path: {path} client: {client} →
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] In safemode, not computing reconstruction work
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:run():[WARN] Could not fetch OOM status. This is expected at shutdown. Exiting., ex
org.apache.hadoop.fs.s3a.S3AInputStream:read(long,byte[],int,int):[ERROR] Read operation failed
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:verifyRMRegistrationResponseForNodeAttributes(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse):[ERROR] Node attributes sent from NM while registration were rejected by RM. ((errorMsgFromRM == null) ? Seems like RM is configured with Centralized Attributes. : And with message + regNMResponse.getDiagnosticsMessage())
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:initAuditLoggers(org.apache.hadoop.conf.Configuration):[ERROR] {} instantiation failed., className, e
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onNodesUpdated(java.util.List):[INFO] onNodesUpdated: <concatenated node updates>
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[ERROR] {} exceptions occurred loading INodes
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntityByTime(byte[],java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl):[DEBUG] Seeker initiated
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[WARN] Balancer already running as a long-service!
org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell():[ERROR] {StringUtils.stringifyException(e)}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:initializeClient(java.net.URI,java.lang.String,java.lang.String,boolean):[TRACE] AbfsClient init complete
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer$TxnBuffer:dumpRemainingEditLogs():[WARN] Unflushed op [ + numTransactions + "]: " + op
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:initializeWriterInRolling(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.datanode.DataNode:join():[WARN] Received exception in Datanode#join: {ex.toString()}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:render():[DEBUG] Invalid Application ID: {aid}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkCommitInternal(long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean):[DEBUG] get commit while still writing to the requested offset
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStop():[INFO] Stopping {}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:restartContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Exception when scheduling the event of restart of Container containerId
org.apache.hadoop.fs.Globber:doGlob():[WARN] File/directory {} not found: it may have been deleted. If this is an object store, this can be a sign of eventual consistency problems.
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Cannot recover task {}
org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Looking for a token with service {}
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[INFO] UserGroupMappingPlacementRule applied
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask:deletionTaskFinished():[ERROR] Unable to remove deletion task + taskId + from state store
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:updateState():[ERROR] Namenode is not operational: <NamenodeDesc>
org.apache.hadoop.mapred.ShuffleHandler:startStore(org.apache.hadoop.fs.Path):[INFO] Using state database at + dbPath + for recovery
org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler:handle(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException):[TRACE] Successfully scanned {block} on {volume}
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:logConf(org.apache.hadoop.conf.Configuration):[INFO] NNTop conf: + DFSConfigKeys.NNTOP_NUM_USERS_KEY + = +
org.apache.hadoop.fs.azure.BlockBlobAppendStream$WriteRequest:run():[DEBUG] command finished for {} ms
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:createBlobClient(java.net.URI,com.microsoft.azure.storage.StorageCredentials):[ERROR] createBlobClient is an invalid operation in SAS Key Mode
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:initCGroupsCpuResourceHandler(org.apache.hadoop.conf.Configuration):[DEBUG] Creating new cgroups cpu handler
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendContainerMonitorStartEvent():[DEBUG] Getting Configuration VMem PMem Ratio
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Planning Algorithm pool size [algoPSize]
org.apache.hadoop.hdfs.server.namenode.CacheManager:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] addCachePool of {info} failed: {e}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:parseArgs(java.lang.String[]):[ERROR] ERROR: + e.getMessage() + \n
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] no existing webapp instance found: {}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing master key + key.getKeyId()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:metaSave(java.io.PrintWriter):[INFO] Mis-replicated blocks that have been postponed:
org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int):[DEBUG] Downgrading EOFException raised trying to read {} bytes at offset {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:initializeState():[WARN] Failed to bootstrap outbound bandwidth configuration
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:tryGetFileStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] File not found {}
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[ERROR] Exception caught in channelRead0
org.apache.hadoop.hdfs.qjournal.server.JournalNode:stop(int):[WARN] Unable to stop HTTP server for ...
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] ApplicationId unregistered successfully.
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] ApplicationHistory Init
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] Allocate from reserved container {} is in final state
org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal():[ERROR] Failed to reEstablish connection with ZooKeeper
org.apache.hadoop.fs.s3a.select.SelectInputStream:close():[DEBUG] While closing stream
org.apache.hadoop.fs.azurebfs.services.AbfsClient:createRequestUrl(java.lang.String,java.lang.String):[DEBUG] Unexpected error.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:ref(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica):[TRACE] this + : + removedFrom + no longer contains + replica + . refCount + (replica.refCount - 1) + -> + replica.refCount + StringUtils.getStackTrace(Thread.currentThread())
org.apache.hadoop.yarn.server.resourcemanager.DBManager$CompactionTimerTask:run():[INFO] Full compaction cycle completed in + duration + msec
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:put(java.lang.String,byte[],boolean):[DEBUG] {} not created
org.apache.hadoop.ha.ActiveStandbyElector:terminateConnection():[DEBUG] Terminating ZK connection for
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[WARN] e.getCause().getMessage()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:reload(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords):[INFO] Received list of auxiliary services: + mapper.writeValueAsString(services)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:registerApplicationMasterForDistributedScheduling(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Forwarding registerApplicationMasterForDistributedScheduling request to the real YARN RM
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean):[INFO] Error deleting {}: {}
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStart():[INFO] Triggering initial evaluation of component {componentName}
org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:run():[INFO] DataNode$DataTransfer, at {DataNode}: Transmitted {block} (numBytes={number}) to {curTarget}
org.apache.hadoop.examples.terasort.TeraInputFormat:writePartitionFile(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path):[INFO] Computing partitions took (t3 - t2) ms
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:getFileLength(java.lang.String):[DEBUG] Get file length. COS key: {}
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:reassembleNonStripedCompositeCrc(long):[DEBUG] flatBlockChecksumData.length={}, numDataUnits={}, checksumLen={}, digest={}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(java.lang.Object):[ERROR] Unexpected health check result null for volume {}
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] SQLException committing split transaction: [details]
org.apache.hadoop.yarn.server.nodemanager.NodeManager:handle(org.apache.hadoop.yarn.event.Event):[WARN] Invalid shutdown event + event.getType() + . Ignoring.
org.apache.hadoop.fs.adl.AdlFileSystem:propagateAccountOptions(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Propagating entries under ${accountPrefix}, accountPrefix
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean):[INFO] Audit failed: renameSnapshot
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:innerList(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor,boolean):[DEBUG] {} is a File, qualifiedPath
org.apache.hadoop.yarn.event.AsyncDispatcher:register(java.lang.Class,org.apache.hadoop.yarn.event.EventHandler):[INFO] Registering + eventType + for + handler.getClass()
org.apache.hadoop.streaming.StreamJob:run(java.lang.String[]):[DEBUG] Error in streaming job
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[WARN] BLOCK* addStoredBlock: block {} moved to storageType {} on node {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.MetricsInvariantChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Queue metrics initialized
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectRequest,java.lang.String):[DEBUG] No version ID to use as a constraint
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumResourceCapability(java.lang.String):[ERROR] Unknown queue: + queueName
org.apache.hadoop.fs.s3a.S3AUtils:closeAll(org.slf4j.Logger,java.io.Closeable[]):[DEBUG] Cleanup with Logger initiated
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Service {serviceName} is stopped.
org.apache.hadoop.yarn.server.federation.policies.router.LocalityRouterPolicy:getHomeSubcluster(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.List):[INFO] Node request: nodeRequest, Rack request: rackRequest, Any request: anyRequest
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Unable to remove master key {}
org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo):[ERROR] Error message with throwable
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateHeartBeatConfiguration(org.apache.hadoop.conf.Configuration):[WARN] Invalid NM Heartbeat Configuration. Required: 0 < minimum ≤ interval ≤ maximum. Got: 0 < [heartBeatIntervalMin] ≤ [nextHeartBeatInterval] ≤ [heartBeatIntervalMax] Setting min and max to configured interval.
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:createLoadedJobCache(org.apache.hadoop.conf.Configuration):[ERROR] The property MR_HISTORY_LOADED_TASKS_CACHE_SIZE is not an integer value. Please set it to a positive integer value.
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:launch():[INFO] Setting up container + masterContainer + for AM + application.getAppAttemptId()
org.apache.hadoop.hdfs.server.datanode.DataStorage:doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,java.io.File,java.io.File,java.io.File,int,org.apache.hadoop.conf.Configuration):[INFO] Upgrade of {} is complete
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Reserve on target node failed, e={}
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] doAsUser = {}, RemoteUser = {} , RemoteAddress = {}
org.apache.hadoop.registry.server.services.RegistryAdminService:purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback):[DEBUG] Match on record @ {} with children
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:parseArgs(java.lang.String[]):[ERROR] ${pe.getMessage()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStop():[INFO] Service map cleared
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[DEBUG] Updating file xattrs for re-encrypting zone {}, starting at {}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceStop():[INFO] Waiting for deletion thread to complete its current action
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String):[ERROR] {message}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:resolveOOM(java.util.concurrent.ExecutorService):[DEBUG] OOM handler failed
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:checksumBlockGroup(org.apache.hadoop.hdfs.protocol.LocatedStripedBlock):[WARN] src={}, datanodes[{}]={}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumEnteringMaintenanceDataNodes():[DEBUG] Failed to get number of entering maintenance nodes
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:loadTokens(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState):[DEBUG] Loading token from + key
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[DEBUG] Checking operation
org.apache.hadoop.ipc.Client$Connection:close():[DEBUG] closing ipc connection to server: closeException.getMessage()
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleCommit(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,int):[INFO] Special commit success
org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData():[WARN] Encountered exception
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream):[DEBUG] Tried to read from deleted edit log segment
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:subClusterHeartbeat(org.apache.hadoop.yarn.server.federation.store.records.SubClusterHeartbeatRequest):[INFO] Heartbeated the StateStore for the specified SubCluster
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:output(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,java.io.FileInputStream,java.util.ArrayList):[DEBUG] Time to output inodes: {}ms
org.apache.hadoop.ipc.Server$Handler:run():[DEBUG] Thread.currentThread().getName(): call for RpcKind + call.rpcKind
org.apache.hadoop.hdfs.server.federation.router.Router:serviceStart():[DEBUG] Jvm metrics set
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:namenodeHeartbeat(org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatRequest):[INFO] NN registration state has changed: {} -> {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:doSingleWrite(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx):[DEBUG] After writing {} at offset {}, updated the memory count, new value: {}, handle.dumpFileHandle(), offset, nonSequentialWriteInMemory.get()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[WARN] couldn't find container + containerId + while processing FINISH_CONTAINERS event
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:stopApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Failed to shutdown the request processing pipeline for app:{applicationId}, ex=""
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:updateLogAggregationStatus(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.LogAggregationStatus,long,java.lang.String,boolean):[WARN] The application: + appId + has already finished, and has been removed from NodeManager, we should not receive the log aggregation status update for this application.
org.apache.hadoop.mapred.pipes.Application$PingSocketCleaner:run():[DEBUG] Connection received from {}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$IBRTaskHandler:run():[INFO] offering IBR service
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:put(java.lang.String,byte[],boolean):[DEBUG] {} not created [EXCEPTION] Cannot write data into znode {}: {error message from exception}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:deleteFiles(java.util.Set):[INFO] {}: Directory entries containing files to delete: {}
org.apache.hadoop.hdfs.server.datanode.DataNode:removeVolumes(java.util.Collection):[INFO] Deactivating volumes (clear failure=%b): %s
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[INFO] Task attempt will be recovered as KILLED
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[ERROR] File does not exist
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse):[ERROR] YARN Service is unavailable or disabled.
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:createApplicationHistoryManager(org.apache.hadoop.conf.Configuration):[WARN] The filesystem based application history store is deprecated.
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:loadPools(java.io.DataInput):[DEBUG] Cache pool added
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:initializePipeline(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,org.apache.hadoop.security.token.Token,org.apache.hadoop.security.token.Token,java.util.Map,boolean,org.apache.hadoop.security.Credentials):[ERROR] Error storing AMRMProxy application context entry for applicationAttemptId, e
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:apply(java.lang.Object):[DEBUG] {}: Stage failure:, getName(), e
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] No delegation token found for url={}, authenticating with {}
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:shutdown():[ERROR] interrupted while waiting for reportCompileThreadPool to terminate, e
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getSnapshotDiffReportListing(java.lang.String,java.lang.String,java.lang.String,byte[],int):[INFO] Audit event success for computeSnapshotDiff
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[WARN] {}
org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration):[INFO] Config key {} 's value {} does not correspond to enum values of java.util.concurrent.TimeUnit. Hence default unit {} will be used
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[INFO] Dispatching event for app attempt
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:addSummaryLog(java.lang.String,java.lang.String,java.lang.String,boolean):[DEBUG] Incoming log {} not present in my summaryLogs list, add it
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized TimelineReader URI=... , clusterId=...
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[INFO] TaskAttempt had not completed, recovering as KILLED
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Application has completed successfully.
org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String):[INFO] Got {} = '{}' (default '{}')
org.apache.hadoop.examples.DBCountPageView:verify():[INFO] sumPageview= [value]
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[INFO] ...
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Completed reading history information of application attempt
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:scan(long):[DEBUG] After remove stream + handle.dumpFileHandle() + , the stream number: + size()
org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration):[DEBUG] ShutdownHookManager completed shutdown.
org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget):[WARN] Proceeding with manual HA state management even though\n+ automatic failover is enabled for + target
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recover():[DEBUG] Recovering application with state: {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[]):[DEBUG] *BLOCK* NameNode.blockReceivedAndDeleted: from {nodeReg} {receivedAndDeletedBlocksLength} blocks.
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Registering the Unmanaged application master {}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:markSuspectBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] {}: suspect block {} is already queued for rescanning.
org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String):[INFO] VM type = {vmBit}-bit
org.apache.hadoop.ha.ZKFailoverController:becomeActive():[ERROR] Couldn't make localTarget active, Throwable t
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.SubClusterRegisterRequest):[WARN] Missing GetSubClusterInfo Request. Please try again by specifying a Get SubCluster information.
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntityEvent(java.util.Set,byte[],int,byte[]):[WARN] Error while decoding tstype
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:close():[DEBUG] ByteBufferInputStream.close() for {}
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:enableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.EnableNameserviceRequest):[INFO] Nameservice {} enabled successfully.
org.apache.hadoop.yarn.client.api.YarnClient:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext):[INFO] Submit application invoked
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:stopThreads():[DEBUG] Stopping expired delegation token remover thread
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute():[INFO] Deleting key: {keyName} from KeyProvider: {provider} <!-- No log for post-println exception, as exception handling occurs -->
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner:run():[INFO] File scanner interrupted
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container):[DEBUG] allocate: applicationAttemptId=<ApplicationAttemptId> container=<ContainerId> host=<Host> type=<Type>
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String):[WARN] Exception running {s}, {e}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread:run():[DEBUG] Heartbeater interrupted
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[DEBUG] Requested datanode ask: %s
org.apache.hadoop.hdfs.server.datanode.DataNode:initDirectoryScanner(org.apache.hadoop.conf.Configuration):[WARN] Periodic Directory Tree Verification scan is disabled because verification is not supported by SimulatedFSDataset
org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long):[INFO] mlocking
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader:getParallelExecutorService():[WARN] Parallel is enabled and {} is set to {}. Setting to the default value {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks():[WARN] Error creating sink '%s'
org.apache.hadoop.yarn.client.RMProxy:createRetryPolicy(org.apache.hadoop.conf.Configuration,long,long,boolean):[WARN] YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS is smaller than YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS. Only try connect once.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Can't handle this event at current state, {e}
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:finishApplicationMaster(java.lang.String,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] Finishing UAM id {} for application {}
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Waiting for application
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error while storing reservation allocation., e
org.apache.hadoop.fs.s3a.select.SelectInputStream:read():[INFO] New instance created
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.registry.server.dns.RegistryDNS:op(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord,org.apache.hadoop.registry.server.dns.RegistryDNS$RegistryCommand):[WARN] Yarn Registry record {} does not contain {} attribute
org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.util.function.Supplier):[DEBUG] Error formatting message
org.apache.hadoop.hdfs.server.namenode.FSImageFormat:renameReservedPathsOnUpgrade(java.lang.String,int):[INFO] Upgrade process renamed reserved path + oldPath + to + path
org.apache.hadoop.hdfs.server.namenode.NameNode:monitorHealth():[WARN] Remote IP {} checking available resources took {}ms
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Using failoverProxy to service {method.getName()}
org.apache.hadoop.mapred.QueueConfigurationParser:parseResource(org.w3c.dom.Element):[INFO] Bad conf file: top-level element not <queues>
org.apache.hadoop.hdfs.server.common.JspHelper:getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,boolean):[DEBUG] getUGI is returning: {ugi.getShortUserName()}
org.apache.hadoop.hdfs.DFSInputStream:blockSeekTo(long):[INFO] Successfully connected to ...
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[DEBUG] saveCacheManagerSection completed
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int):[DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {} from priority queue {}
org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map):[INFO] Adding a node "host" to the list of type hosts from filename
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceStart():[DEBUG] Attempting secure login
org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String):[INFO] Computing capacity for map {mapName}
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:methodAction(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP):[WARN] {} gave an invalid proxy path {}, remoteUser, pathInfo
org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor:init(java.lang.String):[INFO] Error while creating Router RMAdmin Service for user:, user: <user>
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum():[INFO] Fetched new range of seq num, from {} to {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Log sync completed for non-HA setup [AUDIT] Rolling upgrade finalized audit event logged
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[INFO] Available space block placement policy initialized: DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY = balancedPreferencePercent
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:refreshSuperUserGroupsConfiguration():[INFO] Refreshing SuperUser proxy group mapping list
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentEventHandler:handle(org.apache.hadoop.yarn.service.component.ComponentEvent):[ERROR] [COMPONENT {component.getName()}]: Error in handling event type {component.getType()}
org.apache.hadoop.hdfs.server.namenode.FSImage:initNewDirs():[INFO] Wrote VERSION in the new storage, {sd.getCurrentDir()}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:loadToken(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState,byte[]):[INFO] Entering loadToken method
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getMinRecordsPerChunk(org.apache.hadoop.conf.Configuration):[WARN] DistCpConstants.CONF_LABEL_MIN_RECORDS_PER_CHUNK + " should be positive. Fall back to default value: " + DistCpConstants.MIN_RECORDS_PER_CHUNK_DEFAULT
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:copy(java.lang.String,java.lang.String):[DEBUG] Copy source key: [{}] to dest key: [{}].
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$ConnectionCreator:run():[ERROR] Cannot create a new connection
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyBytes(org.apache.hadoop.tools.CopyListingFileStatus,long,java.io.OutputStream,int,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Closing output stream
org.apache.hadoop.yarn.service.ServiceMaster:printSystemEnv():[INFO] {} = {}
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:removeAppFromRegistry(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Removing all registry entries for {}
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest):[WARN] Policy for queue: {} does not exist.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:handle(org.apache.hadoop.yarn.event.Event):[INFO] New REQUEST_RESOURCE_LOCALIZATION localize request for locId, remove old private localizer.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:cleanConfigurationFile():[INFO] delete config file <filePath>
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:recover():[DEBUG] Scheduling deletion of {} logs in {} msec
org.apache.hadoop.tools.rumen.TraceBuilder:run(java.lang.String[]):[WARN] Unable to bind Path
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream):[ERROR] The container {containerIdStr} couldn\'t be found on the node specified: {nodeId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:initializeState():[INFO] Initializing tc state.
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[ERROR] Application attempt Id doesn't exist in ApplicationMasterService cache.
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServices():[DEBUG] Scanner skips for unknown dir {}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Skip killing + rmContainer.getContainerId()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:readEntityTypes(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[DEBUG] Current row key: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractManagedParentQueue:addChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue):[DEBUG] updateChildQueues (action: add queue): + added + + getChildQueuesToPrint()
org.apache.hadoop.yarn.service.component.Component:requestContainers(long):[WARN] Please set memory/vcore in the main section of resource, ignoring this entry={}
org.apache.hadoop.mapreduce.lib.output.PartialOutputCommitter:cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Cleaning up partial output for task
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:serviceStart():[DEBUG] Started audit service {}, auditor
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:startSyncJournalsDaemon():[ERROR] Failed to create directory for downloading log segments: ${journal.getStorage().getEditsSyncDir()}. Stopping Journal Node Sync.
org.apache.hadoop.hdfs.server.datanode.DataNode:parseChangedVolumes(java.lang.String):[INFO] Deactivation request received for active volume: {dir.getRoot()}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Token verified
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:clearTrash():[ERROR] Trash and PreviousDir shouldn't both exist for storage directory {}
org.apache.hadoop.hdfs.DFSUtil:httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String):[INFO] Starting web server as: {serverPrincipal}
org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap():[DEBUG] Updated map for Mac
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:serviceStart():[INFO] Instantiated MRClientService at [dynamic address]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.SchedulerPlacementProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Found non empty SchedulingRequest of AllocateRequest for application=${appAttemptId.toString()}, however the configured scheduler=${scheduler.getClass().getCanonicalName()} cannot handle placement constraints, rejecting this allocate operation
org.apache.hadoop.security.KDiag:validateShortName():[WARN] principal short name: result still contains @ or /
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:checkAddLabelsToNode(java.util.Map):[ERROR] Not all labels being added contained by known label...
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfSlowPeerParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:methodAction(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP):[WARN] {} attempting to access {} that is invalid, remoteUser, appId
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Service added
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getVolumeInfo():[WARN] {IOException message}
org.apache.hadoop.hdfs.server.datanode.DataNode:startDataNode(java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[INFO] dnUserName = {dnUserName}
org.apache.hadoop.fs.s3a.S3AInstrumentation:close():[DEBUG] there is no metric system to unregister {name} from
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing master key
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Looking in service filesystems for implementation class
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:recoverUnfinalizedSegments():[INFO] Moving aside edit log file that seems to have zero transactions elf
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeReservationState(java.lang.String,java.lang.String):[INFO] Removing reservationallocation + reservationIdName + for plan + planName
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[DEBUG] BLOCK* addStoredBlock: {} is added to {} (size={})
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:postContainerReady():[WARN] {} failure getting localization statuses
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[WARN] When aborting {} stream after failing to close it for {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Can't get path for fileId: {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[ERROR] Unable to obtain the SubCluster information for {}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeProxies():[WARN] Other JournalNode addresses not available. Journal Syncing cannot be done
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[DEBUG] {}: the UNIX domain socket associated with this short-circuit memory closed before we could make use of the shm.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {userName}
org.apache.hadoop.yarn.webapp.WebApps$Builder:addFiltersForNewContext(org.eclipse.jetty.webapp.WebAppContext):[INFO] CSRF Protection has been enabled for the {} application. Please ensure that there is an authentication mechanism enabled (kerberos, custom, etc).
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:initDefaultNameService(org.apache.hadoop.conf.Configuration):[INFO] Default name service: {this.defaultNameService}, enabled to read or write
org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket):[TRACE] this: NotificationHandler: read succeeded on sock.fd
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:deleteDir(org.apache.hadoop.fs.Path):[INFO] Deleting path
org.apache.hadoop.util.Shell:isSetsidSupported():[DEBUG] setsid is not available on this machine. So not using it.
org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer):[INFO] Failed to free the buffer
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean):[DEBUG] passing over + elf + because it is in progress + and we are ignoring in-progress logs.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:appendFile(java.lang.String,java.lang.String,java.lang.String,java.util.EnumSet,boolean):[DEBUG] DIR* NameSystem.appendFile: src={}, holder={}, clientMachine={}
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int):[WARN] Could not lower thread count to {} from {}. Too busy.
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Getting Mapper class
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,boolean):[DEBUG] AM resource request: + amAsk.getPerAllocationResource() + exceeds maximum AM resource allowed, + getQueue().dumpState()
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:publishCompressedDataStatistics(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long):[INFO] Input Data Compression Ratio : [ratio]
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:shutdown():[WARN] Exception shutting down SecondaryNameNode
org.apache.hadoop.yarn.service.provider.ProviderUtils:createConfigFileAndAddLocalResource(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,java.util.Map,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams):[DEBUG] Tokens substitution for component instance: {instance.getCompInstanceName()}
org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer:close():[INFO] Stopping InMemoryLevelDBAliasMapServer
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$StatusUpdateWhenHealthyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[INFO] Node {rmNode.nodeId} reported UNHEALTHY with details: {remoteNodeHealthStatus.healthReport}
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeAssignedResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,java.lang.String,java.util.List):[DEBUG] storeAssignedResources: containerId= + container.getContainerId() + , assignedResources= + StringUtils.join(",", assignedResources)
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:uploadCurrentBlock():[DEBUG] Writing block # {}
org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite):[DEBUG] Class {} is not a CryptoCodec.
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:initializeNewPlans(org.apache.hadoop.conf.Configuration):[WARN] Plan based on reservation queue {} already exists.
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectory():[DEBUG] Building listing using multi threaded approach for %s
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Balance succeed!
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[WARN] Received exception in BlockPoolManager#shutDownAll
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:persistBlocks(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean):[DEBUG] persistBlocks: {path} with {file.getBlocks().length} blocks is persisted to the file system
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeFileWithRetry(java.lang.String,java.io.InputStream,byte[],long):[DEBUG] Store file successfully. COS key: [{}], ETag: [{}].
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean):[ERROR] Renaming operation executed
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=GETCONTENTSUMMARY target=path
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:parseDiagnosticOutput(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor,java.lang.String):[WARN] Output of aocl is: ...
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Secret keys
org.apache.hadoop.yarn.service.ServiceScheduler:terminateServiceIfAllComponentsFinished():[INFO] Failed components: []
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[DEBUG] {} is recovering. Skipping notifying ATTEMPT_ADDED, appAttemptId
org.apache.hadoop.hdfs.server.namenode.ImageServlet:doPut(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[ERROR] Expecting boolean obj for setting checking recent image, but got ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[INFO] drop FINISH_CONTAINERS event to + containerId + because container is recovering
org.apache.hadoop.fs.s3a.auth.SignerManager:initCustomSigners():[ERROR] Invalid format for CustomSigner: [customSigner]
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:getBlockToCopy(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$BlockIterator,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[INFO] NextBlock call returned null. No valid block to copy. {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:checkInterfaceCompatibility(java.lang.Class,java.lang.Class):[DEBUG] Checking implemented interface's compatibility: {expectedClass.getSimpleName()}
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String):[DEBUG] Adding request to ask resourceRequestInfo.remoteRequest
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[DEBUG] Storing token to {}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:loadAndRevert(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,org.apache.hadoop.fs.FileStatus):[DEBUG] Committing %s
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant):[WARN] Caching disabled because of slow operation (...)
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubClusters(org.apache.hadoop.yarn.server.federation.store.records.GetSubClustersInfoRequest):[ERROR] Unable to obtain the information for all the SubClusters
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] Exception while closing file {e.getMessage()}
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:extractMagicFileLength(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] Filesystem {} doesn't support XAttr API
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:initializeLeafQueueTemplate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue):[ERROR] Invalid node label {nodeLabel} on configured leaf template on parent queue {parentQueue.getQueuePath()}
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:loadStorageDirectory(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Block pool storage directory for location ... is not formatted. Formatting ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:call():[WARN] Failed to relaunch container.
org.apache.hadoop.fs.s3a.WriteOperationHelper:initiateMultiPartUpload(java.lang.String,org.apache.hadoop.fs.s3a.impl.PutObjectOptions):[DEBUG] Initiating Multipart upload to destKey
org.apache.hadoop.fs.cosn.CosNOutputStream:waitForFinishPartUploads():[ERROR] Cancelling futures.
org.apache.hadoop.mapreduce.task.reduce.Fetcher:copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost):[WARN] copyMapOutput failed for tasks failedTasks
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:calculateRollingMonitorInterval(org.apache.hadoop.conf.Configuration):[INFO] rollingMonitorInterval is set as [interval]. The log rolling monitoring interval is disabled. The logs will be aggregated after this application is finished.
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:serviceStart():[ERROR] Webapps failed to start. Ignoring for now: [dynamic exception]
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isRecoverySupported(org.apache.hadoop.mapreduce.JobContext):[INFO] Probe for isRecoverySupported({}): returning false
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,int):[DEBUG] List objects. prefix: [{}], delimiter: [{}], maxListLength: [{}], priorLastKey: [{}].
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier):[ERROR] Unable to remove token tokenId.getSequenceNumber(), e
org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal):[INFO] Interrupted: {data}
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$CleanupTask:run():[DEBUG] Closing and removing stale pool {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getStaleIntervalFromConf(org.apache.hadoop.conf.Configuration,long):[WARN] The given interval for marking stale datanode = ...
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context):[DEBUG] Error in resource usage emulation! Message:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:normalizeAndValidateRequest(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.api.records.QueueInfo,boolean):[WARN] Some warning message
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] ViewFs InternalDirOfViewFs listStatus call invoked
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:parsePreviousJobHistory():[INFO] Got an error parsing job-history file, ignoring incomplete events.
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.PlanningAlgorithm:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Added new reservation
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Node + node.getNodeName() + offered to queue: + getName() + fairShare: + getFairShare(),
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:modifyAclEntries(java.lang.String,java.util.List):[INFO] logAuditEvent(true, "modifyAclEntries", src, null, auditStat)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Resource {}{} size : {} transitioned from {} to {}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:actOnUnusableNode(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.NodeState):[INFO] TaskAttempt killed because it ran on unusable node {nodeId}. AttemptId:{id}
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {COMMITTER_NAME_DIRECTORY}
org.apache.hadoop.yarn.server.timeline.security.TimelineDelgationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Token max lifetime obtained
org.apache.hadoop.contrib.utils.join.DataJoinJob:createDataJoinJob(java.lang.String[]):[INFO] Using SequenceFileInputFormat: ${args[2]}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedInteractiveOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation):[DEBUG] command array:
org.apache.hadoop.fs.azure.CachingAuthorizer:init(org.apache.hadoop.conf.Configuration):[DEBUG] {} : Initializing CachingAuthorizer instance
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getClusterMetricsInfo():[WARN] Failed to get nodes report
org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String):[INFO] Building PowerShell script to kill {processName} at {host}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:reportNodeUnusable(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.api.records.NodeState):[DEBUG] Metrics updated for deactivated node
org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver:parseUserList(java.net.URI,org.apache.hadoop.conf.Configuration):[ERROR] Error while creating a proxy user
org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker:run():[INFO] Expired: key Timed out after interval secs
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[INFO] Current attempt state of + appAttemptId + is + attemptReport.getYarnApplicationAttemptState() + , waiting for current attempt to reach + attemptState
org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long):[DEBUG] {}
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Initializing SSL Context to channel mode Default
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:markContainerToNonKillable(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Retrieved container's allocated resource
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:writeFile(org.apache.hadoop.fs.Path,byte[]):[ERROR] IOException encountered during write
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.
org.apache.hadoop.fs.http.server.HttpFSServer:post(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] Unset ec policy [path]
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Total VirtualCores: countHere
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:finishApplicationMaster(java.lang.String,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] UAM id {} is unregistered
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:unRegisterNM():[INFO] Successfully Unregistered the Node + this.nodeId + with ResourceManager.
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:checkAndEnableAppAggregators():[INFO] LogAggregation enabled for application
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Boosting the map phase progress.
org.apache.hadoop.hdfs.server.datanode.DataNode:makeInstance(java.util.Collection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[INFO] Storage locations checked
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap):[TRACE] Excluded nodes: ...
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:removeReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[ERROR] Unable to remove reservation: {reservation.getReservationId()} from plan.
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute():[DEBUG] Block compaction: activated with {blockEntries.size()} blocks for {key}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregator:startContainerLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext):[INFO] Starting container log aggregation
org.apache.hadoop.lib.server.Server:init():[INFO] Source Revision : {}
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceStop():[WARN] Interrupted Exception while stopping, {ie}
org.apache.hadoop.examples.dancing.DistributedPentomino:run(java.lang.String[]):[INFO] Job configured successfully
org.apache.hadoop.fs.cosn.CosNFileSystem:delete(org.apache.hadoop.fs.Path,boolean):[DEBUG] Delete the file: {}
org.apache.hadoop.yarn.service.webapp.ApiServer:getProxyUser(javax.servlet.http.HttpServletRequest):[DEBUG] Create proxy user with remote user
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager$CollectorTokenRenewer:renewToken(org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector):[INFO] Renewed token for + appId + with new expiration + timestamp = + newExpirationTime
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:sendContainerRequest():[DEBUG] Application {} sends out requests for {} failed mappers.
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] SecondaryGroupExisting rule: parent rule result: {parentQueue}
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortSingleCommit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit):[INFO] Aborting commit ID {} to object {}{}, uploadId, destKey, origin
org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer):[DEBUG] Received ping message
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[ERROR] Failed to create file:<fileToCreate> at fallback : <linkedFallbackFs.getUri()>
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection):[DEBUG] Node {} is excluded, continuing.
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,java.lang.String,org.slf4j.Logger):[WARN] User {shortUserName} doesn't have permission to call '{method}'
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:mountCGroupController(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController):[INFO] CGroup controller already mounted at: existingMountPath
org.apache.hadoop.nfs.NfsExports$CIDRMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] CIDRNMatcher low = + subnetInfo.getLowAddress() + , high = + subnetInfo.getHighAddress() + , denying client ' + address + ', ' + hostname + '
org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String,java.util.Map):[INFO] addJerseyResourcePackage: packageName= + packageName + , pathSpec= + pathSpec
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:reconstruct():[ERROR] Transfer failed for all targets.
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:initCsiAdaptorCache(java.util.Map,org.apache.hadoop.conf.Configuration):[INFO] CSI Adaptor added to the cache, adaptor name: driverName, driver version: response.getVersion()
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:handleRedirect(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[ERROR] The AM's web app redirected the RM web proxy's request back to the web proxy. The typical cause is that the AM is resolving the RM's address as something other than what it expects. Check your network configuration and the value of the yarn.web-proxy.address property. Once the host resolution issue has been resolved, you will likely need to delete the misbehaving application, id
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:updateDemand():[DEBUG] Counting resource from childQueueName Resource; Total resource demand for FSParentQueueName now UpdatedDemand
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignMapsWithLocality(java.util.List):[DEBUG] Assigned based on host match [host]
org.apache.hadoop.mapred.gridmix.GenerateData:publishPlainDataStatistics(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path):[INFO] Total size of input data : + StringUtils.humanReadableInt(dataSize)
org.apache.hadoop.fs.Globber:doGlob():[DEBUG] Pattern: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil:setupSecurityAndFilters(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager):[INFO] Using RM authentication filter(kerberos/delegation-token) for RM webapp authentication
org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts():[ERROR] decayCurrentCosts exception: ...;
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:closeEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId):[ERROR] Error closing writer for JobID: + jobId
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:commitLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Callback handler does not implement container commit last re-initialization callback methods
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:handleNMContainerStatus(org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus,org.apache.hadoop.yarn.api.records.NodeId):[INFO] Ignoring not found attempt [appAttemptId]
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:run():[DEBUG] {}: wait for {} milliseconds
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:uncacheBlock(java.lang.String,long):[DEBUG] Block with id {}, pool {} does not need to be uncached, because it is not currently in the mappableBlockMap.
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Allocated Memory: memoryHere
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long):[TRACE] Incrementing counter {} by {} with final value {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$InitContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Adding + container.getContainerId() + to application + app.toString()
org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread:run():[WARN] Job [job.getJobID()] out of order
org.apache.hadoop.fs.s3a.S3AInstrumentation:getMetricsSystem():[DEBUG] Metrics system inited {}
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createAppDirs(java.util.List,java.lang.String,java.lang.String):[WARN] Unable to create app directory
org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String):[DEBUG] Returning '{value}' for key: {key}
org.apache.hadoop.ipc.Server:shutdownMetricsUpdaterExecutor():[INFO] Hadoop Metrics Updater executor shutdown interrupted.
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:setRollingUpgradeMarkers(java.util.List):[INFO] {} already exists.
org.apache.hadoop.mapreduce.v2.app.job.Job:loadConfFile():[ERROR] IOException occurred while loading configuration file.
org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:doShuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,org.apache.hadoop.mapred.IFileInputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter):[INFO] Read X bytes from map-output for Y
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogsMeta(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[INFO] Processing node file
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getBlockReports(java.lang.String):[WARN] Storage volume: {volStorageID} missing for the replica block: {b}. Probably being removed!
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovering Reservation system
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object):[DEBUG] BEFORE decResourceRequest: applicationId= priority={} resourceName={} numContainers={}
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectRpcInputStreams(java.util.Collection,long,boolean):[DEBUG] No new edits available in logs; requested starting from ID {}
org.apache.hadoop.hdfs.server.common.Util:receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler):[WARN] Overwriting existing file ...
org.apache.hadoop.yarn.service.utils.JsonSerDeser:fromJson(java.lang.String):[ERROR] Exception while parsing json : {Exception} \n {json}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:requestNewHdfsDelegationTokenAsProxyUser(java.util.Collection,java.lang.String,boolean):[INFO] Received new token [token]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[]):[INFO] *DIR* reportBadBlocks for block: {} on datanode: {}
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgrade(org.apache.hadoop.yarn.service.api.records.Service,java.util.List):[ERROR] {service.getName()} is at {appReport.getYarnApplicationState()} state, upgrade can only be invoked when service is running.
org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Setup connection to ...
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:blockCompaction():[DEBUG] Block compaction: {maxSegmentEnd - maxSegmentBegin} blocks for {key}
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:handleSyncableInvocation():[WARN] Application invoked the Syncable API against stream writing to {}. This is Unsupported
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:poll(long,java.util.concurrent.TimeUnit):[DEBUG] timed poll(): poll() returned null, sleeping for {} ms
org.apache.hadoop.hdfs.DataStreamer:waitForAckedSeqno(long):[WARN] Slow waitForAckedSeqno took {}ms (threshold={}ms). File being written: {}, block: {}, Write pipeline datanodes: {}.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[INFO] {}: Committing job "{}". resilient commit supported = {}
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop():[INFO] {} Component instance state changed from {} to {}
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing master key X
org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int):[INFO] f + ": Failed to open at " + rNode.getFs().getUri()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recover():[DEBUG] Recovering container with state: {}
org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion):[INFO] KMSClientProvider Decryption successful
org.apache.hadoop.hdfs.server.namenode.FSEditLog:registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration):[INFO] Registering new backup node: ...
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier):[DEBUG] Trying to retrieve password for {applicationAttemptId}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Token kind set
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest):[INFO] No remoteRequestTable found with allocationRequestId= + req.getAllocationRequestId()
org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String):[DEBUG] multipleLinearRandomRetry = {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Invalid eventtype UNKNOWN. Ignoring!
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInSnapshot(long):[ERROR] Error while resolving the path : fullName, exception
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:throttle():[DEBUG] Re-encryption handler throttling because queue size {} is larger than number of cores {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromSearchDirs(java.lang.String,java.lang.String[]):[INFO] Found script: {}
org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class):[ERROR] Error creating rate metrics for methodName
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue():[DEBUG] Backup store written
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:closeFileSystems(org.apache.hadoop.security.UserGroupInformation):[WARN] Failed to close filesystems: , e
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:addHandlersFromConfiguredResourcePlugins(java.util.List,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] List of plugins of ResourcePluginManager was empty while trying to add ResourceHandlers from configuration!
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext):[INFO] Reacquiring {} with pid {}
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceStop():[INFO] Stopping JobHistory
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSQuotaUsage operation executed
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int):[DEBUG] Generated Encrypted key for {} number of keys.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:requestBlockReportLeaseId(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[WARN] Failed to find datanode {}
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Duration of deletions: {0}
org.apache.hadoop.security.authentication.server.AuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Delegation token authentication attempted
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:copySucceeded(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MapHost,long,long,long,org.apache.hadoop.mapreduce.task.reduce.MapOutput):[DEBUG] map TaskAttemptID done
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:reencryptEncryptionZone(long):[INFO] Directory with id {} removed during re-encrypt, skipping
org.apache.hadoop.fs.http.server.HttpFSServer:get(java.lang.String,javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [{}] filter [{}]
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:moveBlock(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.Socket,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[DEBUG] Connecting to datanode {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post called for CONCAT
org.apache.hadoop.yarn.webapp.Dispatcher:setMoreParams(org.apache.hadoop.yarn.webapp.Controller$RequestContext,java.lang.String,org.apache.hadoop.yarn.webapp.Router$Dest):[DEBUG] parts={}, params={}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:saveBlockIterator(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$BlockIterator):[WARN] {}: error saving {}., this, iter, e
org.apache.hadoop.fs.s3a.S3AFileSystem:listObjects(org.apache.hadoop.fs.s3a.S3ListRequest,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[DEBUG] LIST {}
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.volume.csi.event.VolumeEvent):[WARN] Unexpected volume event received, event type is + event.getType().name() + , but the volumeId is null.
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doFinalize(java.io.File):[INFO] Finalizing upgrade for storage directory {}.\n cur LV = {}; cur CTime = {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:escalateRenameFailure(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[ERROR] {}: failure to {} {} to {} with source status {} and destination status {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:activateVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.fs.StorageType,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[ERROR] Found duplicated storage UUID: %s in %s.
org.apache.hadoop.hdfs.server.datanode.DataNode:transferReplicaForPipelineRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],java.lang.String):[DEBUG] Replica is being written!
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.lang.String):[DEBUG] Resolved hostName to rackName
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[WARN] Failed to place enough replicas, still in need of ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:checkAndUpdate(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo):[WARN] Removed block + blockId + from memory with missing block file on the disk
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:run():[INFO] Cleaning master[APP_ID]
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setMemoryPerNode(java.lang.String,int):[DEBUG] DRConf - setMemoryPerNode: nodePrefix={}, memory={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[DEBUG] Processing {resourcePath} of type {event.getType()}
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:startInfoServer():[INFO] Web server init done
org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[WARN] Kill preemption policy activated
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics):[ERROR] Cannot initialize record store for simpleName
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:checkAndEnableAppAggregators():[WARN] Enable aggregators failed
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getReportsForNode(java.lang.String):[TRACE] Retrieval of slow peer report is disabled. To enable it, please enable config {}.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String,boolean):[ERROR] Audit event: create, src
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] blacklistDisablePercent is {value}
org.apache.hadoop.tools.dynamometer.Client:init(java.lang.String[]):[INFO] Starting with arguments: ["{}"]
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:addFile(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String,boolean,java.lang.String,java.lang.String):[DEBUG] DIR* addFile: [FileName] is added
org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto):[DEBUG] Have read input token of size + saslToken.length + for processing by saslServer.evaluateResponse()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:canContainerBePreempted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource):[ERROR] Looking to preempt container + container + . Container does not belong to app + getApplicationId()
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[DEBUG] New write buffered with xid {} nextOffset {} req offset={} mapsize={}
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:readAcontainerLogs(java.io.DataInputStream,java.io.Writer,long):[DEBUG] Read a container logs
org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class):[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startFileInt(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String,boolean):[DEBUG] DIR* NameSystem.startFile...
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] CSRF Protection has been enabled for the {} application. Please ensure that there is an authentication mechanism enabled (kerberos, custom, etc).
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:shutdown():[ERROR] Failed to close provider., e
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:parseConfAndFindOtherNN():[ERROR] Could not determine valid IPC address for other NameNode (+info.getNameNodeID()+) , got: +address
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:finalize():[DEBUG] finalize() called
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:getBoundOrNewDT(org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[DEBUG] Returning current token
org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[INFO] Launched fencing command '...' with pid ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceStart():[INFO] Starting SchedulingMonitor=
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent):[INFO] {event.getContainerId()} Container Transitioned from {oldState} to {getState()}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.yarn.service.ServiceManager$CancelUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] [SERVICE] cancel version {}
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder):[INFO] Top limit input is not numeric, using default top value %d.
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:doPostEntities(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[WARN] Skip the timeline entity: { id: " + entity.getEntityId() + ", type: " + entity.getEntityType() + " }, e
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillNewTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Not generating HistoryFinish event since start event not generated for task: task.getID()
org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run():[WARN] Exception in the cleaner thread but it will continue to run
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnfinalizedSegments():[INFO] Successfully started new epoch <epoch value>
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KillingToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] NMAuditLogger log success for container
org.apache.hadoop.fs.s3a.S3AFileSystem:abortOutstandingMultipartUploads(long):[DEBUG] Purging outstanding multipart uploads older than {}
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String):[DEBUG] Creating handler for protocol {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:updateAMRMTokens(org.apache.hadoop.yarn.security.AMRMTokenIdentifier,org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$RequestInterceptorChainWrapper,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[INFO] The local AMRMToken has been rolled-over. Send new local AMRMToken back to application: + pipeline.getApplicationId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizedWhileRunningTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[ERROR] Error when creating symlink link -> rsrcEvent.getLocation()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleReduceContainerRequest(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[INFO] reduceResourceRequest: <reduceResourceRequest>
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempts(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Error when reading history information of some application attempts of application appId
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] SecondaryNameNode principal could not be added
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:doPlacement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.BatchedRequests,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithmOutput,java.util.List,java.util.List,java.util.Map):[WARN] Got exception from TagManager !
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnclosedSegment(long):[INFO] Using longest log: + bestEntry
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog):[INFO] Downloaded file {tmpEditsFile.getName()} of size {tmpEditsFile.length()} bytes.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Finalizing rolling upgrade
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:reclaimOpportunisticContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Container {} will be {} to start the execution of guaranteed container {}.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:logBlockReplicationInfo(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,java.lang.Iterable):[INFO] Block: {block}, Expected Replicas: {curExpectedRedundancy}, live replicas: {curReplicas}, corrupt replicas: {num.corruptReplicas()}, decommissioned replicas: {num.decommissioned()}, decommissioning replicas: {num.decommissioning()}, maintenance replicas: {num.maintenanceReplicas()}, live entering maintenance replicas: {num.liveEnteringMaintenanceReplicas()}, replicas on stale nodes: {num.replicasOnStaleNodes()}, readonly replicas: {num.readOnlyReplicas()}, excess replicas: {num.excessReplicas()}, Is Open File: {bc.isUnderConstruction()}, Datanodes having this block: {nodeList}, Current Datanode: {srcNode}, Is current datanode decommissioning: {srcNode.isDecommissionInProgress()}, Is current datanode entering maintenance: {srcNode.isEnteringMaintenance()}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Storing final state info for app: {appId} at: {nodeUpdatePath}
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Total # of splits generated by getSplits: [splits.size()], TimeTaken: [sw.now(TimeUnit.MILLISECONDS)]
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean):[INFO] Finished iteration of plan follower edit policy for plan: {}
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] SecondaryGroupExisting rule: parent rule found: {parentRule.getName()}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:build():[DEBUG] Block read failed. Getting remote block reader using TCP
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshUserToGroupsMappings():[INFO] Refresh user to groups mapping successful for address
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] RMStatusInfoBean registered
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:logRootNodeAcls(java.lang.String):[DEBUG] {}
org.apache.hadoop.mapreduce.util.MRJobConfUtil:setLocalDirectoriesConfigForTesting(org.apache.hadoop.conf.Configuration,java.io.File):[INFO] {} directory already exists
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:getEntityTypes(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[INFO] Reading entity types
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type:
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] Loaded {} edits file(s) (the last named {}) of total size {}, total edits {}, total load time {} ms
org.apache.hadoop.hdfs.DeadNodeDetector:checkDeadNodes():[DEBUG] Skip to add dead node {datanodeInfo} to check since the node is already in the probe queue.
org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Failed to load token fetcher implementation exception
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner:run():[DEBUG] Active scan starting
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:internalUpdatePendingAsk(org.apache.hadoop.yarn.api.records.SchedulingRequest,boolean):[INFO] Update numAllocation from old= + existingNumAllocations + to new= + newNumAllocations
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onContainerReInitialize(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] No component instance exists for {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:dynamicallyUpdateAppActivitiesMaxQueueLengthIfNeeded():[INFO] Update max queue length of app activities from {} to {}, configured={}, numNodes={}, numAsyncSchedulerThreads={} when multi-node placement disabled.
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:compareCellValues(org.apache.hadoop.hbase.Cell,org.apache.hadoop.hbase.Cell,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter):[ERROR] caught iae during conversion to long
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:parseCredentials(java.util.Map):[DEBUG] Retrieved credentials form RM for {}: {}
org.apache.hadoop.hdfs.DFSClient:renewDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Renewing + DelegationTokenIdentifier.stringifyToken(token)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:abortJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[DEBUG] No job directory to read uploads from
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult):[INFO] Deleted State Store record {}: {}
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:deprecateEnv(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[WARN] Environment variable {} is deprecated and overriding property {}, please set the property in {} instead.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean):[INFO] logAuditEvent: operationName, src
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:start():[INFO] Periodic Directory Tree Verification scan starting in {}ms with interval of {}ms and throttle limit of {}ms/s
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorServer():[ERROR] Error starting ResourceEstimatorServer
org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String):[WARN] dirsProp + "[" + index + "]=" + dirs[index]
org.apache.hadoop.fs.s3a.S3AInstrumentation:lookupQuantiles(java.lang.String):[DEBUG] No quantiles {}
org.apache.hadoop.yarn.service.client.ServiceClient:upgradePrecheck(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Service already at version
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:report(long,java.lang.String,java.lang.String):[DEBUG] a metric is reported: cmd: {} user: {}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:run():[INFO] LifelineSender for + BPServiceActor.this + exiting.
org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set):[DEBUG] parsedDestPath value is:${parsedDestPath}
org.apache.hadoop.hdfs.server.datanode.DataXceiver:checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode,org.apache.hadoop.fs.StorageType[],java.lang.String[]):[DEBUG] Checking block access token for block '{}' with mode '{}'
org.apache.hadoop.fs.s3a.S3AFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Copied {} bytes
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getTopUserOpCounts():[WARN] Failed to fetch TopUser metrics
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]):[ERROR] Error warming up keys for provider with url [provider.getKMSUrl()]
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:waitForRescanIfNeeded():[WARN] Interrupted while waiting for CacheReplicationMonitor rescan
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:shouldSkipNodeSchedule(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler,boolean):[DEBUG] Skip scheduling on node because it haven't heartbeated for + timeElapsedFromLastHeartbeat / 1000.0f + secs
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:getBlocksWithLocations(org.apache.hadoop.hdfs.protocol.DatanodeID,long,long):[WARN] BLOCK* getBlocks: Asking for blocks from an unrecorded node {datanode}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog):[WARN] Deleting {tmpEditsFile} has failed
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:writeAuditLog(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] App succeeded with state: FINISHED
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[DEBUG] Not cleanup up pending uploads to {} as {} is false
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger$ArgsBuilder):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectInputStreams(java.util.Collection,long,boolean):[WARN] Encountered exception while tailing edits >= + fromTxnId + via RPC; falling back to streaming.,ioe
org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp:appendFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,boolean,boolean):[DEBUG] DIR* NameSystem.appendFile: file {srcArg} for {holder} at {clientMachine} block {lb.getBlock()} block size {lb.getBlock().getNumBytes()}
org.apache.hadoop.mapred.gridmix.GenerateData:call():[ERROR] Error while adding input path
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int):[INFO] Waiting in main loop.
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkSubClusterInfo(org.apache.hadoop.yarn.server.federation.store.records.SubClusterInfo):[WARN] Missing SubCluster Information. Please try again by specifying SubCluster Information.
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:processResponseQueue():[DEBUG] Application {} sends out event to clean up its AM container.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:cacheBlock(java.lang.String,long):[WARN] Caching not supported on block with id + blockId + since the volume is backed by RAM.
org.apache.hadoop.hdfs.DataStreamer:run():[DEBUG] Thread interrupted
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List):[TRACE] Added block {} to cachedBlocks
org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String):[WARN] Slow name lookup for + hostname + . Took + elapsedMs + ms.
org.apache.hadoop.yarn.server.nodemanager.NodeManager:stopRecoveryStore():[WARN] Unable to delete + recoveryRoot
org.apache.hadoop.mapred.SortedRanges:remove(org.apache.hadoop.mapred.SortedRanges$Range):[DEBUG] nextRange startIndex: endIndex:
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:shutdown():[ERROR] Error shutting down all UAM clients without killing them, e
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [INIT COMPONENT]: {component.getName()}: {event.getDesired()} instances.
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:handleJobCommit(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobCommitEvent):[ERROR] Could not commit job
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveMetadata(java.lang.String):[DEBUG] Found blob as a directory-using this file under it to infer its properties {}
org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget):[WARN] "Unable to gracefully make {} standby ({})"
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Starting edit log roll due to rolling upgrade finalization
org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner:run():[INFO] Resource usage emulation complete! Matcher exiting
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] nodeBlacklistingEnabled:{value}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] Executing REST operation
org.apache.hadoop.hdfs.server.namenode.NNStorage:reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[DEBUG] current list of storage dirs:{}
org.apache.hadoop.streaming.PipeMapRed:maybeLogRecord():[INFO] info
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:internalError(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType):[ERROR] Invalid event + type + on Task + this.taskId
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:sendContainerRequest():[DEBUG] Application {} sends out requests for {} reducers.
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask:run():[DEBUG] Running DeletionTask : {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[INFO] Recovering AMRMProxyService
org.apache.hadoop.mapred.gridmix.JobMonitor:onSuccess(org.apache.hadoop.mapreduce.Job):[INFO] job.getJobName() + " (" + job.getJobID() + ")" + " success"
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:processHeartbeat(org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.LocalizerStatus):[INFO] Unknown localizer with localizerId + locId + is sending heartbeat. Ordering it to DIE
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:commitSubSection(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder,org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName):[WARN] The requested section for {} is empty. It will not be output to the image
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] AFTER decResourceRequest: applicationId= applicationId.getId() priority= priority.getPriority() resourceName= resourceName numContainers= remoteRequest.getNumContainers() #asks= ask.size()
org.apache.hadoop.hdfs.server.common.Util:deleteTmpFiles(java.util.List):[INFO] Deleting temporary files: + files
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline(java.lang.String):[ERROR] Init ClientRequestInterceptor error for user: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setAcl(java.lang.String,java.util.List):[INFO] logAuditEvent(true, operationName, src, null, auditStat)
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[INFO] Interrupted waiting for countdown latch
org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts():[DEBUG] Decaying costs for the user: ..., its decayedCost: ..., rawCost: ...;
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:serviceStop():[ERROR] Failed to cleanup staging dir:
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:applyRequestLimits():[INFO] Applying ask limit of newReq.getNumContainers() for priority:reqLimit.getPriority() and capability:reqLimit.getCapability()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getVirtualMemorySize(int):[DEBUG] VMEM Comparison: [procfs value] [cgroup value]
org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration):[INFO] Calling getInstance with CipherSuite
org.apache.hadoop.ipc.Server$Responder:run():[ERROR] "Couldn't close write selector in " + Thread.currentThread().getName(), ioe
org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String):[INFO] {percentage}% max memory {StringUtils.TraditionalBinaryPrefix.long2String(maxMemory, "B", 1)} = {StringUtils.TraditionalBinaryPrefix.long2String((long) percentMemory, "B", 1)}
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:addReservationQueue(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue,java.lang.String):[WARN] Exception while trying to activate reservation: {} for plan: {}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the app_flow table
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onContainerReInitializeError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] No component instance exists for {}
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage,boolean):[WARN] {} is not a directory
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:handleNoAccess(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] User doesn't have permissions to <acl>
org.apache.hadoop.mapreduce.JobSubmitter:populateTokenCache(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials):[DEBUG] adding the following namenodes' delegation tokens: null
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector:putDomain(org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain,org.apache.hadoop.security.UserGroupInformation):[DEBUG] putDomain(domain={}, callerUgi={})
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[DEBUG] Failed to delete {0}
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEvent):[ERROR] Failed to store attribute modification to storage
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeIgnore(boolean,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE):[DEBUG] action
org.apache.hadoop.yarn.server.resourcemanager.AdminService:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[ERROR] Exception transitioning to standby
org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException):[INFO] Retrying connect to server: + server + . Already tried + curRetries + time(s); maxRetries= + maxRetries
org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:requestNextBatch():[DEBUG] New listing state: {}
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:heartbeat():[INFO] ApplicationMaster is out of sync with ResourceManager, hence resync and send outstanding requests.
org.apache.hadoop.yarn.server.resourcemanager.AdminService:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[ERROR] Exception transitioning to active
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader:loadEdits():[ERROR] Got RuntimeException at position + inputStream.getPosition()
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishContainerLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent,java.lang.String):[ERROR] Failed to publish Container metrics for container ${container.getContainerId()}
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Error in resource usage emulation! Message:
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:printQueue(java.util.TreeSet):[DEBUG] First Volume : %s, DataDensity : %f, Last Volume : %s, DataDensity : %f
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:doCheckpoint():[WARN] Checkpoint done. New Image Size: ...
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Upgrade of {} is complete
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:run():[WARN] Failed to delete as user {}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:handleFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileHandler):[DEBUG] Handling files with dangling temp data
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:checkDirs():[WARN] {entry.getValue().cause} is unknown for disk error.
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean):[DEBUG] Moving {} to {}
org.apache.hadoop.hdfs.server.federation.router.DFSRouter:main(java.lang.String[]):[ERROR] Failed to start router, e
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the finish data of application [applicationId]
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:initialize(org.apache.hadoop.conf.Configuration,long,long,org.apache.hadoop.fs.Path):[INFO] Compressed input; cannot compute number of records in the split
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action: DNA_CACHE for ... of [...]
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:recalculateChecksum(int,long):[DEBUG] Recalculated checksum for the block index:{}, checksum={}
org.apache.hadoop.mapreduce.task.reduce.Fetcher:openShuffleUrl(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Set,java.net.URL):[WARN] Failed to connect to {host} with {remaining.size()} map outputs
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:topologyAwareSchedule(java.util.Set,int,java.util.Map,java.util.Set,java.util.Map):[ERROR] No cost table initialized!
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:lostFoundInit(org.apache.hadoop.hdfs.DFSClient):[WARN] Cannot initialize /lost+found .
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor):[INFO] DatanodeCommand action : DNA_REGISTER from + actor.nnAddr + with + actor.state + state
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow version: {parts[1]}
org.apache.hadoop.fs.StreamCapabilitiesPolicy:unbuffer(java.io.InputStream):[DEBUG] in.getClass().getName() + : + does not implement StreamCapabilities + and the unbuffer capability
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[INFO] removing RMDelegation token with sequence number: {sequence_number}
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:rollMasterKey():[INFO] Rolling master-key for container-tokens
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[WARN] TaskAttempt found in unexpected state, recovering as KILLED
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:validateConf(org.apache.hadoop.conf.Configuration):[ERROR] YarnConfiguration.NM_LOCAL_CACHE_MAX_FILES_PER_DIRECTORY parameter is configured with very low value.
org.apache.hadoop.yarn.server.nodemanager.NodeManager:stopRecoveryStore():[INFO] Removing state store due to decommission
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$DecommissioningNodeTransition:transition(java.lang.Object,java.lang.Object):[INFO] Preserve original total capability: CAPABILITY_PLACEHOLDER
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getAutoCreatedQueueManagementPolicyClass(java.lang.String):[INFO] Using Auto Created Queue Management Policy: queueManagementPolicyClassName for queue: queueName
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:saveDfsUsed():[WARN] Failed to delete old dfsUsed file in ...
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectoryMultiThreaded():[ERROR] Could not get item from childQueue. Retrying...
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean):[TRACE] Hosts:{}, CNs:{} subjectAlts:{}, ie6:{}, strictWithSubDomains{}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Storing CA Certificate and Private Key
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf:enableParallelSaveAndLoad(org.apache.hadoop.conf.Configuration):[WARN] Parallel Image loading and saving is not supported when {} is set to true. Parallel will be disabled.
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:createNNProtocolProxy(java.net.InetSocketAddress):[DEBUG] Creating non-HA Proxy
org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider:getConfigurationInputStream(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] {filePath} not found
org.apache.hadoop.tools.DistCpSync:sync():[WARN] Failed to use snapshot diff for distcp
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String):[DEBUG] BLOCK* NameSystem.abandonBlock: {b} of file {src}
org.apache.hadoop.hdfs.server.balancer.Balancer:runOneIteration():[INFO] Need to move {StringUtils.byteDesc(bytesLeftToMove)} to make the cluster balanced.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing RMDelegationKey_{}
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogType(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,boolean):[INFO] readAggregatedLogs
org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration):[DEBUG] Using java.security.SecureRandom as random number generator.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Not creating intermediate history logDir: [... based on conf
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:initializeClient(java.net.URI,java.lang.String,java.lang.String,boolean):[TRACE] Fetching token provider
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket):[TRACE] Hosts:{}, CNs:{} subjectAlts:{}, ie6:{}, strictWithSubDomains{}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:chooseProtocolVersion(org.apache.hadoop.hdfs.protocol.EncryptionZone,org.apache.hadoop.crypto.CryptoProtocolVersion[]):[DEBUG] Ignoring unknown CryptoProtocolVersion provided by client: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext):[INFO] Probe for isCommitJobRepeatable({}): returning false
org.apache.hadoop.ipc.Client$Connection:close():[DEBUG] getName(): closed
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[DEBUG] NoOpTimelineWriter is configured. Not storing TimelineEntities.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Failed to move reservation, cannot find source node={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignContainersOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey):[DEBUG] assignContainersOnNode: node= + node.getRMNode().getNodeAddress() + application= + application.getApplicationId().getId() + priority= + schedulerKey.getPriority() + #assigned= + (nodeLocalContainers + rackLocalContainers + offSwitchContainers)
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntityTypes():[DEBUG] Successfully retrieved entity types
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.ReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] Updated reservation using GreedyReservationAgent
org.apache.hadoop.yarn.webapp.Dispatcher:removeCookie(javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[DEBUG] removing cookie {} on {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor):[DEBUG] Initiating caching for Block with id {}, pool {}
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue():[INFO] Value deserialized
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain:main(java.lang.String[]):[ERROR] FATAL, Error while starting FS configuration conversion!, t
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean):[DEBUG] Assigned container + container.getId() + of capacity + container.getResource() + on host + getRMNode().getNodeAddress(), which has + getNumContainers() containers, + getAllocatedResource() used and + getUnallocatedResource() available after allocation
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessorMonitor:run():[ERROR] Returning, interrupted : [Exception Detail]
org.apache.hadoop.hdfs.server.datanode.DataStorage:getParallelVolumeLoadThreadsNum(int,org.apache.hadoop.conf.Configuration):[INFO] Using {} threads to upgrade data directories ({}={}, dataDirs={})
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:submitReservationWhenSpecified():[INFO] RESERVATION SUCCESSFULLY SUBMITTED <placeholder:reservationRequest.getReservationId()>
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[ERROR] Application {} does not exist
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:convertCredentialsFromByteBuffer(java.nio.ByteBuffer):[ERROR] Failed to convert Credentials from ByteBuffer.
org.apache.hadoop.mapreduce.security.TokenCache:mergeBinaryTokens(org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration):[ERROR] Failed to read token storage file
org.apache.hadoop.tools.util.ProducerConsumer$Worker:run():[DEBUG] Interrupted while waiting for requests from inputQueue.
org.apache.hadoop.ipc.Server$Listener:run():[WARN] Out of Memory in server select
org.apache.hadoop.yarn.server.webapp.WrappedLogMetaRequest:getContainerLogMetas():[DEBUG] Setting Container ID: {containerId}
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[ERROR] Application {appId} does not exist
org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver:run():[INFO] FSImageSaver clean checkpoint: txid={} when meet + Throwable., context.getTxId()\n
org.apache.hadoop.hdfs.server.namenode.NNStorage:reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[ERROR] Error reported on storage directory {}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:setKeyACLs(org.apache.hadoop.conf.Configuration):[WARN] Invalid key Operation '{}'
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:calculateHeartBeatInterval(long,long,long,float,float):[DEBUG] Setting heartbeatinterval to: [value] node: [this.nodeId] nodeUtil: [nodeUtil] clusterUtil: [clusterUtil]
org.apache.hadoop.fs.azure.NativeAzureFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean):[DEBUG] Creating directory: {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkFaultTolerantRetry(org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.String,java.io.IOException,org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,java.util.List):[DEBUG] {} does not allow retrying a failed subcluster
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:logResult(java.util.List):[INFO] Mount table entries cache refresh successCount={},failureCount={}
org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map):[INFO] Added filter (name) (class=classname) to context webAppContext.getDisplayName()
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServices():[INFO] Scan for launch type on {}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[INFO] Killed during application recovery
org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long):[INFO] Got {} = '{}' (default '{}'), name, value, defaultValue
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:close():[DEBUG] {}: Closing block #{}: current block= {}, this, blockCount, hasBlock ? block : (none)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:getHeadroom():[DEBUG] Headroom calculation for {}:Min((queueFairShare={} - queueUsage={}), maxAvailableResource={} Headroom={}, this.getName(), queueFairShare, queueUsage, maxAvailableResource, headroom
org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService:updateToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long):[DEBUG] Updating token in FileSystem state store
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceStart():[INFO] MRAppMaster uberizing job ...
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:rollbackLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Exception when scheduling the event Rollback re-initialization of Container ...
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext):[WARN] Error in signalling container {} with {}; exit = {}, pid, signal, retCode, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[ERROR] Unable to decrease container resource
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:launchUAM():[DEBUG] Creates the UAM connection
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[TRACE] Failed to place enough replicas, still in need of...
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:setBoundingQuery(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] Could not find SUBSTITUTE_TOKEN token in query: ${query}; splits may not partition data.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String):[DEBUG] addResourceRequest: applicationId= + applicationId.getId() + priority= + priority.getPriority() + resourceName= + resourceName + numContainers= + remoteRequest.getNumContainers() + #asks= + ask.size()
org.apache.hadoop.hdfs.server.namenode.Checkpointer:initialize(org.apache.hadoop.conf.Configuration):[INFO] Checkpoint Period : + checkpointConf.getPeriod() + secs + ( + checkpointConf.getPeriod() / 60 + min
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:syncWithJournalAtIndex(int):[ERROR] Exception in getting local edit log manifest
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop():[INFO] {Thread.currentThread().getName()} unexpectedly interrupted
org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig):[INFO] Allowed Methods: allowedMethodsHeaderValue
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class):[DEBUG] {rmServiceIds}
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Writing with FileSystemTimelineWriterImpl
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice$AddReplicaProcessor:compute():[WARN] Caught exception while adding replicas from + volume + in subtask. Will throw later.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unknown event arrived at FairScheduler
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] XAttrs not supported on at least one file system: , {e}
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path):[WARN] Can't create trash directory: ...
org.apache.hadoop.fs.s3a.S3AFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[WARN] Access Point usage required
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[INFO] Retrieving block access token.
org.apache.hadoop.util.functional.TaskPool:waitFor(java.util.Collection,int):[DEBUG] Waiting for {} tasks to complete
org.apache.hadoop.hdfs.server.datanode.DataStorage:linkBlocks(java.io.File,java.io.File,int,org.apache.hadoop.fs.HardLink,org.apache.hadoop.conf.Configuration):[ERROR] There are {} duplicate block entries within the same volume.
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initializing Document Store Reader for : {storeType}
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String):[INFO] YarnException occurred
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] AMRMClientAsync initialized
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:getSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.GetSafeModeRequest):[INFO] Safemode status retrieved successfully.
org.apache.hadoop.hdfs.HAUtil:getNameNodeId(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Retrieved trimmed value for namenode id
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:uploadPart(java.io.File,java.lang.String,java.lang.String,int):[INFO] Current thread: [%d], COS key: [%s], upload id: [%s], part num: [%d], exception: [%s]
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String):[DEBUG] *DIR* NameNode.create: file {src} for {clientName} at {clientMachine}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:getSmapBasedRssMemorySize(int):[INFO] SmapBasedCumulativeRssmem (bytes) : total
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$RMAppStateFileProcessor:processChildNode(java.lang.String,java.lang.String,byte[]):[INFO] Unknown child node with name: {childNodeName}
org.apache.hadoop.hdfs.server.namenode.NNStorage:reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] About to remove corresponding storage: {}
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:getAllApplications():[WARN] Unexpected exception from listDirRegistry
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Formatting block pool {blockpoolID} directory {bpSdir.getCurrentDir()}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Creating default headers
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[WARN] Last seen exception:
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shutDownJob():[INFO] Job finished cleanly, recording last MRAppMaster retry
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:main(java.lang.String[]):[ERROR] Error running Client, t
org.apache.hadoop.ha.ZKFailoverController:recheckElectability():[INFO] Would have joined master election, but this node is prohibited from doing so for ... more ms
org.apache.hadoop.yarn.server.resourcemanager.AdminService:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[INFO] Transaction succeeded
org.apache.hadoop.resourceestimator.skylinestore.validator.SkylineStoreValidator:validate(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[ERROR] Resource allocation for <pipelineId> is null.
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename destination cannot be the root
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:allocate(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Skipped app activity without allocation
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:shutdown():[ERROR] interrupted while waiting for masterThread to terminate, e
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[WARN] Access control exception for operation
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceStart():[INFO] Starting AMRMProxyService
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:postEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineEntities):[ERROR] Error putting entities
org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault():[INFO] Successfully instantiated LdapSslSocketFactory with keyStoreLocation = {} and trustStoreLocation = {}
org.apache.hadoop.hdfs.server.namenode.NameNode:doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration):[INFO] RECOVERY COMPLETE
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:shutdown():[INFO] All async lazy persist service threads have been shut down
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:createDefaultReservationQueue(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue,java.lang.String):[WARN] Exception while trying to create default reservation queue for plan: {}, planQueueName, e
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] this + ": allocAndRegisterSlot " + idx + ": allocatedSlots=" + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread())
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:apply(java.lang.Object):[INFO] {}: Executing Stage {}, getName(), stageName
org.apache.hadoop.ipc.Server$Responder:doRunLoop():[WARN] Out of Memory in server select {e}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:addMap(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[DEBUG] Added attempt req to host {host}
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] KMS Hadoop Version:
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:createProcessInfo(java.lang.String):[DEBUG] Expected split length of proc info to be {}. Got {}
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getNamenodesSubcluster(org.apache.hadoop.hdfs.server.federation.store.MembershipStore):[ERROR] Cannot get local host name
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed while checking for/creating history staging path
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:uploadFileToPendingCommit(java.io.File,org.apache.hadoop.fs.Path,java.lang.String,long,org.apache.hadoop.util.Progressable):[DEBUG] Initiating multipart upload from {} to {}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:containerFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerFinishData):[INFO] Finish information of container ...
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[DEBUG] fetchColumnsFromFilterList executed for IS_RELATED_TO
org.apache.hadoop.fs.s3a.S3AFileSystem:isDirectory(org.apache.hadoop.fs.Path):[INFO] Track duration and span invoked
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[INFO] drop FINISH_APPS event to + appID + because container + container.getContainerId() + is recovering
org.apache.hadoop.tools.OptionsParser:parse(java.lang.String[]):[INFO] Set distcp blocksPerChunk to ...
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState):[DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[INFO] schedulerConfDir=...
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Error retrieving tokenInfo [ident.getSequenceNumber()] from ZK
org.apache.hadoop.examples.Sort:run(java.lang.String[]):[INFO] Running on 1 nodes to sort from input into output with 1 reduces.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[INFO] Write lock acquired
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS REMOVE dir fileHandle: {} fileName: {} client: {}
org.apache.hadoop.crypto.key.kms.server.KMS:getKeysMetadata(java.util.List):[DEBUG] Exception in getKeysmetadata.
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:poll(long,java.util.concurrent.TimeUnit):[DEBUG] timed poll(): timed out
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[INFO] STATE* Network topology has {} racks and {} datanodes
org.apache.hadoop.util.VersionInfo:main(java.lang.String[]):[DEBUG] version: x.y.z
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ExitFinishingOnTimeoutTransition:transition(java.lang.Object,java.lang.Object):[WARN] Task attempt is done from TaskUmbilicalProtocol's point of view. However, it stays in finishing state for too long
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[WARN] Failed to cache the block [key= + key + ]!
org.apache.hadoop.yarn.service.ClientAMService:cancelUpgrade(org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto):[INFO] Cancel service upgrade by {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDiscoverer:initialize(org.apache.hadoop.conf.Configuration):[WARN] Failed to pass FPGA devices diagnose
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager:initializeQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] Initialized root queue + root
org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration):[WARN] Cannot parse the value for FS_FTP_TRANSFER_MODE: mode. Using default.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Evicting {} due to space limitations
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* removeStoredBlock: {} from {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceAllocator:updateFpga(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice,java.lang.String,java.lang.String):[INFO] Update IP hash to + newHash
org.apache.hadoop.yarn.service.ServiceScheduler:terminateServiceIfDominantComponentFinished(org.apache.hadoop.yarn.service.component.Component):[INFO] {} Component state changed from {} to {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser:run():[ERROR] Error processing logs for (appId), t
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:parseTokenFromStream(java.io.InputStream,boolean):[DEBUG] Expiry based on expires_in: {expiryPeriodInSecs}
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:unpublishVolumes():[INFO] Un-publishing Volumes
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:getPlanQueue(java.lang.String):[ERROR] The Plan is not an PlanQueue!
org.apache.hadoop.hdfs.server.federation.router.RouterUserProtocol:getGroupsForUser(java.lang.String):[DEBUG] Getting groups for user {}
org.apache.hadoop.hdfs.server.datanode.BlockScanner:addVolumeScanner(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[DEBUG] Not adding volume scanner for {}, because the block scanner is disabled.
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:heartbeat():[ERROR] Could not contact RM after {retryInterval} milliseconds.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedInteractiveOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation):[WARN] logBuilder.toString()
org.apache.hadoop.fs.azure.NativeAzureFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] Did not find any metadata for path: {key}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[]):[INFO] Total num containers requested
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:addFileForEditLog(org.apache.hadoop.hdfs.server.namenode.FSDirectory,long,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,java.util.List,java.util.List,short,long,long,long,boolean,java.lang.String,java.lang.String,byte,byte):[WARN] DIR* FSDirectory.unprotectedAddFile: exception when add existing.getPath() to the file system
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path):[ERROR] Couldnt change the file permissions
org.apache.hadoop.mapred.uploader.FrameworkUploader:addJar(java.io.File):[INFO] Whitelisted /path/to/jar
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:run():[INFO] Start MarkedDeleteBlockScrubber thread
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:getAllocationFile(org.apache.hadoop.conf.Configuration):[WARN] {allocFilePath + " not found on the classpath."}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:init(org.apache.hadoop.conf.Configuration):[INFO] Initializing the GreedyReservationAgent to favor "late" (right) allocations (controlled by parameter: FAVOR_EARLY_ALLOCATION)
org.apache.hadoop.yarn.service.webapp.ApiServer:getProxyUser(javax.servlet.http.HttpServletRequest):[DEBUG] Get login user for proxy
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:stopDaemons():[WARN] Exception shutting down web server
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectInputStreams(java.util.Collection,long,boolean,boolean):[DEBUG] Tailing edits starting from txn ID ... via RPC mechanism
org.apache.hadoop.yarn.service.monitor.ServiceMonitor$ReadinessChecker:run():[INFO] Readiness check failed for {}: {}
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:generateContainerStatusList():[DEBUG] NodeManager {} completed container ({}).
org.apache.hadoop.fs.DU:refresh():[WARN] Could not get disk usage information for path {}, getDirPath(), ioe
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[]):[WARN] {} is shutting down, this, re
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] Failed to choose from local rack (location = {localRack}), retry with the rack of the next replica (location = {nextNode.getNetworkLocation()})
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:kill():[WARN] {}:DataXceiverServer.kill()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCache:loadCache(boolean):[INFO] MembershipNamenodeResolver cache loaded
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:handleRejectedRequests(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Following requests of [{}] were rejected by the PlacementAlgorithmOutput Algorithm: {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:removeInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource):[INFO] Interval extends beyond the end time {timePeriod}
org.apache.hadoop.fs.s3a.S3AInstrumentation:lookupGauge(java.lang.String):[DEBUG] No gauge {}
org.apache.hadoop.mapreduce.JobSubmitter:printTokens(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.security.Credentials):[INFO] Submitting tokens for job: +jobId
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get():[INFO] error prefetching block {}. {}
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[ERROR] Error putting entities, %s
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:loadPlugIns(org.apache.hadoop.conf.Configuration):[DEBUG] Load plugin {} with classpath: {}
org.apache.hadoop.yarn.service.ServiceManager$CancelUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] [SERVICE]: Cannot cancel the upgrade in {} state
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:init(java.util.Properties):[INFO] Using keytab {}, for principal {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map):[INFO] Got environment: {id}, search IP file in localized resources
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[INFO] No operation timeline writer used
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:scheduleReconstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int):[DEBUG] Block {} cannot be reconstructed from any node
org.apache.hadoop.lib.service.scheduler.SchedulerService:destroy():[WARN] ex.getMessage(), ex
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getInitialCachedResources(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[INFO] Querying for all individual cached resource files
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl):[INFO] Adding job token for + oldJobIDString + to jobTokenSecretManager
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType):[INFO] No striped internal block on source {}, block {}. Skipping.
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getHomeDirectory():[ERROR] Unable to get HomeDirectory from original File System
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.Class):[ERROR] Cannot get a connection to {} because the manager isn't running
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[WARN] Exception shutting down DataNode HttpServer
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:offerService():[DEBUG] BP offer service run start time: {}, sendHeartbeat: {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp):[DEBUG] doEditTx() op={} txid={}
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:containerCreated(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,long):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$SendEntity:run():[ERROR] Error when publishing entity:
org.apache.hadoop.fs.azure.BlockBlobAppendStream:close():[ERROR] Time out occurred while close() is waiting for IO request to finish in append for blob : {} , key
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String):[DEBUG] Error getting users for netgroup
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:verifyCommandOptions(java.lang.String,org.apache.commons.cli.CommandLine):[ERROR] %nInvalid argument found for command %s : %s%nValid arguments are : %n\t %s : %s %n
org.apache.hadoop.fs.s3a.S3AUtils:initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration):[DEBUG] Request timeout is too high({} ms). Setting to {} ms instead
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[WARN] Exception closing executor service {}
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Selecting stream
org.apache.hadoop.crypto.key.kms.server.KMS:getCurrentVersion(java.lang.String):[TRACE] Entering getCurrentVersion method.
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(java.lang.Object):[DEBUG] Done
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ContainerId):[INFO] createSuccessLog(user, operation, target, null, null)
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[INFO] Application not found: [aid]
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:safeDelete(org.apache.hadoop.fs.azure.StorageInterface$CloudBlobWrapper,org.apache.hadoop.fs.azure.SelfRenewingLease):[ERROR] Encountered Storage Exception for delete on Blob: ..., Exception Details: ... Error Code: ...
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:deleteDir(org.apache.hadoop.fs.Path):[ERROR] Unable to remove path
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Updated master key log
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:activateApplications():[INFO] Application {applicationId} from user: {user} activated in queue: {queuePath}
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadDirectories(java.io.FileInputStream,java.util.List,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,org.apache.hadoop.conf.Configuration):[INFO] Loading directories
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String,java.lang.String[],java.lang.String,boolean,boolean,org.apache.hadoop.hdfs.server.federation.resolver.order.DestinationOrder,org.apache.hadoop.hdfs.tools.federation.RouterAdmin$ACLEntity):[DEBUG] Updating existing mount table entry
org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer):[DEBUG] Thread.currentThread().getName() + ": processOneRpc from client " + this + " threw exception [" + rse + "]"
org.apache.hadoop.yarn.service.component.Component:assignContainerToCompInstance(org.apache.hadoop.yarn.api.records.Container):[INFO] [COMPONENT {}]: {} allocated, num pending component instances reduced to {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[TRACE] storageTypes={}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] detail
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(long,long,org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Save namespace ...
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture):[DEBUG] Waiting for task completion
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:activeQueue():[INFO] The specified queue: + getQueuePath() + is already in the RUNNING state.
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:queryObjectMetadata(java.lang.String):[ERROR] Retrieve file metadata file failed. COS key: [%s], CosServiceException: [%s].
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanupInterruptedCommit(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[INFO] Delete startJobCommitFile in case commit is not finished as successful or failed.
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[INFO] Loading node into resolver: nodeName --> subClusterId
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:logToCSV(java.util.List):[DEBUG] sb.toString()
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Failed to delete {0}, ignoring exception {1}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Temporary redirect response built
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:identifyContainersToPreempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt):[INFO] Preempting container <container> from queue: <app.getQueueName()>
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:checkManifestOwnerAndPermissions(org.apache.hadoop.fs.FileStatus):[ERROR] Manifest must be owned by YARN admin: + manifest
org.apache.hadoop.yarn.service.ServiceScheduler:terminateServiceIfAllComponentsFinished():[INFO] Succeeded components: [A,B,C]
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable):[TRACE] {}: successfully loaded {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] RMAppManager received completed appId of null, skipping
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Creating file system output stream
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getSnapshotDiffReportListing(java.lang.String,java.lang.String,java.lang.String,byte[],int):[INFO] Audit event failed for computeSnapshotDiff
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:readTrailingEmptyPacket():[TRACE] Reading empty packet at end of read
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:shutdown(org.apache.hadoop.hdfs.shortcircuit.DfsClientShm):[WARN] this + ": error shutting down shm: got IOException calling " + "shutdown(SHUT_RDWR)"
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:readStats():[DEBUG] classId -> bytes sent {}
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse):[ERROR] Authentication required
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[INFO] Initialized Docker runtime
org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker:schedule(org.apache.hadoop.hdfs.server.datanode.checker.Checkable,java.lang.Object):[DEBUG] Skipped checking {}. Time since last check {}ms is less than the min gap {}ms., target, msSinceLastCheck, minMsBetweenChecks
org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long):[WARN] Unknown method + methodName + called on + connectionProtocolName + protocol.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:setRootNodeAcls():[DEBUG] After setting ACLs\n
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine):[ERROR] failover: incorrect arguments
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger):[INFO] createStartupShutdownMessage(classname, hostname, args)
org.apache.hadoop.fs.s3a.S3AFileSystem:setInputPolicy(org.apache.hadoop.fs.s3a.S3AInputPolicy):[WARN] setInputPolicy is no longer supported
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeDirectorySectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String):[ERROR] {} exceptions occurred loading INodeDirectories
org.apache.hadoop.lib.server.Server:init():[INFO] Built information:
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:nextTcpPeer():[TRACE] nextTcpPeer: created newConnectedPeer {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:startDaemons():[ERROR] failed to start web server
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Containers Allocated: countHere
org.apache.hadoop.fs.s3a.ProgressableProgressListener:uploadCompleted():[DEBUG] S3A write delta changed after finished: {} bytes
org.apache.hadoop.tools.dynamometer.ApplicationMaster:cleanup():[INFO] Exception thrown in thread join: ...
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doUpgrade(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.Storage):[ERROR] Unable to rename temp to previous for + sd.getRoot(), ioe
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getNumberOfDatanodes(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Acquired read lock
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:renameScriptFile(org.apache.hadoop.fs.Path):[INFO] User + appSubmitterUgi.getUserName() + added suffix(.sh/.bat) to script file as + renamedScriptPath
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Saved output of task '{attemptId}' to {outputPath}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:enableAsyncAuditLog(org.apache.hadoop.conf.Configuration):[WARN] Log4j is required to enable async auditlog
org.apache.hadoop.fs.cosn.BufferPool:createDir(java.lang.String):[DEBUG] Buffer dir: [{}] is created successfully.
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[ERROR] File is not closed
org.apache.hadoop.hdfs.server.namenode.CacheManager:removeCachePool(java.lang.String):[INFO] removeCachePool of + poolName + failed: , e
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[WARN] Ending block pool service for: this
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Node not in list!
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:innerCommitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] task statistics\n{}
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(long,byte[],int,int):[DEBUG] read requested b.length = {} offset = {} len = {}, b.length, off, len
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:verifyAndGetJob(org.apache.hadoop.mapreduce.v2.api.records.JobId,boolean):[INFO] getLoginUser called
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String):[WARN] createFailureLog(user, operation, target, description, null, null)
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[INFO] negotiable preemption : A resourceReq, B containers
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:squashCGroupOperations(java.util.List):[WARN] Invalid argument: + arg
org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion):[INFO] DefaultCryptoExtension Decryption successful
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:throwApplicationDoesNotExistInCacheException(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[ERROR] Application doesn't exist in cache ...
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Entity document writing process initiated
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[INFO] Waiting up to 30 seconds for transfer threads to complete
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseArguments(java.lang.String[]):[ERROR] No filesystem specified in either fs or target.
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:startSyncJournalsDaemon():[ERROR] JournalNodeSyncer daemon received Runtime exception.
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] Initialized KeyProviderCryptoExtension
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[DEBUG] Completed container: {} in state: {} event:{}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent):[INFO] Event Writer setup for JobId: [jobId], File: [historyFile]
org.apache.hadoop.hdfs.server.namenode.FSImage:doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Starting upgrade of local storage directories. old LV = ...; old CTime = ... new LV = ...; new CTime = ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:killContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Killing container ${container}
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration):[DEBUG] concat: result: + dstfs.getFileStatus(firstChunkFile)
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:refresh():[DEBUG] Host list file [{}] has not been modified from last refresh
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:addPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy):[INFO] The policy name already exists
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path):[INFO] Input data generation successful.
org.apache.hadoop.registry.server.dns.RegistryDNSServer:main(java.lang.String[]):[DEBUG] Generic options parsed
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:chooseDatanode(org.apache.hadoop.hdfs.server.namenode.NameNode,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus):[DEBUG] DataNode {} was requested to be excluded, but it was not found.
org.apache.hadoop.tools.SimpleCopyListing:writeToFileListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path):[DEBUG] REL PATH: {}, FULL PATH: {}
org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE):[DEBUG] Service: {} entered state {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getMountPointStatus(java.lang.String,int,long):[DEBUG] Cannot get remote user: {msg}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:signalContainersIfOvercommitted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,boolean):[INFO] {} is overcommitted ({}), preempt/kill containers
org.apache.hadoop.security.token.Token:getRenewer():[WARN] No TokenRenewer defined for token kind {}
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:checkAndDeleteCgroup(java.io.File):[WARN] Failed to read cgroup tasks file.
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[]):[DEBUG] Executing command {SelectTool.NAME}
org.apache.hadoop.streaming.StreamJob:parseArgv():[WARN] -file option is deprecated, please use generic option -files instead.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:getContainerPid():[DEBUG] Accessing pid for container {} from pid file {}
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:seek(long):[DEBUG] set nextReadPos to {}
org.apache.hadoop.hdfs.qjournal.server.Journal:syncLog(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL):[INFO] Synchronizing log [segment] from [url]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[WARN] Failed to cache + key + : could not reserve + more bytes in the cache: + cacheLoader.getCacheCapacity() + exceeded when try to reserve + length + bytes.
org.apache.hadoop.fs.s3a.impl.RequestFactoryImpl:newUploadPartRequest(java.lang.String,java.lang.String,int,long,java.io.InputStream,java.io.File,long):[DEBUG] Creating part upload request for {} #{} size {}, uploadId, partNumber, size
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[DEBUG] Waiting for Event Handling thread to complete
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$CleanupTask:run():[DEBUG] Cleaning up {}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread:run():[INFO] Shutdown requested. Stopping callback.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] Can't get path for fileId: {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,long,java.lang.String):[WARN] Unable to parse credentials for <applicationId>
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:run():[INFO] Namesystem is not running, skipping decommissioning/maintenance checks.
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy):[INFO] YarnConfiguration initialized
org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory):[INFO] Enabled protocols: {}
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:addSpillIndexFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] addSpillIndexFileCB... Path: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppState(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] Attempting to get RMApp for appId
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[DEBUG] The first job has a submit time of [firstJobSubmitTime]
org.apache.hadoop.hdfs.DFSClient:getBlockSize(java.lang.String):[WARN] Problem getting block size
org.apache.hadoop.yarn.service.ServiceMaster:recordTokensForContainers():[INFO] token.toString()
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.PlanningAlgorithm:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Added new reservation for periodic allocation
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:processResponseQueue():[INFO] Application {} goes to finish.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Additional complete request on completed container {containerId}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] User {} added to activeUsers, currently: {}
org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.util.function.Supplier):[TRACE] Adding context entry {}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:getNextBlockToScan():[WARN] {}: nextBlock error on {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid FSINFO request
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initDriver():[ERROR] Invalid root directory, unable to initialize driver.
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:start(java.lang.String):[INFO] Starting SyncJournal daemon for journal + jid
org.apache.hadoop.hdfs.DFSUtil:getJournalNodeAddresses(org.apache.hadoop.conf.Configuration):[ERROR] The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not properly set with correct journal node hostnames
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:analyseBlocksStorageMovementsAndAssignToDN(org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy):[WARN] The storage policy {} is not suitable for Striped EC files. So, ignoring to move the blocks
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:onAMLaunchFailed(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Exception):[INFO] Error launching {appAttemptId}. Got exception: {stringifiedException}
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean):[INFO] Starting directory copy
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:render():[DEBUG] Your relevant debug message here
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[WARN] Exception
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:serviceStop():[INFO] App-level aggregator shutdown timed out, shutdown now.
org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider:getToken():[DEBUG] AAD Token is missing or expired: Calling refresh-token from abstract base class
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAllocateBlockId(long):[INFO] AllocateBlockIdOp instance created and block ID set
org.apache.hadoop.yarn.service.client.ServiceClient:checkPermissions(org.apache.hadoop.fs.Path):[ERROR] User must be on the {} or {} list to have permission to upload AM dependency tarball
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getRuntimeSpec(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Cannot get allocation for container: [containerId]
org.apache.hadoop.hdfs.util.MD5FileUtils:renameMD5File(java.io.File,java.io.File):[WARN] deleting + fromFile.getAbsolutePath() + FAILED
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:moveBlocksToPending():[DEBUG] There are no pending or blocks yet to be processed
org.apache.hadoop.crypto.key.kms.server.KMSACLs:run():[WARN] Could not reload ACLs file: '%s'
org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]):[DEBUG] Have read input token of size ${inBuf.length} for processing by saslServer.unwrap()
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:loadDriver():[ERROR] Cannot initialize State Store driver {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadRMAppState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Unknown child node with name {} under {}
org.apache.hadoop.fs.azure.CachingAuthorizer:get(java.lang.Object):[DEBUG] {}: CACHE HIT: {}, {}
org.apache.hadoop.ha.StreamPumper:pump():[WARN] logPrefix + ": " + line
org.apache.hadoop.hdfs.web.resources.ExceptionHandler:toResponse(java.lang.Throwable):[TRACE] GOT EXCEPITION, e
org.apache.hadoop.hdfs.server.namenode.NNStorage:attemptRestoreRemovedStorage():[INFO] restoring dir {}
org.apache.hadoop.nfs.nfs3.Nfs3Base:startTCPServer():[ERROR] Failed to start the TCP server.
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersAllocated(java.util.List):[ERROR] "Exception when removing the matching requests. ", e
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans():[DEBUG] Stacktrace:
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[INFO] EDEKCacheLoader interrupted before warming up.
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:checkPauseForTesting():[INFO] Continuing re-encrypt handler after pausing.
org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean:isSecurityEnabled():[DEBUG] Checking federal security status
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot():[DEBUG] Taking snapshot of IOStatisticsContext id {}
org.apache.hadoop.examples.DBCountPageView:shutdown():[WARN] Exception occurred while closing connection : + StringUtils.stringifyException(ex)
org.apache.hadoop.hdfs.qjournal.server.Journal:doRollback():[INFO] storage.getJournalManager().doRollback called
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:storeEdits(byte[],long,long,int):[ERROR] Edits cache is out of sync; looked for next txn id at %d but got start txn id for cache put request at %d. Reinitializing at new request.
org.apache.hadoop.yarn.service.webapp.ApiServer:updateLifetime(java.lang.String,org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.security.UserGroupInformation):[INFO] Service (appName)'s lifeTime is updated to newLifeTime, updateAppData.getLifetime() seconds remaining
org.apache.hadoop.yarn.service.webapp.ApiServer:getProxyUser(javax.servlet.http.HttpServletRequest):[DEBUG] Create remote user with remote user
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,int,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] got reply from datanode:{} for blockIdx:{}, checksum:{}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntityTypes(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Adding block pool
org.apache.hadoop.fs.s3a.commit.impl.CommitContext:buildThreadPool(int):[DEBUG] creating thread pool of size {}, numThreads
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:start():[ERROR] Unable to update current master key in state store, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] SchedulerAttempt + getApplicationAttemptId() + is recovering container + rmContainer.getContainerId()
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... is a file.
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage):[WARN] Storage directory {} does not exist, rootPath
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid READ request
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close():[ERROR] Error closing provider with url
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfSlowDiskParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}, property, newVal
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:finish():[INFO] Application completed. Stopping running containers
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:sortLocatedBlocks(java.lang.String,java.util.List):[DEBUG] Sorting located block
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:getSpillFileCB(org.apache.hadoop.fs.Path,java.io.InputStream,org.apache.hadoop.conf.Configuration):[ERROR] Could not get inputStream position.. Path {}
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService$OpportunisticAMSProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[ERROR] Calling allocate on previous or removed or non existent application attempt {appAttemptId}
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] # applications = {}, # total tasks = {}, average # tasks per application = {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:schedule(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler):[INFO] New instance created
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:moveToDone():[INFO] Moved {} to {}, appDirPath, doneAppPath
org.apache.hadoop.fs.FileSystem:debugLogFileSystemClose(java.lang.String,java.lang.String):[DEBUG] FileSystem.{}() by method: {}); {}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads():[ERROR] Could not stop Delegation Token Counter
org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getRecordClass(java.lang.Class):[ERROR] Logger error message with arguments
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[ERROR] Audit event failed
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:rebootNodeStatusUpdaterAndRegisterWithRM():[ERROR] Unexpected error rebooting NodeStatusUpdater, e
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getEstimatedResourceAllocation(java.lang.String):[DEBUG] Query the skyline store for pipelineId: {}. + pipelineId
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] Service {} is already in a terminated state {}
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[DEBUG] RegisterUAM returned existing running container {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean):[TRACE] Block {} numExpected={}, numLive={}
org.apache.hadoop.mapred.JobConf:getTaskJavaOpts(org.apache.hadoop.mapreduce.TaskType):[INFO] Task java-opts do not specify heap size. Setting task attempt jvm max heap size to + xmxArg
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[DEBUG] Setting output format
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[WARN] User '{}' is not allowed to do placement based on application tag
org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] RPC Server's Kerberos principal name for protocol= + protocol.getCanonicalName() + is + serverPrincipal
org.apache.hadoop.tools.CopyListing:validateFinalListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[DEBUG] Copy list entry idx: lastFileStatus.getPath().toUri().getPath()
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[INFO] Found {} INodes in the INode section
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[INFO] Path access checked successfully
org.apache.hadoop.yarn.service.client.ServiceClient:addKeytabResourceIfSecure(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Using a keytab from localhost: keytabURI
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:addInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource):[INFO] Cannot set capacity beyond end time: {timePeriod} was ({interval.toString()})
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[WARN] Unknown DatanodeCommand action: ...
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[WARN] Error cleaning up a HistoryFile that is out of date.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:attemptScheduling(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[INFO] Skipping scheduling as the node {nodeID} has been removed
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:handleRollingUpgradeStatus(org.apache.hadoop.hdfs.server.protocol.HeartbeatResponse):[ERROR] Invalid BlockPoolId ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:parseOutput(java.lang.String):[WARN] Unknown key {key}, ignored
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean):[TRACE] this can't register a slot because the ShortCircuitRegistry is not enabled.
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[ERROR] Usage: MapFile inFile outFile
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Attempted to delete a non-existing znode {nodeRemovePath}
org.apache.hadoop.portmap.RpcProgramPortmap:set(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR):[DEBUG] Portmap set key= + key
org.apache.hadoop.hdfs.server.federation.router.FederationUtil:getJmx(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.web.URLConnectionFactory,java.lang.String):[DEBUG] JMX URL: {}
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Saved output of task '{attemptId}' to {committedTaskPath}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:processResponseQueue():[DEBUG] Application {} starts to launch a container ({}).
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[WARN] Exception running child ...
org.apache.hadoop.hdfs.server.common.Util:isDiskStatsEnabled(int):[INFO] DFSConfigKeys.DFS_DATANODE_FILEIO_PROFILING_SAMPLING_PERCENTAGE_KEY set to ${fileIOSamplingPercentage}. Enabling file IO profiling
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectMetadataRequest,java.lang.String):[DEBUG] Restricting metadata request to version {}
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] {}: waiting for loading to finish...
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] RefreshTokenBasedTokenProvider initialized
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadToken(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState,org.apache.hadoop.fs.Path,long):[DEBUG] Token identifier fields read
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int,int):[INFO] Waiting in main loop.
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[INFO] Setting destination locations
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponents(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List):[INFO] PUT: upgrade components {} for service {} + user = {}
org.apache.hadoop.yarn.server.webproxy.ProxyCA:init():[WARN] Could not verify Certificate, Public Key, and Private Key: regenerating
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:getMapCompletionEvents():[DEBUG] Got + events.length + map completion events from + fromEventIdx
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:readFile(org.apache.hadoop.fs.Path,long):[DEBUG] Cleanup with logger
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String,boolean):[DEBUG] Retrying getTokenSingleCall. RetryCount = {}
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFile(java.lang.String,java.lang.String,long,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3AReadOpContext):[DEBUG] copyFile {srcKey} -> {dstKey}
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Endpoint {epr} is the standard one; declare region as null
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Failed: countHere
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter:saveNextReplica():[WARN] Failed to save replica {block}. re-enqueueing it.
org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call):[INFO] ${Thread.currentThread().getName() + ", call " + call}: ${e}
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:cleanupRunningContainers():[ERROR] Failed to stop Container [containerId] when stopping NMClientImpl
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:confirmMutation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore$LogMutation,boolean):[WARN] pendingMutation or tempConfigPath is null, do nothing
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Adding csi-driver-adaptor for csi-driver {}
org.apache.hadoop.hdfs.server.namenode.sps.DatanodeCacheManager:getLiveDatanodeStorageReport(org.apache.hadoop.hdfs.server.namenode.sps.Context):[DEBUG] LIVE datanodes: {}
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[DEBUG] Unexpected node event: ...
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Generating {hamlet} methods
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:verifyFileLocation(java.io.File,java.io.File,long):[WARN] Block: + blockId + found in invalid directory. Expected directory: + expectedBlockDir + . Actual directory: + actualBlockDir
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] SecondaryGroupExisting rule: parent rule failed
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] amRegistrationResponse recovered for {}
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:setNextDirectoryInputStream():[INFO] \nOpening file [currentFileName] *************************** .
org.apache.hadoop.mapred.LocalJobRunner$Job:run():[INFO] Failed to createOutputCommitter
org.apache.hadoop.hdfs.server.common.Util:deleteTmpFiles(java.util.List):[WARN] Deleting {file} has failed
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[ERROR] copyBlocksToLostFound: error processing {fullName}
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:getApplicationAttemptEntity(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Map):[DEBUG] Fields parameter provided
org.apache.hadoop.hdfs.server.federation.router.security.token.DistributedSQLCounter:selectCounterValue(boolean,java.sql.Connection):[DEBUG] Select counter statement: + query
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] onStartContainerError received unknown container ID: {containerId}
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[DEBUG] TaskType set to MAP
org.apache.hadoop.mapreduce.CryptoUtils:wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean):[DEBUG] IV read from Stream [encodedIV]
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction):[DEBUG] AzureBlobFileSystem.access path : {}, mode : {}
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:updateTimelineCollectorContext(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow name: {}, flowName
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleCacheCleanup():[DEBUG] stats.toStringDetailed()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:doFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain):[INFO] Redirecting to AHS ...
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAclEntries(java.lang.String,java.util.List):[INFO] ACL entries removed successfully
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:updateNonSequentialWriteInMemory(long):[DEBUG] Update nonSequentialWriteInMemory by {} new value: {}, count, newValue
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing [EVENT_ID] of type [EVENT_TYPE]
org.apache.hadoop.fs.s3a.tools.MarkerTool:execute(org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanArgs):[INFO] The directory marker policy of ... is "..."
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:run():[WARN] Returning, interrupted : e
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:initializeProcessingChain(org.apache.hadoop.conf.Configuration):[WARN] Found PlacementProcessor= + p.getClass().getCanonicalName() + defined in + YarnConfiguration.RM_APPLICATION_MASTER_SERVICE_PROCESSORS + , however PlacementProcessor handler should be configured + by using + YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_HANDLER + , this processor will be ignored.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler:parseAndConvert(java.lang.String[]):[INFO] Missing command line arguments
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getStorageDirs(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] !!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:\n\t\t- shutdown and restart NameNode with configured "propertyName" in hdfs-site.xml;\n\t\t- use Backup Node as a persistent and up-to-date storage of the file system meta-data.
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[TRACE] {ThreadId}: Call -> {remoteId}: {methodName}: {TextFormat.shortDebugString(theRequest)}
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:plan(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode):[INFO] Compute Plan for Node : {}:{} took {} ms, node.getDataNodeName(), node.getDataNodePort(), endTime - startTime
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] {}: shared memory segment access is disabled.
org.apache.hadoop.yarn.service.component.Component:areDependenciesReady():[ERROR] Couldn't find dependency {dependency} for {getName()} (should never happen)
org.apache.hadoop.mapred.YARNRunner:generateResourceRequests():[DEBUG] AppMaster capability = {capability}
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] looking for configuration option {}
org.apache.hadoop.fs.s3a.S3AUtils:initUserAgent(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration):[DEBUG] Using User-Agent: {}
org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion):[DEBUG] Decryption process started
org.apache.hadoop.mount.MountdBase:start(boolean):[ERROR] Failed to register the MOUNT service.
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceStart():[INFO] Login user acquired
org.apache.hadoop.mapreduce.task.reduce.Fetcher:getMapOutputURL(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Collection):[DEBUG] MapOutput URL for host -> url
org.apache.hadoop.hdfs.net.DomainPeerServer:close():[ERROR] error closing DomainPeerServer: , e
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveFailureTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Unexpected event for REDUCE task {event.getType()}
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Looking for Hadoop tarball for version: {version}
org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory:buildAmazonS3Client(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters):[WARN] ** FAILURE INJECTION ENABLED. Do not run in production! **
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner$NodeHealthMonitorExecutor:reportHealthStatus(org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner$HealthCheckerExitStatus):[WARN] Unknown HealthCheckerExitStatus - ignored.
org.apache.hadoop.yarn.service.webapp.ApiServer:cancelUpgradeService(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {} cancelling upgrade
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:deleteCgroup(java.lang.String):[DEBUG] deleteCgroup: {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver:save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression):[INFO] Saving image file {newFile} using {compression}
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[ERROR] Failed to create the workingDir:
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[TRACE] AzureBlobFileSystemStore init complete
org.apache.hadoop.mapreduce.Cluster:initProviderList():[INFO] Failed to instantiate ClientProtocolProvider, please check the /META-INF/services/org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider files on the classpath
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:run():[DEBUG] Dumper woke up
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:finalizeFileSystemFile():[INFO] finalize temp configuration file successfully, finalConfigPath= + finalConfigPath
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shutDownJob():[WARN] Graceful stop failed. Exiting..
org.apache.hadoop.security.authentication.util.KerberosName:resetDefaultRealm():[DEBUG] resetting default realm failed, current default realm will still be used.
org.apache.hadoop.fs.s3a.select.SelectInputStream:seek(long):[DEBUG] ignoring seek to current position.
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean):[DEBUG] Running plan follower edit policy for plan: {}
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:getFailoverVirtualIP(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Name service ID {} will use virtual IP {} for failover, nameServiceID, virtualIP
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest):[INFO] Signal to container successful
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:createTimelineReaderStore(org.apache.hadoop.conf.Configuration):[INFO] Using store: + timelineReaderClassName
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[DEBUG] Creating FSImage
org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call):[WARN] ${Thread.currentThread().getName() + ", call " + call}, ${e}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:checkAndGetApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application ' + applicationId + ' is submitted without priority hence considering default queue/cluster priority: + appPriority.getPriority()
org.apache.hadoop.hdfs.server.datanode.DataStorage:linkBlocks(java.io.File,java.io.File,java.lang.String,int,org.apache.hadoop.fs.HardLink,org.apache.hadoop.conf.Configuration):[INFO] Start linking block files from {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[DEBUG] {}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureTokens(org.apache.hadoop.security.token.Token,org.apache.hadoop.security.Credentials,java.util.Map):[INFO] Adding # + credentials.numberOfTokens() + tokens and # + credentials.numberOfSecretKeys() + secret keys for NM use for launching container
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappedBlock:close():[WARN] Failed to delete the mapped File: {}!
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:free():[WARN] this + ": failed to munmap", e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] [Container ID] is released by application
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String):[DEBUG] Receiver: Block replaced
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Returning null.
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[DEBUG] Gracefully shutting down executor service {}. Waiting max {} {}
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:localizeClasspathJar(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] localizeClasspathJar: jarPath target o:owner
org.apache.hadoop.hdfs.DFSUtilClient:connectToDN(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,javax.net.SocketFactory,boolean,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[DEBUG] Connecting to datanode dnAddr
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath):[DEBUG] DIR* FSDirectory.unprotectedDelete: failed to remove ${iip.getPath()} because it does not exist
org.apache.hadoop.yarn.service.client.ServiceClient:actionDestroy(java.lang.String):[INFO] Service ' + serviceName + ' doesn't exist at hdfs path: + appDir
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[DEBUG] Block files moved to rbw directory
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler):[INFO] Sent total: {total} bytes. Size of last segment intended to send: {num} bytes.
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal():[INFO] Moving har to original location
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:tryDatanode(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] got reply from {}: blockChecksum={}, blockChecksumType={}, datanode, blockChecksumForDebug, getBlockChecksumType()
org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler:printEventQueueDetails():[INFO] Event type: {entry.getKey()}, Event record counter: {num}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[ERROR] Application {} does not exist
org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException):[WARN] Bad checksum at {position}. Skipping entries.
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock():[DEBUG] block[{}]: closeBlock()
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:initializeAuthHandler(java.lang.String,java.util.Properties):[ERROR] Failed to initialize authentication handler + authHandlerClassName, ex
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[DEBUG] Adding trackID:{} for the file id:{} back to retry queue as some of the blocks movement failed.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[INFO] STATE* Safe mode is OFF
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Roll back resource for container + containerId
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Provided block pool slice fetched and logged
org.apache.hadoop.yarn.service.component.Component$CheckStableTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] Component is stable after upgrade or cancel upgrade ]]>
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:checkAndInitializeLocalDirs():[WARN] msg, e
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] Adding: {}
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,int,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] write to {}: {}, block={}
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] Setting truststore location to
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Failed to submit application to parent-queue: + parent.getQueuePath(), ace
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[DEBUG] Shutting down timer for {mi}
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop():[WARN] Gave up waiting for the app check task to shutdown.
org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration):[ERROR] ex.toString()
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:flush():[INFO] Starting flush of map output
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:verifyRMHeartbeatResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] Node Labels {{}} were Accepted by RM, StringUtils.join(,, getPreviousValue())
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext):[INFO] Initializing Federation Interceptor
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:clearDirectory(java.lang.String):[INFO] Delete current dump directory {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String):[WARN] this.prefix + metrics system already initialized!
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone():[DEBUG] #{}: {}, getCallId(), r.getState()
org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute():[INFO] Alias has been successfully created.
org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[]):[INFO] Stream closed
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:start():[INFO] Default Metrics System initialized
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setCapacity(java.lang.String,float):[DEBUG] CSConf - setCapacity: queuePrefix={}, capacity={}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver:serializeFilesUCSection(java.io.OutputStream):[WARN] Fail to find inode + id + when saving the leases.
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object):[TRACE] program + procedure # + call.getProcedure()
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error storing app: {appId}, {e}
org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration):[ERROR] ShutdownHookManager interrupted while waiting for termination.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:scriptBasedInit(java.util.function.Function,java.lang.String[]):[INFO] Use {} as script name.
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$LeafQueueBlock:render():[INFO] Max Running Applications:
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:checkIfAlreadyBootstrapped(java.lang.String):[WARN] Failed to match regex: regex Current state: state
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[INFO] File created with overwrite rule set
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:moveBlocksToPending():[INFO] There are {} blocks pending replication and the limit is {}. A further {} blocks are waiting to be processed. The replication queue currently has {} blocks
org.apache.hadoop.mapred.TaskAttemptListenerImpl$TaskProgressLogPair:update(float):[DEBUG] Reset log
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:signalContainersIfOvercommitted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,boolean):[INFO] Send {} to {} to free up {}
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[INFO] Application {} with appId {} submitted on {}
org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options):[DEBUG] Generating key material
org.apache.hadoop.metrics2.impl.MetricsConfig:getClassName(java.lang.String):[DEBUG] Class name for prefix {} is {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:checkInterfaceCompatibility(java.lang.Class,java.lang.Class):[ERROR] Method {method.getName()} is not found in plugin
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[INFO] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:processNodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppNodeUpdateEvent$RMAppNodeUpdateType,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Received node update event:{} for node:{} with state:
org.apache.hadoop.hdfs.DataStreamer:endBlock():[DEBUG] Closing old block {}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$MRAppMasterShutdownHook:run():[INFO] MRAppMaster received a signal. Signaling RMCommunicator and JobHistoryEventHandler.
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded:clearQueuesWithNotification():[WARN] Failed to remove SPS xattr for track id [itemInfo.getFile()]
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList):[INFO] New instance created
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature):[INFO] End checkpoint for + registration.getAddress()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration):[INFO] Backup node ... re-registers
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache$StreamMonitor:run():[TRACE] StreamMonitor can still have a sleep:...
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:printConfiguredHosts(boolean):[DEBUG] include:
org.apache.hadoop.fs.s3a.impl.RequestFactoryImpl:copyEncryptionParameters(com.amazonaws.services.s3.model.ObjectMetadata,com.amazonaws.services.s3.model.CopyObjectRequest):[DEBUG] Propagating SSE-KMS settings from source {sourceKMSId}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager:reinitializeQueues(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration):[INFO] Parsed new queues
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute():[DEBUG] Block compaction finished for {TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startCompaction)} ms with {blockEntries.size()} blocks for {key}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder):[TRACE] Fetch SAS token for {} on {}
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain):[ERROR] {e.getMessage(), e}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery():[INFO] Not attempting to recover. Recovery disabled. To enable recovery, set MRJobConfig.MR_AM_JOB_RECOVERY_ENABLE
org.apache.hadoop.hdfs.qjournal.client.QuorumCall:waitFor(int,int,int,int,java.lang.String):[WARN] Waited A ms (timeout=B ms) for a response for C...
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Do not start Router RPC metrics
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:main(java.lang.String[]):[ERROR] Error running ApplicationMaster, t
org.apache.hadoop.yarn.nodelabels.store.AbstractFSNodeStore:recoverFromStore():[INFO] Finished write mirror at: ${mirrorPath}
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:getVolumeReports():[WARN] Error compiling report. Continuing.
org.apache.hadoop.yarn.client.RMProxy:createRetryPolicy(org.apache.hadoop.conf.Configuration,boolean):[WARN] YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS is smaller than YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS. Only try connect once.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:parseSummaryLogs(org.apache.hadoop.yarn.server.timeline.TimelineDataManager):[DEBUG] Try to parse summary log for log {} in {}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:evictOldStartTimes(long):[DEBUG] Preparing to delete a batch of {batchSize} old start times
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getEditLogManifest(long):[DEBUG] Getting EditLog
org.apache.hadoop.yarn.server.webproxy.ProxyCA:createCert(boolean,java.lang.String,java.lang.String,java.util.Date,java.util.Date,java.security.PublicKey,java.security.PrivateKey):[INFO] Created Certificate for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Privileged container requested for : [containerId]
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[WARN] Received null queue for application {appId} from home subcluster. Will use default queue name {YarnConfiguration.DEFAULT_QUEUE_NAME} for getting AMRMProxyPolicy
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:shutdown():[INFO] All async disk service threads have been shut down
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:createWorkPlan(org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[WARN] Disk Balancer - Source and destination volumes are same: {volUuid}
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[ERROR] Application ${appId} does not exist
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:getConsumptionForUserOverTime(java.lang.String,long,long):[WARN] Exception while trying to merge periodic and non-periodic user allocations: {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] RegisterAM processing finished in ... ms for application ...
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Application [...].
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:uploadPart(java.io.InputStream,java.lang.String,java.lang.String,int,long):[DEBUG] Current thread: [%d], COS key: [%s], upload id: [%s], part num: [%d], exception: [%s]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:sigKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Interrupted while waiting for processes to disappear
org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node):[DEBUG] NetworkTopology became:\n{}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] blockSize = {store.getHadoopBlockSize()}
org.apache.hadoop.crypto.key.kms.server.KMS:deleteKey(java.lang.String):[DEBUG] Exception in deleteKey., e
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:getSpillFileCB(org.apache.hadoop.fs.Path,java.io.InputStream,org.apache.hadoop.conf.Configuration):[WARN] getSpillFileCB.. Could not find spilled file .. Path: {}
org.apache.hadoop.hdfs.DFSInputStream:read(long,byte[],int,int):[INFO] New instance created
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startRollingUpgrade():[INFO] Audit event logged
org.apache.hadoop.conf.Configuration:parse(java.io.InputStream,java.lang.String,boolean):[DEBUG] parsing input stream + is
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions):[INFO] bpos.toString() + ": scheduling an incremental block report " + "to namenode: " + nnAddr + "."
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.ContextFactory:createContext(java.lang.Class[],java.util.Map):[WARN] Exception caught during context creation
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:getCredentials():[DEBUG] No credentials from {provider}: {e.toString()}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=[recoveredFinalState]
org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread:run():[DEBUG] Handling uplink command
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler$UpdateThread:run():[WARN] Scheduler UpdateThread interrupted. Exiting.
org.apache.hadoop.fs.http.server.HttpFSServer:put(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [{}] allowed snapshot
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Reported NameNode version '{nnVersion}' does not match DataNode version '{dnVersion}' but is within acceptable limits. Note: This is normal during a rolling upgrade.
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.SetSubClusterPolicyConfigurationRequest):[WARN] Missing SetSubClusterPolicyConfiguration Request. Please try again by specifying an policy insertion information.
org.apache.hadoop.hdfs.server.namenode.FSDirectory:normalizePaths(java.util.Collection,java.lang.String):[ERROR] {} ignoring path {} with scheme
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Failed to create <dirToCreate_value> at fallback : <linkedFallbackFs.getUri_value>
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:execute(org.apache.commons.cli.CommandLine):[DEBUG] Processing report command
org.apache.hadoop.yarn.service.component.Component:assignContainerToCompInstance(org.apache.hadoop.yarn.api.records.Container):[INFO] [COMPONENT {}]: Assigned {} to component instance {} and launch on host {}
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat):[DEBUG] Received stat error from Zookeeper. code: + code.toString()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getResourceTypePrefix(org.apache.hadoop.mapreduce.v2.api.records.TaskType):[INFO] TaskType + taskType + does not support custom resource types - this support can be added in + getClass().getSimpleName()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:verifyRMRegistrationResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse):[ERROR] NodeLabels sent from NM while registration were rejected by RM. + ((errorMsgFromRM == null) ? "Seems like RM is configured with Centralized Labels." : "And with message " + regNMResponse.getDiagnosticsMessage())
org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler:handle(org.apache.hadoop.yarn.event.Event):[INFO] Size of event-queue is + qSize
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:loadServiceUpgrade(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.lang.String,java.lang.String):[INFO] Loading service definition from {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Removing RMDelegationToken and SequenceNumber
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[DEBUG] {}: Job Summary {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:rememberTargetTransitionsAndStoreState(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent,java.lang.Object,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState):[INFO] Updating application attempt with final state
org.apache.hadoop.yarn.server.timeline.LogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[DEBUG] Parsing for log dir {} on attempt {}
org.apache.hadoop.yarn.service.component.Component$NeedsUpgradeTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [COMPONENT {}]: need upgrade to {}
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path):[ERROR] File not found exception caught
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator):[DEBUG] {}: retrying {}
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:launch():[INFO] Done launching container + masterContainer + for AM + application.getAppAttemptId()
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getOwnerForPath(org.apache.hadoop.fs.Path):[DEBUG] Retrieved '{}' as owner for path - {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Reserved container + container.getContainer().getId() + on node + this + for application attempt + application.getApplicationAttemptId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:parseDiagnosticOutput(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor,java.lang.String):[WARN] Failed to retrieve major/minor number for device
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogType(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,boolean):[ERROR] containerLogNotFound
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Application {app.applicationId} is registered for timeout monitor, type={timeout.getKey()} remaining timeout={remainingTime / 1000} seconds
org.apache.hadoop.fs.cosn.BufferPool:returnBuffer(org.apache.hadoop.fs.cosn.ByteBufferWrapper):[DEBUG] Return the buffer to the buffer pool.
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:setErasureCodingPolicy(java.lang.String,java.lang.String):[DEBUG] Set erasure coding policy {} on {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[ERROR] The application {} does exist but was overwritten
org.apache.hadoop.hdfs.server.federation.router.RouterFsck:fsck():[INFO] Federated FSCK started by current_user from remoteAddress at current_time
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[DEBUG] Processing {} of type {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$NvidiaCommandExecutor:searchBinary():[INFO] Search binary..
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] {} reported usable
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:makeRemoteRequest():[INFO] getResources() for applicationId: ask= ask.size release= release.size newContainers= allocateResponse.getAllocatedContainers().size finishedContainers= numCompletedContainers resourcelimit= availableResources knownNMs= clusterNmCount
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl$FSAction:runWithRetries():[INFO] Maxed out FS retries. Giving up!
org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[DEBUG] Renewing delegation token {}
org.apache.hadoop.hdfs.HAUtilClient:cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection):[DEBUG] Mapped HA service delegation token for logical URI + haUri + to namenode + singleNNAddr
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$StatusUpdateWhenUnHealthyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[DEBUG] Node transitioned to RUNNING state
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Waiting for %d DataNodes to register with the NameNode...
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path):[INFO] Track duration and span executed successfully
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:checkIfAlreadyBootstrapped(java.lang.String):[DEBUG] Matched regex: regex
org.apache.hadoop.mapreduce.JobSubmitter:submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster):[INFO] number of splits: maps
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:executeStage(java.lang.Object):[INFO] {}: Creating Job Attempt directory {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Removing attempt someAttemptId from app: someAppId
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[INFO] The storage policy of [path]: [policy]
org.apache.hadoop.fs.http.server.HttpFSServer:post(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] Truncate [path] to length [newLength]
org.apache.hadoop.mapred.QueueManager:hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation):[INFO] Cannot submit job to parent queue + q.getName()
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:changeProxy(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo):[DEBUG] Changed current proxy from {} to {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[INFO] The application {} was not inserted in the StateStore because it was already present in SubCluster {}
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] There is no reducer, no need to use native output collector
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[WARN] interrupted when wait copies to finish
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:makeUberDecision(long):[INFO] Uberizing job + jobId + ": " + numMapTasks + "m+" + numReduceTasks + "r tasks (" + dataInputLength + " input bytes) will run sequentially on single node.
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy$LocalDatanodeInfo:getDatanodeProxy(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.conf.Configuration,int,boolean):[WARN] encountered exception
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$StatusUpdaterRunnable:run():[INFO] Missed heartbeat
org.apache.hadoop.streaming.StreamJob:parseArgv():[ERROR] <Exception message>
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:dispatch():[INFO] Successfully moved + this
org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter:run():[INFO] Status reporter thread exiting
org.apache.hadoop.hdfs.server.federation.router.Router:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Heartbeat is enabled but there are no namenodes to monitor
org.apache.hadoop.hdfs.server.namenode.Checkpointer:doCheckpoint():[DEBUG] Doing checkpoint. Last applied: {lastApplied}
org.apache.hadoop.yarn.service.ClientAMService:upgrade(org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto):[INFO] Upgrade container {containerId}
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] Native output collector doesn't support customized java comparator
org.apache.hadoop.hdfs.DFSInputStream:pread(long,java.nio.ByteBuffer):[INFO] Fetching block byte range
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:attemptScheduling(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] No container is allocated on node {node}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[DEBUG] saveSecretManagerSection completed
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:getEncryptedStreams(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream):[DEBUG] Server using encryption algorithm + dnConf.getEncryptionAlgorithm()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:render():[INFO] Configuration script appended
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:run():[WARN] DatanodeAdminMonitor caught exception when processing node.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest):[INFO] {}: Saving manifest file to {}
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[WARN] System service launcher thread interrupted
org.apache.hadoop.hdfs.server.datanode.DataXceiver:run():[INFO] Failed to read expected SASL data transfer protection handshake from client at ...
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[DEBUG] Running at: miniKdc.getHost():miniKdc.getHost()
org.apache.hadoop.yarn.applications.distributedshell.Client:setAMResourceCapability(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.Map,java.util.List):[WARN] AM vcore not specified, use {DEFAULT_AM_VCORES} mb as AM vcores
org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater:run():[DEBUG] InterruptedException in block key updater thread
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[DEBUG] Setting entity creation time
org.apache.hadoop.yarn.service.client.ServiceClient:flexComponents(java.lang.String,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Application ID doesn't exist for <serviceName>
org.apache.hadoop.fs.s3a.S3AFileSystem:initMultipartUploads(org.apache.hadoop.conf.Configuration):[DEBUG] Failed to purge multipart uploads against {}, FS may be read only
org.apache.hadoop.mapred.LocalDistributedCacheManager:symlink(java.io.File,java.lang.String,java.lang.String):[INFO] Creating symlink: %s <- %s
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:commitJob(org.apache.hadoop.mapreduce.JobContext):[WARN] Commit failure for job {}
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Cannot locate RPC service address for NN {}, using RPC address {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] AMRMProxy is ignoring event: {}, event.getType()
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:nextTcpPeer():[TRACE] nextTcpPeer: reusing existing peer {}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RecoverTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] Access check complete
org.apache.hadoop.examples.terasort.TeraOutputFormat$TeraRecordWriter:close(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Operation hsync is not supported so far on path with erasure code policy set
org.apache.hadoop.yarn.webapp.WebApp:joinThread():[INFO] interrupted
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint():[INFO] TrashPolicyDefault#deleteCheckpoint for trashRoot: + trashRoot
org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions):[DEBUG] Failed to create raw erasure encoder {rawCoderName}, fallback to next codec if possible
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[INFO] New instance created
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier):[DEBUG] Trying to retrieve password for {applicationAttemptId}
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[INFO] rpcClient.invokeConcurrent executed
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:init():[INFO] Additional docker CLI options from plugin to run GPU containers: cliOptions
org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocol:addToClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsRequest):[DEBUG] Adding to cluster node labels in RouterRMAdminService
org.apache.hadoop.hdfs.server.datanode.DataXceiver:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[]):[DEBUG] opWriteBlock: stage={}, clientname={} block={}, newGs={}, bytesRcvd=[{}, {}] targets={}; pipelineSize={}, srcDataNode={}, pinning={}
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:createRMNodeLabelsMappingProvider(org.apache.hadoop.conf.Configuration):[ERROR] Failed to create RMNodeLabelsMappingProvider based on Configuration
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:initializeLogDir(org.apache.hadoop.fs.FileContext,java.lang.String):[WARN] Could not initialize log dir {logDir}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:nodeHeartbeat(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[INFO] Node not found resyncing + remoteNodeStatus.getNodeId()
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo:setState(org.apache.hadoop.hdfs.server.protocol.DatanodeStorage$State):[INFO] Provided storage {} transitioning to state {}
org.apache.hadoop.fs.azurebfs.utils.IdentityHandler:lookupForLocalGroupIdentity(java.lang.String):[INFO] Lookup for local group identity started
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:containerCompleted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,java.lang.String):[INFO] Container completed with partition
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:checkReplaceLabelsOnNode(java.util.Map):[ERROR] Not all labels being replaced contained by known label collections, please check, new labels=[...]
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:getReplanner(java.lang.String):[INFO] Using Replanner: {plannerClassName} for queue: {planQueueName}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:parseAclsWithPrefix(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType,java.util.Map):[WARN] Invalid KEY_OP '{}' for {}, ignoring
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:doCopy(java.util.Set):[DEBUG] LocalFetcher going to fetch
org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean):[INFO] Got {} = '{}' (default '{}'), name, value, defaultValue
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:invokeRefresh(java.util.List):[ERROR] Mount table cache refresher was interrupted.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Error in handling event type + event.getType() + for node + nodeId, t
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:cleanupStagingDirs():[DEBUG] Deleting magic directory Path
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:updateToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long):[DEBUG] Updating token #
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[WARN] Event of type [ + containerEvent.getType() + ] not expected here..
org.apache.hadoop.mapred.pipes.BinaryProtocol:start():[DEBUG] starting downlink
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown in handler for event [RESTART_CONTAINER] for Container
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:start():[DEBUG] JvmMetrics singleton initialized
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:requestNewHdfsDelegationTokenAsProxyUser(java.util.Collection,java.lang.String,boolean):[INFO] RM proxy-user privilege is not enabled. Skip requesting hdfs tokens.
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Can't create a new BR lease for DN {dn.getDatanodeUuid()}, because numPending equals maxPending at {numPending}. Current leases: {allLeases.toString()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[DEBUG] Application attempt {application.getApplicationAttemptId()} released container {container.getId()} on node: {(node == null ? nodeID : node)} with event: {event}
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersAllocated(java.util.List):[INFO] containers.size() + " containers allocated."
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getRandomActiveSubCluster(java.util.Map):[ERROR] No active subclusters available
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppQueue(javax.servlet.http.HttpServletRequest,java.lang.String):[DEBUG] Configuring readable endpoints
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getMaxChunksTolerable(org.apache.hadoop.conf.Configuration):[WARN] DistCpConstants.CONF_LABEL_MAX_CHUNKS_TOLERABLE should be positive. Fall back to default value: DistCpConstants.MAX_CHUNKS_TOLERABLE_DEFAULT
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getDevices():[ERROR] Error during scanning devices
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] To-be-moved container already updated.
org.apache.hadoop.yarn.client.cli.ApplicationCLI:listApplicationAttempts(java.lang.String):[DEBUG] ApplicationAttempt details printed
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:getMemorySize(java.io.File):[WARN] Failed to parse cgroups + memswStat
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] Unable to inspect storage directory
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List):[WARN] Caught exception when adding...
org.apache.hadoop.tools.dynamometer.Client:launchAndMonitorWorkloadDriver(java.util.Properties):[INFO] Workload job completed successfully!
org.apache.hadoop.registry.server.dns.RegistryDNS:op(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord,org.apache.hadoop.registry.server.dns.RegistryDNS$RegistryCommand):[DEBUG] Creating ApplicationServiceRecordProcessor for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:findNextResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Incorrect path for PRIVATE localization.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[]):[WARN] Took {} ms to process {} commands from NN, processCommandsMs, cmds.length
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error removing app: [appId], [e]
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int):[DEBUG] this + : + caller + starting sendCallback for fd + fd
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode):[WARN] DIR* FSDirectory.unprotectedRenameTo: Source {src} and destination {dst} must both be directories
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:getGroups(java.lang.String[]):[INFO] User group mappings retrieved
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.CopyObjectRequest,java.lang.String):[DEBUG] Restricting metadata request to version {}
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask:run():[ERROR] Async data service got error: , t
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Executing command {Uploads.NAME}
org.apache.hadoop.tools.mapred.CopyMapper:handleFailures(java.io.IOException,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context):[ERROR] Failure in copying [path] [additional details based on conditions]
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[DEBUG] Processing event of type {event.getType()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:storeRetryContext():[WARN] Unable to update remainingRetryAttempts in state store for + containerId
org.apache.hadoop.security.KDiag:execute():[WARN] Security is not enabled for the Hadoop cluster
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.Class):[ERROR] Cannot add more than {} connections at the same time
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] NodeManager information
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getHostname():[ERROR] Error getting localhost name. Using 'localhost'...
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:clean():[INFO] Cleaning up history files
org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable):[DEBUG] Log throwable from afterExecute
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionUpgradeExpress(java.lang.String,java.io.File):[INFO] Upgrade in progress. Please wait..
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doLogAggregationOutOfBand():[INFO] Do OutOfBand log aggregation
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:initNetworkResourceHandler(org.apache.hadoop.conf.Configuration):[INFO] Using network-tagging-handler.
org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int):[WARN] Unable to transition local node to standby: {exceptionMessage}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[ERROR] Exception when recovering attemptId, removing it from NMStateStore and move on
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:handleInteraction(org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter$HttpInteraction):[TRACE] Proceeding with interaction since the request doesn't access WebHDFS API
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:shouldRetry(java.io.IOException,int,java.lang.String):[DEBUG] Retrying connect to Remote service:{} Already tried {} time(s); retry policy is {}, delay {}ms.
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean,java.util.EnumMap):[TRACE] storageTypes={}, storageTypes
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[INFO] The configurated fileControllers:
org.apache.hadoop.yarn.api.ApplicationClientProtocol:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest):[INFO] Submitting reservation through RouterClient
org.apache.hadoop.yarn.service.client.ApiServiceClient:getRMWebAddress():[DEBUG] Authorization: Negotiate {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:initAuxService(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord,org.apache.hadoop.conf.Configuration,boolean):[WARN] The Auxiliary Service named [service_name] in the configuration...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: {event.getClass()}
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login():[DEBUG] Hadoop login
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:addOrUpdateReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String,org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction,boolean):[DEBUG] Storing reservation: {reservationIdName} in plan:{planName} at: {reservationPath}
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock):[DEBUG] Got invalid encryption key error in response to OP_BLOCK_CHECKSUM
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[WARN] Interrupted while waiting for current attempt of + appId + to reach + attemptState
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse):[ERROR] output
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:updateContainerStatus(org.apache.hadoop.yarn.api.records.ContainerStatus):[WARN] Unable to process container ports mapping: {e}
org.apache.hadoop.hdfs.server.namenode.CacheManager:startMonitorThread():[INFO] CacheReplicationMonitor started
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadIndexedLogsMeta(org.apache.hadoop.fs.Path,long,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] loadIndexedLogsMeta
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[ERROR] Fencing method + method + misconfigured
org.apache.hadoop.security.token.DtUtilShell$Edit:validate():[ERROR] must pass -alias field with dtutil edit command
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:getCommitterFactory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[INFO] Using OutputCommitter factory class {} from key {}
org.apache.hadoop.fs.s3a.impl.V2Migration:v1DelegationTokenCredentialProvidersUsed():[WARN] The credential provider interface has changed in AWS SDK V2, custom credential providers used in delegation tokens binding classes will need to be updated once S3A is upgraded to SDK V2
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:addReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation,boolean):[ERROR] The specified Reservation with ID ... is not mapped to any user
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to start Container {}, containerId, t
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:proxyLink(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.net.URI,javax.servlet.http.Cookie,java.lang.String,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] local InetAddress for proxy host: {localAddress}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:moveLazyPersistReplicasToFinalized(java.io.File):[WARN] Failed to mkdirs + targetDir
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[INFO] Begin saveSecretManagerSection
org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] Get kerberos info proto: ...
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[INFO] Operation check started
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[INFO] submitApplication appId {appId} try #{i} on SubCluster {subClusterId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:parseDiagnosticOutput(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor,java.lang.String):[WARN] aocl output is: ...
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Finish time set for task attempt
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Removing RMDTMasterKey., e
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing event for {} of type {}, appAttemptID, event.getType()
org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType):[DEBUG] No ACL available for key, denying access for {opType}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection):[DEBUG] Reported block {block} on {dn} size {block.getNumBytes()} replicaState = {reportedState}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handle(org.apache.hadoop.yarn.event.Event):[INFO] Size of event-queue in RMContainerAllocator is + qSize
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[INFO] Loaded token cache.
org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient:requestRole(java.lang.String,java.lang.String,java.lang.String,long,java.util.concurrent.TimeUnit):[DEBUG] Requesting role {} with duration {}; policy = {}, roleARN, duration, policy
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease,boolean):[WARN] Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob
org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int):[WARN] A failover has occurred since the start of call # + callId + + proxyInfo.getString(method.getName())
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getAncestor(org.apache.hadoop.fs.Path):[DEBUG] Found ancestor {}, for path: {}, ancestor.toString(), f.toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] List of warnings:
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:getNextBlockToScan():[INFO] {}: finished scanning block pool {}
org.apache.hadoop.crypto.key.kms.server.KMS:reencryptEncryptedKeys(java.lang.String,java.util.List):[INFO] reencryptEncryptedKeys {} keys for key {} took
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:createDelegationToken(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken,javax.servlet.http.HttpServletRequest,org.apache.hadoop.security.UserGroupInformation):[INFO] Create delegation token request failed
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[WARN] Proxy user '{}' from application tag does not have access to queue '{}'. The placement is done for user '{}'
org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable):[ERROR] Failed to setup deferred error response. ThreadName= + Thread.currentThread().getName() + , Call= + this
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getEditLogManifest(long):[DEBUG] Checking NN startup
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest):[WARN] Missing SetSubClusterPolicyConfiguration Request. Please try again by specifying an policy insertion information.
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:removeVeryOldStoppedContainersFromCache():[ERROR] Unable to remove container ...
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue):[INFO] current name + name + not equal to + newState.getName()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:handleJobCommit(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobCommitEvent):[WARN] Exception in committer.isCommitJobRepeatable()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:get(java.lang.Class):[ERROR] Cannot get children for "{}": {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[INFO] Aggregator executed
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long):[INFO] Update RMDT with sequence number
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState):[DEBUG] convertToByteBufferState is invoked, not efficiently. Please use direct ByteBuffer inputs/outputs
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:getNextBlockToScan():[DEBUG] Failed to get access time of block {}
org.apache.hadoop.hdfs.tools.DFSHAAdmin:transitionToObserver(org.apache.commons.cli.CommandLine):[ERROR] transitionToObserver: incorrect number of arguments
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:parseListFilesResponse(java.io.InputStream):[ERROR] Unable to deserialize list results
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container is localizing
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String):[ERROR] {t.toString()}
org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils:getIndexInfo(java.lang.String):[WARN] Unable to parse submit time from job history file [jhFileName] : [e]
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose():[DEBUG] Closing {}
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[WARN] Ignoring service {} for the user {} as it is already present, filename = {}
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:sendReadResult(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status):[INFO] Could not send read status ( + statusCode + ) to datanode + peer.getRemoteAddressString() + : + e.getMessage()
org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run():[DEBUG] <name> sending #<id> <rpcRequest>
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall):[DEBUG] add + call
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMDTSecretManagerKeys(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loaded RM delegation key from keyId={}, expirationDate={}
org.apache.hadoop.security.token.DtUtilShell$Edit:validate():[ERROR] must pass -service field with dtutil edit command
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendLaunchEvent():[INFO] Recovering previously launched container
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=LISTSTATUS target=path
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:getLogAggPolicyInstance(org.apache.hadoop.conf.Configuration):[INFO] this.appId specifies ContainerLogAggregationPolicy of policyClass
org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.fs.StorageType[][],java.lang.String[][]):[WARN] Failed to transfer block {}
org.apache.hadoop.util.JvmPauseMonitor$Monitor:run():[WARN] formatMessage(extraSleepTime, gcTimesAfterSleep, gcTimesBeforeSleep)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getSnapshotDiffReport(java.lang.String,java.lang.String,java.lang.String):[INFO] Snapshot diff report generated
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:activateNextMasterKey():[INFO] Activating next master key with id: ...
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleLaunchForLaunchType(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext,org.apache.hadoop.yarn.api.ApplicationConstants$ContainerLaunchType):[INFO] PrivilegedOperation type unsupported in launch: {op.getOperationType()}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet,boolean):[INFO] Audit log creation failed
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:scheduleContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Enqueue Container
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {COMMITTER_NAME_FILE}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$UserLogDir:scanIfNeeded(org.apache.hadoop.fs.FileStatus):[ERROR] Error while trying to scan the directory + p
org.apache.hadoop.yarn.applications.distributedshell.Client:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Got application report from ASM for, appId=?, clientToAMToken=?, appDiagnostics=?, appMasterHost=?, appQueue=?, appMasterRpcPort=?, appStartTime=?, yarnAppState=?, distributedFinalState=?, appTrackingUrl=?, appUser=?
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:checkVersion():[ERROR] Incompatible version for timeline state store: expecting version {getCurrentVersion()}, but loading version {loadedVersion}
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:launch(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent):[INFO] Launching {taskAttemptID}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:addConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint,boolean):[INFO] Cannot add constraint to application {}, as it has not been registered yet.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float):[INFO] completedMapPercent
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startActiveServices():[INFO] Starting services required for active state
org.apache.hadoop.yarn.service.monitor.probe.PortProbe:ping(org.apache.hadoop.yarn.service.component.instance.ComponentInstance):[DEBUG] error
org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] getName() + ": initing services, size=" + services.size()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext):[WARN] {}: rename failures were recovered from. Number of recoveries: {}, committerConfig.getName(), recoveries
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Cannot rename the root of a filesystem
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Num retry attempts [this.retryAttempts]
org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName):[DEBUG] Unregistering [Dynamic Value: mbeanName]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptSucceededTransition:transition(java.lang.Object,java.lang.Object):[INFO] Issuing kill to other attempt
org.apache.hadoop.yarn.service.provider.ProviderUtils:resolveHadoopXmlTemplateAndSaveOnHdfs(org.apache.hadoop.fs.FileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.ConfigFile,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.service.ServiceContext):[INFO] Failed to load config file: configFile, e
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadAliasMap(java.net.URL,java.io.File,boolean):[INFO] Downloaded file + aliasMap.getName() + size + aliasMap.length() + bytes.
org.apache.hadoop.hdfs.server.datanode.DataNode:requestShortCircuitFdsForRead(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,int):[DEBUG] requestShortCircuitFdsForRead failed
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getPoliciesConfigurations(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPoliciesConfigurationsRequest):[ERROR] Cannot get policies: {e.getMessage()}
org.apache.hadoop.minikdc.MiniKdc:delete(java.io.File):[WARN] WARNING: cannot delete directory + f.getAbsolutePath()
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] File rename has taken place: recovery {}
org.apache.hadoop.mapred.gridmix.StressJobFactory:checkLoadAndGetSlotsToBackfill():[DEBUG] System.currentTimeMillis() + " [MAP-LOAD] Overloaded is " + Boolean.TRUE.toString() + " MapSlotsBackfill is " + loadStatus.getMapLoad()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:registerWithRM():[INFO] Registering with RM using containers :{containerReports}
org.apache.hadoop.fs.s3a.S3AFileSystem:uploadPart(com.amazonaws.services.s3.model.UploadPartRequest,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[INFO] Incrementing put start statistics
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getCallerUgi(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Error getting UGI
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials):[DEBUG] Using delegation token {} from service:{}, dToken, service
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:addNoChecksumAnchor():[TRACE] {}: {} no-checksum anchor to slot {}
org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider:isTokenAboutToExpire():[DEBUG] AADToken: token expiring: {token.getExpiry().toString()} : Five-minute window: {new Date(approximatelyNow).toString()}
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:createIgnoredSnapshotException(long):[DEBUG] No snapshot name found for inode {}
org.apache.hadoop.mapreduce.lib.partition.InputSampler:run(java.lang.String[]):[INFO] Partition file written
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:rescanPostponedMisreplicatedBlocks():[DEBUG] BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block {} no longer found in block map.
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue():[ERROR] Got sink exception and over retry limit, suppressing further error messages
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:verifyECWithTopology(int,int,int,int,java.lang.String):[DEBUG] %d racks are required for the erasure coding policies: %s. The number of racks is only %d.
org.apache.hadoop.mapred.TaskAttemptListenerImpl:done(org.apache.hadoop.mapred.TaskAttemptID):[INFO] Done acknowledgment from + taskAttemptID.toString()
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler:killTask(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskRequest):[INFO] Kill task ${taskId} received from ${callerUGI} at ${Server.getRemoteAddress()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest):[DEBUG] Container pre-start checks completed
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:deleteAsUser(org.apache.hadoop.fs.Path):[WARN] Failed to delete [Path placeholder], e
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[INFO] Nodes checked and resources updated
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerKilled(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] storeContainerKilled: containerId={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:findNodeToUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource):[ERROR] node to unreserve doesn't exist, nodeid: + idToUnreserve
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[ERROR] Failed to move aside pre-upgrade storage in image directory + sd.getRoot()
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int):[DEBUG] Proceeding with shuffle since usedMemory ([usedMemory]) is lesser than memoryLimit ([memoryLimit]). CommitMemory is ([commitMemory])
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:loadAndAbort(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,org.apache.hadoop.fs.FileStatus,boolean,boolean):[DEBUG] Aborting %s
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:parseMtab(java.lang.String):[WARN] Error while reading
org.apache.hadoop.hdfs.server.namenode.FSImage:waitForThreads(java.util.List):[ERROR] Caught interrupted exception while waiting for thread {thread.getName()} to finish. Retrying join
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:getStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage):[WARN] Reserved storage {} reported as non-provided from {}
org.apache.hadoop.hdfs.protocol.ClientProtocol:refreshNodes():[INFO] RouterClientProtocol refresh invoked
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuNodeResourceUpdateHandler:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[INFO] Initializing configured GPU resources for the NodeManager.
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Creating a new file: [{}] in COS.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:pullImageFromRemote(java.lang.String,java.lang.String):[DEBUG] now pulling docker image. image name: {imageName}, container: {containerIdStr}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:validateAppTimeoutRequest(javax.servlet.http.HttpServletRequest,java.lang.String):[DEBUG] RMAuditLogger log failure due to static user
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishContainerLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent,java.lang.String):[ERROR] Seems like client has been removed before the event could be published for ${container.getContainerId()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:pullImageFromRemote(java.lang.String,java.lang.String):[DEBUG] pull docker image done with {pullImageTimeMs}ms specnt. image name: {imageName}, container: {containerIdStr}
org.apache.hadoop.security.authentication.server.AuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[WARN] sample log statement for warning
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[WARN] Error retrieving hostname:
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAllContainersLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[DEBUG] emptyLogDir
org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib():[WARN] Failed to load/initialize native-zlib library
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getFields(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord):[ERROR] Cannot execute getter {methodName} on {record}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[DEBUG] Datanode {} is using BR lease id 0x0 to bypass rate-limiting.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:setCGroupParameters():[DEBUG] Swap monitoring is turned off in the kernel
org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey):[INFO] Thread.currentThread().getName() + ": readAndProcess caught InterruptedException"
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceStart():[DEBUG] Setting up servlets
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeAttributesProvider(org.apache.hadoop.conf.Configuration):[DEBUG] Distributed Node Attributes is enabled with provider class as : ScriptBasedNodeAttributesProvider
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:serviceStop():[WARN] InterruptedException while stopping
org.apache.hadoop.hdfs.server.namenode.FSEditLog:close():[DEBUG] Closing log when already closed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread$PreemptContainersTask:run():[INFO] Killing container ...
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:discardOldEntities(long):[INFO] Discarded 0 entities for timestamp 0 and earlier in 0.0 seconds
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KilledDuringCommitTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Handling job abort event
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[DEBUG] Task succeeded
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber:clearCorruptLazyPersistFiles():[WARN] Removing lazyPersist file [bc.getName()] with no replicas.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:showRequests():[INFO] Scheduling request: {schedulingRequest}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:shutdownExecutor():[ERROR] Disk Balancer : Scheduler did not terminate.
org.apache.hadoop.fs.s3a.S3AUtils:deleteQuietly(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean):[DEBUG] Failed to delete {}, path, e
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:subClusterHeartbeat(org.apache.hadoop.yarn.server.federation.store.records.SubClusterHeartbeatRequest):[ERROR] SubCluster does not exist; cannot heartbeat
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:moveTmpToDone(org.apache.hadoop.fs.Path):[INFO] Moved tmp to done: + tmpPath + to + path
org.apache.hadoop.fs.s3a.impl.RenameOperation:removeSourceObjects(java.util.List):[DEBUG] Initiating delete operation for {} objects
org.apache.hadoop.hdfs.server.balancer.KeyManager:close():[WARN] Exception shutting down access key updater thread
org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class):[DEBUG] name
org.apache.hadoop.mapred.LocalJobRunner$Job:run():[WARN] Error cleaning up [JobID]: [IOException Detail]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Failed to move reservation, node updated or removed, moving cancelled.
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent):[ERROR] Shuffle error + e
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Node is not chosen due to being too busy (load: [nodeLoad] > [maxLoad])
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:populateBlockChecksumBuf(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto):[DEBUG] {MD5Hash.toString()}
org.apache.hadoop.hdfs.server.datanode.DataXceiver:copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[INFO] Copied ... to ...
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] [AdminHelper.prettifyException(e)]
org.apache.hadoop.tools.mapred.CopyCommitter:trackMissing(org.apache.hadoop.conf.Configuration):[INFO] Target listing {sortedTargetListing}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:store(byte[],org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl$Tables):[INFO] Invalid table name provided.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using ResourceCalculatorProcessTree: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi):[DEBUG] LazyWriter: Finish persisting RamDisk block: block pool Id: bpId block id: blockId to block file savedFiles[1] and meta file savedFiles[0] on target volume targetVolume
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadTokensFromBucket(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState,org.apache.hadoop.fs.Path):[WARN] Skipping unexpected file in history server token bucket: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:lookUpAutoDiscoveryBinary(org.apache.hadoop.conf.Configuration):[WARN] Please check the configuration value of NM_GPU_PATH_TO_EXEC. It should point to an DEFAULT_BINARY_NAME binary.
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishApplicationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[DEBUG] {} is not a desired ApplicationEvent which needs to be published by NMTimelinePublisher, event.getType()
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:waitBlocksMapAndNameCacheUpdateFinished():[INFO] Completed update blocks map and name cache, total waiting duration {}ms.
org.apache.hadoop.tools.mapred.CopyCommitter:trackMissing(org.apache.hadoop.conf.Configuration):[INFO] Source listing {sourceSortedListing}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStop():[WARN] The cleaner service was interrupted while shutting down the task.
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getTask(org.apache.hadoop.mapred.JvmContext):[INFO] JVM with ID: + jvmId + is invalid and will be killed.
org.apache.hadoop.yarn.server.webapp.ContainerBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Failed to read the container ${containerid}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Node + node.getNodeName() + offered to parent queue: + getName() + visiting + childQueues.size() + children
org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client):[DEBUG] stopping client from cache: [ClientInstance]
org.apache.hadoop.yarn.service.provider.ProviderFactory:createServiceProviderFactory(org.apache.hadoop.yarn.service.api.records.Artifact):[DEBUG] Loading service provider type {type}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:updateMetricsForGracefulDecommission(org.apache.hadoop.yarn.api.records.NodeState,org.apache.hadoop.yarn.api.records.NodeState):[WARN] Unexpected initial state
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration):[WARN] Edits URI Ignoring duplicates.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:offerService():[WARN] IOException in offerService
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$CancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] {} pending cancellation
org.apache.hadoop.hdfs.DFSClient:getDelegationToken(java.lang.String):[INFO] Cannot get delegation token from [renewer]
org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver:updateSubclusterMapping():[DEBUG] Wait to get the mapping for the first time
org.apache.hadoop.hdfs.server.namenode.FSEditLog:initJournals(java.util.List):[ERROR] No edits directories configured!
org.apache.hadoop.hdfs.DataStreamer:handleBadDatanode():[WARN] Error Recovery for block in pipeline datanodes: datanode index is reason
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List):[DEBUG] path already present: {}
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue():[INFO] Input value counter incremented
org.apache.hadoop.yarn.util.resource.ResourceUtils:addResourcesFileToConf(java.lang.String,org.apache.hadoop.conf.Configuration):[ERROR] Exception trying to read resource types configuration ' + resourceFile + '., ex
org.apache.hadoop.yarn.server.webapp.ContainerBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Bad request: requires container ID
org.apache.hadoop.lib.server.Server:setService(java.lang.Class):[ERROR] Could not set service [{}] programmatically -server shutting down-, {}
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:write(byte[],int,int):[DEBUG] writing more data than block has capacity -triggering upload
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:checkAcls(java.lang.String):[WARN] Couldn't get current user
org.apache.hadoop.hdfs.DFSInputStream:seek(long):[DEBUG] Exception while seek to {} from {} of {} from {}
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue():[INFO] name thread interrupted.
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:handleInteraction(org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter$HttpInteraction):[TRACE] Got request user: {}, remoteIp: {}, query: {}, path: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getDevices():[INFO] Found devices:
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] Remove write {} from the list
org.apache.hadoop.tools.dynamometer.ApplicationMaster:init(java.lang.String[]):[INFO] Application master for app: appId=<ID>, clusterTimestamp=<TIMESTAMP>, attemptId=<ID>
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Real Authentication method: %s
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:processOneManifest(org.apache.hadoop.fs.FileStatus):[WARN] {}: Failed to write manifest for task {}
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] Validating request made by...
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* removeStoredBlock: {} from {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:handleContainerExitWithFailure(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.fs.Path,java.lang.StringBuilder):[ERROR] Failed to get tail of the container's prelaunch error log file
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceStart():[INFO] Node ID assigned is : + this.nodeId
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not update cgroup for container
org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable):[DEBUG] Execution exception when running task in [current thread name]
org.apache.hadoop.hdfs.server.namenode.FSEditLog:startLogSegment(long,boolean,int):[ERROR] Unable to start log segment + segmentTxId
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Start information is missing for application + appId
org.apache.hadoop.ipc.Server$Responder:run():[INFO] Thread.currentThread().getName() + ": starting"
org.apache.hadoop.hdfs.server.namenode.FSDirectory:initUsersToBypassExtProvider(org.apache.hadoop.conf.Configuration):[INFO] Add user {tmp} to the list that will bypass external attribute provider.
org.apache.hadoop.hdfs.server.namenode.CacheManager:addDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.util.EnumSet):[INFO] addDirective of {} successful., info
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[ERROR] Unable to recover task attempt
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:stop():[DEBUG] Storage policy is not enabled, ignoring
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches():[ERROR] Cache update failed for cache {}
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:reinitialize(java.util.Map,java.util.Set):[INFO] {} subcluster active, {} subclusters active and enabled
org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run():[INFO] Property %s is not configurable: old value: %s, new value: %s
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder,java.lang.String):[TRACE] SAS token fetch complete for {operation} on {path}
org.apache.hadoop.yarn.util.resource.Resources:multiplyAndRound(org.apache.hadoop.yarn.api.records.Resource,double,org.apache.hadoop.yarn.util.resource.Resources$RoundingDirection):[WARN] Resource is missing:...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:processConf():[INFO] Initialized the Default Decommission and Maintenance monitor
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$LoggingAuditSpan:activate():[TRACE] [{}] {} Activate {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:allocateFromReservedContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Starting app allocation recording
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:getRestoreDirectory(java.io.File):[INFO] Restoring {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:createAMRMProxyService(org.apache.hadoop.conf.Configuration):[INFO] AMRMProxyService is disabled
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[DEBUG] PmemMappableBlockLoader used for block loading
org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext:acquire(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] taskId + " acquired " + chunkFile.getPath()
org.apache.hadoop.fs.azurebfs.services.AbfsClient:close():[INFO] Executor service shutdown
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.ConversionOptions:handleWarning(java.lang.String,org.slf4j.Logger):[WARN] msg
org.apache.hadoop.util.Progress:set(float):[DEBUG] Illegal progress value found, progress is larger than 1. Progress will be changed to 1
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:render():[DEBUG] Admin button rendered
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Connecting to url {} with token {} as {}
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[DEBUG] Executing task
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleMapContainerRequest(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[INFO] mapResourceRequest: + mapResourceRequest
org.apache.hadoop.hdfs.server.datanode.DataNode:getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[TRACE] getBlockLocalPathInfo for block={} returning null
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String):[INFO] this.prefix + metrics system started (again)
org.apache.hadoop.tools.dynamometer.Client:getAMCommand():[INFO] Completed setting up app master command: [...]
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool():[INFO] Loading edits into backupnode to try to catch up from txid ...
org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable):[DEBUG] Current Caller: {} Priority: {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[DEBUG] scanning file: ...
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:write(org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogKey,org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue):[WARN] Aggregated logs truncated by approximately ...
org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean):[WARN] Cannot get all trash roots
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater$RMDelegatedNodeLabelsUpdaterTimerTask:run():[ERROR] Failed to update node Labels
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processMisReplicatedBlocks(java.util.List):[INFO] Caught InterruptedException while scheduling replication work for mis-replicated blocks
org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean):[WARN] Service is not ready to become active, but forcing: {}
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:close():[ERROR] Thread [THREAD_NAME] failed unexpectedly
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveCacheDirectiveInfo(java.lang.Long,boolean):[DEBUG] Calling logRpcIds
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean):[TRACE] this: registered blockId with slot slotId (isCached=true)
org.apache.hadoop.hdfs.DFSInputStream:close():[DEBUG] DFSInputStream has been closed already
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:run(int):[WARN] Failed to renew lease for [clientsString] for [elapsed/1000] seconds. Aborting ...
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:verifyRestEndPointAvailable():[ERROR] TimelineClient has reached to max retry times : <dynamic_value>, but failed to fetch timeline service address. Please verify Timeline Auxiliary Service is configured in all the NMs
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$IntermediateMemoryToMemoryMerger:merge(java.util.List):[INFO] Initiating Memory-to-Memory merge with + noInMemorySegments + segments of total-size: + mergeOutputSize
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueues(java.lang.String,java.lang.String[]):[DEBUG] CSConf - setQueues: qPrefix={}, queues={}
org.apache.hadoop.crypto.key.kms.server.KMS:handleEncryptedKeyOp(java.lang.String,java.lang.String,java.util.Map):[DEBUG] Decrypting key for {}, the edek Operation is {}.
org.apache.hadoop.tools.DistCpOptions$Builder:setOptionsForSplitLargeFile():[INFO] Enabling preserving blocksize since {DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch()} is passed.
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:put(java.lang.Object,java.lang.Object):[ERROR] GenericObjectMapper cannot write [entity class name] into a byte array. Write aborted!
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStopContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to stop NameNode container ID + containerId
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse):[INFO] output
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] Java runtime version : {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:tailFile(org.apache.hadoop.fs.Path,long,long):[INFO] Cleanup completed
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics:shutdownAndWait():[ERROR] Disk Outlier Detection daemon did not shutdown
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[DEBUG] Recovered UAM in {} from NMSS
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:loadServices(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords,org.apache.hadoop.conf.Configuration,boolean):[INFO] No auxiliary services changes detected
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Resource handler chain enabled = true
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:refreshToken():[TRACE] CustomTokenProvider Access token fetch was successful with retry count {}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:completeMultipartUpload(java.lang.String,java.lang.String,java.util.List):[INFO] Complete the multipart upload. bucket: [{}], COS key: [{}], upload id: [{}].
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] User entry: "{userEntry}"
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:close():[ERROR] Error freeing leases
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder):[INFO] No top limit specified, using default top value %d.
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[ERROR] Application doesn't exist in cache
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:mark(int):[DEBUG] mark at {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] pMemCheck [current={} + asked={} > allowed={}], this.containersAllocation.getPhysicalMemory(), (pMemBytes >> 20), (getContainersMonitor().getPmemAllocatedForContainers() >> 20)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StartContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown from onContainerStarted for Container containerId
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Render job counters
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] onContainerStopped received unknown container ID: [ContainerId]
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] SPNEGO starting for url: {request.getRequestURL()}
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:validatePaths(java.lang.String[]):[WARN] paths[i] + " is not a valid path. Path should be with " + FILE_SCHEME + " scheme or without scheme"
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanUpPreviousJobOutput():[INFO] Previous job temporary files do not exist, no clean up was necessary.
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition:checkJobAfterTaskCompletion(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl):[INFO] Job failed as tasks failed. failedMaps:{job.failedMapTaskCount} failedReduces:{job.failedReduceTaskCount} killedMaps:{job.killedMapTaskCount} killedReduces: {job.killedReduceTaskCount}
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:closeBlock():[DEBUG] block[{}]: skipping re-entrant closeBlock(), index
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler):[INFO] Connection closed by client...
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter():[INFO] {}: no suitable block pools found to scan. Waiting {} ms.
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextOp():[INFO] Fast-forwarding stream...
org.apache.hadoop.hdfs.server.datanode.DataNode:handleAddBlockPoolError(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.AddBlockPoolException):[DEBUG] HandleAddBlockPoolError called with empty exception list
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] Got token: {}.
org.apache.hadoop.mapreduce.JobResourceUploader:uploadResourcesInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path):[WARN] Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntityTypes(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitShm(java.lang.String):[WARN] Failed to send success response back to the client. Shutting down socket for {...}
org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean):[WARN] Thread.currentThread().getName(), call call: output error
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:addZoneIfNecessary(java.lang.Long,java.lang.String,org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto):[DEBUG] Adding zone {} for re-encryption status
org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[]):[DEBUG] GZIP stream written
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:logProxyException(java.lang.Exception,java.lang.String):[DEBUG] Invocation returned standby exception on [{}], proxyInfo, ex
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:convertProtoToDeletionTaskRecoveryInfo(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[INFO] Conversion of proto to deletion task
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocateContainersInternal(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$AllocationParams,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$ContainerIdGenerator,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.Map,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$EnrichedResourceRequest,int):[INFO] Nodes for scheduling has a blacklisted node [rNodeHost].
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:fetchEntityTypes(java.lang.String,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[DEBUG] Querying Collection : {} , with query {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand):[DEBUG] using docker's cgroups options
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:getGpuDeviceInformation():[DEBUG] msg
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceStart():[DEBUG] S3A Delegation support token {} with {}, identifierToString(), tokenBinding.getDescription()
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:getEntityTypes(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[DEBUG] NoOpTimelineReader is configured. Response to all the read requests would be empty
org.apache.hadoop.security.authentication.examples.RequestLoggerFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] xRequest.getResquestInfo().toString()
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError():[INFO] Error Recovery for + block + waiting for responder to exit.
org.apache.hadoop.hdfs.server.namenode.ImageServlet:validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String):[WARN] Received non-NN/SNN/administrator request for image or edits from {request.getUserPrincipal().getName()} at {request.getRemoteHost()}
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[DEBUG] Task complete
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object):[DEBUG] Registered + name
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] Delete path {} - recursive {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot remove {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:updateReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Successfully updated reservation: {} in plan.
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean):[INFO] Deleted trash checkpoint: + dir
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:getConstraints(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Application {} is not registered in the Placement Constraint Manager., appId
org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter:run():[INFO] Exception while running the status reporter thread!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Caught exception while scanning {v}. Will throw later.
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:handleNodeReport(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder,java.lang.String,java.lang.String):[INFO] Reporting volume information for DataNode(s). These DataNode(s) are parsed from '<nodeVal>'.
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:addContainerRequest(org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest):[WARN] ContainerRequest has duplicate nodes: ...
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[INFO] Deleting path : {}
org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long):[INFO] {buffer.toString(StandardCharsets.UTF_8.name())}
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:run():[INFO] Sleeping for {delay} ms
org.apache.hadoop.mapred.MultiFileSplit:getLocations():[DEBUG] Processing paths
org.apache.hadoop.mapred.ClientServiceDelegate:checkAndGetHSProxy(org.apache.hadoop.yarn.api.records.ApplicationReport,org.apache.hadoop.mapreduce.v2.api.records.JobState):[WARN] Job History Server is not configured.
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:startSpill():[INFO] Spilling map output
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:updateAppsRunnability(java.util.List,int):[ERROR] Can't make app runnable that does not already exist in queue as non-runnable: {}. This should never happen.
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:init(org.apache.hadoop.yarn.util.Clock,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,java.util.Collection):[INFO] Initializing Plan Follower Policy: org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:renewDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.RenewDelegationTokenRequest):[DEBUG] Setting next expiration time
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdownDatanode(boolean):[INFO] shutdownDatanode command received (upgrade=...)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] BLOCK* block RECEIVED_BLOCK: block {} is received from {}
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:scanEditLog(java.io.File,long,boolean):[INFO] scanEditLog
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveKilledTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[ERROR] Unexpected event for REDUCE task + event.getType()
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:deleteCgroup(java.lang.String):[WARN] Unable to delete cgroup at: {} , tried to delete for {} ms
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:checkDirs():[WARN] Directory {dir} error, {errorInformation.message}, removing from list of valid directories
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[DEBUG] Job Selected: <JobID>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Assign container precheck for queue + getName() + on node + node.getNodeName() + failed
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineStorageUtils:getTimelineServiceHBaseConf(org.apache.hadoop.conf.Configuration):[INFO] Using hbase configuration at + timelineServiceHBaseConfFilePath
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:getAllowedLocalityLevel(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,int,double,double):[TRACE] SchedulingOpportunities: ..., nodeLocalityThreshold: ..., change allowedLocality from NODE_LOCAL to RACK_LOCAL..., priority: ..., app attempt id: ...
org.apache.hadoop.hdfs.util.ECPolicyLoader:loadECPolicies(java.io.File):[INFO] Loading EC policy file + policyFile
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] --container={} resource={}
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readRemote(long,byte[],int,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] issuing HTTP GET request params position = {} b.length = {} offset = {} length = {}
org.apache.hadoop.crypto.key.kms.server.KMS:createKey(java.util.Map):[TRACE] Entering createKey Method.
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerRecoveredTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent):[WARN] RMContainer received unexpected recover event with container state {state} while recovering.
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Invalid Input Errors raised
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection):[DEBUG] Chosen node {} from first random
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] {} Marked container={} from queue={} to be preemption candidates
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long):[INFO] Starting maintenance of {node} {storage} with {storage.numBlocks()} blocks
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[INFO] System.out.println("No EC policy parsed out from " + filePath)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[DEBUG] Outputted {} INodes.
org.apache.hadoop.tools.DistCp:main(java.lang.String[]):[ERROR] Couldn’t complete DistCp operation: , e
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient:getWriter(org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Writer$Options,java.lang.String):[INFO] Loading InMemoryAliasMapWriter for block pool id {}
org.apache.hadoop.fs.azure.PageBlobOutputStream:close():[DEBUG] Caught InterruptedException
org.apache.hadoop.mapred.nativetask.handlers.CombinerHandler:create(org.apache.hadoop.mapred.nativetask.TaskContext):[INFO] NativeTask Combiner is enabled, class = …
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:blockIdCK(java.lang.String):[WARN] Block + blockId + + NONEXISTENT_STATUS
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [FLEX COMPONENT {component.getName()}]: already has {event.getDesired()} instances, ignoring
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] NameNode container completed; marking application as done
org.apache.hadoop.hdfs.server.datanode.DataNode:initDataXceiver():[INFO] Opened streaming server at {}
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceStart():[ERROR] Could not start proxy web server
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[INFO] Creating file
org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession():[WARN] {IOException.toString()}
org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float):[INFO] Got {} = '{}' (default '{}'), name, value, defaultValue
org.apache.hadoop.fs.azurebfs.services.AbfsIoUtils:dumpHeadersToDebugLog(java.lang.String,java.util.Map):[DEBUG] {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:parseGpuDevicesFromUserDefinedValues():[WARN] CPU device is duplicated: device
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean):[DEBUG] Skipping scheduling since node {} is reserved by application {}
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:commitBeforeRead(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long):[DEBUG] No opened stream for fileId: {fileHandle.dumpFileHandle()} commitOffset={commitOffset}. Return success in this case.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStart():[INFO] Starting
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet:skipMisConfiguredVolume(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume):[ERROR] Real capacity is negative. This usually points to some kind of mis-configuration. Capacity : %d Reserved : %d realCap = capacity - reserved = %d. Skipping this volume from all processing. type : %s id :%s
org.apache.hadoop.ipc.Client:stop():[TRACE] Interrupted while waiting on all connections to be closed.
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] Registered webapp guice modules
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] Queues refreshed
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:initialize(org.apache.hadoop.conf.Configuration,long,long,org.apache.hadoop.fs.Path):[INFO] Expecting " + numRecordsRemainingInSplit + " records each with a length of " + recordLength + " bytes in the split with an effective size of " + splitSize + " bytes
org.apache.hadoop.tools.dynamometer.ApplicationMaster:main(java.lang.String[]):[INFO] Application Master failed. exiting
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfDataXceiverParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {property} to {newVal}
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[WARN] Invalid RPC call program + call.getProgram()
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action from standby NN {}: DNA_ACCESSKEYUPDATE
org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData):[WARN] Service interrupted by {interruptData}
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] USERNAME: + userName
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logDeleteSnapshot(java.lang.String,java.lang.String,boolean,long):[DEBUG] Logging RPC IDs
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getBlockPoolId():[DEBUG] Getting the block pool id
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogCleaner:run():[DEBUG] Cleaner starting
org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext():[DEBUG] Successfully retrieved next source value
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[DEBUG] Adding trackID:{} for the file id:{} back to retry queue as some of the blocks are low redundant.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.yarn.service.ServiceMaster:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Service AppAttemptId: applicationAttemptId
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[INFO] YARN containers restricted to ... cores.
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStart():[DEBUG] Initializing node attribute store
org.apache.hadoop.hdfs.web.resources.ExceptionHandler:toResponse(java.lang.Exception):[TRACE] GOT EXCEPITION
org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]):[WARN] Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i]
org.apache.hadoop.mapreduce.v2.util.MRApps:setClassLoader(java.lang.ClassLoader,org.apache.hadoop.conf.Configuration):[INFO] Setting classloader + classLoader + on the configuration and as the thread context classloader
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:checkType(java.lang.String):[WARN] Missing Policy Type. Please try again by specifying a Policy Type.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:getEntryToEvict():[WARN] No eviction candidate. All streams have pending work.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:addConstraintToMap(java.util.Map,java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint,boolean):[INFO] Replacing the constraint associated with tag {} with {}., sourceTag, placementConstraint
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getReader(java.lang.String):[ERROR] Cannot open read stream for record {}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:removeGlobalCleanerPidFile():[INFO] Removed the global cleaner pid file at + pidPath.toString()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] GETATTR for fileHandle: {} client: {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:populateAbfsInputStreamContext(java.util.Optional):[DEBUG] Buffered pread option processed
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handleComponentInstanceRelaunch(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent,boolean,java.lang.String):[ERROR] builder.toString()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp):[INFO] Logger debug executed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:createNewQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue,java.util.List):[ERROR] Can't create queue ' + queueName + ', since + FifoPolicy.NAME + is only for leaf queues.
org.apache.hadoop.examples.terasort.TeraInputFormat:writePartitionFile(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path):[INFO] Computing input splits took (t2 - t1) ms
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:addService(java.lang.String,org.apache.hadoop.yarn.server.api.AuxiliaryService,org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord):[INFO] Adding auxiliary service [serviceRecord.getName()] version [serviceRecord.getVersion()]
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.tools.rumen.ParsedJob:dumpParsedJob():[INFO] ParsedJob details: [counters and other details]
org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long):[DEBUG] The option {} has a negative value {}, replacing with the default {}
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes):[INFO] No opened stream for fileHandle: <dumpedHandle>
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:nullOp(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] MOUNT NULLOP : client: client
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:requestCreated(com.amazonaws.AmazonWebServiceRequest):[INFO] Creating a request outside an audit span
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorApp():[DEBUG] Base URI added
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getDevices():[ERROR] Error during executing external binary
org.apache.hadoop.fs.s3a.S3AFileSystem:innerCreateFile(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.fs.s3a.impl.CreateFileBuilder$CreateFileOptions):[DEBUG] Skipping existence/overwrite checks
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render():[INFO] Sorry, can't do anything without a JobID.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:run():[DEBUG] Uncaching of {} completed. usedBytes = {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when executing command., e
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[DEBUG] Storing token {}
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[]):[ERROR] Invalid URI:
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readInternal(long,byte[],int,int,boolean):[DEBUG] issuing read ahead requestedOffset = ... requested size ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] before vMemCheck + [isEnabled={}, current={} + asked={} > allowed={}], getContainersMonitor().isVmemCheckEnabled(), this.containersAllocation.getVirtualMemory(), (vMemBytes >> 20), (getContainersMonitor().getVmemAllocatedForContainers() >> 20)
org.apache.hadoop.hdfs.server.namenode.sps.DatanodeCacheManager:getLiveDatanodeStorageReport(org.apache.hadoop.hdfs.server.namenode.sps.Context):[DEBUG] elapsedTimeMs > refreshIntervalMs : {} > {}, so refreshing cache
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:sendRequest(byte[],int,int):[DEBUG] Getting output stream failed with expect header enabled, returning back
org.apache.hadoop.yarn.server.sharedcache.SharedCacheUtil:getCacheDepth(org.apache.hadoop.conf.Configuration):[WARN] Specified cache depth was less than or equal to zero. Using default value instead. Default: {}, Specified: {}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:finalizeBlock(long):[INFO] Received block X size Y from address
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:storeEdits(byte[],long,long,int):[INFO] Initializing edits cache starting from txn ID %d
org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType):[DEBUG] Checking user [{ugi.getShortUserName()}] for: {opType.toString()}: {acl.getAclString()}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.files.SuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] No summary directory set in OPT_SUMMARY_REPORT_DIR
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[INFO] Looking for the active RM in ...
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Response temporary redirect configured
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator:getCSAssignmentFromAllocateResult(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[INFO] assignedContainer application attempt={application.getApplicationAttemptId()} container={updatedContainer.getContainerId()} queue={appInfo.getQueueName()} clusterResource={clusterResource} type={assignment.getType()} requestedPartition={updatedContainer.getNodeLabelExpression()}
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:namenodeHeartbeat(org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatRequest):[INFO] Inserting new NN registration: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Not cleaning up tc rules. classId unknown for container: + containerId.toString()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path):[WARN] Cannot get one of the children's( + path + ) target path( + link.getTargetFileSystem().getUri() + ) file status.
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:reserve(org.apache.hadoop.mapreduce.TaskAttemptID,long,int):[DEBUG] Stalling shuffle since usedMemory ([usedMemory]) is greater than memoryLimit ([memoryLimit]). CommitMemory is ([commitMemory])
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Initializing the ViewFileSystemOverloadScheme with the uri: ...
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:serviceStop():[INFO] Stopping Document Timeline Store reader...
org.apache.hadoop.fs.cosn.BufferPool:createDir(java.lang.String):[DEBUG] buffer dir: {} already exists.
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int):[INFO] Waiting in main loop.
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[INFO] Starting to load key cache.
org.apache.hadoop.tools.HadoopArchiveLogs:generateScript(java.io.File):[INFO] Generating script at: + localScript.getAbsolutePath()</item>
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Instance directory is {}
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,org.slf4j.Logger):[WARN] Couldn't get current user
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[DEBUG] Removing ZKDTSMDelegationToken_ [ident.getSequenceNumber()]
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:buildAmazonS3EncryptionClient(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters):[DEBUG] KMS region used: {cryptoConfigurationV2.getAwsKmsRegion()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleCacheCleanup():[INFO] stats.toString()
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:genImpl(java.lang.Class,java.lang.String,int):[INFO] Generating class {}<T>
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:renewTokenForAppCollector(org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector):[INFO] Delegation token not available for renewal for app + appCollector.getTimelineEntityContext().getAppId()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:queryRollingUpgrade():[INFO] Auditing event: queryRollingUpgrade
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Updating ClusterNode [{rmNode.getNodeID()}] with queue wait time [{estimatedQueueWaitTime}] and wait queue length [{waitQueueLength}]
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:rescanPostponedMisreplicatedBlocks():[INFO] Rescan of postponedMisreplicatedBlocks completed in {} msecs. {} blocks are left. {} blocks were removed.
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:start():[INFO] Sink + name + started
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getLegacyBlockReaderLocal():[WARN] {}: error creating legacy BlockReaderLocal. Disabling legacy local reads.
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:resolve(org.apache.hadoop.mapred.TaskCompletionEvent):[INFO] Ignoring obsolete output of <eventStatus> map-task: '<eventTaskId>'
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[INFO] Log output complete
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:checkDiskScheduler():[WARN] Device <partition> does not use the CFQ scheduler; disk isolation using CGroups will not work on this partition.
org.apache.hadoop.mapred.LocalJobRunner$Job:createReduceExecutor():[DEBUG] Max local threads: maxReduceThreads
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Removing reservation allocation. + reservationEvent.getReservationIdName()
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:run():[WARN] IOException caught when re-encrypting zone {}
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:finalizeLogSegment(long,long):[INFO] Finalizing edits file inprogressFile -> dstFile
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:firstStep():[WARN] Unable to place reservation: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:tailFile(org.apache.hadoop.fs.Path,long,long):[INFO] File opened successfully
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:close():[TRACE] closed {}{} this suffix
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:main(java.lang.String[]):[DEBUG] SSL Configuration read
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Enabling OAuth2 in WebHDFS
org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider:close():[DEBUG] Autocloseables closed
org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String):[DEBUG] Loaded IP list of size = + lines.size() + from file = + fileName
org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeDescriptorsScriptRunner:run():[WARN] Execution of Node Labels script failed, Caught exception : + e.getMessage(), e
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:close():[DEBUG] Shutting down connection pool \"connectionPoolIdPlaceholder\" used timeSinceLastActivePlaceholder seconds ago
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:writeAuditLog(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] App failed with unknown state
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String):[INFO] Replacing block
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage):[DEBUG] Dest file: + f
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:serviceStart():[ERROR] Error while starting the Secret Manager threads
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:ahsRedirectPath(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp):[DEBUG] Error parsing {} as an ApplicationId
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getDestinationForPath(java.lang.String):[ERROR] Cannot get main namespace for path {} with order {}, path, order
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:methodAction(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet$HTTP):[INFO] {} is accessing unchecked {} which is the app master GUI of {} owned by {}, remoteUser, toFetch, appId, runningUser
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Rolling back storage directory {}. target LV = {}; target CTime = {}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:destroy():[INFO] {CompInstanceId} Flexed down by user, destroying.
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint():[WARN] Unexpected item in trash: + dir + . Ignoring.
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:createLoggers(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory,java.lang.String):[WARN] Quorum journal URI ' + uri + ' has an even number of Journal Nodes specified. This is not recommended!
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renewDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Renew delegation token operation started
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean):[WARN] Gridmix will not emulate Distributed Cache load because the input trace source is a stream instead of file.
org.apache.hadoop.hdfs.server.datanode.DataNode:getDiskBalancerSetting(java.lang.String):[ERROR] Disk Balancer - Unknown key in get balancer setting. Key: {}
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector:selectToken(java.net.URI,java.util.Collection,org.apache.hadoop.conf.Configuration):[INFO] Attempting to create address for host without nnServiceName
org.apache.hadoop.yarn.client.api.AppAdminClient:actionStop(java.lang.String):[INFO] Action stop called on service client
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Reducer output key fields set
org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File):[WARN] Could not parse line '<String line>'. Lines should be of the form '[uid|gid] [remote id] [local id]'. Blank lines and everything following a '#' on a line will be ignored.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:satisfyStoragePolicy(java.lang.String,boolean):[INFO] Audit success: satisfyStoragePolicy
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:setNextDirectoryInputStream():[INFO] This file, [inputDirectoryCursor + 1]/[inputDirectoryFiles.length], starts with line [lineNumber].
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Instantiating Proxy at {}: {}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:loadAndCommit(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,org.apache.hadoop.fs.FileStatus):[DEBUG] Loading and committing files in pendingset %s
org.apache.hadoop.security.authentication.client.AuthenticatedURL:extractToken(java.net.HttpURLConnection,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[TRACE] Setting token value to null ({token}), resp={respCode}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:checkPauseForTesting():[INFO] Sleeping in the re-encryption updater for unit test.
org.apache.hadoop.yarn.nodelabels.store.AbstractFSNodeStore:recoverFromStore():[INFO] Finished create editlog file at: ${editLogPath}
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:initializeAuthHandler(java.lang.String,java.util.Properties):[INFO] Successfully initialized Authentication handler of type + authHandlerClassName
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:putAuxiliaryServices(javax.servlet.http.HttpServletRequest,org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords):[ERROR] Fail to reload auxiliary services, reason: {e}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:startReconfiguration():[INFO] Reconfiguration task started
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureJobJar(org.apache.hadoop.conf.Configuration,java.util.Map):[INFO] The job-jar file on the remote FS is ...
org.apache.hadoop.mapred.gridmix.ClusterSummarizer:update(org.apache.hadoop.mapred.gridmix.Statistics$ClusterStats):[INFO] Error in processing cluster status at <timestamp>
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object):[TRACE] Failed to register MBean \ + name + \, iaee
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:reset():[DEBUG] reset
org.apache.hadoop.yarn.service.ClientAMService:stop():[ERROR] Interrupted while stopping
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable):[INFO] got stale replica ... Removing this replica from the replicaInfoMap and retrying.
org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding:createDelegationToken(java.util.Optional,org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[DEBUG] Created token {} with token identifier {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:receivedNewWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider):[DEBUG] Repeated write request which is already served: xid={}, resend response.
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:commit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit,java.lang.String):[DEBUG] Committing single commit {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerStartFailedEvent(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[ERROR] Container end event could not be published for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:scriptBasedInit(java.util.function.Function,java.lang.String[]):[ERROR] Script not found in scriptPaths
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:populateNMTokens(java.util.List):[DEBUG] Received new token for : {nodeId}
org.apache.hadoop.yarn.service.utils.ConfigHelper:loadFromResource(java.lang.String):[DEBUG] loaded resources from {}
org.apache.hadoop.fs.s3a.S3AFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] {}
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Creating split : ..., bytes in split: ...
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[INFO] Instance created
org.apache.hadoop.fs.azure.NativeAzureFileSystem:open(org.apache.hadoop.fs.Path,int,java.util.Optional):[DEBUG] Opening file: {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:initializePriorityDigraph():[DEBUG] Initializing priority preemption directed graph:
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:commit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit,java.lang.String):[DEBUG] Successful commit of file length {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String):[DEBUG] Stacktrace: , e
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeBlocksAndUpdateSafemodeTotal(org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo):[DEBUG] Adjusting safe-mode totals for deletion. decreasing safeBlocks by {}, totalBlocks by {}
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg$AppLevelAggregator:aggregate():[WARN] App-level collector is not ready, skip aggregation.
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[INFO] BR lease 0x{} is not valid for unknown datanode {}
org.apache.hadoop.hdfs.DFSInputStream:tryReadZeroCopy(int,java.util.EnumSet):[DEBUG] Unable to perform a zero-copy read from offset {curPos} of {src}; BlockReader#getClientMmap returned null.
org.apache.hadoop.yarn.service.ServiceManager:handle(org.apache.hadoop.yarn.event.Event):[ERROR] [SERVICE]: Invalid event {1} at {2}.
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using exact match for 'host' and READ_ONLY
org.apache.hadoop.hdfs.DFSInputStream:refetchLocations(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[WARN] Could not obtain block: {blockInfo}{errMsg}. Throwing a BlockMissingException
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:listCorruptFileBlocks(java.lang.String,java.lang.String[]):[DEBUG] there are no corrupt file blocks.
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[DEBUG] Skipping malformed line in machine list: line
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkRead(java.lang.String):[DEBUG] Reading {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:sortLocatedBlocks(java.lang.String,java.util.List):[DEBUG] Sorting located striped block
org.apache.hadoop.hdfs.server.namenode.NNStorage:reportErrorsOnDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] Unable to unlock bad storage directory: {}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[ERROR] Unexpected Event.. [ + containerEvent.getType() + ]
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] Native-Task doesn't support sort class
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isStickyBitCheckViolated(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata,boolean):[DEBUG] current user violates sticky bit check
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:setTimelineDelegationToken(org.apache.hadoop.yarn.api.records.Token,java.lang.String):[WARN] Timeline token to be updated should be of kind TimelineDelegationTokenIdentifier.KIND_NAME
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader:nextJob():[ERROR] Its submit time is + result.getSubmitTime() + ,but the previous one was + returnedLatestSubmitTime
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:printBlockDeletionTime():[INFO] The block deletion will start around {}
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:serviceStop():[WARN] Abnormal shutdown of UAMPoolManager, still {} UAMs in map
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics:detectAndUpdateDiskOutliers(java.util.Map,java.util.Map,java.util.Map):[DEBUG] Updated disk outliers.
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:deleteRenamePendingFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[WARN] rename pending file {redoFile} is already deleted
org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]):[ERROR] -format must be ' + DtFileOperations.FORMAT_JAVA + ' or ' + DtFileOperations.FORMAT_PB + ' not ' + format + '
org.apache.hadoop.registry.server.services.RegistryAdminService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Registry System ACLs:, RegistrySecurity.aclsToString(registrySecurity.getSystemACLs())
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:plan(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode):[INFO] Starting plan for Node : {}:{}, node.getDataNodeName(), node.getDataNodePort()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long):[DEBUG] Adding event to entity
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logDisableErasureCodingPolicy(java.lang.String,boolean):[DEBUG] logEdit operation
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream:abort():[ERROR] Unable to abort stream {stream}
org.apache.hadoop.net.TableMapping$RawTableMapping:load():[WARN] NET_TOPOLOGY_TABLE_MAPPING_FILE_KEY not configured.
org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib():[INFO] Successfully loaded & initialized native-zlib library
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:cleanupContainerFiles(org.apache.hadoop.fs.Path):[WARN] {} exec failed to cleanup
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Initialized App Name queue mappings, override: {overrideWithQueueMappings}
org.apache.hadoop.hdfs.server.datanode.DataStorage:doUpgradePreFederation(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Upgrading storage directory {}. old LV = {}; old CTime = {}. new LV = {}; new CTime = {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop():[WARN] prefix + metrics system not yet started!, new MetricsException(Illegal stop)
org.apache.hadoop.yarn.service.component.Component:areDependenciesReady():[INFO] [COMPONENT {getName()}]: Dependency {dependency} not satisfied, only {dependentComponent.getNumReadyInstances()} of {dependentComponent.getNumDesiredInstances()} instances are ready or the dependent component has not completed
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainerLogFile(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean):[INFO] getLogFile called
org.apache.hadoop.mapred.gridmix.Gridmix:startThreads(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver):[INFO] Submission policy is ...
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:refreshAdminAcls():[INFO] Successfully logged refreshAdminAcls action
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector:preemptFromLeastStarvedApp(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map):[DEBUG] totalPreemptedResourceAllowed for preemption at this round is :{}
org.apache.hadoop.fs.s3a.S3AFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter):[DEBUG] listLocatedStatus({}, {})
org.apache.hadoop.crypto.key.kms.server.KMSACLs:setKeyACLs(org.apache.hadoop.conf.Configuration):[INFO] KEY_NAME '{}' KEY_OP '{}' ACL '{}'
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadDriver:getJobForSubmission(org.apache.hadoop.conf.Configuration,java.lang.String,long,java.lang.Class):[INFO] The workload will start at + startTimestampMs + ms ( + startTimeString + )
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:throttle():[DEBUG] Throttling re-encryption, sleeping for {} ms
org.apache.hadoop.tools.HadoopArchiveLogsRunner:main(java.lang.String[]):[DEBUG] Exception
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:analyzeMetricsAndUpdateSleepDuration(org.apache.hadoop.fs.azurebfs.services.AbfsOperationMetrics,int):[DEBUG] %5.5s, %10d, %10d, %10d, %10d, %6.2f, %5d, %5d, %5d
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForRead(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Fallback to getPathStatus REST call as provided filestatus + is not of type VersionedFileStatus
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:load():[TRACE] load({}, {}): loaded iterator {}: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:format():[INFO] delete config file [fileStatuses[i].getPath()]
org.apache.hadoop.mapred.TaskLog:getRealTaskLogFileLocation(org.apache.hadoop.mapred.TaskAttemptID,boolean,org.apache.hadoop.mapred.TaskLog$LogName):[ERROR] getTaskLogFileDetail threw an exception {exception}
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:addCounter(org.apache.hadoop.mapreduce.Counter):[WARN] name + "is not a known counter."
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStart():[INFO] Scheduled the in-memory scm store app check task to run every + checkPeriodMin + minutes.
org.apache.hadoop.mapred.gridmix.StressJobFactory:update(java.lang.Object):[ERROR] Couldn't get the new Status, e
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$BaseFinalTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[ERROR] Cannot get this state!! Error!!
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumNamenodes():[ERROR] Cannot retrieve numNamenodes for JMX: {}
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[WARN] Unknown failure while trying to fence via ssh
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:writeStringTableSection():[DEBUG] STRING_TABLE writing header: {header details here}
org.apache.hadoop.hdfs.server.namenode.BackupImage:waitUntilNamespaceFrozen():[INFO] BackupNode namespace frozen.
org.apache.hadoop.yarn.service.utils.ConfigHelper:loadFromResource(java.lang.String):[DEBUG] failed to find {} on the classpath
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkRepeatedWriteRequest(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] Got a repeated request, same range, with a different xid: {} xid in old request: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:appAdminClientCleanUp(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl):[WARN] Could not run type-specific cleanup on application {app.applicationId} of type {app.applicationType}, {exception}
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:setPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.SetSubClusterPolicyConfigurationRequest):[ERROR] Cannot set policy: {exception message}
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$PollTimerTask:run():[INFO] DECOMMISSIONING {} timeout
org.apache.hadoop.examples.pi.Util:runJob(java.lang.String,org.apache.hadoop.mapreduce.Job,org.apache.hadoop.examples.pi.DistSum$Machine,java.lang.String,org.apache.hadoop.examples.pi.Util$Timer):[INFO] Util Timer Starting
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:dispatch():[INFO] Cancel moving + this + as iteration is already cancelled due to + dfs.balancer.max-iteration-time is passed.
org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext():[ERROR] Cleanup invoked due to NoSuchElementException
org.apache.hadoop.yarn.server.federation.store.metrics.FederationStateStoreClientMetrics:failedStateStoreCall():[ERROR] UNKOWN_FAIL_ERROR_MSG, methodName
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean):[DEBUG] Deleting file: {}
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:shutdown():[INFO] Shutdown has been called
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Scanner skips for unknown dir {}
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable):[DEBUG] Exception in channel handler, cause
org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport:translateDeleteException(java.lang.String,com.amazonaws.services.s3.model.MultiObjectDeleteException):[INFO] {item}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotalBlocks():[DEBUG] Failed to get number of blocks
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Found matching rule, subnet: {}, path: {}; returned true
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:releaseContainer(org.apache.hadoop.yarn.api.records.ContainerId,boolean):[DEBUG] Released container [containerId] of capacity [containerResource] on host [rmNodeAddress], which currently has [numContainers] containers, [allocatedResource] used and [unallocatedResource] available, release resources=[true]
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadReservationSystemState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loading plan from znode: {planName}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:removeAll(java.lang.Class):[ERROR] Cannot remove {znode}: {exception message}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:reInitializeContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerLaunchContext,boolean):[DEBUG] {} requested reinit, containerId
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:run():[WARN] Log aggregation did not complete for application
org.apache.hadoop.hdfs.StripeReader:readToBuffer(org.apache.hadoop.hdfs.BlockReader,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.ByteBufferStrategy,org.apache.hadoop.hdfs.protocol.ExtendedBlock):[WARN] Found Checksum error for ...
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:containerBasedPreemptOrKill(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] {selector.getClass().getName()} uses {clock.getTime() - startTime} millisecond to run
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processMisReplicatedBlocks():[DEBUG] BLOCK* processMisReplicatedBlocks: Re-scanned block {}, result is {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppLogInitFailTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] Log Aggregation service failed to initialize, there will be no logs for this application
org.apache.hadoop.fs.s3a.audit.AuditIntegration:createAndInitAuditor(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.s3a.audit.OperationAuditorOptions):[DEBUG] Auditor class is {}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path):[WARN] This committer will abort these uploads in job cleanup
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet):[INFO] Checked operation WRITE
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[INFO] Node update recording finished
org.apache.hadoop.fs.s3a.S3AFileSystem:doBucketProbing():[DEBUG] skipping check for bucket existence
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:dump():[ERROR] Dump data failed: {} OpenFileCtx state: {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getAclStatus(java.lang.String):[DEBUG] Check operation
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Could not rename {taskAttemptPath} to {committedTaskPath}
org.apache.hadoop.mapred.YarnChild:configureTask(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task,org.apache.hadoop.security.Credentials,org.apache.hadoop.security.token.Token):[DEBUG] APPLICATION_ATTEMPT_ID: ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:deleteBlockPool(java.lang.String,boolean):[WARN] bpid + has some block files, cannot delete unless forced
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int):[DEBUG] Block added
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] In all {} UAMs {} running containers including AM recovered for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[INFO] Sent signal x (...) to pid a as user b for container y, result=failed
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[DEBUG] Incrementing AUDIT_REQUEST_EXECUTION counter
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean):[ERROR] No shared edits directory configured for namespace + nsId + namenode + namenodeId
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:calculateRollingMonitorInterval(org.apache.hadoop.conf.Configuration):[INFO] Log aggregation debug mode enabled. Skipped checking minimum limit.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:tryCreatingHistoryDirs(boolean):[INFO] Waiting for FileSystem at {doneDirPrefixPath.toUri().getAuthority()} to be out of safe mode
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized Yarn-registry with Filesystem
org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute():[INFO] keyName has not been created.
org.apache.hadoop.oncrpc.RpcProgram:unregister(int,int):[INFO] The bound port is X, different with configured port Y
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] File/Directory does not exist: [path]
org.apache.hadoop.hdfs.server.federation.router.FederationUtil:getJmx(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.web.URLConnectionFactory,java.lang.String):[ERROR] Cannot read JMX bean {} from server {}
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] User [remoteUser] is not authorized to view the logs for [logEntity]
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:cacheAllocatedContainers(java.util.List,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[DEBUG] Adding container {}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$RecommissionNodeTransition:transition(java.lang.Object,java.lang.Object):[INFO] Node {rmNode.nodeId} in DECOMMISSIONING is recommissioned back to RUNNING.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:getMajorAndMinorNumber(java.lang.String):[WARN] Command output: {shexec.getOutput()}, exit code: {shexec.getExitCode()}
org.apache.hadoop.hdfs.server.federation.router.Quota:aggregateQuota(java.lang.String,java.util.Map):[DEBUG] Get quota usage for path: nsId: {}, dest: {}, nsCount: {}, ssCount: {}, typeCount: {}.
org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int):[WARN] IPC_FCQ_DECAYSCHEDULER_THRESHOLDS_KEY is deprecated. Please use IPC_DECAYSCHEDULER_THRESHOLDS_KEY
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(org.apache.hadoop.conf.Configuration,java.lang.String[]):[DEBUG] Executing command {subCommand}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[TRACE] Sending receipt verification byte for slot {slot}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:updateAppsRunnability(java.util.List,int):[ERROR] Waiting app {} expected to be in usersNonRunnableApps, but was not. This should never happen.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner:run():[ERROR] Error scanning active files
org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Found multiple CostProviders; using: {providers.get(0).getClass()}
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue):[ERROR] Number of children for queue + newState.getName() + in newState is + newChildrenSize + which is not equal to + childrenSize + in the current state.
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceStop():[ERROR] Couldn't delete data file for leveldb timeline store
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:runDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when writing command to temp file, command={dockerVolumeCommand}
org.apache.hadoop.hdfs.server.namenode.NNStorage:processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int):[WARN] Clusterid mismatch - current clusterid: {}, Ignoring given clusterid: {}
org.apache.hadoop.fs.s3a.S3ListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Prefix: {}
org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite):[DEBUG] No crypto codec classes with cipher suite configured.
org.apache.hadoop.hdfs.tools.DFSck:listCorruptFileBlocks(java.lang.String,java.lang.String):[INFO] The filesystem under path 'dir' has no CORRUPT files
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[ERROR] FATAL, State store fenced even though the resource manager is not configured for high availability. Shutting down this resource manager to protect the integrity of the state store.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[INFO] Erasure coding policy unset successfully
org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI):[DEBUG] initialized local file as 'file'.
org.apache.hadoop.fs.azure.BlockBlobAppendStream$WriteRequest:run():[DEBUG] Encountered exception during execution of command for Blob : + {} Exception : {}
org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String):[INFO] changing property {property} to {newVal}
org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int):[INFO] f + " " + srcNode + "->" + dstNode + ": Failed to repair"
org.apache.hadoop.yarn.service.client.ServiceClient:updateLifetime(java.lang.String,long):[INFO] Successfully updated lifetime for an service: serviceName = {serviceName}, appId = {appId}. New expiry time in ISO8601 format is {newTimeout}
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:doUnregistration():[INFO] Setting job diagnostics to {diagnostics}
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] DistCpMapper::map(): Received {sourcePath}, {relPath}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeApplicationAttemptInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Removing info for attempt: ${appAttemptId} at: ${nodeRemovePath}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error storing appAttempt:
org.apache.hadoop.mapred.IndexCache:removeMap(java.lang.String):[INFO] Map ID + mapId + not found in cache
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:getComputedResourceLimitForActiveUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] userLimit is fetched. userLimit={}, userSpecificUserLimit={}, schedulingMode={}, partition={}
org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget):[DEBUG] Transitioned to standby successfully
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] Got host: {host} path: {path} →
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppLogInitDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] failed to update application state in state store
org.apache.hadoop.mapreduce.lib.db.FloatSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[WARN] imprecise representation of floating-point values in Java, this
org.apache.hadoop.registry.server.dns.RegistryDNS:updateDNSServer(org.apache.hadoop.conf.Configuration):[INFO] DNS servers: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:updateLeafQueueState():[INFO] managedParentQueue.getQueuePath() + " : Removed partition " + partition + " from leaf queue state"
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[WARN] {containerId} exception trying to reap container. Ignoring. {ioe}
org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long):[WARN] Unknown method {methodName} called on {connectionProtocolName} protocol.
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Initializing SSL Context to channel mode Default_JSSE
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId,boolean):[INFO] Flushing subClusters from cache and rehydrating from store, most likely on account of RM failover.
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Error storing info for RMDTMasterKey with keyID: {delegationKey.getKeyId()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Interrupted while waiting to reload alloc configuration
org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler:handle(org.apache.hadoop.yarn.event.Event):[WARN] Very low remaining capacity in the event-queue: + remCapacity
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:addFile(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String,boolean,java.lang.String,java.lang.String):[INFO] DIR* addFile: failed to add [FileName]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Assigned container in queue:{} container:{}, getName(), assigned
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] App ended with state: ... and status: ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Removing RMDTMasterKey.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: SPECIFIED_NOT_FIRST
org.apache.hadoop.hdfs.server.balancer.Balancer:getLong(org.apache.hadoop.conf.Configuration,java.lang.String,long):[INFO] key = v (default=defaultValue)
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[INFO] Successfully authenticated to ZooKeeper using SASL.
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[WARN] {} pending uploads were found -aborting
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:getClientMmap(java.util.EnumSet):[TRACE] can't get an mmap for {} of {} since SKIP_CHECKSUMS was not given, we aren't skipping checksums, and the block is not mlocked.
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Balance failed, error code: [retCode]
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getMountPointStatus(java.lang.String,int,long):[ERROR] Cannot get mount point: {e.getMessage()}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client skipping handshake in secured configuration with unsecured cluster for addr = {}, datanodeId = {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getAccessibleNodeLabels(java.lang.String):[WARN] Accessible node labels for root queue will be ignored, it will be automatically set to "*".
org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:moveToNextQueue():[DEBUG] Moving to next queue from queue index {} to index {}, number of requests left for current queue: {}.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[WARN] commitBeforeRead didn’t succeed with ret={}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Staging dir does not exist ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainerInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest,java.lang.String):[INFO] Start request for containerIdStr by user remoteUser with resource containerResource
org.apache.hadoop.metrics2.sink.KafkaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[DEBUG] Broker list {brokerList}
org.apache.hadoop.mapred.TaskLog:writeToIndexFile(java.lang.String,boolean):[WARN] Unable to clean resources
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:bootstrap(java.lang.String,int,int):[INFO] TC configuration is incomplete. Wiping tc state before proceeding
org.apache.hadoop.hdfs.server.datanode.DataNode:checkDiskErrorAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi):[DEBUG] checkDiskErrorAsync: no volume failures detected
org.apache.hadoop.mapred.FadvisedChunkedFile:close():[WARN] Failed to manage OS cache for "<identifier>" fd <fd.toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:killAllAppsInQueue(java.lang.String):[INFO] Application killed due to expiry of reservation queue {queueName}.
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[INFO] AM container found in context, has credentials: {}
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:createNewApplication(javax.servlet.http.HttpServletRequest):[WARN] Unable to create a new ApplicationId in SubCluster {subClusterId.getId()}
org.apache.hadoop.tools.DistCpSync:checkNoChange(org.apache.hadoop.hdfs.DistributedFileSystem,org.apache.hadoop.fs.Path):[WARN] Failed to compute snapshot diff on ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadRMAppStateFromAppNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String,java.lang.String):[DEBUG] Loading application from znode: {}
org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[WARN] Unable to execute ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:updateLabelsOnNode(org.apache.hadoop.yarn.api.records.NodeId,java.util.Set):[WARN] There's something wrong, some RMContainers running on a node, but we cannot find SchedulerApplicationAttempt for it. Node= + node.getNodeID() + applicationAttemptId= + rmContainer.getApplicationAttemptId()
org.apache.hadoop.resourceestimator.solver.impl.LpSolver:solve(java.util.Map):[DEBUG] time interval: {}, container: {}.
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto):[DEBUG] storeApplication: appId={}, proto={}
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:periodicInvoke():[INFO] Attempting to open state store driver.
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache:prepareAddressMap():[ERROR] FS:fsScheme, Namenode ID collision for path:exportPath nnid:namenodeId uri being added:exportURI existing uri:value
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementDispatcher:collect(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithmOutput):[DEBUG] Planning Algorithm has placed for application [{}] the following [{}]
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:precommitCheckPendingFiles(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit):[DEBUG] Preflight Load of pending files
org.apache.hadoop.hdfs.protocol.ClientProtocol:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean):[INFO] Calling RouterRpcServer mkdirs
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:executeStage(org.apache.commons.lang3.tuple.Triple):[INFO] {}: Executing Manifest Job Commit with {} files
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils:isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.slf4j.Logger):[DEBUG] Skipping 'rack' {} for {} since it has been blacklisted
org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName):[DEBUG] Stacktrace:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Trying to assign containers to child-queue of {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: MAX_RESOURCES
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.MetricsInvariantChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] JVM metrics initialized
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Stopping service {serviceName}, with appId = {currentAppId}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkConfiguration(org.apache.hadoop.conf.Configuration):[WARN] Only one namespace edits storage directory (DFS_NAMENODE_EDITS_DIR_KEY) configured. Beware of data loss due to lack of redundant storage directories!
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startFile(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String,boolean):[INFO] Audit event: create, src, status
org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket):[TRACE] this: NotificationHandler: got EOF on sock.fd
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[ERROR] Unable to update token {tokenId.getSequenceNumber()}, {e}
org.apache.hadoop.yarn.service.ServiceManager$CancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.ServiceManager,org.apache.hadoop.yarn.service.ServiceEvent):[INFO] [SERVICE] cancel version {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanUpApplicationsOnNMShutDown():[INFO] Done waiting for Applications to be Finished. Still alive: ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS PATHCONF fileHandle: {} client: {}, handle.dumpFileHandle(), remoteAddress
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:doneReading(org.apache.hadoop.fs.azurebfs.services.ReadBuffer,org.apache.hadoop.fs.azurebfs.contracts.services.ReadBufferStatus,int):[TRACE] ReadBufferWorker completed read file {} for offset {} outcome {} bytes {}
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:render():[DEBUG] Invalid containerId CONFIGURED
org.apache.hadoop.yarn.service.component.Component$DecommissionInstanceTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] Instance was null for decommissioned instance {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:recoverTrackerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTracker,org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$LocalResourceTrackerState):[DEBUG] Recovering localized resource {} at {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesLogger$APP:recordAppActivityWithoutAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityState,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityLevel):[WARN] Request key should not be null at level level.
org.apache.hadoop.fs.s3a.S3AFileSystem:uploadPart(com.amazonaws.services.s3.model.UploadPartRequest,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[DEBUG] Tracking duration of supplier
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:enableBlockPoolId(java.lang.String):[TRACE] {}: loaded block iterator for {}.
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:init(javax.servlet.FilterConfig):[WARN] {} does not appear to be a valid URL
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests:preemptReduce(int):[INFO] Preempting {id}
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Keytab <loginKeytabStatus>
org.apache.hadoop.tools.dynamometer.ApplicationMaster$LaunchContainerRunnable:getContainerStartCommand():[INFO] Completed setting up command for namenode: ...
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[ERROR] Unable to insert the ApplicationId {appId} into the FederationStateStore
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:get(java.lang.Class):[WARN] Removing {} as it's an old temporary record
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[ERROR] Can't handle this event at current state
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeRemoveAuxService(java.lang.String):[INFO] Removing aux service
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Time taken to replay the logs in ms: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppFinishTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[INFO] Removing containerID from application app.toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:processHeartbeat(java.util.List):[WARN] {req} failed for {localizerId} : {diagnostics}
org.apache.hadoop.tools.dynamometer.Client:setupRemoteResourcesGetEnv():[INFO] Set the environment for the application master
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:reportContainerResourceUsage(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,java.lang.Long,java.lang.Float):[ERROR] Seems like client has been removed before the container metric could be published for container.getContainerId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] FPGA plugin failed to downloaded IP, please check the value of environment viable: {REQUEST_FPGA_IP_ID_KEY} if you want YARN to program the device
org.apache.hadoop.fs.s3a.Invoker:quietlyEval(java.lang.String,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[DEBUG] Action {} failed
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner:shouldRetry(java.io.IOException,int):[INFO] Retrying connect to namenode: {}. Already retried {} time(s); retry policy is {}, delay {}ms.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] Exception while canceling delayed flush timer. Likely caused by a failed flush {e.getMessage()}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:executeWriteBack():[DEBUG] The openFileCtx is not active anymore, fileId: {}
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration):[INFO] Formatting using clusterid: {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[DEBUG] Exception caught, ignoring node:{}.
org.apache.hadoop.hdfs.server.datanode.DataXceiver:run():[DEBUG] {datanode.getDisplayName()}:Number of active connections is: {...}
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:clearRollingUpgradeMarkers(java.util.List):[WARN] Failed to delete {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:readReplicasFromCache(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Replica Cache file: path cannot be deleted
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Assigning container failed on node '{}' because it has reserved containers.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:createOutputCommitter(org.apache.hadoop.conf.Configuration):[INFO] OutputCommitter is <committer_class_name>
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.util.List):[DEBUG] Could not resolve {hostname}. Falling back to {rack}
org.apache.hadoop.nfs.nfs3.Nfs3Base:start(boolean):[ERROR] Failed to register the NFSv3 service.
org.apache.hadoop.portmap.RpcProgramPortmap:getport(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR):[WARN] Warning, no mapping for key: + key
org.apache.hadoop.streaming.PipeMapRed$MRErrorThread:incrCounter(java.lang.String):[WARN] Cannot parse counter increment ' + columns[2] + ' from line: + line
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout():[DEBUG] Hadoop logout
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleRunningAppOnNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.NodeId):[WARN] Cannot get RMApp by appId= + appId + , just added it to finishedApplications list for cleanup
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:health(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] Timeline services health check: timeline reader reported connection failure
org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context):[INFO] Slept for ${duration}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:cleanup():[INFO] Current OpenFileCtx is already inactive, no need to cleanup.
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:loadNodeChildrenHelper(org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node,java.lang.String,java.lang.String[]):[TRACE] Skipping XMLEvent ...
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter:handle(com.sun.jersey.api.client.ClientRequest):[INFO] Retrying request with TimelineClientRetryOp
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteFilesWithDanglingTempData(org.apache.hadoop.fs.Path):[DEBUG] Deleting files with dangling temp data in {}, root
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleDestroyApplicationResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application):[WARN] Removing uninitialized application + application
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Starting 5 threads
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnclosedSegment(long):[INFO] None of the responders had a log to recover: + QuorumCall.mapToString(prepareResponses)
org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String):[INFO] Can't map group [GROUP]. Use its string hashcode: [HASHCODE]
org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException):[DEBUG] Detailed exit debug info
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:uploadBlockAsync(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock,java.lang.Boolean):[DEBUG] Completed upload of {} to part {}
org.apache.hadoop.hdfs.server.datanode.DataNode:checkDiskError():[WARN] checkDiskError got {unhealthyVolumes.size()} failed volumes - {unhealthyVolumes}
org.apache.hadoop.hdfs.DFSClient:cancelDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Cancelling token
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[ERROR] No more jobs to process in the trace with 'starts-after' set to [startsAfter] ms.
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager:allocSlot(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,org.apache.hadoop.hdfs.ExtendedBlockId,java.lang.String):[TRACE] this + ": the DfsClientShmManager is closed."
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration):[DEBUG] Getting configuration value for cipher suite
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List):[DEBUG] Corruption detected! Parent node is not contained in the list of known ids!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getSubdirEntries():[TRACE] getSubdirEntries({}, {}): purging entries cache for {} + after {} ms.
org.apache.hadoop.fs.s3a.S3AFileSystem:s3GetFileStatus(org.apache.hadoop.fs.Path,java.lang.String,java.util.Set,boolean):[DEBUG] S3GetFileStatus {}
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Getting UGI for current user
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:isResourceCalculatorAvailable():[INFO] ResourceCalculatorPlugin is unavailable on this system. {} is disabled.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:registerMBean():[INFO] Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
org.apache.hadoop.mapreduce.JobResourceUploader:addLog4jToDistributedCache(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path):[DEBUG] Copying Log4j property file
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:initializePluggableDevicePlugins(org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.conf.Configuration,java.util.Map):[INFO] The pluggable device framework enabled, trying to load the vendor plugins
org.apache.hadoop.hdfs.qjournal.server.JournalNode$ErrorReporter:reportErrorOnFile(java.io.File):[ERROR] Error reported on file ... exiting
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path):[WARN] {} active upload(s) in progress under {}
org.apache.hadoop.portmap.RpcProgramPortmap:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] PortmapHandler unknown rpc procedure= + portmapProc
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] Event ... sent to absent application ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: MAPPED_DYNAMIC_QUEUE
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersReceivedFromPreviousAttempts(java.util.List):[INFO] Containers recovered after AM registered: {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSectionHeader(java.io.InputStream,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[INFO] Loading + numInodes + INodes.
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] GridMix is configured to use a compression ratio of ... for the job output data.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[DEBUG] Resource {resourcePath}{(->localPath)} size : {getSize()} transitioned from {oldState} to {newState}
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] Truncating file starting with recordModification
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] User {} has been removed!
org.apache.hadoop.mapred.gridmix.ClusterSummarizer:update(java.lang.Object):[INFO] Error in processing cluster status at ...
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[WARN] Please set EXECUTE permissions on this directory
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getWriter(java.lang.String):[DEBUG] Writing file: {filename}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:showRequests():[DEBUG] showRequests: application=... headRoom=... currentConsumption=...
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnfinalizedSegments():[DEBUG] newEpoch(<epoch value>) responses:\n<responses>
org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[]):[DEBUG] Exception in closing {}
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader:newConnectedPeer(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.InetSocketAddress,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID):[INFO] Successfully connected to peer
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Dispatcher setup and services added
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:removeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] removeContainerQueued: containerId={}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFolderContentsToDelete(org.apache.hadoop.fs.azure.FileMetadata,java.util.ArrayList):[ERROR] User does not have permissions to delete {}. Parent directory has sticky bit set.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:init():[DEBUG] Found volume-driver:volumeDriver
org.apache.hadoop.nfs.nfs3.Nfs3Interface:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] FSSTAT operation completed
org.apache.hadoop.hdfs.DFSInputStream:actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[WARN] Connection failure: Failed to connect to [datanode] for file [src] for block [block]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:checkInterfaceCompatibility(java.lang.Class,java.lang.Class):[DEBUG] Try to find method: {method.getName()}
org.apache.hadoop.mapred.ClientCache:instantiateHistoryProxy():[DEBUG] Connecting to HistoryServer at: serviceAddr
org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker:processErasureCodingTasks(java.util.Collection):[WARN] Failed to reconstruct striped block {block info}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[DEBUG] Duplicate: deleting
org.apache.hadoop.lib.server.Server:initServices(java.util.List):[DEBUG] Initializing service [{}]
org.apache.hadoop.hdfs.DataStreamer:run():[DEBUG] stage={}, {}
org.apache.hadoop.yarn.server.nodemanager.DeletionService:delete(org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask):[DEBUG] Scheduling DeletionTask (delay {}) : {}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecommissioningDataNodes():[DEBUG] Failed to get number of decommissioning nodes
org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token kind is {} and the token's service name is {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for dir fileId: {}
org.apache.hadoop.fs.s3a.S3AUtils:clearBucketOption(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[DEBUG] Unset {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[WARN] Incorrect version exception message DN: dnReg
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getWriter(java.lang.String):[ERROR] Cannot open write stream for record {filename}, {e}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters):[DEBUG] AzureBlobFileSystem.openFileWithOptions path: {}
org.apache.hadoop.streaming.PipeMapRed$MROutputThread:run():[INFO] "Records R/W=" + numRecRead_ + "/" + numRecWritten_
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:getEncryptedStreams(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,javax.crypto.SecretKey):[DEBUG] DataNode overwriting downstream QOP
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Interrupted while waiting for application
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[INFO] Executing with tokens: ...
org.apache.hadoop.hdfs.DataStreamer:createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient):[DEBUG] Send buf size {}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invoke(java.lang.String,int,java.lang.reflect.Method,java.lang.Object,java.lang.Object[]):[ERROR] Unexpected exception while proxying API
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:serviceStart():[INFO] ContainerManager started at <!-- connectAddress value -->
org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI):[DEBUG] the local file exists and is size 'file.length()'.
org.apache.hadoop.yarn.service.client.ServiceClient:parseNumberOfContainers(org.apache.hadoop.yarn.service.api.records.Component,java.lang.String):[WARN] [COMPONENT {component.getName()}]: component count goes to negative ({orig}{newNumber} = {ret}), ignore and reset it to 0.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[DEBUG] String.format(STATE_CHANGE_MESSAGE, appID, oldState, getState(), event.getType())
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printContainerReport(java.lang.String):[INFO] Container Report :
org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server):[INFO] Reloading keystore and truststore certificates.
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int):[DEBUG] this + : + caller + : sendCallback not + closing fd + fd
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[ERROR] Unable to store token
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore$AppCheckTask:run():[INFO] Checking the initial app list for finished applications.
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeGenericKeys(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[DEBUG] Setting {} to {}
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerCompleted(org.apache.hadoop.yarn.api.records.ContainerId,int):[DEBUG] storeContainerCompleted: containerId={}
org.apache.hadoop.examples.Sort:run(java.lang.String[]):[INFO] Job started: startTime
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[WARN] Unknown block status code reported by {}: {}
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] delete: path is a directory: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean,java.util.List,java.util.Queue):[INFO] Processing directory file
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] User to Groups mapping refreshed
org.apache.hadoop.fs.s3a.S3AUtils:initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration):[ERROR] Proxy error: proxyPort set without proxyHost
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KilledDuringSetupTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[DEBUG] EventHandler handling CommitterJobAbortEvent
org.apache.hadoop.fs.s3a.impl.ChangeTracker:processNewRevision(java.lang.String,java.lang.String,long):[DEBUG] Revision ID changed from {} to {}
org.apache.hadoop.mapreduce.task.reduce.Fetcher:verifyConnection(java.net.URL,java.lang.String,java.lang.String):[DEBUG] url=msgToEncode;encHash=encHash;replyHash=replyHash
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:markZoneCompleted(java.lang.Long):[INFO] Zone {} completed re-encryption.
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Using line limit
org.apache.hadoop.yarn.util.ResourceCalculatorPlugin:getResourceCalculatorPlugin(java.lang.Class,org.apache.hadoop.conf.Configuration):[WARN] Throwable: Failed to instantiate default resource calculator.
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:pullCachedLogAggregationReports():[WARN] The log aggregation is disabled. There is no cached log aggregation status.
org.apache.hadoop.tools.util.RetriableCommand:execute(java.lang.Object[]):[ERROR] Failure in Retriable command: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:killReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] {}:{}, SchedulerEventType.KILL_RESERVED_CONTAINER, container
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer):[WARN] Failed to report bad + block + from datanode + srcDataNode + to namenode
org.apache.hadoop.mapred.gridmix.JobSubmitter:add(org.apache.hadoop.mapred.gridmix.GridmixJob):[INFO] Total number of queued jobs:
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getMapCompletionEvents(org.apache.hadoop.mapred.JobID,int,int,org.apache.hadoop.mapred.TaskAttemptID):[INFO] MapCompletionEvents request from [taskAttemptID]. startIndex [startIndex] maxEvents [maxEvents]
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Storing RMDTMasterKey.
org.apache.hadoop.tools.DistCh:setup(java.util.List,org.apache.hadoop.fs.Path):[INFO] OP_COUNT_LABEL=opCount
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:disableBlockPoolId(java.lang.String):[WARN] {}: can't remove block pool {}, because it was never added., this, bpid
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:createTaskCommitter(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Using committer {name} to output data to {outputPath}
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long):[ERROR] Unable to update token {token_sequence_number}, {exception}
org.apache.hadoop.hdfs.DeadNodeDetector:idle():[DEBUG] Got interrupted while DeadNodeDetector is idle.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:updateRuleSet(java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Empty rule set defined, ignoring update
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:setPlanParams(java.util.List):[DEBUG] Setting max error to {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:stopApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] No interceptor pipeline for application {}, likely because its AM is not run in this node.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[INFO] Sent signal x (...) to pid a as user b for container y, result=success
org.apache.hadoop.util.Shell:isSetsidSupported():[INFO] Avoiding JDK-8047340 on BSD-based systems.
org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:fetchNextBatchAsyncIfPresent():[DEBUG] [{}], Requesting next {} objects under {}
org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String):[DEBUG] Handling deprecation for all properties in config...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:deleteSync(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[INFO] Deleting block.getLocalBlock() replica replicaToDelete
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean):[DEBUG] This node {} doesn't have sufficient available or preemptible resource for minimum allocation
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] Path: [{}] is a directory. COS key: [{}]
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl$ForwardingEventHandler:handle(org.apache.hadoop.yarn.event.Event):[INFO] Handling NodeAttributesStoreEvent
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:getMinResourceNormalized(java.lang.String,java.util.Map,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Updating min resource for Queue: {name} as {ret.getResourceInformation(i)}, Actual resource: {nResourceInformation.getValue()}, ratio: {ratio.floatValue()}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:shutdown():[WARN] AsyncLazyPersistService has already shut down.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$LocalizationStatusRetriever:run():[ERROR] {} Failed to get localization statuses for {} {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] msg, e
org.apache.hadoop.mapred.Merger$MergeQueue:merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress):[INFO] Merging sorted segments
org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] Serialization class not found: , e
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean):[ERROR] Aborting since the Request has failed with all KMS providers(depending on {}={} setting and numProviders={}) in the group OR the exception is not recoverable
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path):[INFO] Generating {} of test data...
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[INFO] ====== Beginning Service Fencing Process... ======
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:isAvailable():[WARN] Failed to get Operating System name.
org.apache.hadoop.hdfs.LocatedBlocksRefresher:waitForInterval():[DEBUG] Interrupted during wait interval
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration):[INFO] Native Bzip2 library found; proceeding to use native library name
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:getNamenodeStatusReport():[ERROR] Cannot fetch safemode state for {}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:processQueueMessages():[WARN] {}{}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:deleteLocalDir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.yarn.server.nodemanager.DeletionService,java.lang.String):[WARN] Failed to delete this local Directory: [path]
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error while removing reservation allocation., e
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Setting node count for blacklist to {}
org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector:close(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Progressing
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[WARN] Get error accessing file, fileId: {}
org.apache.hadoop.yarn.server.timeline.security.TimelineDelgationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Service initialized
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore$AppCheckTask:run():[ERROR] Unexpected exception thrown during in-memory store app check task. Rescheduling task.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[DEBUG] resolveDuplicateReplicas decide to keep + replicaToKeep + . Will try to delete + replicaToDelete
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[INFO] couldn't find application + appID + while processing + FINISH_APPS event. The ResourceManager allocated resources for this application to the NodeManager but no active containers were found to process.
org.apache.hadoop.hdfs.server.balancer.Balancer:logUtilizationCollections():[DEBUG] logUtilizationCollection("above-average", aboveAvgUtilized)
org.apache.hadoop.fs.cosn.CosNFileSystem:open(org.apache.hadoop.fs.Path,int):[INFO] Open the file: [{f}] for reading.
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[WARN] Unable to connect to + host + as user + args.user
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable):[DEBUG] Updating attr cache...
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState
org.apache.hadoop.ha.StreamPumper:pump():[INFO] logPrefix + ": " + line
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedWriter:transferData2Targets():[WARN] {e.getMessage()}
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration):[WARN] [dirStrings[i]] is not writable
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[DEBUG] {} is not a desired LocalizationEvent which needs to be published by NMTimelinePublisher, event.getType()
org.apache.hadoop.util.Shell:runCommand():[WARN] Error while closing the input stream,
org.apache.hadoop.fs.azure.PageBlobOutputStream:close():[DEBUG] Timed out after 10 minutes waiting for IO requests to finish ioThreadPool.toString()
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Completing previous finalize for storage directory {}
org.apache.hadoop.fs.adl.AdlFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Applying UMask
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:silentlyStopSchedulingMonitor(java.lang.String):[WARN] Exception while stopping monitor= + mon.getName(), e
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:recoverTransitionRead(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Analyzing storage directories for bpid {}
org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long):[WARN] Not attempting to re-login since the last re-login was attempted less than (kerberosMinSecondsBeforeRelogin / 1000) seconds before. Last Login= user.getLastLogin()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptFailedTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] The number of failed attempts in previous + app.attemptFailuresValidityInterval + milliseconds is + numberOfFailure + . The max attempts is + app.maxAppAttempts
org.apache.hadoop.lib.service.scheduler.SchedulerService:destroy():[DEBUG] Scheduler shutdown
org.apache.hadoop.yarn.client.cli.TopCLI:run(java.lang.String[]):[ERROR] Unable to parse options
org.apache.hadoop.fs.s3a.S3AInputStream:readSingleRange(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer):[WARN] Exception while reading a range {} from path {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:visit(java.io.RandomAccessFile):[INFO] Loading inode references
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:initializePipeline(java.lang.String):[ERROR] Init RMAdminRequestInterceptor error for user: {}
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal():[INFO] Deleting original logs
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:safeDelete(org.apache.hadoop.fs.azure.StorageInterface$CloudBlobWrapper,org.apache.hadoop.fs.azure.SelfRenewingLease):[DEBUG] Swallowing delete exception on retry: ...
org.apache.hadoop.fs.azurebfs.services.ShellDecryptionKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[ERROR] Unable to get key from credential providers.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] Executing with tokens:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Removed node {nodeInfo.getNodeAddress()} clusterResource: {getClusterResource()}
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:processArgs(java.util.List,java.io.PrintStream):[DEBUG] No mode specified, defaulting to -LIST,
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:createQueues(org.apache.hadoop.conf.Configuration):[WARN] Not able to initialize queue
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager:recycle(byte[]):[DEBUG] , freeQueue.offer
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,java.net.InetAddress):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:put(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo):[DEBUG] Added Execution Type={}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handle(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent):[WARN] Very low remaining capacity in the event-queue of RMContainerAllocator: {remCapacity}
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection):[ERROR] BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}
org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils:getConfiguredHistoryIntermediateUserDoneDirPermissions(org.apache.hadoop.conf.Configuration):[WARN] Unsupported permission configured in + JHAdminConfig.MR_HISTORY_INTERMEDIATE_USER_DONE_DIR_PERMISSIONS + , the user and the group permission must be 7 (rwx). + The permission was set to + permission.toString()
org.apache.hadoop.mapred.LocalJobRunner$Job:createReduceExecutor():[DEBUG] Starting reduce thread pool executor.
org.apache.hadoop.mapreduce.lib.join.Parser$WNode:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Getting splits for the job context
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:unreference():[DEBUG] Decrease reference count <= 0 on ...
org.apache.hadoop.yarn.api.ApplicationClientProtocol:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest):[INFO] Submitting reservation to ResourceManager
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[INFO] ====== Fencing successful by method + method + ======
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] Attempting to reacquire classId for container: {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateNodeLabelsFromNMReport(java.util.Set,org.apache.hadoop.yarn.api.records.NodeId):[DEBUG] Node Labels {<nodeLabels>} from Node <nodeId> were Accepted from RM
org.apache.hadoop.yarn.client.util.YarnClientUtils:generateToken(java.lang.String):[DEBUG] Got valid challenge for host {}
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[INFO] strict preemption : X containers to kill
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.ConvertedConfigValidator:validateConvertedConfig(java.lang.String):[INFO] Capacity scheduler was successfully started
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:copyBlocks(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] Disk Balancer - Unable to find dest volume: {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] FATAL, State store fenced even though the resource manager is not configured for high availability. Shutting down this resource manager to protect the integrity of the state store.
org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map):[ERROR] Can't update mapName map
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$ContainerDoneTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing + containerEvent.getContainerID() + from application + app.toString()
org.apache.hadoop.util.Shell:runCommand():[WARN] Error while closing the error stream
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:getMajorNumber(java.lang.String):[ERROR] Failed to parse device major number from stat output
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: event.getClass()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:addCredentials():[DEBUG] Adding new framework-token for {} for log-aggregation: {}; userUgi={}
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Verifying access-type {} for {} on application {} owned by {}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:removeApplicationFromRenewal(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Removing delegation token for appId=org.apache.hadoop.yarn.api.records.ApplicationId; token=dttr.token.getService()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] requested offset={} and current filesize={}
org.apache.hadoop.fs.s3a.S3AInputStream:read(long,byte[],int,int):[INFO] Read operation on path
org.apache.hadoop.hdfs.server.datanode.DataXceiver:copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[INFO] Not able to copy block ... because threads quota exceeded
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:wrapExceptionWithMessage(java.lang.Exception,java.lang.String):[DEBUG] Unable to wrap exception of type {}, it has no (String) constructor.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] Using blacklist for AM: additions({}) and removals({})
org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String):[TRACE] Name lookup for + hostname + took + elapsedMs + ms.
org.apache.hadoop.yarn.service.client.ServiceClient:addYarnSysFs(org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[WARN] Failed to delete temp file: [tmpDir.getAbsolutePath()]
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumLiveDataNodes():[DEBUG] Failed to get number of live nodes
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String):[DEBUG] Sender: Block replaced
org.apache.hadoop.hdfs.DFSInputStream:read(long,byte[],int,int):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:defaultScheduleAction(java.util.Set,java.util.Map,java.util.Set,org.apache.hadoop.yarn.api.records.ContainerId,int):[DEBUG] Using default scheduler. Allowed: + allowed + ,Used: + used + , containerId: + containerId
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Container + containerId + not running, nothing to signal.
org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress):[DEBUG] Acquired token
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:runLoop(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] {}: saving block iterator {} after {} ms., this, curBlockIter, saveDelta
org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite):[DEBUG] Crypto codec {} not found.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStart():[INFO] Recovery ended
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Could not delete + taskAttemptPath
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Adding block pool [bpid] to volume with id [getStorageID()]
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[DEBUG] Initializing Generic Keys
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Total Nodes: countHere
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:handleContainerRemove(java.lang.String,java.util.Map):[INFO] Delayed removal requested and allowed, skipping removal - [containerId]
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadIndexedLogsMeta(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] loadIndexedLogsMeta
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:deactivateLeafQueuesIfInActive(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue,java.lang.String,java.util.Map):[WARN] Could not find queue in scheduler while trying to deactivate for [parentQueue]
org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration):[DEBUG] AuthenticationMethod.TOKEN authentication enabled for secret manager
org.apache.hadoop.mapreduce.protocol.ClientProtocol:getTaskDiagnostics(org.apache.hadoop.mapreduce.TaskAttemptID):[INFO] Fetching task diagnostics
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getLegacyBlockReaderLocal():[TRACE] {}: can't construct BlockReaderLocalLegacy because the address + {} is not local
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:setNodeId(org.apache.hadoop.yarn.api.records.NodeId):[DEBUG] updating nodeId : {}
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:run():[INFO] EventFetcher is interrupted.. Returning
org.apache.hadoop.yarn.service.utils.JsonSerDeser:fromResource(java.lang.String):[ERROR] Exception while parsing json resource {}
org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator:setRandomTextDataGeneratorWordSize(org.apache.hadoop.conf.Configuration,int):[DEBUG] Random text data generator is configured to use a dictionary with words of length [wordSize]
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$IntermediateMemoryToMemoryMerger:merge(java.util.List):[INFO] reduceId + Memory-to-Memory merge of the + noInMemorySegments + files in-memory complete.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:scanForLogs():[DEBUG] scan for log file: {filename}
org.apache.hadoop.hdfs.server.balancer.Dispatcher:dispatchBlockMoves():[WARN] DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY={maxMoverThreads} is too small for moving blocks to {targets.size()} targets. Balancing may be slower.
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer):[INFO] this + : datanode does not support short-circuit + shared memory access: + error
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:reapContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReapContext):[WARN] Error in reaping container {} exit = {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:copySource(java.lang.String,org.apache.hadoop.fs.s3a.S3ObjectAttributes,org.apache.hadoop.fs.s3a.S3AReadOpContext,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Copy file from %s to %s (length=%d)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String):[WARN] Specified path envBinaryPath is a directory
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:moveBlocksToPending():[DEBUG] {} blocks are now pending replication
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo):[DEBUG] Adding jobId to job list cache with fileInfo.getJobIndexInfo()
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] Invalid input: , {e}
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskStriped:recover():[WARN] Failed to recover block (block=" + block + ", datanode=" + id + ")
org.apache.hadoop.yarn.appcatalog.application.YarnServiceClient:upgradeApp(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Error in stopping application: {e}
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation):[INFO] {} configured as false. Blob metadata will be treated case insensitive.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:initNetworkResourceHandler(org.apache.hadoop.conf.Configuration):[INFO] Using traffic control bandwidth handler
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unknown event type on UpdateContainer: ${event.getType()}
org.apache.hadoop.yarn.sls.SLSRunner:waitForNodesRunning():[INFO] SLSRunner is waiting for all nodes RUNNING. {} of {} NMs initialized.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Edit log sync completed
org.apache.hadoop.lib.server.Server:destroyServices():[ERROR] Could not destroy service [{}], {}
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] count=count
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[INFO] Expanded source {expanded}
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[INFO] Cannot find matching entity of type ...
org.apache.hadoop.mapred.uploader.FrameworkUploader:checkSymlink(java.io.File):[DEBUG] Not a link
org.apache.hadoop.lib.server.Server:init():[INFO] Built by : {}
org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List):[DEBUG] argumentString
org.apache.hadoop.hdfs.server.datanode.DataNode:checkDiskError():[ERROR] Interrupted while running disk check, e
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleCommit(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,int):[ERROR] Should not get commit return code: + ret.name()
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:listStatusIterator(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.listStatusIterator path : {}
org.apache.hadoop.hdfs.server.namenode.sps.Context:removeSPSHint(long):[DEBUG] Removing SPS hint
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:deleteApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[ERROR] Cannot delete app: e.getMessage()
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createUserCacheDirs(java.util.List,java.lang.String):[WARN] Unable to create app cache directory : {}
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[INFO] Indeterminate response from trying to kill service. Verifying whether it is running using nc...
org.apache.hadoop.hdfs.server.datanode.BPOfferService:updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat):[INFO] Namenode actor trying to claim ACTIVE state with txid= txid
org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run():[WARN] Trash cannot close FileSystem: {exception}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getEditLogManifest(long):[INFO] Checking superuser privilege
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:cleanLogs(org.apache.hadoop.fs.Path,long):[DEBUG] Checking application log directory
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler:exceptionCaught(java.lang.Throwable):[WARN] INTERNAL_SERVER_ERROR
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCorruptFilesList():[DEBUG] Get corrupt file blocks returned error: {e.getMessage()}
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int):[WARN] Interrupted before adjusting thread count: {}
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadStorageDirectory(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List):[INFO] Storage directory with location {} does not exist
org.apache.hadoop.hdfs.DFSOutputStream:closeImpl():[DEBUG] Closing an already closed stream. [Stream:{}, streamer:{}]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateStarvedApps():[INFO] Processing apps with fairshare starvation
org.apache.hadoop.yarn.YarnUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Thread [thread] threw an Throwable, but we are shutting down, so ignoring this
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:doUnregistration():[INFO] History url is {historyUrl}
org.apache.hadoop.hdfs.server.balancer.Balancer$Cli:run(java.lang.String[]):[INFO] Balancing took ...
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:authenticateWithoutTlsExtension(java.lang.String,java.lang.String):[DEBUG] Authentication successful for {}
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:satisfyStoragePolicy(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.blockmanagement.BlockManager,java.lang.String,boolean):[INFO] Skipping satisfy storage policy on path:{} as this file doesn't have any blocks!
org.apache.hadoop.service.CompositeService:stop(int,boolean):[DEBUG] Stopping service # i: service
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logLegacyGenerationStamp(long):[DEBUG] Logging legacy generation stamp
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:initializeQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[ERROR] Failed to updatePlacementRules
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:refreshScheduler(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationDefinition,java.lang.String):[DEBUG] Reservation {} is within threshold so attempting to create synchronously., reservationId
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:execute(org.apache.commons.cli.CommandLine):[INFO] Executing "execute plan" command
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[WARN] Inconsistent number of corrupt replicas for {} blockMap has {} but corrupt replicas map has {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:setPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.SetSubClusterPolicyConfigurationRequest):[ERROR] Unable to insert the newly generated policy for the queue : {queue}
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:queryDocuments(java.lang.String,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,java.lang.Class,long):[DEBUG] Querying Collection : {} , with query {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:commitQueueManagementChanges(java.util.List):[DEBUG] Queue is already de-activated. Skipping de-activation : {}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed while getting the configured log directories
org.apache.hadoop.hdfs.server.namenode.FSEditLog:selectInputStreams(long,long,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean,boolean):[ERROR] Exception while selecting input streams
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkCommit(org.apache.hadoop.hdfs.DFSClient,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean):[DEBUG] Got commit status: {}
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:convertProtoToDeletionTask(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[DEBUG] Unable to get task type, trying FileDeletionTask
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:init(javax.servlet.FilterConfig):[INFO] listAllowedUsers=<listAllowedUsers>
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:getLatestImages():[ERROR] Image checkpoint time X > edits checkpoint time Y
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:callCOSClientWithRetry(java.lang.Object):[INFO] Call cos sdk failed, retryIndex: [1 / max], call method: uploadPart, exception: ...
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleLaunchForLaunchType(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext,org.apache.hadoop.yarn.api.ApplicationConstants$ContainerLaunchType):[ERROR] Failed to squash cgroup operations!
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KilledForReInitializationTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Relaunching Container [ID] for re-initialization !!
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[]):[INFO] Removed storage {} from DataNode {}
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:readEntityTypes(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[DEBUG] Scanned {} records for {} types
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[WARN] Invalid RPC call version + ver
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long):[INFO] Resolved OOM in X ms
org.apache.hadoop.fs.s3a.impl.DeleteOperation:deleteObjectAtPath(org.apache.hadoop.fs.Path,java.lang.String,boolean):[DEBUG] delete: {} {}, (isFile ? "file" : "dir marker"), key
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Found non empty SchedulingRequest in AllocateRequest for application=...
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop():[INFO] Shutting down the background thread.
org.apache.hadoop.mapred.SortedRanges:add(long,long):[DEBUG] added + recRange
org.apache.hadoop.registry.server.dns.RegistryDNS:enableDNSSECIfNecessary(org.xbill.DNS.Zone,org.apache.hadoop.conf.Configuration,org.xbill.DNS.SOARecord,org.xbill.DNS.NSRecord):[INFO] Registering {}
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.TimelineCollectionReader:readDocuments(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,long):[DEBUG] Fetching documents for entity type ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:notifyStoreOperationFailedInternal(java.lang.Exception):[ERROR] State store operation failed
org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException):[DEBUG] Failed to connect to server: + server + : + action.reason, ioe
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanupStagingDir():[INFO] Deleting staging directory + FileSystem.getDefaultUri(getConfig()) + jobTempDir
org.apache.hadoop.hdfs.tools.DFSAdmin:run(java.lang.String[]):[DEBUG] Exception encountered:
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogCleaner:run():[DEBUG] Cleaner finished
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Unexpected error when publishing entity
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionCancelUpgrade(java.lang.String):[ERROR] Failed to cancel upgrade: , e
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:heartbeat():[INFO] Event from RM: shutting down Application Master
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:getMajorAndMinorNumber(java.lang.String):[WARN] Failed to get major-minor number from reading /dev/{devName}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] OUTCOME: SUCCESS, Reservation ID: reservationId.toString(), Contract: contract.toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:initAuxService(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord,org.apache.hadoop.conf.Configuration,boolean):[ERROR] Failed to initialize [service_name]
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext):[INFO] Error while creating of RM app master service proxy for attemptId,..., user:...
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$ConnectionCreator:run():[DEBUG] Cannot add more than {} connections to {}
org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl):[WARN] Unknown method called
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getDefaultWorkFile(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String):[DEBUG] Work file for {} extension '{}' is {}
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[]):[ERROR] Invalid Port - int required
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[DEBUG] Cleaning up resources
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... but app not found (Took ... ms.)
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:init(java.util.Properties,javax.servlet.ServletContext,long):[INFO] The secret znode already exists, retrieving data
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.SubClusterDeregisterRequest):[WARN] Missing GetSubClusterInfo Request. Please try again by specifying a Get SubCluster information.
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop():[INFO] Stopping service scheduler
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:trackContainerForPreemption(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Container added for preemption
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:init(java.util.Properties,javax.servlet.ServletContext,long):[INFO] Creating secret znode
org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Saver:buildINodeReference(org.apache.hadoop.hdfs.server.namenode.INodeReference,long):[ERROR] FSImageFormatPBSnapshot: Missing referred INodeId ...
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] RMAppManager processing event for applicationId of type APP_COMPLETED
org.apache.hadoop.tools.rumen.ParsedTaskAttempt:dumpParsedTaskAttempt():[INFO] ParsedTaskAttempt details: {obtainCounters()};DiagnosticInfo={obtainDiagnosticInfo()} \n {obtainTrackerName()};{obtainHttpPort()};{obtainShufflePort()};rack={getHostName().getRackName()};host={getHostName().getHostName()}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlows(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL <url> from user <user>
org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec):[INFO] Got brand-new decompressor [<default extension>]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Get volume create request from plugin:{} for container: {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster:main(java.lang.String[]):[INFO] Application Master completed successfully. exiting
org.apache.hadoop.yarn.service.ServiceScheduler:buildInstance(org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.conf.Configuration):[INFO] Set registry user accounts: sasl:username
org.apache.hadoop.fs.GetSpaceUsed$Builder:build():[WARN] Error trying to create an instance of [CLASS_NAME], [Exception]
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] Ignoring directory: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Virtual memory check enabled: {}
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] Server status = {apiServer}
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[DEBUG] Starting the block retrieval process
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Recovering container
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[WARN] error stopping existing instance: {}
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to start + containerId, t
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:start():[INFO] Given mode: {} is invalid
org.apache.hadoop.hdfs.DataStreamer:addDatanode2ExistingPipeline():[WARN] Error transferring data from ...
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:loadState():[INFO] Loaded + numKeys + master keys and + numTokens + tokens from leveldb, and latest sequence number is + state.getLatestSequenceNumber()
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:loadStateFromRegistry(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Failed reading registry key, skipping subcluster {}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[DEBUG] Block {}: can't add new cached replicas, + because there is no record of this block + on the NameNode.
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client doing general handshake for addr = {}, datanodeId = {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:isContainerOutOfLimit(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Container %s is out of its limits, using %d when requested only %d
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:run():[INFO] EventFetcher: Got new map-outputs
org.apache.hadoop.fs.azurebfs.services.ShellDecryptionKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Executing shell command
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Received an invalid request file transfer request from + request.getRemoteAddr() + : + msg
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:delete(java.lang.String,boolean,boolean):[DEBUG] Audit event logged
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:nodeUnpublishVolume(org.apache.hadoop.yarn.api.protocolrecords.NodeUnpublishVolumeRequest):[DEBUG] Translate to CSI proto message: {}
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using CIDR match for 'host' and READ_WRITE
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore$EntityDeletionThread:run():[ERROR] e.toString()
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:renewDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Renew delegation token
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric was emitted with no name.
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration):[DEBUG] concat: firstchunk: + dstfs.getFileStatus(firstChunkFile)
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[DEBUG] saveErasureCodingSection completed
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceStart():[INFO] Router RMAdminService listening on address: + this.server.getListenerAddress()
org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Renewing delegation token {} with url:{}, as:{}
org.apache.hadoop.mapred.lib.FieldSelectionMapReduce:configure(org.apache.hadoop.mapred.JobConf):[INFO] specToString()
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using leveldb path /path/to/leveldb
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[DEBUG] Inode {} EZ key version unchanged, skipping re-encryption.
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path):[ERROR] Gridmix input data directory {} already exists when -generate option is used.
org.apache.hadoop.applications.mawo.server.common.MawoConfiguration:readConfigFile():[INFO] Configuration file being loaded: + CONFIG_FILE + . Found in classpath at + MawoConfiguration.class.getClassLoader().getResource(CONFIG_FILE)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List):[DEBUG] AzureBlobFileSystem.modifyAclEntries path: {path}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:processApplicationStartRequest(org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest):[INFO] Callback received for initializing request processing pipeline for an AM
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] RMAppManager processing event for applicationId of type APP_MOVE
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String,java.lang.String[],java.lang.String,boolean,boolean,org.apache.hadoop.hdfs.server.federation.resolver.order.DestinationOrder,org.apache.hadoop.hdfs.tools.federation.RouterAdmin$ACLEntity):[INFO] Mount point updated successfully
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] logCommitterStatisticsAtDebug
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS READDIR fileHandle: {} cookie: {} count: {} client: {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:remove(java.lang.String):[ERROR] Cannot remove {}
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Executing command {BucketInfo.NAME}
org.apache.hadoop.security.authentication.client.Authenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[INFO] Pseudo authentication attempted
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRename(java.lang.String,java.lang.String,long,boolean,org.apache.hadoop.fs.Options$Rename[]):[INFO] logEdit called
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:setTimelineCollectorInfo(org.apache.hadoop.yarn.api.records.CollectorInfo):[INFO] Updated timeline service address to + timelineServiceAddress
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Cannot rollback resource for container + containerId + . The container does not exist.
org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]):[DEBUG] Attempting active election for + this
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS PATHCONF fileHandle: {} client: {}
org.apache.hadoop.registry.server.dns.RegistryDNS:updateDNSServer(org.apache.hadoop.conf.Configuration):[ERROR] Can not resolve DNS servers:
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[DEBUG] Got the information about the specified SubCluster {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Allocated to {}: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeCachePool(java.lang.String,boolean):[INFO] Success: removeCachePool, {poolName: {poolName}}, true
org.apache.hadoop.fs.azure.NativeAzureFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] Found path as a file
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:getMultiNodeSortIterator(java.util.Collection,java.lang.String,java.lang.String):[WARN] MultiNode policy 'policyName' is configured, however yarn.scheduler.capacity.multi-node-placement-enabled is false
org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String):[ERROR] ioe.toString()
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceStop():[INFO] Stopping {}
org.apache.hadoop.yarn.service.webapp.ApiServer:getService(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] GET: getService for appName = {} user = {}
org.apache.hadoop.fs.Globber:glob():[INFO] glob path pattern
org.apache.hadoop.yarn.server.webproxy.ProxyCA:createCACertAndKeyPair():[DEBUG] CA Certificate: \n{}
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] allocate(arrayLength): count=count
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:startTimelineReaderServer(java.lang.String[],org.apache.hadoop.conf.Configuration):[ERROR] Error starting TimelineReaderWebServer
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL):[DEBUG] Fetcher request verfied. enc_str=enc_str;reply=reply
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Removing existing BR lease 0x{Long.toHexString(node.leaseId)} for DN {dn.getDatanodeUuid()} in order to issue a new one.
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:putObject():[WARN] Interrupted object upload
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device):[DEBUG] stat output:{output}
org.apache.hadoop.fs.DelegationTokenRenewer:removeRenewAction(org.apache.hadoop.fs.FileSystem):[DEBUG] Exception in removeRenewAction: {}
org.apache.hadoop.streaming.PipeMapRed:setStreamJobDetails(org.apache.hadoop.mapred.JobConf):[INFO] JobConf set minRecWrittenToEnableSkip_ = + minRecWrittenToEnableSkip_
org.apache.hadoop.mapred.SortedRanges:add(org.apache.hadoop.mapred.SortedRanges$Range):[DEBUG] added <range>
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] User [] is not authorized to view the logs for
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyBytes(org.apache.hadoop.tools.CopyListingFileStatus,long,java.io.OutputStream,int,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Writing bytes to output stream
org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration):[WARN] Unable to parse configuration UMASK_LABEL with value confUmask as octal or symbolic umask.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] [message]
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node):[INFO] Adding a new node: + NodeBase.getPath(node)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationOnRecovery(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[DEBUG] Application applicationId is recovering. Skip notifying APP_ACCEPTED
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.
org.apache.hadoop.mapreduce.JobResourceUploader:uploadFiles(org.apache.hadoop.mapreduce.Job,java.util.Collection,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,short,java.util.Map,java.util.Map):[INFO] Created directory for job submission
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkStat(java.lang.String):[DEBUG] Stat {}
org.apache.hadoop.resourceestimator.skylinestore.validator.SkylineStoreValidator:validate(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation):[ERROR] Resource allocation for {pipelineId} is null.
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:onClose(org.eclipse.jetty.websocket.api.Session,int,java.lang.String):[WARN] session.getRemoteAddress().getHostString() + " closed, status: " + status + " Reason: " + reason
org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider:toResponse(java.lang.Throwable):[ERROR] User request caused exception.
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameserviceAggregatedLong(java.util.function.ToLongFunction):[ERROR] Unable to extract metrics: {e.getMessage()}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List):[DEBUG] AzureBlobFileSystem.removeAclEntries path: {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Response JSON location configured
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Copying local file from {} to {}
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:updateState():[DEBUG] Reporting non-HA namenode as operational: <NamenodeDesc>
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:findNextResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] local path for PRIVATE localization could not be found. Disks might have failed.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:copyBlockFiles(long,long,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration):[DEBUG] Copied {srcReplica.getMetadataURI()} meta to {dstMeta} and calculated checksum
org.apache.hadoop.yarn.server.resourcemanager.AdminService:addToClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsRequest):[INFO] Operation addToClusterNodeLabels succeeded
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[]):[DEBUG] Executing command {Uploads.NAME}
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:sendContainerRequest():[INFO] Application {} sends out request for {} containers.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assign(java.util.List):[WARN] Container allocated at unwanted priority: ...
org.apache.hadoop.ha.ZKFailoverController:setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State):[INFO] Local service {localTarget} entered state: {newState}
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] Starting ApplicationMaster
org.apache.hadoop.tools.DistCp:cleanup():[ERROR] Unable to cleanup meta folder: {metaFolder}
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Remove RMDT master key with key id: {delegationKey.getKeyId()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:addUnallocatedResource(org.apache.hadoop.yarn.api.records.Resource):[ERROR] Invalid resource addition of null resource for + rmNode.getNodeAddress()
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:toResponse(java.lang.Throwable):[DEBUG] GOT EXCEPITION
org.apache.hadoop.nfs.NfsExports$ExactMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] ExactMatcher ' + ipOrHost + ', allowing client ' + address + ', ' + hostname
org.apache.hadoop.tools.mapred.CopyCommitter:commitData(org.apache.hadoop.conf.Configuration):[ERROR] Pre-existing final-path found at: finalDir
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List):[TRACE] Removed block {} from PENDING_CACHED list.
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getHttpRequest(java.lang.String[],java.lang.String,java.util.List,int,java.lang.String,boolean):[DEBUG] SecureWasbRemoteCallHelper#getHttpRequest() {}
org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String):[INFO] name + not found
org.apache.hadoop.fs.s3a.S3AFileSystem$OperationCallbacksImpl:finishRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] source & dest parents are different; fix up dir markers
org.apache.hadoop.hdfs.HAUtil:getNameNodeId(org.apache.hadoop.conf.Configuration,java.lang.String):[ERROR] Configuration must be suffixed with nameservice and namenode ID for HA configuration
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadDirectoriesInINodeSection(java.io.InputStream):[DEBUG] Scanned {} inodes.
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getDelegationToken(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Delegation token from cache - {}
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:flush():[DEBUG] Writing out keystore.
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId():[DEBUG] Thread interrupted while performing keyId increment
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline(java.lang.String):[INFO] Initializing request processing pipeline for application for the user: {}
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:renew():[DEBUG] Did not renew lease for client {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client skipping handshake in unsecured configuration for addr = {}, datanodeId = {}
org.apache.hadoop.hdfs.server.diskbalancer.planner.PlannerFactory:getPlanner(java.lang.String,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode,double):[DEBUG] Creating a GREEDY_PLANNER for Node : nodeName IP : nodeIP ID : nodeUUID
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics:init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration):[DEBUG] Configuring metrics output directory
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:getContainerStatus(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor,org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] Container Status: {DockerContainerStatus.NONEXISTENT.getName()} ContainerId: {containerId}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeSequential(java.util.List,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod):[ERROR] Unexpected exception {exception} proxying {methodName} to {nameServiceId}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[WARN] Event of type [] not expected here..
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[DEBUG] Map task identified
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[WARN] Error cleaning up a HistoryFile that is out of date.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore:checkVersion():[INFO] Loaded configuration store version info {}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:uploadObject(java.lang.String,java.io.File):[DEBUG] result.getETag()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:submitLazyPersistTask(java.lang.String,long,long,long,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[DEBUG] LazyWriter schedule async task to persist RamDisk block pool id: + bpId + block id: + blockId
org.apache.hadoop.hdfs.DFSStripedOutputStream:hsync():[DEBUG] DFSStripedOutputStream does not support hsync {}. Caller should check StreamCapabilities before calling.
org.apache.hadoop.hdfs.server.datanode.DataStorage:upgradeProperties(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.conf.Configuration):[INFO] Updating layout version from {} to {} for storage {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Void):[INFO] {}: scanning directory {}, getName(), taskAttemptDir
org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String):[WARN] Can't find user name for uid + uid + . Use default user name + unknown
org.apache.hadoop.mapreduce.split.JobSplitWriter:writeNewSplits(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.InputSplit[],org.apache.hadoop.fs.FSDataOutputStream):[WARN] Max block location exceeded for split: {split} splitsize: {locations.length} maxsize: {maxBlockLocations}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Privileged container being requested but privileged containers are not enabled on this cluster
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:verifyRMHeartbeatResponseForNodeAttributes(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] Node attributes {{}} were Accepted by RM
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,org.apache.hadoop.fs.Options$Rename[]):[DEBUG] DIR* FSDirectory.unprotectedRenameTo: src is renamed to dst
org.apache.hadoop.hdfs.server.namenode.BackupImage:setState(org.apache.hadoop.hdfs.server.namenode.BackupImage$BNState):[DEBUG] State transition <currentState> -> <newState>
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[INFO] System.out.println(response)
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close():[ERROR] Cache file {entry.path} deletion would not be attempted as write lock could not be acquired within {PREFETCH_WRITE_LOCK_TIMEOUT} {PREFETCH_WRITE_LOCK_TIMEOUT_UNIT}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[INFO] nodeId + Node Transitioned from + oldState + to + getState()
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[INFO] Applying default permissions
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Container [ + container.getContainerId() + ] + re-initialization failure..
org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean):[WARN] Log warning as the lock was held for too long
org.apache.hadoop.hdfs.DFSUtil:httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String):[INFO] Starting Web-server for {name} at: {uri}
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename destination directory is not empty: {dst}
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[WARN] Unknown DatanodeCommand action: {} from standby NN {}
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:compare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,boolean):[ERROR] The resource manager is in an inconsistent state. It is safe for the resource manager to be restarted as the error encountered should be transitive. If high availability is enabled, failing over to a standby resource manager is also safe.
org.apache.hadoop.registry.server.services.RegistryAdminService:submit(java.util.concurrent.Callable):[DEBUG] Submitting {}
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action: DNA_UNCACHE for ... of [...]
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:cleanupKeysWithPrefix(java.lang.String):[DEBUG] cleanup {} from leveldb
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[INFO] No pending uploads were found
org.apache.hadoop.lib.service.scheduler.SchedulerService:destroy():[WARN] Gave up waiting for scheduler to shutdown
org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean):[DEBUG] Launched service {name}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] All datanode containers completed; marking application as done
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:initReplicaRecoveryImpl(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long):[INFO] initReplicaRecovery: + block + , recoveryId= + recoveryId + , replica= + replica
org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer:startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int):[INFO] Log segment started
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:scanActiveLogs(org.apache.hadoop.fs.Path):[DEBUG] scan logs for {appId} in {stat.getPath()}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition:transition(java.lang.Object,java.lang.Object):[INFO] Num completed Tasks: + job.completedTaskCount
org.apache.hadoop.util.Shell:isSetsidSupported():[DEBUG] setsid exited with exit code (shexec != null ? shexec.getExitCode() : "(null executor)")
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:configureAzureStorageSession():[DEBUG] AzureNativeFileSystemStore init. Settings={},{},{},{{},{},{},{}},{{},{},{}}
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[ERROR] Invalid parent directory, throwing PathNotFoundException
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean):[WARN] Gridmix will not emulate Distributed Cache load because <iopath> provided is on local file system.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:loadMmapInternal():[TRACE] {}: created mmap of size {}, this, channel.size()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DiagnosticInformationUpdater:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Diagnostics report from [taskAttempt.attemptId]: [diagEvent.getDiagnosticInfo()]
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[WARN] Error closing the HTTP response's inputstream.
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:singleCopy(java.lang.String,java.lang.String):[DEBUG] Copy object ETag
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recover():[INFO] Not a recoverable state store. Nothing to recover.
org.apache.hadoop.yarn.webapp.Dispatcher:service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[ERROR] error handling URI: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[INFO] Version check passed
org.apache.hadoop.hdfs.DataStreamer:transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.security.token.Token):[INFO] sendTransferBlock called
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] {} Lifeline RPC address: {}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllTables(org.apache.hadoop.conf.Configuration,boolean):[WARN] Skip and continue on: IOException
org.apache.hadoop.fs.azure.NativeFileSystemStore:delete(java.lang.String,org.apache.hadoop.fs.azure.SelfRenewingLease):[INFO] Attempting to delete blob
org.apache.hadoop.hdfs.web.URLConnectionFactory:getSSLConnectionConfiguration(int,int,org.apache.hadoop.conf.Configuration):[WARN] Cannot load customized ssl related configuration. Fallback to system-generic settings.
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceStart():[ERROR] Unexpected error starting NodeStatusUpdater
org.apache.hadoop.oncrpc.RpcProgram:register(int,int):[ERROR] Registration failure with <host>:<port>, portmap entry: <mapEntry>
org.apache.hadoop.mapreduce.lib.partition.InputSampler$RandomSampler:getSample(org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Job):[DEBUG] seed: <seed_value>
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:logLineFromTasksFile(java.io.File):[WARN] Failed to read cgroup tasks file.
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Time taken to get FileStatuses: ...
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:initSecurity():[INFO] Registry User ACLs ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDiscoverer:initialize(org.apache.hadoop.conf.Configuration):[INFO] Trying to diagnose FPGA information ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:canDelete(java.lang.String):[INFO] Need to stop the specific queue: + queueName + first.
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:collectBlocksSummary(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[INFO] Fsck: ignoring open file + path
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:getResources():[INFO] Requested node-label-expression is invalid.
org.apache.hadoop.yarn.applications.distributedshell.Client:main(java.lang.String[]):[ERROR] Application failed to complete successfully
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:copyBlockFiles(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration):[DEBUG] Copied srcReplica.getMetadataURI() meta to dstMeta and calculated checksum
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:getImageTxIdToRetain(org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector):[INFO] Going to retain {toRetain} images with txid >= {minTxId}
org.apache.hadoop.hdfs.server.federation.router.Router:createNamenodeHeartbeatServices():[ERROR] Wrong Namenode to monitor
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator:createTimelineSchema(java.lang.String[],org.apache.hadoop.conf.Configuration):[INFO] Using {} for creating Timeline Service Schema
org.apache.hadoop.yarn.service.ServiceScheduler:syncSysFs(org.apache.hadoop.yarn.service.api.records.Service):[INFO] YARN sysfs synchronized.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkCommitInternal(long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean):[DEBUG] get commit while still writing to the requested offset, + with empty queue
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[DEBUG] Reading input file
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:run():[ERROR] Re-encryption updater thread exiting.
org.apache.hadoop.crypto.key.kms.server.KMS:getKey(java.lang.String):[TRACE] Entering getKey method.
org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:run():[DEBUG] DFSClient {}
org.apache.hadoop.mapred.YARNRunner:createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials):[INFO] SUBMITTING ApplicationSubmissionContext app:{applicationId} to queue:{appContext.getQueue()} with reservationId:{appContext.getReservationID()}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest):[DEBUG] Resource for node: nid; is adjusted from: capability; to: dynamicLoadCapability
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateSignature(com.nimbusds.jwt.SignedJWT):[WARN] JWT signature verification failed.
org.apache.hadoop.hdfs.server.namenode.NameNode:doRollback(org.apache.hadoop.conf.Configuration,boolean):[ERROR] "rollBack" will remove the current state of the file system...
org.apache.hadoop.resourceestimator.translator.impl.BaseLogParser:parseStream(java.io.InputStream):[DEBUG] Date conversion error
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches(boolean):[ERROR] Error updating cache for {}
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Setting unmanaged AM
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus):[DEBUG] Container Status: id= + containerId + , status= + containerStatus
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:apply(java.lang.Object):[INFO] {}: Stage {} completed after {}, getName(), stageName, OperationDuration.humanTime(stageExecutionTracker.asDuration().toMillis())
org.apache.hadoop.registry.server.dns.RegistryDNS:addAnswer(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int,int,int):[INFO] found local record? true
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadINodeDirSection(java.io.FileInputStream,java.util.List,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,org.apache.hadoop.conf.Configuration,java.util.List):[INFO] Finished loading INode directory section in {}ms
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[DEBUG] Constructing ProcessTree for: PID = pId ContainerId = containerId
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:initExisting():[INFO] Found + timestampedDirList.size() + directories to load
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:getExpiry(java.lang.String):[ERROR] Error parsing se query parameter ({}) from SAS.
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin):[INFO] Removing CPU constraints for YARN containers.
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$UpdateNodeResourceWhenUnusableTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[WARN] Try to update resource on a + rmNode.getState().toString() + node: + rmNode.toString()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:clean():[DEBUG] Retrieved job index info
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {unknown}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTask(org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[INFO] Re-encryption was canceled.
org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path):[ERROR] Not able to initialize target file system. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:createNewApplication(javax.servlet.http.HttpServletRequest):[ERROR] Unable to create new app from RM web service
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[DEBUG] Token not required, returning default user parameters
org.apache.hadoop.mapreduce.lib.partition.InputSampler:run(java.lang.String[]):[ERROR] Sampler requires more than one reducer
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:unpublishVolume(org.apache.hadoop.yarn.server.volume.csi.VolumeMetaData):[INFO] Local mount {} no longer exist, skipping cleaning up the volume
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:verifyNodeUUID(org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[ERROR] Disk Balancer - Plan was generated for another node.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler$ResourceCommitterService:run():[INFO] ResourceCommitterService exited!
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String):[DEBUG] Retrying getTokenSingleCall. RetryCount = 1
org.apache.hadoop.mapred.nativetask.Platforms:support(java.lang.String,org.apache.hadoop.mapred.nativetask.serde.INativeSerializer,org.apache.hadoop.mapred.JobConf):[DEBUG] platform {platform.name()} support key class {keyClassName}
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:updateYarnSysFS(org.apache.hadoop.yarn.server.nodemanager.Context,java.lang.String,java.lang.String,java.lang.String):[WARN] Unable to delete {sysFSPath}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:forceKillApplication(org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest):[ERROR] Unable to kill the application report for applicationId to SubCluster subClusterIdId
org.apache.hadoop.yarn.server.resourcemanager.AdminService:checkRMStatus(java.lang.String,java.lang.String,java.lang.String):[DEBUG] ResourceManager is not active. Can not [message]
org.apache.hadoop.fs.cosn.CosNFileSystem:createParent(org.apache.hadoop.fs.Path):[DEBUG] Create parent key: {parentKey}
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutBlock:render():[INFO] Timeline Server Overview
org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close():[DEBUG] File deleted in BlockUploadData close: {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] Service ACLs refreshed
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:scanForLogs():[DEBUG] scanForLogs on {appDirPath}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Skip killing
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat):[WARN] Lock monitoring failed because session was lost
org.apache.hadoop.tools.HadoopArchiveLogs:checkFilesAndSeedApps(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Skipping [appName] due to not having enough log files ([files.length] < [minNumLogFiles])
org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket):[TRACE] this: NotificationHandler: doing a read on sock.fd
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:process():[WARN] The cleaner task was interrupted. Aborting.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:copyBlockToLazyPersistLocation(java.lang.String,long,long,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,int,org.apache.hadoop.conf.Configuration):[WARN] LazyWriter failed to create {lazyPersistDir}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner:run():[DEBUG] {}: finishing cache cleaner run started at {}. Demoted {} mmapped replicas; purged {} replicas.
org.apache.hadoop.hdfs.server.datanode.DNConf:initBlockReportDelay():[INFO] DFS_BLOCKREPORT_INITIAL_DELAY_KEY is greater than or equal to DFS_BLOCKREPORT_INTERVAL_MSEC_KEY. Setting initial delay to 0 msec.
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans():[DEBUG] MBean for source registered.
org.apache.hadoop.yarn.util.AbstractLivelinessMonitor$PingChecker:run():[INFO] getName thread interrupted
org.apache.hadoop.oncrpc.SimpleTcpClientHandler:channelActive(io.netty.channel.ChannelHandlerContext):[DEBUG] sending PRC request
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity):[DEBUG] Verifying the access of user on the timeline domain
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeDirectorySection(java.io.InputStream):[WARN] Failed to add the inode {} to the directory {}
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed to get FileSystem for registry, e
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppActivities(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.util.Set,java.util.Set,java.lang.String,java.lang.String,java.util.Set,boolean):[ERROR] Cannot find application with given appId
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[WARN] Application already registered
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameTo(java.lang.String,java.lang.String,boolean):[WARN] Access denied for rename (options=[…]) src dst
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:takeAndProcessTasks():[INFO] Exception when processing re-encryption task for zone {zoneId}, retrying...
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[DEBUG] drain or abort reason {} remaining={} abort={}
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkStreamers():[DEBUG] checkStreamers: ... <!-- actual content -->
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Job %s setting up
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[INFO] BLOCK* registerDatanode: from ...
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:logException(java.lang.String,java.lang.Throwable):[ERROR] comment, t
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:close():[DEBUG] Closed {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[INFO] Container y not launched. Not sending the signal
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] {} post complete, containerId
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:toResponse(java.lang.Exception):[TRACE] GOT EXCEPITION, e
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] {} reported unusable
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit):[WARN] Thread interrupted while trying to acquire READ lock
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Can't get path for dirHandle: {}
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Creating a new file: [{}] in COS.
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:main(java.lang.String[]):[INFO] HttpFSServerWebServer initialized
org.apache.hadoop.fs.s3a.S3AFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Initializing S3AFileSystem for {}
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:serviceStop():[WARN] Could not wait for the thread to join
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String):[INFO] Waiting for application to be successfully unregistered.
org.apache.hadoop.hdfs.DFSInputStream:closeCurrentBlockReaders():[ERROR] error closing blockReader
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop():[DEBUG] RPC client proxy stopped
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementForUser(java.lang.String):[DEBUG] Creating placement context for user {} using primary group current user mapping
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:init():[INFO] Using FileSystemAccess Kerberos authentication, principal {} keytab {}
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[INFO] User ACL Info for Queue
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] SQLException closing resultset: [details]
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path):[INFO] Saved output of task '${attemptId}' to ${outputPath}
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unable to record log deleter state
org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]):[ERROR] Aborted
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:handleSyncableInvocation():[DEBUG] Downgrading Syncable call
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:handleTaskAbort(org.apache.hadoop.mapreduce.v2.app.commit.CommitterTaskAbortEvent):[WARN] Task cleanup failed for attempt + event.getAttemptID(), e
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Removing state for app + removeAppId
org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll():[INFO] Fail closing ViewFileSystem's child filesystem
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[ERROR] App: + appID + can't handle this event at current state
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[INFO] Job instance created
org.apache.hadoop.mapred.ResourceMgrDelegate:getBlacklistedTrackers():[WARN] getBlacklistedTrackers - Not implemented yet
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] BLOCK* block DELETED_BLOCK: block {} is received from {}
org.apache.hadoop.hdfs.tools.DFSck:doWork(java.lang.String[]):[ERROR] Connecting to namenode via [url]
org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration):[INFO] Configuring filter for proxies
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:canAssign(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[INFO] Queue activity recorded
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Created a new BR lease 0x{Long.toHexString(node.leaseId)} for DN {dn.getDatanodeUuid()}. numPending = {numPending}
org.apache.hadoop.hdfs.StripedDataStreamer:nextBlockOutputStream():[WARN] Excluding datanode
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RecoverTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[DEBUG] Recovering task information
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:fitsInMaxShare(org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Resource usage plus resource request: usagePlusAddition exceeds maximum resource allowed: getMaxShare in queue getName
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable):[DEBUG] Error , cause
org.apache.hadoop.yarn.service.timelineservice.ServiceMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord):[DEBUG] Publishing service metrics. {}
org.apache.hadoop.hdfs.server.namenode.FSDirectory:updateCountForQuota(int):[INFO] Quota update count info
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader:getSelectQuery():[DEBUG] Using query: ...
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getNamenodesSubcluster(org.apache.hadoop.hdfs.server.federation.store.MembershipStore):[ERROR] Cannot get Namenodes from the State Store
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:copyBlocks(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] Disk Balancer - Unable to support transient storage type.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:run():[DEBUG] Dumper checking OpenFileCtx activeState: {} enabledDump: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recoverApplication(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto):[INFO] Recovering application {}
org.apache.hadoop.lib.server.Server:destroy():[INFO] Server [{}] shutdown!, name
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.PlanningAlgorithm:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Updated reservation for periodic allocation
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[]):[TRACE] HTTP {op.getValue().getType()}: {op}, {path}, ugi={ugi}, {username}, {doAsUser} {Param.toSortedString(", ", parameters)}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanUpApplicationsOnNMShutDown():[INFO] All applications in FINISHED state
org.apache.hadoop.hdfs.server.datanode.DataStorage:removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList):[WARN] Unexpectedly short length on {}.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:commit(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS COMMIT fileHandle: [handle] offset=[offset] count=[count] client: [remoteAddress]
org.apache.hadoop.hdfs.server.balancer.Balancer:getInt(org.apache.hadoop.conf.Configuration,java.lang.String,int):[INFO] key = v (default=defaultValue)
org.apache.hadoop.mapreduce.JobSubmitter:printTokens(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.security.Credentials):[INFO] Executing with tokens: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendLaunchEvent():[INFO] Container Resumed as some resources freed up
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.commons.logging.Log):[INFO] createStartupShutdownMessage
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Recovering storage directory {} from previous upgrade
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:waitExecutorTerminated(java.util.concurrent.ExecutorService):[DEBUG] Waiting to executor service terminated duration {}ms.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] IP already in device "{allowed.get(i).getAliasDevName()},{majorMinorNumber}", skip reprogramming
org.apache.hadoop.hdfs.qjournal.server.Journal:startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int):[INFO] Updating lastWriterEpoch from ...
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Incremented storage statistics OpCounter
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:writeStringTableSection():[TRACE] Writing string table entry: {entry details here}
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys():[DEBUG] Calling with Connection
org.apache.hadoop.registry.server.dns.RegistryDNSServer:launchDNSServer(org.apache.hadoop.conf.Configuration,org.apache.hadoop.registry.server.dns.RegistryDNS):[ERROR] Error starting Registry DNS Server, t
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Error during set operation type
org.apache.hadoop.hdfs.server.common.Storage:nativeCopyFileUnbuffered(java.io.File,java.io.File,boolean):[DEBUG] Failed to preserve last modified date from '{srcFile}' to '{destFile}'
org.apache.hadoop.yarn.server.nodemanager.NodeManager:reregisterCollectors():[DEBUG] Remove collector data for done app {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using ResourceCalculatorPlugin: {}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:deleteCorruptedFile(java.lang.String):[INFO] Fsck: deleted corrupt file
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:deleteTaskAttemptPathQuietly(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Delete task attempt path
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder,java.lang.String):[TRACE] Fetch SAS token for {operation} on {path}
org.apache.hadoop.mapred.gridmix.Gridmix:start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver):[DEBUG] Startup failed
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Error in handling event type + event.getType() + for application + appID, t
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token):[DEBUG] Token not set, looking for delegation token. Creds:{}, size:{}
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:addReport(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.protocol.OutlierMetrics):[TRACE] Adding slow peer report is disabled. To enable it, please enable config {}.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[WARN] Exception
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:handleVolumeFailures(java.util.Set):[DEBUG] Handling volume failures
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(java.lang.Object,java.lang.Object):[INFO] Storing CA Certificate and Private Key
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:writeRegistry(org.apache.hadoop.registry.client.api.RegistryOperations,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String,boolean):[ERROR] Registry write key {key} failed
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:cleanupStagingDirs():[INFO] MagicS3GuardCommitter staging dirs cleaned
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[WARN] Unable to submit the application {} to SubCluster {}
org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String):[INFO] {percentage}% max memory {StringUtils.TraditionalBinaryPrefix.long2String(maxMemory, "B", 1)} = {StringUtils.TraditionalBinaryPrefix.long2String((long) percentMemory, "B", 1)}
org.apache.hadoop.yarn.util.resource.ResourceUtils:addResourceTypeInformation(java.lang.String,java.lang.String,java.util.Map):[DEBUG] Setting value for resource type [resourceType] to [resourceValue] with units [units]
org.apache.hadoop.registry.server.dns.RegistryDNS:signSiteRecord(org.xbill.DNS.Zone,org.xbill.DNS.Record):[INFO] Adding {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateHeartBeatConfiguration(org.apache.hadoop.conf.Configuration):[WARN] Heartbeat scaling factors must be ≥ 0 SpeedupFactor: [heartBeatIntervalSpeedupFactor] SlowdownFactor: [heartBeatIntervalSlowdownFactor]. Using Defaults
org.apache.hadoop.hdfs.server.namenode.FSImage:renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile):[WARN] Unable to rename checkpoint in directory
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:rewind():[WARN] Exception in getting reader from provided alias map
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:run():[INFO] Processing the event TASK_ABORT
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerResourceUpdated(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] onContainerResourceUpdated: {}, {}
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:updateLayoutVersion(int,long):[INFO] Updating edits cache to use layout version {newLayoutVersion} starting from txn ID {newStartTxn}; previous version was {layoutVersion}; old entries will be cleared.
org.apache.hadoop.yarn.server.resourcemanager.blacklist.SimpleBlacklistManager:getBlacklistUpdates():[WARN] Ignoring Blacklists, blacklist size currentBlacklistSize is more than failure threshold ratio blacklistDisableFailureThreshold out of total usable nodes numberOfNodeManagerHosts
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Error when reading history information of some containers of application attempt [appAttemptId]
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitShm(java.lang.String):[WARN] Failed to shut down socket in error handler, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Skip recovering container for unknown SchedulerApplication. Application current state is [state]
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid SYMLINK request
org.apache.hadoop.util.Shell:checkIsBashSupported():[INFO] Bash execution is not allowed by the JVM security manager.Considering it not supported.
org.apache.hadoop.hdfs.server.namenode.NameNode:parseArguments(java.lang.String[]):[ERROR] Must specify a valid cluster ID after the CLUSTERID flag
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionCancelUpgrade(java.lang.String):[INFO] Cancel upgrade in progress. Please wait..
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:syncBlock(java.util.List):[DEBUG] syncBlock for block {}, all datanodes don't have the block or their replicas have 0 length. The block can be deleted.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:convert(org.apache.hadoop.conf.Configuration):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.AdminService:mapAttributesToNodes(org.apache.hadoop.yarn.server.api.protocolrecords.NodesToAttributesMappingRequest):[INFO] Map Attributes to Nodes operation initiated
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[INFO] The machine list file path is not specified in the configuration
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Application ID doesn't exist for service {serviceName}
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler):[INFO] Save namespace ...
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:getBoundOrNewDT(org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[DEBUG] Delegation token requested
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:allocate(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] There is no available memory: + resource.getMemorySize() + in numa nodes for + containerId
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext):[INFO] {} configured to be {}, should be positive. Using default of {}.
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:getSplits(org.apache.hadoop.conf.Configuration,int,long):[DEBUG] Creating split : + split + , bytes in split: + currentSplitSize
org.apache.hadoop.crypto.key.kms.server.KMS:getCurrentVersion(java.lang.String):[TRACE] Exiting getCurrentVersion method.
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] CachedHistoryStorage Init
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] User rule: parent rule result: {parent.getQueue()}
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Cannot rename source file: [{}] to dest file: [{}], because the file already exists.
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processJobLine(org.apache.hadoop.tools.rumen.ParsedLine):[WARN] HadoopLogsAnalyzer.processJobLine: bad numerical format, at line
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Filesystem {} defined in configuration option
org.apache.hadoop.fs.cosn.CosNInputStream:reopen(long):[WARN] An interrupted exception occurred when waiting a read buffer.
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[INFO] Container {} was marked as inactive. Returning terminated error
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Operation type set
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:abort():[WARN] Unable to abort multipart upload, you may need to purge uploaded parts
org.apache.hadoop.yarn.sls.SLSRunner:startAMFromSynthGenerator():[WARN] Warning: reset job {} start time to 0.
org.apache.hadoop.hdfs.qjournal.server.Journal:startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int):[WARN] Client is requesting a new log segment ...
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] DelegateToFileSystem listStatus call invoked
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path):[DEBUG] Could not flush Keystore.. + attempting to reset to previous state !!
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToActive():[INFO] Already in active state
org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String):[INFO] createSuccessLog(user, operation, target)
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[DEBUG] Processing {} of type {}
org.apache.hadoop.fs.s3a.S3AFileSystem:createFakeDirectoryIfNecessary(org.apache.hadoop.fs.Path):[DEBUG] Creating new fake directory at {}
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:reencryptEdeks():[WARN] Failed to re-encrypt one batch of {} edeks, start:{}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[ERROR] Error in updating persisted RMDelegationToken with sequence number: sequence_number_value
org.apache.hadoop.hdfs.DFSStripedOutputStream:enqueueCurrentPacketFull():[DEBUG] enqueue full {}, src={}, bytesCurBlock={}, blockSize={}, appendChunk={}, {}
org.apache.hadoop.hdfs.DFSInputStream:tryReadZeroCopy(int,java.util.EnumSet):[DEBUG] Reducing read length from {maxLength} to {length63} to avoid going more than one byte past the end of the block. blockPos={blockPos}; curPos={curPos}; curEnd={curEnd}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean):[DEBUG] (isUpdate ? Updating : Storing ) + ZKDTSMDelegationToken_ + ident.getSequenceNumber()
org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper():[INFO] Auth info added
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAcl(java.lang.String):[ERROR] Access Control Exception: removeAcl failed
org.apache.hadoop.hdfs.server.namenode.FSDirConcatOp:unprotectedConcat(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile[],long):[DEBUG] NameSystem.concat to {}
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:remove(java.io.File):[INFO] Removing block level storage: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:executeStage(java.lang.Void):[INFO] {}: Committing task \"{}\"
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render():[ERROR] User [remoteUser] is not authorized to view the logs for [logEntity]
org.apache.hadoop.mapred.TaskStatus:setFinishTime(long):[ERROR] Trying to set finish time for task + taskid + when no start time is set, stackTrace is : + StringUtils.stringifyException(new Exception())
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:run():[ERROR] Error occurred while aggregating the log for the application
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreHeartbeat:run():[WARN] Exception when trying to heartbeat:
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose():[DEBUG] Block {}: Buffer file {} exists —close upload stream
org.apache.hadoop.util.LogAdapter:warn(java.lang.String,java.lang.Throwable):[WARN] Logger warning with msg and throwable
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Authentication exception: [EXCEPTION_MESSAGE]
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[DEBUG] Securing socket communication.
org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object):[DEBUG] val + is a zero-length value
org.apache.hadoop.mapreduce.CryptoUtils:wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream):[DEBUG] IV read from Stream [ + Base64.encodeBase64URLSafeString(iv) + ]
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:trimWriteRequest(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx,long):[DEBUG] Trim request [offset-(offset + count)), current offset currentOffset, drop the overlapped section [offset-currentOffset) and write new data [currentOffset-(offset + count))
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:assignContainersToChildQueues(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Decrease parentLimits {limit} for {thisQueuePath} by {resourceToSubtract} as childQueue={childQueuePath} is blocked
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[DEBUG] Forwarding allocate request to the real YARN RM
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[ERROR] Application Master is trying to unregister before registering for: appId
org.apache.hadoop.mapred.YarnChild:reportError(java.lang.Exception,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[ERROR] Fast fail the job because the cluster storage capacity was exceeded.
org.apache.hadoop.fs.s3a.impl.DirMarkerTracker:removeAllowedMarkers(org.apache.hadoop.fs.s3a.impl.DirectoryPolicy):[DEBUG] Removing {}
org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]):[INFO] ===file===
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[ERROR] The job trace is empty
org.apache.hadoop.yarn.service.ServiceManager:handle(org.apache.hadoop.yarn.service.ServiceEvent):[ERROR] [SERVICE]: Invalid event {1} at {2}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:setActionForProperty(java.lang.String):[INFO] No rule set for {}, defaulting to WARNING
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:deleteFileWithRetries(org.apache.hadoop.fs.Path):[INFO] Deleting file with retries
org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Looking for a token with service
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:createAMRMProxyService(org.apache.hadoop.conf.Configuration):[INFO] AMRMProxyService is enabled. All the AM->RM requests will be intercepted by the proxy
org.apache.hadoop.tools.DistCh:run(java.lang.String[]):[ERROR] NAME + failed:, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] capacityConfigType is '{}' for queue {}
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,long):[DEBUG] DIR* FSDirectory.unprotectedDelete: " + iip.getPath() + " is removed
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:getLogAggPolicyInstance(org.apache.hadoop.conf.Configuration):[WARN] this.appId specified invalid log aggregation policy className
org.apache.hadoop.fs.azure.NativeAzureFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.azure.SelfRenewingLease):[DEBUG] Creating file: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:recoverAssignedGpus(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Finished recovery of GpuDevice for {}.
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker:logRecoverBlock(java.lang.String,org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock):[INFO] BlockRecoveryWorker: <who> calls recoverBlock(<block>, targets=[<targets>], newGenerationStamp=<newGenerationStamp>, newBlock=<newBlock>, isStriped=<isStriped>)
org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping():[INFO] Using/Reloading 'staticMappingFile' for static UID/GID mapping...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] requested offset={} and current filesize={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:logContainerStatus(java.lang.String,org.apache.hadoop.yarn.api.records.ContainerStatus):[INFO] ContainerStatus: [ContainerId: ..., ExecutionType: ..., State: ..., Capability: ..., Diagnostics: ..., ExitStatus: ..., IP: ..., Host: ..., ExposedPorts: ..., ContainerSubState: ...]
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateHeartBeatConfiguration(org.apache.hadoop.conf.Configuration):[WARN] HeartBeat interval: [nextHeartBeatInterval] must be greater than 0, using default.
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage):[WARN] Unable to rename edits file from + tmpFile + to + finalizedFile
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[INFO] rc: {rc}
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherThread:run():[ERROR] Failed to refresh mount table entries cache at router {}, adminAddress, e
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:createCgroup(java.lang.String,java.lang.String):[DEBUG] createCgroup: {}
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop():[INFO] {Thread.currentThread().getName()}: connection aborted from {key.attachment()}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS CREATE dir fileHandle: {} filename: {} client: {}
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node):[INFO] Removing a node: + NodeBase.getPath(node)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager$CollectorTokenRenewer:run():[INFO] Cannot find active collector while renewing/regenerating token for appId
org.apache.hadoop.hdfs.DFSStripedInputStream:refreshLocatedBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock):[DEBUG] refreshLocatedBlock for striped blocks, offset= ...
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readOneBlock(byte[],int,int):[DEBUG] created new buffer size {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:moveToDone():[DEBUG] moveToDone: + historyFile
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:markSuspectBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] {}: Not scheduling suspect block {} for rescanning, because we rescanned it recently.
org.apache.hadoop.net.DNS:resolveLocalHostIPAddress():[WARN] Unable to determine address of the host -falling back to 'localhost' address
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getAdditionalBlock(java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],java.util.EnumSet):[DEBUG] BLOCK* getAdditionalBlock: {} inodeId {} for {}
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:init(javax.servlet.FilterConfig):[INFO] adminAclList not set, hence setting it to ""
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[DEBUG] Setting input format
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleInitApplicationResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application):[DEBUG] Created application tracking structs for app: {appIdStr}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[DEBUG] Container allocated on a single node
org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto):[WARN] Unknown request source: + proto.getReqSource()
org.apache.hadoop.mapred.LocalJobRunner$Job:createMapExecutor():[DEBUG] Max local threads: {{maxMapThreads}}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Failed to load local runC image to hash file. Config not set
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getNamenodesSubcluster(org.apache.hadoop.hdfs.server.federation.store.MembershipStore):[ERROR] Cannot get address for {}: {}
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[ERROR] Audit failure, incrementing AUDIT_FAILURE counter
org.apache.hadoop.fs.s3a.S3AInputStream:reopen(java.lang.String,long,long,boolean):[DEBUG] reopen({}) for {} range[{}-{}], length={}, streamPosition={}, nextReadPosition={}, policy={}
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:updateStateStore():[WARN] Cannot heartbeat router {}
org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp:addSymlink(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean):[DEBUG] addSymlink: [path] is added
org.apache.hadoop.hdfs.NameNodeProxiesClient:createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean):[DEBUG] Couldn't create proxy provider null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:safeCreateZkData(java.lang.String,byte[]):[WARN] NODEEXISTS_MSG
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeReservationState(java.lang.String,java.lang.String):[DEBUG] Removing reservationallocation {reservationIdName} for plan {planName}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:checkForCompletedNodes(java.util.List):[DEBUG] Node {} is currently in maintenance
org.apache.hadoop.fs.s3a.S3AFileSystem:createRequestFactory():[WARN] Configuration property UPLOAD_PART_COUNT_LIMIT shouldn't be overridden by client
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:serviceDependencySatisfied(org.apache.hadoop.yarn.service.api.records.Service):[WARN] Caught exception:
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[]):[INFO] Placement Spec received
org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget):[INFO] Should fence: + target
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread:doWork():[INFO] Checkpoint finished successfully.
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean):[DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {} added as corrupt on {} by {} {}
org.apache.hadoop.hdfs.server.federation.router.Router:serviceStart():[INFO] Router is running now
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Unknown event arrived at OpportunisticContainerAllocatorAMService: {event}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:deleteDir(org.apache.hadoop.fs.Path):[ERROR] Unable to remove path with exception
org.apache.hadoop.mapreduce.lib.db.DBSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[INFO] Splitting using IntegerSplitter
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:isStale():[TRACE] {} is stale because it's {} ms old and staleThreadholdMS={}
org.apache.hadoop.yarn.service.ClientAMService:restart(org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto):[INFO] Restart service by {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:rescanPostponedMisreplicatedBlocks():[DEBUG] BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block {}, result is {}
org.apache.hadoop.crypto.key.kms.server.KMS:getCurrentVersion(java.lang.String):[DEBUG] Getting key version for key with name {}., name
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:submit():[INFO] JobName got an error while submitting, ioe
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:start():[WARN] [{}] {} Start {}
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:init():[INFO] StreamBaseRecordReader.init: start_=start_ end_=end_ length_=length_ start_ > in_.getPos() = (start_ > in_.getPos()) start_ > in_.getPos
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:moveToDone():[DEBUG] Move no longer pending
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEvent):[WARN] Move Application has failed: errorMessage
org.apache.hadoop.streaming.PipeMapRed$MROutputThread:run():[WARN] "{}" + outerrThreadsThrowable
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler:failTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.FailTaskAttemptRequest):[INFO] Fail task attempt TASK_ATTEMPT_ID received from USER_NAME at REMOTE_ADDRESS
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:waitForAckHead(long):[DEBUG] {}: seqno={} waiting for local datanode to finish write.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeDefaultAcl(java.lang.String):[ERROR] Audit event failed: AccessControlException
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path):[WARN] No Output found for ${attemptId}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubClusters(org.apache.hadoop.yarn.server.federation.store.records.GetSubClustersInfoRequest):[INFO] SubCluster info validated and added
org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean):[WARN] Failed to delete file or dir [ + f.getAbsolutePath() + ]: it still exists.
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get():[DEBUG] error prefetching block {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processOverWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.security.IdMappingServiceProvider):[WARN] Treat this jumbo write as a real random write, no support.
org.apache.hadoop.hdfs.DFSClient:getDelegationToken(java.lang.String):[INFO] Created [token]
org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] CostProvider not specified, defaulting to DefaultCostProvider
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod(org.apache.hadoop.security.UserGroupInformation,java.util.List,java.lang.Class,java.lang.reflect.Method,java.lang.Object[]):[ERROR] No namenode available to invoke...
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:cleanupExpiredQueues(java.lang.String,boolean,java.util.Set,java.lang.String):[INFO] Queue: {} removed
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Stopping decommissioning of {} node {} [node.isAlive() ? "live" : "dead"]
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveAllKeys(java.io.DataOutputStream,java.lang.String):[INFO] Begin step SAVING_CHECKPOINT
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[INFO] Namespace ID found
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loaded delegation key: keyId={}, expirationDate={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationOnRecovery(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] FATAL: Queue named queueName is no longer a leaf queue during application recovery. Changing a leaf queue to a parent queue during recovery is not presently supported by the capacity scheduler.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getECTopologyResultForPolicies(java.lang.String[]):[INFO] Audit event: getECTopologyResultForPolicies
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Completing previous upgrade for storage directory {}
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean):[ERROR] Error getting metrics from source + name, e
org.apache.hadoop.mapreduce.lib.db.DBOutputFormat$DBRecordWriter:close(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] {warning message based on warn method usage}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[INFO] ignore signal command x
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:abort():[DEBUG] Ignoring abort() as stream is already closed
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$AddNodeTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[ERROR] Node health check failed
org.apache.hadoop.mapred.pipes.PipesReducer:close():[INFO] waiting for finish
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[INFO] Session connected.
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] Unrecognized attribute value for HADOOP_SECURITY_AUTHENTICATION of {}
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Scan complete: shutting down
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Pending: countHere
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler):[ERROR] NameNode process will exit now... The saved FsImage <nnf> is potentially corrupted.
org.apache.hadoop.net.TableMapping$RawTableMapping:load():[WARN] Line does not have two columns. Ignoring. {line}
org.apache.hadoop.hdfs.DFSClient:createWrappedOutputStream(org.apache.hadoop.hdfs.DFSOutputStream,org.apache.hadoop.fs.FileSystem$Statistics):[DEBUG] Start decrypting EDEK for file: {dfsos.getSrc()}, output stream: 0x{Integer.toHexString(dfsos.hashCode())}
org.apache.hadoop.yarn.service.provider.ProviderFactory:createServiceProviderFactory(org.apache.hadoop.yarn.service.api.records.Artifact):[DEBUG] Loading service provider type default
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter:run():[WARN] Ignoring exception in LazyWriter:
org.apache.hadoop.mapred.ClientServiceDelegate:getProxy():[INFO] Job is running, but the host is unknown. Verify user has VIEW_JOB access.
org.apache.hadoop.util.HostsFileReader:readFileToSetWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Set):[INFO] Adding a node "node" to the list of hosts from filename
org.apache.hadoop.yarn.server.resourcemanager.AdminService:updateNodeResource(org.apache.hadoop.yarn.server.api.protocolrecords.UpdateNodeResourceRequest):[INFO] Update resource on node({nodeId}) with resource({newResourceOption.toString()})
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[TRACE] Exception interrupting DataXceiverServer
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:render():[INFO] Rendering FifoScheduler Queue with QueueInfoBlock
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:deleteSavedFiles():[WARN] Failed to delete block file
org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int):[WARN] Exception occurred: ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:saveDfsUsed():[WARN] Failed to write dfsUsed to ...
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleUpdatedNodes(org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[INFO] Killing taskAttempt:tid because it is running on unusable node:taskAttemptNodeId
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:run():[INFO] Processing the event
org.apache.hadoop.hdfs.server.namenode.FSImage:saveFSImage(org.apache.hadoop.hdfs.server.namenode.SaveNamespaceContext,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile):[ERROR] Detected errors while saving FsImage
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[INFO] Creating password for identifier: [formatted_identifier], currentKey: [currentKeyId]
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[DEBUG] Remote directory retrieved
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:logApplicationSummary(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application summary logged
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:getMaxShare():[WARN] String.format(Queue %s has max resources %s less than + min resources %s, getName(), maxResource, minShare)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext):[WARN] Exit code from container {locId} startLocalizer is : {exitCode}
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:doSpnegoSequence(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Using subject: ...
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] Failed to choose from local rack (location = {}); the second replica is not found, retry choosing randomly
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[DEBUG] {applicationAttemptId} is recovering. Skipping notifying ATTEMPT_ADDED
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is balancedSpaceTolerance, Default value DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_RACK_FAULT_TOLERANT_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_DEFAULT will be used instead.
org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration):[ERROR] ShutdownHookManager shutdown forcefully after {} seconds.
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:evictOldStartTimes(long):[INFO] Deleted {startTimesCount}/{totalCount} start time entities earlier than {minStartTime}
org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier):[DEBUG] Creating password for {} for user {} to be run on NM {}
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] SliderFileSystem initialized
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo):[INFO] Replica state finalized, adding stored block
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Probe for needsTaskCommit({context.getTaskAttemptID()})
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown in handler for event [REINITIALIZE_CONTAINER] for Container
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[DEBUG] Inode {} existing edek changed, skipping re-encryption
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] Native-Task doesn't support secure shuffle
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader:createBlockReader(long):[INFO] Exception while creating remote block reader, datanode {source}
org.apache.hadoop.fs.FileSystem:debugLogFileSystemClose(java.lang.String,java.lang.String):[TRACE] FileSystem.{}() full stack trace:
org.apache.hadoop.fs.cosn.CosNFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] owner:<actual_owner>, group:<actual_group>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:editSchedule():[DEBUG] Total time used= + (clock.getTime() - startTs) + ms.
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:put(java.lang.Object,java.lang.Object):[ERROR] [exception message]
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:computeMove(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume):[DEBUG] Next Step: {}, nextStep
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[DEBUG] Inode {} EZ key changed, skipping re-encryption.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:transition(java.lang.Object,java.lang.Object):[INFO] Rolling back Container reInitialization for [container.getContainerId()] !!
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:serviceStart():[INFO] Starting SchedulingMonitor= + getName()
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] Remove write {} which is already written from the list
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:getNMCollectorService():[INFO] nmCollectorServiceAddress:
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:initReplicaRecoveryImpl(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long):[INFO] initReplicaRecovery: changing replica state for + block + from + replica.getState() + to + rur.getState()
org.apache.hadoop.minikdc.MiniKdc:stop():[INFO] MiniKdc stopped.
org.apache.hadoop.tools.dynamometer.Client:monitorInfraApplication():[WARN] Interrupted while joining workload job thread; continuing to cleanup.
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSync(long):[DEBUG] logSync
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:selectInputStreams(java.util.Collection,long,boolean):[DEBUG] this: selecting input streams starting at fromTxId (inProgress ok) from among elfs.size() candidate file(s)
org.apache.hadoop.yarn.service.ClientAMService:stop():[INFO] Stop the service by {}
org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run():[DEBUG] Current time is {}, next refresh is {}
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:run():[ERROR] ReplayThread encountered exception; exiting.
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Dead node {} is decommissioned immediately.
org.apache.hadoop.mapreduce.util.ProcessTree:isProcessGroupAlive(java.lang.String):[WARN] Error executing shell command + shexec.toString() + ioe
org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[ERROR] Cannot build PowerShell script
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:syncWithJournalAtIndex(int):[INFO] Syncing Journal...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer):[DEBUG] Trying to allocate from reserved container in async scheduling mode
org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception):[DEBUG] <log message, ex>
org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl:createServer(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,int,org.apache.hadoop.thirdparty.protobuf.BlockingService,java.lang.String):[INFO] Adding protocol + pbProtocol.getCanonicalName() + to the server
org.apache.hadoop.fs.s3a.S3AFileSystem:doBucketProbing():[INFO] DNS address
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:getTotalResourceUsagePerUser(java.lang.String):[WARN] User ' + userName + ' is not present in active/non-active. This is highly unlikely. + We can consider this user in non-active list in this case.
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getOwnerForPath(org.apache.hadoop.fs.Path):[DEBUG] Cannot find file/folder - '{}'. Returning owner as empty string
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Checking superuser privilege
org.apache.hadoop.hdfs.qjournal.server.Journal:prepareRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long):[INFO] Prepared recovery for segment {segmentTxId}: {TextFormat.shortDebugString(resp)} ; journal id: {journalId}
org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String):[INFO] Setting the excludes file to [excludesFile]
org.apache.hadoop.yarn.client.api.YarnClient:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext):[ERROR] Application not found
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Audit log for success: [user: {user}, operation: {operation}, target: {target}]
org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress):[WARN] Failed to get token for service
org.apache.hadoop.mapred.ShuffleHandler:serviceStart():[INFO] ShuffleHandler listening on port <port>
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:reencryptEncryptionZone(long):[INFO] Re-encrypting zone {}(id={})
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:appAdminClientCleanUp(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl):[INFO] Type-specific cleanup of application {app.applicationId} of type {app.applicationType} succeeded
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:checkLimit(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$ProcessTreeInfo,long,long):[WARN] {msg}
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer):[DEBUG] Done
org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver:run():[INFO] FSImageSaver clean checkpoint: txid={} when meet + Throwable., context.getTxId()
org.apache.hadoop.util.ShutdownHookManager:executeShutdown():[WARN] ShutdownHook 'SimpleName' timeout, ex.toString(), ex
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric name + name +, value + value + has no type.
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:createProcessInfo(java.lang.String):[DEBUG] Error parsing procInfo.
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:closeInMemoryFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput):[INFO] Starting inMemoryMerger's merge since commitMemory= ... > mergeThreshold= ... . Current usedMemory= ...
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache$FlushTimerTask:run():[DEBUG] {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$KillAllocatedAMTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] Event handler called
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[INFO] Status: OK
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[INFO] Running new API mapper
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long):[INFO] Recover RBW replica
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] It is not allowed to rename a parent directory: [{}] to its subdirectory: [{}]
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handleExcludeNodeList(boolean,int):[INFO] Forcefully decommission node with state not DECOMMISSIONED
org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable:run():[INFO] Starting task: <reduceId>
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:get(java.lang.Class):[ERROR] Cannot create record type "{}" from "{}": {}
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:createTaskCommitter(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:channelOpen(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent):[INFO] Current number of shuffle connections (%d) is greater than or equal to the max allowed shuffle connections (%d)
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processReduceAttemptLine(org.apache.hadoop.tools.rumen.ParsedLine):[WARN] A map attempt status you don't know about is ...
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator:main(java.lang.String[]):[ERROR] Error while creating Timeline Schema : , e
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx):[DEBUG] Trigger the write back task. Current nextOffset: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:render():[INFO] Rendering FifoScheduler Queue with default queue
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaNodeResourceUpdateHandler:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[INFO] Initializing configured FPGA resources for the NodeManager.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean):[INFO] Assigned container + container.getId() + of capacity + container.getResource() + on host + getRMNode().getNodeAddress() + ", which has " + getNumContainers() + " containers, " + getAllocatedResource() + " used and " + getUnallocatedResource() + " available after allocation
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[DEBUG] Rejecting appliance of allocation due to existing pending allocation request for schedulerContainer
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:serviceStart():[INFO] Instantiating NMWebApp at [bindAddress]
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteUnnecessaryFakeDirectories(org.apache.hadoop.fs.Path):[DEBUG] While deleting keys {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String):[INFO] Found script: envBinaryPath
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNodesImpl(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[ERROR] Cannot get {} nodes
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$SetupCompletedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[DEBUG] Job event handled
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:cleanOldLogs(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.security.UserGroupInformation):[ERROR] Failed to clean old logs
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:isStale():[TRACE] {} is not stale because it's only {} ms old and staleThresholdMs={}
org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock(org.apache.hadoop.hdfs.server.datanode.BPOfferService,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[WARN] Cannot find FsVolumeSpi to report bad block: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:removeFromCluserNodeLabels(java.util.Set,javax.servlet.http.HttpServletRequest):[INFO] Node labels removed from cluster
org.apache.hadoop.mapreduce.security.TokenCache:mergeBinaryTokens(org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration):[INFO] No binary token filename provided
org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps():[INFO] Update cache now
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[WARN] Failed to place enough replicas...
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setErasureCodingPolicy(java.lang.String,java.lang.String,boolean):[WARN] Audit event failure: AccessControlException for operation: setErasureCodingPolicy on srcArg
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error removing attempt: + attemptId, e
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:cleanLogs(org.apache.hadoop.fs.Path,long):[INFO] Application log directory cleaned
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[WARN] Unexpected exception in block pool this
org.apache.hadoop.fs.s3a.tools.MarkerTool:scan(org.apache.hadoop.fs.Path,boolean,int,int,int,org.apache.hadoop.fs.s3a.impl.DirectoryPolicy):[DEBUG] marker scan %s
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:initRollingLevelDB(java.lang.Long,org.apache.hadoop.fs.Path):[INFO] Added rolling leveldb instance {dbName} to {getName()}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing ZKDTSMDelegationKey_{key.getKeyId()}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for fileId: {}
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier):[INFO] Waiting in main loop.
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:reportNodeUnusable(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.api.records.NodeState):[DEBUG] Handling NodesListManagerEvent
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadIndexedLogsMeta(org.apache.hadoop.fs.Path,long,org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Some warning message
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.CopyObjectRequest,java.lang.String):[DEBUG] No version ID to use as a constraint
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getListing(java.lang.String,byte[],boolean):[INFO] Audit event for listStatus operation denied on src
org.apache.hadoop.hdfs.server.datanode.DataNode:getDiskBalancerStatus():[DEBUG] Reading diskbalancer Status failed.
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Using Committer {} for {}, outputCommitter, outputPath
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:isParentZnodeSafe(java.lang.String):[ERROR] Mismatched cluster! The other RM seems to be from a different cluster. Current cluster = clusterId Other RM's cluster = proto.getClusterId()
org.apache.hadoop.hdfs.server.datanode.BPOfferService:updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat):[WARN] NN actor tried to claim ACTIVE state at txid= txid but there was already a more recent claim at txid= lastActiveClaimTxId
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Rejecting recoverTask({}) call
org.apache.hadoop.hdfs.DFSOutputStream:enqueueCurrentPacketFull():[DEBUG] enqueue full {}, src={}, bytesCurBlock={}, blockSize={}, appendChunk={}, {}, currentPacket, src, getStreamer().getBytesCurBlock(), blockSize, getStreamer().getAppendChunk(), getStreamer()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing master key {}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:parseAclsWithPrefix(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType,java.util.Map):[INFO] {} for KEY_OP '{}' is set to '*'
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:handleVolumeFailures(java.util.Set):[ERROR] Unexpected IOException
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:checkVersion():[INFO] Loaded timeline store version info
org.apache.hadoop.tools.CommandShell:run(java.lang.String[]):[ERROR] Exception occurred
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post called for UNSETSTORAGEPOLICY
org.apache.hadoop.fs.s3a.S3AFileSystem:incrementPutProgressStatistics(java.lang.String,long):[DEBUG] PUT {}: {} bytes
org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse():[DEBUG] {connectionName} got value #{callId}
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:doCopy(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet):[INFO] Renaming temporary target file path {targetPath} to {target}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:getRemoteUgi():[WARN] Cannot obtain the user-name. Got exception: ...
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectoryMultiThreaded():[DEBUG] Starting thread pool of {} listStatus workers.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:addApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User):[INFO] Application added - appId: application.getApplicationId() user: application.getUser() leaf-queue: getQueuePath() #user-pending-applications: user.getPendingApplications() #user-active-applications: user.getActiveApplications() #queue-pending-applications: getNumPendingApplications() #queue-active-applications: getNumActiveApplications() #queue-nonrunnable-applications: getNumNonRunnableApps()
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[ERROR] Cannot add application home subcluster for appId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:canAssign(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[INFO] Node allocation skipped
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Container {} completed with event {}, but corresponding RMContainer doesn't exist.
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:umnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] MOUNT UMNT path: {path} client: {client}
org.apache.hadoop.tools.SimpleCopyListing:writeToFileListingRoot(org.apache.hadoop.io.SequenceFile$Writer,java.util.LinkedList,org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[DEBUG] Skip {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long):[DEBUG] OP_ADD: ... numblocks: ....
org.apache.hadoop.util.Shell:checkIsBashSupported():[WARN] Bash is not supported by the OS
org.apache.hadoop.hdfs.server.namenode.FSDirectory:copyINodeDefaultAcl(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] child: {}, posixAclInheritanceEnabled: {}, modes: {}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:enableBlockPoolId(java.lang.String):[DEBUG] {}: failed to load block iterator: + e.getMessage()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK markBlockAsCorrupt: {} cannot be marked as corrupt as it does not belong to any file
org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean):[ERROR] Unable to make {toSvc} active ({sfe.getMessage()}). Failing back.
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int):[DEBUG] Check the condition for main loop.
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:doGetEvents(java.lang.String,java.util.SortedSet,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,org.apache.hadoop.security.UserGroupInformation):[WARN] Error when verifying access for user ...
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] nodeId + Node Transitioned from + oldState + to + getState()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$StatusUpdaterRunnable:updateTimelineCollectorData(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] Sync a new collector address: {} for application: {} from RM
org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration):[DEBUG] Reinit compressor with new compression configuration
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List):[WARN] Timeout submitting entries to {}
org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List):[DEBUG] startupShutdownMessage
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] [attemptId] transitioned from state [oldState] to [newState], event type is [eventType] and nodeId=[nodeId]
org.apache.hadoop.yarn.service.client.ApiServiceClient:getInstances(java.lang.String,java.util.List,java.lang.String,java.util.List):[ERROR] Fail to get containers {}
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:registerAndInitializeHeartbeat():[INFO] Started federation membership heartbeat with interval: {}
org.apache.hadoop.hdfs.tools.DFSHAAdmin:addSecurityConfiguration(org.apache.hadoop.conf.Configuration):[DEBUG] Using NN principal: + nameNodePrincipal
org.apache.hadoop.fs.s3a.S3AInputStream:readSingleRange(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer):[DEBUG] Start reading range {} from path {}
org.apache.hadoop.mapred.nativetask.handlers.NativeCollectorOnlyHandler:create(org.apache.hadoop.mapred.nativetask.TaskContext):[INFO] [NativeCollectorOnlyHandler] combiner is not null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean):[ERROR] Trying to schedule on a removed node, please double check, + nodeId= {}
org.apache.hadoop.mapred.pipes.Submitter:run(java.lang.String[]):[WARN] -jobconf option is deprecated, please use -D instead.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:computeQueueManagementChanges():[DEBUG] Parent queue = ..., nodeLabel = ..., deactivated leaf queues = ...
org.apache.hadoop.tools.rumen.ParsedTask:dumpParsedTask():[INFO] ... (additional logs from dumpParsedTaskAttempt if any) ...
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:take():[DEBUG] take(): poll() returned null, sleeping for {} ms
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[WARN] Exception, e
org.apache.hadoop.yarn.service.ServiceScheduler:syncSysFs(org.apache.hadoop.yarn.service.api.records.Service):[WARN] Error synchronize YARN sysfs: {response.getEntity(String.class)}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor:run():[INFO] Stopping RedundancyMonitor.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:processRecovery():[INFO] Attempting to recover.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$LogMonitorThread:run():[WARN] Container [pid=PID,containerID=containerId] is logging beyond the container single log directory limit... Killing container.
org.apache.hadoop.tools.HadoopArchiveLogs:checkMaxEligible():[INFO] Removing ...
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:cacheAllocatedContainers(java.util.List,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[WARN] Duplicate containerID: {} found in the allocated containers from same sub-cluster: {}, so ignoring.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[INFO] {}: Committing job with file count: {}; total size {} bytes
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread:run():[WARN] Continuous scheduling thread interrupted. Exiting.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:addDirectoryToSerialNumberIndex(org.apache.hadoop.fs.Path):[WARN] Could not find serial portion from path: [serialDirPath]. Continuing with next
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] access for filesystem: {}, path: {}, mode: {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:dumpOutDebugInfo():[INFO] System env: key=..., val=...
org.apache.hadoop.fs.s3a.S3AFileSystem:getAmazonS3ClientForTesting(java.lang.String):[WARN] Access to S3A client requested, reason {}
org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run():[INFO] Change property: change.prop from ((change.oldVal == null) ? "<default>" : oldValRedacted) to ((change.newVal == null) ? "<default>" : newValRedacted)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(java.lang.Object):[INFO] {}: Deleting job directory {}, getName(), baseDir
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerStoppedTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[WARN] [COMPONENT {0}]: Failed {1} times, exceeded the limit - {2}. Shutting down now...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[WARN] Failed to cache + key + : Underlying blocks are not backed by files.
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[DEBUG] Auth parameters added for proxy user
org.apache.hadoop.util.RunJar:run(java.lang.String[]):[ERROR] Error opening job jar
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:run():[DEBUG] NM deleting absolute path : {}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder):[TRACE] SAS token fetch complete for {} on {}
org.apache.hadoop.hdfs.DFSOutputStream:computePacketChunkSize(int,int):[DEBUG] computePacketChunkSize: src={}, chunkSize={}, chunksPerPacket={}, packetSize={}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx):[DEBUG] The write back thread is working.
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:getTaskOutput(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Scanning {} for files to commit
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:getVolumeReports():[ERROR] Unexpected IOException by closing FsVolumeReference
org.apache.hadoop.yarn.service.utils.SliderFileSystem:deleteComponentDir(java.lang.String,java.lang.String):[DEBUG] deleted dir {}, path;
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String):[INFO] Checking script path: envBinaryPath
org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long):[WARN] interrupted while sleeping
org.apache.hadoop.hdfs.server.federation.router.ErasureCoding:setErasureCodingPolicy(java.lang.String,java.lang.String):[DEBUG] Retrieved locations for path
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[INFO] {}, Token={}
org.apache.hadoop.yarn.server.resourcemanager.placement.SpecifiedPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[ERROR] Specified queue name not valid: '{}'
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[WARN] Found unexpected column for entity ...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getNetworkDependencies(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[ERROR] The dependency call returned null for host + node.getHostName()
org.apache.hadoop.service.CompositeService:addService(org.apache.hadoop.service.Service):[DEBUG] Adding service + service.getName()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] DefaultMetricsSystem initialized
org.apache.hadoop.fs.s3a.S3AFileSystem:setAmazonS3Client(com.amazonaws.services.s3.AmazonS3):[DEBUG] Setting S3 client to {}
org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager:createPassword(org.apache.hadoop.yarn.security.ContainerTokenIdentifier):[DEBUG] Creating password for {} for user {} to be run on NM {}, identifier.getContainerID(), identifier.getUser(), identifier.getNmHostAddress()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[ERROR] Exception in app dir creation, disabling log aggregation
org.apache.hadoop.hdfs.server.datanode.DataNode:makeInstance(java.util.Collection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[DEBUG] Metrics system initialized
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.volume.csi.event.VolumeEvent):[INFO] VolumeImpl + volumeId + transitioned from + oldState + to + newState
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error storing appAttempt: $attemptState.getAttemptId(), $e
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[ERROR] No block listing files were found! Cannot run with 0 DataNodes.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$StartedAfterUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] {} received started but cancellation pending
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[INFO] Got APPLICATION_INIT for service {serviceId}
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] UserPasswordTokenProvider initialized
org.apache.hadoop.hdfs.server.datanode.DataXceiver:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[WARN] "{}: Got exception while serving {} to {}: ", dnR, block, remoteAddress, ioe
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[INFO] Reinitializing SchedulingMonitorManager ...
org.apache.hadoop.examples.dancing.DistributedPentomino:createInputDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.examples.dancing.Pentomino,int):[DEBUG] Directory created
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:chooseTarget(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.List,org.apache.hadoop.hdfs.server.balancer.Matcher,java.util.EnumMap,java.util.List):[DEBUG] Datanode:{} storage type:{} doesn’t have sufficient space:{} to move the target block size:{}
org.apache.hadoop.fs.azure.BlockBlobAppendStream:close():[DEBUG] Lease free update blob {} encountered Storage Exception: {} Error Code : {}, key, ex, ex.getErrorCode()
org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration):[ERROR] Failed to get the AbstractFileSystem for path: + uri
org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials):[DEBUG] selected by alias={} token={}
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection):[WARN] Time zone {clientTimeZone} could not be set on Oracle database.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:uncacheBlock(java.lang.String,long):[DEBUG] Cancelling caching for block with id {}, pool {}.
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized nodemanager with : physical-memory= ... virtual-memory= ... virtual-cores= ...
org.apache.hadoop.fs.s3a.S3AFileSystem:abortMultipartUpload(com.amazonaws.services.s3.model.MultipartUpload):[DEBUG] Aborting multipart upload {} to {} initiated by {} on {}, uploadId, destKey, upload.getInitiator(), df.format(upload.getInitiated())
org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.GpuDeviceInformationParser:parseXml(java.lang.String):[ERROR] Failed to parse XML output of GPU_SCRIPT_REFERENCE!
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Storing RMDelegationToken and SequenceNumber , e
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:removeAclEntries(java.lang.String,java.util.List):[DEBUG] Acquired locations for path
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationsHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationsHomeSubClusterRequest):[INFO] FederationStateStoreClientMetrics succeeded state store call
org.apache.hadoop.crypto.key.kms.server.KMS:getKey(java.lang.String):[TRACE] Exiting getKey method.
org.apache.hadoop.hdfs.server.datanode.DataNode:triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions):[INFO] Triggering block report
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceStart():[INFO] Starting Router RMAdmin Service
org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean):[DEBUG] running sort pass
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:serviceStart():[INFO] Router RPC up at: ...
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] ClientCredsTokenProvider initialized
org.apache.hadoop.examples.dancing.DistributedPentomino:createInputDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.examples.dancing.Pentomino,int):[INFO] Prefixes generated
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler):[WARN] SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean):[DEBUG] DIR* NameSystem.delete: {iip.getPath()}
org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory):[DEBUG] Removed {} from exclude protocol list
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain):[DEBUG] Verifying the access of [user]
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:onClose(org.eclipse.jetty.websocket.api.Session,int,java.lang.String):[INFO] session.getRemoteAddress().getHostString() + " closed, status: " + status
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,boolean,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] appending SAS token to query
org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token kind is and the token's service name is
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getLegacyBlockReaderLocal():[TRACE] {}: trying to construct BlockReaderLocalLegacy
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationStartData):[ERROR] Error when writing start information of application X
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkCreate(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List):[DEBUG] Creating {} with {} bytes of data and ACL {}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:takeAndProcessTasks():[WARN] Failure processing re-encryption task for zone {zoneId}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:open(org.apache.hadoop.fs.Path,int):[DEBUG] Opening file: {f.toString()}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$KillAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[DEBUG] App state before killing logged
org.apache.hadoop.crypto.key.kms.server.KMS:invalidateCache(java.lang.String):[DEBUG] Invalidating cache with key name {}.
org.apache.hadoop.hdfs.server.balancer.Balancer:logUtilizationCollections():[DEBUG] logUtilizationCollection("over-utilized", overUtilized)
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:deregisterSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterDeregisterRequest):[ERROR] SubCluster {subClusterId} not found
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] ResourceBase = {Collections.singletonList(apiServer.getWebAppContext().getResourceBase())}
org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState):[DEBUG] rechecking for electability from bad state
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:newConnection():[ERROR] Unsupported protocol for connection to NameNode: null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[DEBUG] Log audit event: successful operation addCachePool
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMNodeLabelsHandler:verifyRMRegistrationResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse):[INFO] RMRegistration Success message
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo,java.lang.String):[WARN] Error reading the stream
org.apache.hadoop.tools.DistCp:createMetaFolderPath():[DEBUG] Meta folder location: + metaFolderPath
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.security.UserGroupInformation:print():[INFO] 
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:enqueue(long,boolean,long,org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status):[DEBUG] {}: enqueue {}
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[ERROR] Unable to fence service by any configured method.
org.apache.hadoop.hdfs.DFSStripedOutputStream:handleStreamerFailure(java.lang.String,java.lang.Exception,org.apache.hadoop.hdfs.StripedDataStreamer):[WARN] Failed: [err], [this], [e]
org.apache.hadoop.lib.server.BaseService:init(org.apache.hadoop.lib.server.Server):[INFO] Configuration resolved
org.apache.hadoop.registry.server.services.RegistryAdminService:purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback):[DEBUG] Scheduling for deletion with children
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[INFO] Datanode storage report obtained
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Audit manager initialized with audit service {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS SYMLINK, target: {}, link: {}, namenodeId: {}, client: {}
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[ERROR] Resource profiles is not enabled
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServices():[WARN] System service directory {} doesn't not exist.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:initializePlugins(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context,java.lang.String[]):[INFO] Initialized plugin {}
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] getRunCommand: %s exists:%b
org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline:createStreams(boolean,org.apache.hadoop.util.DataChecksum):[DEBUG] writeTo blockfile is ... of size ...
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[DEBUG] krb5conf: krb5conf
org.apache.hadoop.hdfs.server.datanode.BlockScanner:markSuspectBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock):[INFO] Not scanning suspicious block {block} on {storageId}, because there is no volume scanner for that storageId.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:closeFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile):[DEBUG] closeFile: {} with {} blocks is persisted to the file system
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[INFO] Satisfy storage policy
org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:requestNextBatch():[DEBUG] [{}], Requesting next {} uploads prefix {}, next key {}, next upload id {}
org.apache.hadoop.yarn.client.cli.TopCLI:getNodesInfo():[ERROR] Unable to fetch cluster metrics
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanUpPreviousJobOutput():[INFO] Starting to clean up previous job's temporary files
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:run():[TRACE] {} exiting because of InterruptedException.
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:purgeLogsOlderThan(long):[INFO] Purging logs older than + minTxIdToKeep
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads():[INFO] TokenCache is enabled
org.apache.hadoop.mapreduce.JobSubmitter:submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster):[DEBUG] Configuring job jobId with submitJobDir as the submit dir
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfDataXceiverParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {property} to {newVal}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Setting file size is not supported when creating file: {} + dir fileId: {}
org.apache.hadoop.mapred.LocalDistributedCacheManager:symlink(java.io.File,java.lang.String,java.lang.String):[WARN] Failed to create symlink: %s <- %s
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ExitFinishingOnTimeoutTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[WARN] Task attempt ${taskAttempt.getID()} is done from TaskUmbilicalProtocol's point of view. However, it stays in finishing state for too long
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] New instance created for InputFormat
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:fetchTaskManifest(org.apache.hadoop.fs.FileStatus):[INFO] {}: Task Attempt {} file {}: File count: {}; data size={}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] AzureBlobFileSystem.create path: {f} permission: {permission} overwrite: {overwrite} bufferSize: {blockSize}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuResourcePlugin:checkGpuResourceHandler():[WARN] Linux Container Executor is not configured for the NodeManager. To fully enable GPU feature on the node also set YarnConfiguration.NM_CONTAINER_EXECUTOR properly.
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils:getContainerLogFile(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.Context):[WARN] Failed to find log file
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:writeJobConf():[ERROR] Error reading/writing job conf file for job: jobId
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[ERROR] IOException during updateDynamicResourceConfiguration
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doAppLogAggregationPostCleanUp():[WARN] An exception occurred while getting file information
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:unreserveInternal(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[INFO] Application ... unreserved on node ..., currently has ... at priority ...; currentReservation ...
org.apache.hadoop.examples.DBCountPageView:verify():[INFO] totalPageview= [value]
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:cancelTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path):[DEBUG] Cancelled token for token.getService()
org.apache.hadoop.hdfs.server.datanode.DataStorage:createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,int,org.apache.hadoop.conf.Configuration):[INFO] Generated new storageID {} for directory {} {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:handleMaxCapacityPercentage(java.lang.String):[INFO] Configuring max capacity percentage for queue
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Invalid container release by application + appAttemptId
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappedBlock:close():[INFO] Successfully uncached one replica:{} from persistent memory, [cached path={}, length={}]
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Number of original ranges size {}, Number of combined ranges {}, ranges.size(), combinedFileRanges.size()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long):[ERROR] Unable to store token {sequence number}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[ERROR] Attempting to remove non-existent node
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:init(org.apache.hadoop.conf.Configuration):[INFO] AMRMTokenKeyRollingInterval: this.rollingInterval ms and AMRMTokenKeyActivationDelay: this.activationDelay ms
org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean):[TRACE] Enabling fallbackToSimpleAuth for target, as we are allowed to fall back.
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:internalUpdateAttributesOnNodes(java.util.Map,org.apache.hadoop.yarn.server.api.protocolrecords.AttributeMappingOperationType,java.util.Map,java.lang.String):[INFO] Updated NodeAttribute event to RM: + newNodeToAttributesMap
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:updateApplicationPriority(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationPriorityRequest):[INFO] Success in updating application priority
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:init(javax.servlet.FilterConfig):[INFO] allowedUsersAclList=<allowedUsersAclList.getUsers()>
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:replayLog(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand):[WARN] Unsupported/invalid command:
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:handleContainerKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal):[DEBUG] Container status is {}, skipping kill - {}
org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:validate(java.util.Map,int):[ERROR] Solver timeInterval {} is invalid, please specify a positive value., timeInterval
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateNodeLabelsFromNMReport(java.util.Set,org.apache.hadoop.yarn.api.records.NodeId):[ERROR] Node Labels {<nodeLabels>} reported from NM with ID <nodeId> was rejected from RM with exception message as: <exceptionMessage>
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Released read lock
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:requestNewHdfsDelegationTokenAsProxyUser(java.util.Collection,java.lang.String,boolean):[INFO] Received new tokens for [referringAppIds]. Received [newTokens.length] tokens.
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer):[INFO] Queue refreshed
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[]):[DEBUG] Executing command {BucketInfo.NAME}
org.apache.hadoop.fs.aliyun.oss.OSSListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Prefix count = {}; object count={}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[INFO] Shutting down CacheReplicationMonitor.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeUpgrade():[INFO] finalizing upgrade completed by superuser
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:chooseDatanode(org.apache.hadoop.hdfs.server.namenode.NameNode,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:addVolume(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List):[INFO] Added volume - location, StorageType: storageType
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] Couldn't find RM app for {applicationId} to set queue name on.
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getAclStatus(java.lang.String):[INFO] Invoke sequential execution
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean):[DEBUG] Source is a directory
org.apache.hadoop.hdfs.server.namenode.LeaseManager:checkLeases(java.util.Collection):[WARN] Cannot release the path {} in the lease {}. It will be retried.
org.apache.hadoop.mapred.JobQueueClient:displayQueueAclsInfoForCurrentUser():[INFO] User <shortUserName> does not have access to any queue.
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class):[WARN] Could not get information of requester, ignoring for now.
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionLaunch(java.lang.String,java.lang.String,java.lang.Long,java.lang.String):[ERROR] Fail to launch application:
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteUnnecessaryFakeDirectories(org.apache.hadoop.fs.Path):[TRACE] To delete unnecessary fake directory {} for {}
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:updateHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[ERROR] Trying to updateHistory {recurrenceId} with empty resource skyline
org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String):[WARN] unable to return groups for user {user} {pge}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStop():[INFO] Executor terminated
org.apache.hadoop.security.authentication.server.AuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] LDAP authentication attempted
org.apache.hadoop.hdfs.qjournal.server.Journal:acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL):[INFO] Synchronizing log
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:getHAServiceState(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo):[DEBUG] Failed to connect to {} while fetching HAServiceState
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] {} is in multiple subclusters
org.apache.hadoop.fs.s3a.S3AFileSystem:toString():[DEBUG] Appending block size info
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:checkVersion():[INFO] Storing timeline store version info getCurrentVersion()
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Updating info for attempt: {}
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel:reserveQueueSpace(int):[WARN] Pending edits to IPCLoggerChannel.this is going to exceed limit size: queueSizeLimitBytes, current queued edits size: queuedEditsSizeBytes, will silently drop size bytes of edits!
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline service v1 batch publishing enabled
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[INFO] MapFile writer created
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:isAvailable():[INFO] ProcfsBasedProcessTree currently is supported only on Linux.
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersions(java.lang.String):[DEBUG] Getting key versions for key {}
org.apache.hadoop.fs.s3a.S3AFileSystem:bindAWSClient(java.net.URI,boolean):[DEBUG] Using credential provider {}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getQueueInfo(org.apache.hadoop.yarn.api.protocolrecords.GetQueueInfoRequest):[INFO] RMAuditLogger logSuccess
org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Config has been overridden during init
org.apache.hadoop.hdfs.server.balancer.Balancer:chooseStorageGroups(org.apache.hadoop.hdfs.server.balancer.Matcher):[INFO] Placeholder for log message
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:analyseBlocksStorageMovementsAndAssignToDN(org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy):[INFO] File: {} is not having any blocks. So, skipping the analysis.
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[INFO] Killing running jobs...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getCleanupDockerVolumesCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Exception thrown in onDeviceReleased of + devicePlugin.getClass() + for container: + container.getContainerId().toString(), e
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[ERROR] Fsck: could not copy block {block} to {target}
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.util.List):[DEBUG] Resolved {hostname} to {rack}
org.apache.hadoop.nfs.NfsExports$RegexMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] RegexMatcher 'pattern', denying client 'address', 'hostname'
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:findNodeToUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] checked to see if could unreserve for app but nothing reserved that matches for this app
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FifoPolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy):[ERROR] FifoPolicy policy is only for leaf queues. Please choose DominantResourceFairnessPolicy or FairSharePolicy for parent queues.
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:genericRefresh(java.lang.String[],int):[INFO] Refresh Responses:
org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run():[WARN] Cleaner thread interrupted, will stop
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[ERROR] Error running child ...
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNodeHealthyForDecommissionOrMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Node {} hasn't sent its first block report.
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int,int):[INFO] Waiting in main loop.
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker:cancelAllTasks():[INFO] Cancelling {} re-encryption tasks
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getGroupsForUser(java.lang.String):[DEBUG] Getting groups for user + user
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler:logAndStdErr(java.lang.Throwable,java.lang.String):[DEBUG] Stack trace
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectoryMultiThreaded():[DEBUG] Recording source-path: {} for copy.
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[DEBUG] logRpcIds called
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:nodeHeartbeat(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[INFO] Too far behind rm response id: + lastNodeHeartbeatResponse.getResponseId() + nm response id: + remoteNodeStatus.getResponseId()
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:authorizeInternal(java.lang.String,java.lang.String,java.lang.String):[ERROR] Authorization failed with error
org.apache.hadoop.yarn.server.timeline.LogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[DEBUG] Parsing {} at offset {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:shutdown():[INFO] Shutting down all async lazy persist service threads
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:readOp(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream):[DEBUG] Tried to read from deleted or moved edit log segment
org.apache.hadoop.hdfs.DFSOutputStream:flushInternalWithoutWaitingAck():[INFO] Packet queued
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not update cgroup for container, e
org.apache.hadoop.tools.rumen.Folder:run():[ERROR] All of your job[s] have the same submit time. Please just use your input file.
org.apache.hadoop.tools.DistCpSync:deleteTargetTmpDir(org.apache.hadoop.hdfs.DistributedFileSystem,org.apache.hadoop.fs.Path):[ERROR] Unable to cleanup tmp dir: {tmpDir}, {exception}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container metrics released
org.apache.hadoop.fs.s3a.WriteOperationHelper:operationRetried(java.lang.String,java.lang.Exception,int,boolean):[DEBUG] Stack
org.apache.hadoop.mapred.LocalJobRunner$Job:createMapExecutor():[DEBUG] Map tasks to process: {{this.numMapTasks}}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeFile(java.lang.String,java.io.InputStream,byte[],long):[INFO] Store file from input stream. COS key: [{}], length: [{}].
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:tryCreatingHistoryDirs(boolean):[INFO] Waiting for FileSystem at {intermediateDoneDirPath.toUri().getAuthority()} to be out of safe mode
org.apache.hadoop.fs.s3a.S3AFileSystem:putObject(com.amazonaws.services.s3.model.PutObjectRequest):[DEBUG] PUT {} bytes to {} via transfer manager
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerRecoveredTransition:transition(java.lang.Object,java.lang.Object):[WARN] RMContainer received unexpected recover event with container state report.getContainerState while recovering.
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Writing with HBaseTimelineWriterImpl
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:completeFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[DEBUG] DIR* NameSystem.completeFile: String srcArg for String holder
org.apache.hadoop.yarn.client.cli.TopCLI:getQueueMetrics():[ERROR] Unable to get queue information
org.apache.hadoop.ha.ActiveStandbyElector:createConnection():[DEBUG] Created new connection for this
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:getAggregatorThreadPoolSize(org.apache.hadoop.conf.Configuration):[WARN] Invalid thread pool size. Setting it to the default value in YarnConfiguration
org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean):[WARN] Error closing the stream
org.apache.hadoop.mapred.pipes.BinaryProtocol:close():[DEBUG] closing connection
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockPoolSlice:fetchVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,org.apache.hadoop.fs.FileSystem):[ERROR] Got null reader from BlockAliasMap
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] OK response built with JSON location
org.apache.hadoop.hdfs.server.namenode.FSEditLog:endCurrentLogSegment(boolean):[INFO] Ending log segment
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaNodeResourceUpdateHandler:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[INFO] Didn't find any usable FPGAs on the NodeManager.
org.apache.hadoop.yarn.service.client.ServiceClient:actionDecommissionInstances(java.lang.String,java.util.List):[ERROR] persistedService.getName() + " is at " + appReport.getYarnApplicationState() + " state, decommission can only be invoked when service is running"
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration):[DEBUG] DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for {}
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Service initialized with configuration
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int,java.io.InputStream):[DEBUG] No space available. Available: availableSize MinSize: minSize
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[DEBUG] Task failed
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getRssMemorySize(int):[DEBUG] MEM Comparison:<value_from_procfs> <value_from_cgroup>
org.apache.hadoop.tools.DistCpSync:checkNoChange(org.apache.hadoop.hdfs.DistributedFileSystem,org.apache.hadoop.fs.Path):[WARN] The target has been modified since snapshot ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] pre-assignContainers for application + getApplicationId()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[ERROR] Error in handling event type + event.getType() + for application + appID, t
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:getProxyAddresses():[DEBUG] proxy address is: {}
org.apache.hadoop.hdfs.DFSClient:renewLease():[WARN] Failed to renew lease for clientName for (elapsed / 1000) seconds (>= hard-limit = (dfsClientConf.getleaseHardLimitPeriod() / 1000) seconds.) Closing all files being written ...
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:putTimelineDataInJSONFile(java.lang.String,java.lang.String):[INFO] Timeline domains are successfully put
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction:runWithRetries():[INFO] Exception while executing an FS operation.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] Unrecognized value [...] for property ....
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils:openLogFileForRead(java.lang.String,java.io.File,org.apache.hadoop.yarn.server.nodemanager.Context):[ERROR] Exception reading log file {logFile.getAbsolutePath()}
org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean):[ERROR] Exception raised {}
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:run():[WARN] anomalous line #<lineNumber>:<line>
org.apache.hadoop.crypto.key.kms.server.KMS:deleteKey(java.lang.String):[TRACE] Exiting deleteKey method.
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:seek(long):[DEBUG] requested seek to position {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler$LogDeleterRunnable:run():[WARN] Unsupported file system used for log dir
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:loadMap(java.util.HashMap,java.lang.String,int,int):[DEBUG] Loaded map stats - File: {}, Loaded: {}, Error: {}
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler):[WARN] SIMULATING A CORRUPT BYTE IN IMAGE TRANSFER!
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:savePools(java.io.DataOutputStream,java.lang.String):[INFO] Incrementing counter for each cache pool
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementContextWithParent(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping,java.lang.String):[WARN] Placement rule specified a parent queue {}, but it is ambiguous.
org.apache.hadoop.mapred.ShuffleHandler:recoverJobShuffleInfo(java.lang.String,byte[]):[DEBUG] Configuring job jar
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:setPathProperties(org.apache.hadoop.fs.Path,java.util.Hashtable,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] setFilesystemProperties for filesystem: {client.getFileSystem()} path: {path} with properties: {properties}
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:tryLock():[ERROR] Unable to acquire file lock on path {}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainers(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Completed reading history information of all containers of application attempt [appAttemptId]
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:main(java.lang.String[]):[ERROR] Application Master failed. exiting
org.apache.hadoop.ha.ZKFailoverController:waitForActiveAttempt(int,long):[WARN] timeoutMillis + ms timeout elapsed waiting for an attempt to become active
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:activateNextMasterKey():[INFO] Activating next master key with id: [dynamic value]
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.SubClusterHeartbeatRequest):[WARN] Missing GetSubClusterInfo Request. Please try again by specifying a Get SubCluster information.
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader:nextJob():[ERROR] The current job was submitted earlier than the previous one
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open(org.apache.hadoop.fs.Path,java.util.Optional):[INFO] File qualified
org.apache.hadoop.ha.HealthMonitor:shutdown():[INFO] Stopping HealthMonitor thread
org.apache.hadoop.tools.SimpleCopyListing$FileStatusProcessor:processItem(org.apache.hadoop.tools.util.WorkRequest):[ERROR] Exception in listStatus. Will send for retry.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Failed to fetch user credentials from application
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Ignoring invalid eventtype
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:resetCGroupParameters():[DEBUG] Swap monitoring is turned off in the kernel
org.apache.hadoop.fs.cosn.BufferPool:createDir(java.lang.String):[DEBUG] Buffer dir: [{}] does not exists. create it first.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:initInternal(org.apache.hadoop.conf.Configuration):[ERROR] Invalid format for ...
org.apache.hadoop.tools.mapred.CopyCommitter:cleanupTempFiles(org.apache.hadoop.mapreduce.JobContext):[WARN] Unable to cleanup temp files
org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String):[WARN] Unable to return groups for user '...' as shell group lookup command '...' ran longer than the configured timeout limit of ... seconds.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[TRACE] {}: save('{}, {}, {}')
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceStop():[INFO] Clean up complete and service stopped
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanupContainersOnNMResync():[INFO] All containers in DONE state
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:addDeviceSet(java.lang.String,java.util.Set):[INFO] Adding new resource: type: + resourceName + , + deviceSet
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection):[DEBUG] Node {} is excluded, continuing.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser:handleTaskAttemptFailedEvent(org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent):[WARN] TaskInfo is null for TaskAttemptUnsuccessfulCompletionEvent taskId: [event.getTaskId().toString()]
org.apache.hadoop.hdfs.DFSInputStream:getCurrentBlockLocationsLength():[INFO] Found null currentLocatedBlock. pos={}, blockEnd={}, fileLength={}
org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask:run():[WARN] Failed to submit [JobName] as [Ugi]
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[WARN] Event {event} sent to absent container {event.getContainerID()}
org.apache.hadoop.mapreduce.Cluster:initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration):[DEBUG] Cannot pick ... as the ClientProtocolProvider - returned null protocol
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path):[WARN] Output Path is null in commitTask()
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[INFO] Shutdown complete.
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] System Service Directory is configured to {}
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getRMWebAppURLWithScheme(org.apache.hadoop.conf.Configuration):[DEBUG] Get HTTP scheme prefix
org.apache.hadoop.mapred.uploader.FrameworkUploader:buildPackage():[INFO] Compressing tarball
org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader:readDataChecksum(java.io.FileInputStream,int,java.io.File):[WARN] Unexpected meta-file version for {name}: version in file is {header.getVersion()} but expected version is {VERSION}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Docker inspect output for + containerId + : + output
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Path could not be found: {target}
org.apache.hadoop.ipc.Server$Responder:doRunLoop():[WARN] Exception in Responder {e}
org.apache.hadoop.mapred.TaskAttemptListenerImpl:fsError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String):[ERROR] Task: taskAttemptID - failed due to FSError: message
org.apache.hadoop.ipc.Client$Connection:close():[ERROR] The connection is not in the closed state
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:getInfoServer():[DEBUG] Will connect to NameNode at + address
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor:processFileDiffEntry():[DEBUG] Processing fileDiffEntry
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:checkNotSymlink(org.apache.hadoop.hdfs.server.namenode.INode,byte[][],int):[DEBUG] UnresolvedPathException path: ${path} preceding: ${preceding} count: ${i} link: ${link} target: ${target} remainder: ${remainder}
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches():[INFO] Skipping State Store cache update, driver is not ready.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateSourceTags(java.util.Set):[WARN] A placement constraint cannot be associated with an empty set of tags.
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:delete(java.lang.String,boolean):[DEBUG] *DIR* Namenode.delete: src={src}, recursive={recursive}
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:stop():[INFO] Stopping security manager
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:loadConversionRules(java.lang.String):[INFO] Reading conversion rules file from: + rulesFile
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[DEBUG] Application submitted to RMWebServices
org.apache.hadoop.mapred.ClientCache:getClient(org.apache.hadoop.mapreduce.JobID):[WARN] Could not connect to History server.
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] [getName()] is not supported for filesystem [fs.getScheme()] on path [path]
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:run():[DEBUG] <lineNumber> <line.second()>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.api.records.Container):[DEBUG] allocate: applicationAttemptId=... container=... host=... type=...
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:getPreviousJobHistoryStream(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Previous history file is at ...
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the hbase Connection
org.apache.hadoop.examples.Sort:run(java.lang.String[]):[DEBUG] Sampling input to effect total-order sort...
org.apache.hadoop.fs.s3a.S3AFileSystem:getObjectMetadata(org.apache.hadoop.fs.Path):[DEBUG] HEAD {key} with change tracker {changeTracker}
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[INFO] Set storage policy [policyName] on [path]
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LocalizationFailedToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container failed with state: <state>
org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String):[DEBUG] poking parent ' + getParent().getClass().getSimpleName() + ' for key: ' + key
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:sendRequestsToResourceManagers(java.util.Map):[ERROR] UAM not found for attemptId in sub-cluster subClusterId
org.apache.hadoop.hdfs.qjournal.server.Journal:moveTmpSegmentToCurrent(java.io.File,java.io.File,long):[ERROR] The endTxId of the temporary file is not less than the last committed transaction id. Aborting move to final file {finalFile} ; journal id: {journalId}
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket):[TRACE] createNewMemorySegment: ShortCircuitRegistry is not enabled.
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:submitCurrentBatch(java.lang.Long):[INFO] Submitted batch (start:{}, size:{}) of zone {} to re-encrypt.
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfCacheReportParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] Recovered {} running containers from UAM in {}
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:authorizeInternal(java.lang.String,java.lang.String,java.lang.String):[INFO] Authorization successful
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:finishSingleFileRename(java.lang.String):[WARN] Attempting to complete rename of file {srcKey}/{fileName} during folder rename redo, and file was not found in source or destination {dstKey}/{fileName}. This must mean the rename of this file has already completed
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean,java.util.List,java.util.Queue):[ERROR] Error adding replica to map
org.apache.hadoop.tools.DistCh:run(java.lang.String[]):[ERROR] Input error:, e
org.apache.hadoop.hdfs.tools.DFSAdmin:shutdownDatanode(java.lang.String[],int):[DEBUG] Submitted a shutdown request to datanode
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[INFO] Updating final state {attemptState.getState()} for attempt: {attemptState.getAttemptId()}
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:init(org.apache.hadoop.conf.Configuration):[ERROR] Cannot create base directories:
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:notifyIsLastAMRetry(boolean):[INFO] Notify JHEH isAMLastRetry: [isLastAMRetry]
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:addDirectoryToSerialNumberIndex(org.apache.hadoop.fs.Path):[WARN] Could not find timestamp portion from path: [serialDirPath]. Continuing with next
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + but flowrun not found (Took + (Time.monotonicNow() - startTime) + ms.)
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext):[DEBUG] Recording source-path: {path} for copy.
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Writing with DocumentStoreTimelineWriterImpl
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyBytes(org.apache.hadoop.tools.CopyListingFileStatus,long,java.io.OutputStream,int,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Seeking if required
org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession():[DEBUG] Establishing zookeeper connection for {this}
org.apache.hadoop.ipc.Server$Listener:run():[INFO] Stopping <placeholder>
org.apache.hadoop.yarn.server.resourcemanager.blacklist.SimpleBlacklistManager:getBlacklistUpdates():[DEBUG] blacklist size {} is less than failure threshold ratio {} out of total usable nodes {}, currentBlacklistSize, blacklistDisableFailureThreshold, numberOfNodeManagerHosts
org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean):[WARN] Finished time + finished + is ahead of started time + started
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:convertProtoToDeletionTask(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[DEBUG] Converting recovered FileDeletionTask
org.apache.hadoop.yarn.service.client.ServiceClient:flexComponents(java.lang.String,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[ERROR] <serviceName> is at <appReport.getYarnApplicationState()> state, flex can only be invoked when service is running
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean):[DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: {} added as corrupt on {} by {}
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Max virtual cores capabililty of resources in this cluster {}
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Submitting application to ASM
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector:selectToken(java.net.URI,java.util.Collection,org.apache.hadoop.conf.Configuration):[INFO] Retrieving service name from configuration
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map):[ERROR] CONTAINER_REMOTE_LAUNCH contains a map task
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Authentication method: %s
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] (offset,count,nextOffset): ({offset},{count},{nextOffset})
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:initializePriorityDigraph():[DEBUG] - Added priority ordering edge: {q2} >> {q1}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[INFO] Added Application Attempt {applicationAttemptId} to scheduler from user: {user}
org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Add token with service {}
org.apache.hadoop.hdfs.DFSOutputStream:flushInternalWithoutWaitingAck():[DEBUG] Closer check completed
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEvent):[DEBUG] RMAppManager processing event for ApplicationId of type eventType
org.apache.hadoop.fs.FileSystem:close():[DEBUG] Closing FileSystem: Key: someKey; URI: someUri; Object Identity Hash: someHash
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:setResponseHeaders(org.jboss.netty.handler.codec.http.HttpResponse,boolean,long):[INFO] Content Length in shuffle : + contentLength
org.apache.hadoop.mapreduce.task.reduce.Fetcher:connect(java.net.URLConnection,int):[WARN] Sleep in connection retry get interrupted.
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[ERROR] Error while submitting services for user {}
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:putEntities(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[]):[DEBUG] Writing entity log for {} to {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:run():[INFO] Checked {} blocks this tick. {} nodes are now in maintenance or transitioning state. {} nodes pending. {} nodes waiting to be cancelled.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(java.lang.Object,java.lang.Object):[INFO] Storing info for app: + appId
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:retrieve(java.lang.String,long,long):[ERROR] Exception thrown when store retrieves key: {key}, exception: {e}
org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem):[DEBUG] Could not compare file-systems. Unknown host:
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[ERROR] Unsupported operation exception
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersOnMultiNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet):[DEBUG] This partition '{}' doesn't have available or killable resource
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getSnapshottableDirListing():[DEBUG] Audit log true for operation listSnapshottableDirectory
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[INFO] Killed workload app
org.apache.hadoop.crypto.key.kms.server.KMS:getCurrentVersion(java.lang.String):[DEBUG] Exception in getCurrentVersion., e
org.apache.hadoop.yarn.event.EventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Size of {getName()} event-queue is {qSize}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:put(org.apache.hadoop.nfs.nfs3.FileHandle,org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx):[DEBUG] Evict stream ctx: + pairs.getValue()
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getClusterId():[ERROR] Cannot fetch cluster ID metrics: {exception_message}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData):[DEBUG] {}: Saving _SUCCESS file to {} via {}, successFile, getName(), successTempFile
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[WARN] DataNode {} cannot be found with UUID {} + , removing block invalidation work., dn, dn.getDatanodeUuid()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Allocate processing finished in {} ms for application {}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[WARN] Ignored {} nodes while loading token cache.
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:doCheckpoint():[WARN] Failed to write legacy OIV image: ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Partial read. Asked offset: {} count: {} and read back: {} file size: {}
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,java.lang.String,org.slf4j.Logger):[TRACE] {method} invoked by user {shortUserName}
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Error
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver:save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression):[INFO] Image file {newFile} of size {newFile.length()} bytes saved in {(monotonicNow() - startTime) / 1000} seconds.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:checkRemoveParentZnode(java.lang.String,int):[DEBUG] Unable to remove app parent node {} as it has children., parentZnode
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser:run():[INFO] Log parser interrupted
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] Forwarding finish application request to the real YARN Resource Manager
org.apache.hadoop.registry.client.impl.zk.CuratorService:createCurator():[INFO] Creating CuratorService with connection {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[DEBUG] Cache cleanup handled
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner:run():[TRACE] CacheCleaner: purging replica
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaBinaryHelper):[WARN] Failed to discover GPU information from system, exception message: ... continue...
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initRecordStorage(java.lang.String,java.lang.Class):[INFO] {} data directory doesn't exist, creating it
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] NameNode information: %s
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:setNextRollingTimeMillis(long):[INFO] Next rolling time for + getName() + is + fdf.format(nextRollingCheckMillis)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(long,long,int):[DEBUG] before cpuCheck [asked={} > allowed={}]
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReader:doReadMinimumSources(int,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[INFO] Read data interrupted.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:init():[WARN] IOException of NvidiaDockerV1CommandPlugin init:, e
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processMapAttemptLine(org.apache.hadoop.tools.rumen.ParsedLine):[WARN] HadoopLogsAnalyzer.processMapAttemptLine: bad numerical format, at line<lineNumber>.
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path):[DEBUG] Entering exist check
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile):[DEBUG] Directive {}: caching {}: {}/{} bytes
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] IOException occurred: Throwing YarnRuntimeException
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Job Overview generated
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:unfinalizeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[WARN] Block + b + unfinalized and removed.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits:addBlockedHeadroom(org.apache.hadoop.yarn.api.records.Resource):[DEBUG] New instance created
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:completeExecute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[TRACE] {} REST operation complete
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] Processing operation for req=({}), token: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadProxyCAManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Couldn't find Proxy CA data
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:startWebApp():[ERROR] AHSWebApp failed to start.
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTask(org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[INFO] Processing returned re-encryption task for zone {}({},), batch size {}, start:{}
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using Regex match for 'host' and READ_ONLY
org.apache.hadoop.yarn.client.cli.ApplicationCLI:shellToContainer(java.lang.String,org.apache.hadoop.yarn.api.records.ShellContainerCommand):[INFO] Shelling to container
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:setErasureCodingPolicy(java.lang.String,java.lang.String):[DEBUG] No policy name is specified, set the default policy name instead
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:validateOneFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry):[WARN] Length of dest file {}: {} does not match that of manifest entry {}
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[WARN] No KEY found for persisted identifier + identifier.toString()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:scanDirectoryTree(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,int,boolean):[INFO] Ignoring FS object ...
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:sendContainerRequest():[DEBUG] Application {} sends out request for {} mappers.
org.apache.hadoop.examples.terasort.TeraInputFormat:writePartitionFile(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path):[INFO] Sampling (samples) splits of (splits.size())
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[ERROR] Failed to fetch user credentials from application:
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:enableBlockPoolId(java.lang.String):[TRACE] {}: created new block iterator for {}.
org.apache.hadoop.hdfs.server.datanode.StorageLocation:makeBlockPoolDir(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Invalid directory in: {data.getCanonicalPath()}: {e.getMessage()}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:updateStorageStats(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary):[INFO] Number of failed storages changes from {} to {}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[INFO] Stopping StoragePolicySatisfier.
org.apache.hadoop.hdfs.DFSInputStream:close():[WARN] closing file ${src}, but there are still unreleased ByteBuffers allocated by read(). Please release ${builder.toString()}.
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy:splitResourceRequests(java.util.List,java.util.Set):[DEBUG] ERROR resolving sub-cluster for resourceName: {}, picked a random subcluster to forward: {}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:disableHostsFileReader(java.lang.Exception):[WARN] Failed to init hostsReader, disabling, ex
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] FieldSelectionHelper.specToString(fieldSeparator, mapOutputKeyValueSpec, allMapValueFieldsFrom, mapOutputKeyFieldList, mapOutputValueFieldList) + \nignoreInputKey: + ignoreInputKey
org.apache.hadoop.hdfs.DeadNodeDetector:probeCallBack(org.apache.hadoop.hdfs.DeadNodeDetector$Probe,boolean):[WARN] Probe failed, add suspect node to dead node list: {}.
org.apache.hadoop.yarn.service.ServiceScheduler$ServiceEventHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Error in handling event type {0}
org.apache.hadoop.yarn.appcatalog.application.YarnServiceClient:getStatus(org.apache.hadoop.yarn.appcatalog.model.AppEntry):[ERROR] Error in fetching application status: , e
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:isAvailable():[INFO] CGroupElasticMemoryController currently is supported only on Linux.
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:kill():[INFO] KILLING {taskAttemptID}
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection):[WARN] Total Nodes in scope : {} are less than Available Nodes : {}
org.apache.hadoop.yarn.server.webapp.AppAttemptBlock:render():[ERROR] Failed to read the application attempt
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processPerfectOverWrite(org.apache.hadoop.hdfs.DFSClient,long,int,org.apache.hadoop.nfs.nfs3.Nfs3Constant$WriteStableHow,byte[],java.lang.String,org.apache.hadoop.nfs.nfs3.response.WccData,org.apache.hadoop.security.IdMappingServiceProvider):[INFO] Perfect overwrite has same content, updating the mtime, then return success
org.apache.hadoop.yarn.server.resourcemanager.AdminService:mapAttributesToNodes(org.apache.hadoop.yarn.server.api.protocolrecords.NodesToAttributesMappingRequest):[ERROR] IOException during mapAttributesToNodes
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:checkReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Received an RBW replica for {} on {}: ignoring it, since it is complete with the same genstamp
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:doTailEdits():[WARN] Edits tailer failed to find any streams. Will try again later.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] Max mem capability of resources in this cluster
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:serviceStart():[WARN] Log Aggregation is disabled. So is the LogAggregationStatusTracker.
org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration):[INFO] Canceled tokenKind:tokenService
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:deregisterSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterDeregisterRequest):[INFO] Deregistered the SubCluster [subClusterId] state to [state]
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:getSpillFileCB(org.apache.hadoop.fs.Path,java.io.InputStream,org.apache.hadoop.conf.Configuration):[DEBUG] getSpillFileCB... access incorrect position.. Path {}; Pos: {}
org.apache.hadoop.ipc.Server$Connection:authorizeConnection():[DEBUG] Successfully authorized
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}, handle.getFileId()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:handleApplicationAttemptStateOp(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData,org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$AppAttemptOp):[DEBUG] {} info for attempt: {} at: {}, operation, appAttemptId, path
org.apache.hadoop.security.authentication.server.AuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Kerberos authentication attempted
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:getApplicationAttemptEntities(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.util.Map,long,java.lang.String):[INFO] FromId parameter added
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] token validation failed - sending redirect to: + loginURL
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] List COS key: [{}] to check the existence of the path.
org.apache.hadoop.hdfs.server.datanode.DataNode:deleteBlockPool(java.lang.String,boolean):[WARN] The block pool {} is still running, cannot be deleted.
org.apache.hadoop.crypto.key.kms.server.KMSConfiguration:isACLsFileNewer(long):[TRACE] Checking file {}, modification time is {}, last reload time is {}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:flushOrSync(boolean):[WARN] Slow flushOrSync took {duration} ms (threshold={datanodeSlowLogThresholdMs} ms), isSync:{isSync}, flushTotalNanos={flushTotalNanos} ns, volume={getVolumeBaseUri()}, blockId={replicaInfo.getBlockId()}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkConfiguration(org.apache.hadoop.conf.Configuration):[WARN] Only one image storage directory (DFS_NAMENODE_NAME_DIR_KEY) configured. Beware of data loss due to lack of redundant storage directories!
org.apache.hadoop.yarn.applications.distributedshell.Client:sendStopSignal():[INFO] Waiting for Client to exit loop
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:release(byte[]):[DEBUG] , freeQueueSize=
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[INFO] Starting CacheReplicationMonitor with interval {intervalMs} milliseconds
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleInitContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationRequestEvent):[DEBUG] Localizing {req.getPath()} for container {c.getContainerId()}
org.apache.hadoop.fs.s3a.auth.STSClientFactory:builder(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String):[DEBUG] STS Endpoint={}; region='{}'
org.apache.hadoop.mapreduce.tools.CLI:getJob(org.apache.hadoop.mapreduce.JobID):[INFO] Could not obtain job info after ... attempt(s). Sleeping for ... seconds and retrying.
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[INFO] Output directory:
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:cancelUpgrade():[INFO] {} cancelling upgrade, container.getId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:getDevices():[DEBUG] Failed to get output from {pathOfGpuBinary}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] Listing status for {f}
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute():[INFO] Deleting credential: <alias> from CredentialProvider: <provider>
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetXAttrs(java.lang.String,java.util.List,boolean):[DEBUG] logRpcIds
org.apache.hadoop.security.KDiag:loginFromKeytab():[DEBUG] Failed to reset UGI: {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] placing the following ReservationRequest: + contract
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] excluded nodes = <p.getExcludedNodes()>
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerStartFailedEventOnTimelineServiceV2(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[INFO] Container start failed event attempted to publish
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[DEBUG] BLOCK* addStoredBlock: Redundant addStoredBlock request received for {} on node {} size {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[WARN] logWarningWhenAuxServiceThrowExceptions during CONTAINER_STOP
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceStart():[INFO] Thread pool created
org.apache.hadoop.tools.HadoopArchiveLogs:main(java.lang.String[]):[DEBUG] Exception
org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable):[DEBUG] afterExecute in thread: [current thread name], runnable type: [runnable type]
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] Sending finish application request to RM {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:copyEmptyDirectoryMarkers(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.impl.DirMarkerTracker):[DEBUG] copying dir marker from {key} to {newDestKey}
org.apache.hadoop.mapred.MapTask:createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter):[WARN] Unable to initialize MapOutputCollector + clazz.getName()
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getListingInt(java.lang.String,byte[],boolean):[DEBUG] Cannot get locations for {}, {}.
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] Proxy user Authentication successful
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device):[DEBUG] Try to get device type from device path: {devName}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[WARN] Incorrect version exception
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyToFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,long,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet,org.apache.hadoop.fs.FileChecksum):[DEBUG] Getting replication factor
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[DEBUG] Scheduling move to done of ...
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] rename({}, {}) failure {}; retry={} etag {}
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Task timeout must be as least twice as long as the task status report interval. Setting task timeout to {taskTimeOut}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:parseGpuDevicesFromUserDefinedValues():[WARN] Cannot parse GPU device numbers: device
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:addLabelsToNode(java.util.Map):[ERROR] NODE_LABELS_NOT_ENABLED_ERR
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:handleContainerExitCode(int,org.apache.hadoop.fs.Path):[INFO] Container + containerId + succeeded
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doUpgrade(java.lang.String,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.io.File,java.io.File,java.io.File,int,org.apache.hadoop.conf.Configuration):[INFO] Upgrade of {} is complete, name
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection):[INFO] Time zone has been set to {clientTimeZone}
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:run():[INFO] EventFetcher: Thread started
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:sigKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Cannot list more tasks in container {containerId} to kill.
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[ERROR] No parent path found, throwing PathNotFoundException
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:run():[INFO] Shutting down DataXceiverServer before restart
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:loadDfsUsed():[WARN] elapsed time:{} is greater than threshold:{}, mtime:{} in file:{}, will proceed with Du for space computation calculation
org.apache.hadoop.hdfs.DFSClient:reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.LocatedBlock[]):[INFO] Found corruption while reading {file}. Error repairing corrupt blocks. Bad blocks remain., ie
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateAppCollectorsMap(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[WARN] Cannot update collector info because application ID: {appId} is not found in RMContext!
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:resolveNetworkLocationWithFallBackToDefaultLocation(org.apache.hadoop.hdfs.protocol.DatanodeID):[ERROR] Unresolved topology mapping. Using + NetworkTopology.DEFAULT_RACK + for host + node.getHostName()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:handleContainerExitCode(int,org.apache.hadoop.fs.Path):[DEBUG] Container {} completed with exit code {}, containerId, exitCode
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:signalContainersIfOvercommitted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,boolean):[DEBUG] Enough unallocated resources {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Setting file size is not supported when setattr, fileId: {}
org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[]):[INFO] Loaded properties from {fname}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf):[INFO] Initializing cache loader: MemoryMappableBlockLoader.
org.apache.hadoop.lib.server.Server:init():[INFO] ++++++++++++++++++++++++++++++++++++++++++++++++++++++
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext):[DEBUG] cwdApp: /path/to/cwdApp
org.apache.hadoop.yarn.util.resource.Resources:multiplyAndAddTo(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,double):[WARN] Resource is missing: [exception message]
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:initializeProcessTrees(java.util.Map$Entry):[DEBUG] Tracking ProcessTree {} for the first time
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:beginStep(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[DEBUG] Beginning of the step. Phase: {}, Step: {}
org.apache.hadoop.mapred.JobConf:getMaxPhysicalMemoryForTask():[WARN] The API getMaxPhysicalMemoryForTask() is deprecated. Refer to the APIs getMemoryForMapTask() and getMemoryForReduceTask() for details.
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node):[DEBUG] NetworkTopology became:\n + this.toString()
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getNumUnderConstructionBlocks():[INFO] Number of blocks under construction: {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:getEncryptedStreams(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,javax.crypto.SecretKey):[DEBUG] Client using encryption algorithm {}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingDeletionBlocks():[DEBUG] Failed to get number of blocks pending deletion
org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int):[DEBUG] waiting to acquire block: {}
org.apache.hadoop.hdfs.server.namenode.NNStorage:determineClusterId():[INFO] current cluster id for sd=;lv=;cid=
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] New instance created
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Getting UGI from keytab....
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[INFO] FPGA Plugin bootstrap success.
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate):[TRACE] Hosts:{}, CNs:{} subjectAlts:{}, ie6:{}, strictWithSubDomains{}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeErasureCodingPolicy(java.lang.String,boolean):[INFO] Audit event logged
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getBlock(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,byte[]):[TRACE] getBlock for file {} position {} thread {}, stream.getPath(), position, Thread.currentThread().getName()
org.apache.hadoop.hdfs.DeadNodeDetector:scheduleProbe(org.apache.hadoop.hdfs.DeadNodeDetector$ProbeType):[DEBUG] Schedule probe datanode for probe type: {}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:normalizeNodeLabelExpressionInRequest(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.QueueInfo):[DEBUG] Queue Info : {queueInfo}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[ERROR] Unknown WritingApplicationHistoryEvent type: [eventType]
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:doTailEdits():[DEBUG] lastTxnId:
org.apache.hadoop.yarn.util.resource.ResourceUtils:addMandatoryResources(java.util.Map):[DEBUG] Adding resource type - name = VCORES, units = ResourceInformation.VCORES.getUnits(), type = ResourceTypes.COUNTABLE
org.apache.hadoop.hdfs.tools.DFSAdmin:genericRefresh(java.lang.String[],int):[INFO] Refresh Responses:
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:sendRequestsToResourceManagers(java.util.Map):[INFO] Allocating asynchronously to sub-cluster via UAM pool
org.apache.hadoop.yarn.server.resourcemanager.DBManager$CompactionTimerTask:run():[INFO] Starting full compaction cycle
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Vmem enforcement enabled
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] DN {node} joining cluster has expanded a formerly single-rack cluster to be multi-rack. Re-checking all blocks for replication, since they should now be replicated cross-rack
org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int):[ERROR] The state code: <stateCode> is unrecognized!
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:mkdir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Perms after creating ... , Expected: ...
org.apache.hadoop.hdfs.DFSInputStream:refreshBlockLocations(java.util.Map):[DEBUG] Failed to refresh DFSInputStream for path {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:checkManifestPermissions(org.apache.hadoop.fs.FileStatus):[ERROR] Manifest file and parents must not be writable by group or others. The current Permission of {status.getPath()} is {status.getPermission()}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Creating request URL
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:fsck():[INFO] {}
org.apache.hadoop.fs.azurebfs.services.AbfsListStatusRemoteIterator:getNextIterator():[ERROR] Thread got interrupted: {exception}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:removeAclEntries(java.lang.String,java.util.List):[DEBUG] Concurrent invocation initiated
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[INFO] Error storing info for attempt: + appAttemptId, e
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:shouldRun(java.lang.String,java.lang.String):[WARN] File {} for script "{}" can not be executed.
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceStop():[DEBUG] Closing RBF metrics
org.apache.hadoop.yarn.sls.SLSRunner:startAMFromSLSTrace(java.lang.String):[ERROR] Failed to create an AM: {exception message}
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:update(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int,int,int):[DEBUG] BLOCK* NameSystem.LowRedundancyBlock.update: {block} has only {curReplicas} replicas and needs {curExpectedReplicas} replicas so is added to neededReconstructions at priority level {priority}
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner:setConf(org.apache.hadoop.conf.Configuration):[WARN] Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:loadDfsUsed():[WARN] cachedDfsUsed not found in file:{}, will proceed with Du for space computation calculation
org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo:initContext():[ERROR] Error parsing the policy.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] setPermission filesystem: {client.getFileSystem()} path: {path} permission: {permission}
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:run():[INFO] Processing the event JOB_ABORT
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[INFO] Shutting down CacheReplicationMonitor
org.apache.hadoop.tools.mapred.CopyCommitter:trackMissing(org.apache.hadoop.conf.Configuration):[INFO] Tracking file changes to directory {trackDir}
org.apache.hadoop.mapreduce.v2.hs.JobHistory$MoveIntermediateToDoneRunnable:run():[INFO] Starting scan to move intermediate done files
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$KilledAfterSuccessTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Ignoring killed event for successful reduce task attempt + taskAttempt.getID().toString()
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:nextOpImpl(boolean):[DEBUG] skipping skipAmt bytes at the end of edit log getName(): reached txid txId out of lastTxId
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File[]):[ERROR] Edits file + f + has improperly formatted + transaction ID
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set default replication to {info.getDefaultReplication()}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:finish():[ERROR] Failed to unregister application
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:stopContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId):[WARN] Exception when scheduling the event of stopping Container + containerId
org.apache.hadoop.mapred.lib.MultithreadedMapRunner:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[DEBUG] Awaiting all running Mappper.map calls to finish, job {jobName}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeConcurrent(java.util.Collection,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,boolean,long,java.lang.Class):[ERROR] Invocation to "{location}" for "{method.getMethodName()}" timed out
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client skipping handshake in secured configuration with no SASL protection configured for addr = {}, datanodeId = {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getECTopologyResultForPolicies(java.lang.String[]):[DEBUG] getECTopologyResultForPolicies
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Destination listing completed in {0}
org.apache.hadoop.yarn.nodelabels.NodeLabelsStore:recover():[INFO] Recovering node labels
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Issuing AuthenticationToken for user.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.ConvertedConfigValidator:validateConvertedConfig(java.lang.String):[ERROR] Could not start Capacity Scheduler
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:removeAclEntries(java.lang.String,java.util.List):[DEBUG] Sequential invocation initiated
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureTokens(org.apache.hadoop.security.token.Token,org.apache.hadoop.security.Credentials,java.util.Map):[INFO] Putting shuffle token in serviceData
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[WARN] Could not list directory {path}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:putEntities(java.util.TreeMap,java.util.TreeMap,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity,org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse):[ERROR] Error putting entity [entityId] of type [entityType]
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:call():[WARN] Recovered container exited with a non-zero exit code [retCode]
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendLaunchEvent():[INFO] Launching new container
org.apache.hadoop.hdfs.ViewDistributedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Mount tree initialization failed with the reason => {}. Falling back to regular DFS initialization. Please re-initialize the fs after updating mount point.
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream):[ERROR] Can not find log metadata for container: {containerIdStr}
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:writeUnlock():[INFO] Number of suppressed write-lock reports: ... [additional formatted placeholders]
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:updateAMRMTokens(org.apache.hadoop.yarn.security.AMRMTokenIdentifier,org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$RequestInterceptorChainWrapper,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[ERROR] Error storing AMRMProxy application context entry for + context.getApplicationAttemptId(), e
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Running Client
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:splitAllocateRequest(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[DEBUG] Creating heart beat request for subClusters
org.apache.hadoop.hdfs.server.mover.Mover:run():[INFO] namenodes = + namenodes
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions):[INFO] bpos.toString() + ": scheduling a full block report " + "to namenode: " + nnAddr + "."
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onClose(org.eclipse.jetty.websocket.api.Session,int,java.lang.String):[INFO] session.getRemoteAddress().getHostString() + " closed!"
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[ERROR] Error in storing RMDelegationToken with sequence number: + identifier.getSequenceNumber()
org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node):[DEBUG] NetworkTopology became:\n{}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[DEBUG] Refreshing all components
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[DEBUG] Storing info for attempt: $attemptState.getAttemptId()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] maxTaskFailuresPerNode is {value}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long):[DEBUG] Updating {}{}, DELEGATION_TOKEN_PREFIX, rmDTIdentifier.getSequenceNumber()
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:launchAM(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Error reading the error stream
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:finalMerge(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem,java.util.List,java.util.List):[INFO] Merging X segments, Y bytes from memory into reduce
org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread:run():[DEBUG] DELETED {context.fullPath}
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:initCsiAdaptorCache(java.util.Map,org.apache.hadoop.conf.Configuration):[INFO] Initializing cache for csi-driver-adaptors
org.apache.hadoop.mapred.gridmix.JobFactory:getNextJobFiltered():[DEBUG] Ignoring job [jobID] from the input trace. Reason: [reasons]
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] SecondaryNameNode principal could not be added
org.apache.hadoop.yarn.client.api.impl.TimelineWriter:doPostingObject(java.lang.Object,java.lang.String):[DEBUG] PUT to {}/{}
org.apache.hadoop.mapred.nativetask.StatusReportChecker:run():[WARN] Update native status got exception
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppState(javax.servlet.http.HttpServletRequest,java.lang.String):[ERROR] Trying to get state of an absent application
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Object):[INFO] {}: scanning directory {}, getName(), taskAttemptDir
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* removeStoredBlock: {} has already been removed from node {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:removeAll(java.lang.Class):[INFO] Deleting {path}
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized NMTimelinePublisher UGI to [nmLoginUGI]
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:readJustAMInfos():[WARN] Could not parse the old history file. Will not have old AMinfos
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:callCOSClientWithRetry(java.lang.Object):[INFO] Call cos sdk failed, retryIndex: [1 / max], call method: putObject, exception: ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[WARN] Failed to delete meta file for replica + replicaToDelete
org.apache.hadoop.yarn.appcatalog.application.YarnServiceClient:deleteApp(java.lang.String):[ERROR] Error in deleting application: {exception}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:getMapOutputInfo(java.lang.String,int,java.lang.String,java.lang.String):[DEBUG] Retrieved pathInfo for + identifier + check for corresponding loaded messages to determine whether + it was loaded or cached
org.apache.hadoop.examples.pi.DistSum$ReduceSide$PartitionMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] parts[i] = parts[i]
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppLogInitDoneTransition:transition(java.lang.Object,java.lang.Object):[WARN] failed to update application state in state store, ex
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:setShouldUnregister(boolean):[INFO] RMCommunicator notified that shouldUnregistered is: true/false
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog):[INFO] Skipping download of remote edit log {log} since it's already stored locally at {finalEditsFile}
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[ERROR] Unable to kill infrastructure app ({})
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl$SASCloudBlobContainerWrapperImpl:getBlockBlobReference(java.lang.String):[ERROR] Encountered SASKeyGeneration exception while generating SAS Key for relativePath : {relativePath} inside container : {getName()} Storage account : {storageAccount}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread:run():[ERROR] Exception on heartbeat
org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger:logAuditEvent(org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$OpStatus,org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$AuditEvent):[INFO] {status}[op={}, key={}, user={}, accessCount={}, interval={}ms] {}
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {InternalCommitterConstants.COMMITTER_NAME_STAGING}
org.apache.hadoop.hdfs.server.datanode.DataXceiver:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]):[INFO] transferBlock {} received exception {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateStarvedApps():[INFO] Computing extent of minshare starvation
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {userName}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=LISTXATTRS target=path
org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab():[INFO] Logout successful for user using keytab file
org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream():[WARN] Abandoning block: + block
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean,java.util.EnumMap):[TRACE] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[DEBUG] Creating encryption zone
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:run():[INFO] Relaying an out of band ack of type
org.apache.hadoop.hdfs.util.MD5FileUtils:readStoredMd5(java.io.File):[INFO] Matcher matches and returns result
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Write locked
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String):[WARN] {message}
org.apache.hadoop.yarn.server.webapp.AppBlock:generateApplicationTable(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,org.apache.hadoop.security.UserGroupInformation,java.util.Collection):[WARN] Container not found
org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map):[DEBUG] add to mapName map: nameId[0] id: nameId[1]
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LocalizedTransition:transition(java.lang.Object,java.lang.Object):[INFO] Localized resource <resourceRequest> for container <container.containerId>
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] Reading Manifest in file {}
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:parseConfAndFindOtherNN():[INFO] Found nn: +info.getNameNodeID()+, ipc: +info.getIpcAddress()
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[DEBUG] Building remote locations list
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:initializePipeline(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,org.apache.hadoop.security.token.Token,org.apache.hadoop.security.token.Token,java.util.Map,boolean,org.apache.hadoop.security.Credentials):[INFO] Initializing request processing pipeline for application. ApplicationId: applicationAttemptId for the user: user
org.apache.hadoop.hdfs.util.MD5FileUtils:saveMD5File(java.io.File,org.apache.hadoop.io.MD5Hash):[DEBUG] Saved MD5 digestString to md5File
org.apache.hadoop.mapreduce.task.reduce.Fetcher:verifyConnection(java.net.URL,java.lang.String,java.lang.String):[DEBUG] for url=msgToEncode sent hash and received reply
org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String):[WARN] Jetty request log for {} was of the wrong class
org.apache.hadoop.streaming.PipeMapRed:mapRedFinished():[INFO] mapRedFinished
org.apache.hadoop.fs.http.server.HttpFSServer:get(java.lang.String,javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [{}] offset [{}] len [{}]
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[INFO] Stopped plug-in {}
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentEventHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] No component exists for + event.getName()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doAppLogAggregation():[WARN] PendingContainers queue is interrupted
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[WARN] Error retrieving hostname:
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadDelegationKey(byte[]):[DEBUG] DataInputStream initialized
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun():[ERROR] Layout version on remote node (...) does not match this node's layout version (...)
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Storing info for attempt: {}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeMethod(org.apache.hadoop.security.UserGroupInformation,java.util.List,java.lang.Class,java.lang.reflect.Method,java.lang.Object[]):[ERROR] Get connection for...
org.apache.hadoop.examples.QuasiMonteCarlo:estimatePi(int,long,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Job Finished in
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:recoverNumaResource(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Unexpected number: + assignedResources.size() + of assigned numa resources for + containerId + while recovering.
org.apache.hadoop.hdfs.server.balancer.Balancer:runOneIteration():[INFO] Total target DataNodes in this iteration: {dispatcher.moveTasksTotal()}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] BLOCK* addToInvalidates: {storedBlock} {datanodes}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:heartbeat():[DEBUG] After Scheduling:
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:removeReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Sucessfully deleted reservation: {reservation.getReservationId()} in plan.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:validateAuthUserWithEntityUser(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager,org.apache.hadoop.security.UserGroupInformation,java.lang.String):[DEBUG] Authenticated User: {} Requested User:{}
org.apache.hadoop.security.UserGroupInformation:print():[INFO] Group Ids:
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] Could not find {} cookie, so user will not be set, WebAppProxyServlet.PROXY_USER_COOKIE_NAME
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextOp():[ERROR] Failing over to edit log...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getConfigurationFromFileSystem():[INFO] upload conf from fileSystem took X ms
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Preemption monitor:org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy
org.apache.hadoop.security.Groups:refresh():[WARN] Error refreshing groups cache
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenUsingClientCreds(java.lang.String,java.lang.String,java.lang.String):[DEBUG] AADToken: starting to fetch token using client creds for client ID ${clientId}
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.InvariantsChecker:logOrThrow(java.lang.String):[WARN] message
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture):[DEBUG] Ignoring exception raised in task completion:
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:writeDocument(org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.TimelineDocument,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType):[ERROR] Unable to perform upsert for Document Id : {} under Collection : {} under Database {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[ERROR] Unable to obtain the application information for the specified application {}
org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget):[INFO] Successfully transitioned + target + to standby state without fencing
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanupContainersOnNMResync():[INFO] Waiting for containers to be killed
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logUpdateBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean):[DEBUG] logRpcIds
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:loadDriver():[INFO] Connection to the State Store driver {} is open and ready
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration,boolean,boolean):[INFO] Formatting using clusterid: {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.event.Event):[INFO] Application stop event received for stopping AppId: + event.getApplicationID().toString()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:cleanupStagingDirs():[DEBUG] cleanup magic directory Path
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream):[ERROR] {exception_message}
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[INFO] [COMPONENT {}] Health has gone below threshold. Starting health threshold timer at ts = {} ({})
org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean):[DEBUG] responding to call Wrote numBytes bytes.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] NodeManager's totalPmem could not be calculated. Setting it to {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks():[DEBUG] Stopping metrics sink entry.getKey(): class=entry.getValue().sink().getClass()
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl):[INFO] Service stopped, return null for the storage
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long):[DEBUG] Storing token + tokenId.getSequenceNumber()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State):[INFO] Job Abort statistics {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:sendRequestsToResourceManagers(java.util.Map):[INFO] Requests sent to resource managers for new sub-clusters
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementForUser(java.lang.String):[DEBUG] Creating placement context for user {} using secondary group current user mapping
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[INFO] Container {containerIdStr} not launched. No cleanup needed to be done
org.apache.hadoop.util.Progress:set(float):[DEBUG] Illegal progress value found, progress is less than 0. Progress will be changed to 0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat):[INFO] Namenode actor relinquishing ACTIVE state with txid= nnHaState.getTxId()
org.apache.hadoop.mapred.pipes.BinaryProtocol:endOfInput():[DEBUG] Sent close command
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] Stopping JobHistoryEventHandler. Size of the outstanding queue size is {eventQueue.size()}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:setKMSACLs(org.apache.hadoop.conf.Configuration):[INFO] '{} ACL '{}'
org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData):[DEBUG] Error changing ownership of + item, e
org.apache.hadoop.fs.s3a.S3AFileSystem:waitForUploadCompletion(java.lang.String,org.apache.hadoop.fs.s3a.UploadInfo):[INFO] Interrupted: aborting upload
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkStreamers():[DEBUG] original failed streamers: ...
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[INFO] Creating symlink
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:updateContainerInternal(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.ContainerTokenIdentifier):[ERROR] Container resource is being increased or undergoing ExecutionType promotion
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Unexpected state filter for inactive RM node
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink):[DEBUG] some debug message here
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Rename source path: [{}] to dest path: [{}]
org.apache.hadoop.yarn.server.router.clientrm.AbstractClientRequestInterceptor:setupUser(java.lang.String):[INFO] Error while creating Router ClientRM Service for user:, user: + user
org.apache.hadoop.ha.ZKFailoverController:fatalError(java.lang.String):[ERROR] Fatal error occurred: err
org.apache.hadoop.tools.rumen.Folder:run():[INFO] starts-after time is specified. Initial job submit time : ...<br>
org.apache.hadoop.tools.DistCp:createAndSubmitJob():[INFO] DistCp job-id: + jobID
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:shutdown():[WARN] AsyncDataService has already shut down.
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[INFO] Updated xattrs on {}({}) files in zone {} for re-encryption, starting:{}.
org.apache.hadoop.mapred.gridmix.LoadJob$StatusReporter:run():[INFO] Status reporter thread started.
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[DEBUG] Configuration set
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$AddNodeTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[DEBUG] Node added without containers
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf):[INFO] Initialized MemoryMappableBlockLoader
org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Bypassing cache to create filesystem {uri}
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] Requested NameNode ask: %s
org.apache.hadoop.hdfs.DFSClient:createWrappedOutputStream(org.apache.hadoop.hdfs.DFSOutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long):[DEBUG] Start decrypting EDEK for file: {}, output stream: 0x{}
org.apache.hadoop.util.AsyncDiskService:awaitTermination(long):[INFO] All AsyncDiskService threads are terminated.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanUpApplicationsOnNMShutDown():[INFO] Applications still running : ...
org.apache.hadoop.security.ShellBasedIdMapping:reportDuplicateEntry(java.lang.String,java.lang.Integer,java.lang.String,java.lang.Integer,java.lang.String):[WARN] \n + header + String.format("new entry (%d, %s), existing entry: (%d, %s).%n%s%n%s", key, value, ekey, evalue, "The new entry is to be ignored for the following reason.", DUPLICATE_NAME_ID_DEBUG_INFO)
org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver:run():[ERROR] Unable to save image for + sd.getRoot(), t\n
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateApplicationTimeout(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppTimeoutInfo,javax.servlet.http.HttpServletRequest,java.lang.String):[DEBUG] RMAuditLogger log failure for absent application
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Initializing SSL Context to channel mode {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device):[WARN] Non device number provided, cannot decide the device type
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Got exception while resuming container: + StringUtils.stringifyException(e)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:createNewZkPath(java.lang.String):[WARN] NODEEXISTS_MSG
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:preemptOrkillSelectedContainerAfterWait(java.util.Map,long):[DEBUG] Starting to preempt containers for selectedCandidates and size:{}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getRootDir():[DEBUG] Unable to create a temporary directory. Fall back to the default system temp directory {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:unregister(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Can't unregister DN {} because it is not currently registered.
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:createTempAppForResCalculation(org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Collection,org.apache.hadoop.yarn.api.records.Resource,java.util.Map):[DEBUG] TempUser:{}
org.apache.hadoop.util.SysInfoLinux:readProcNetInfoFile():[WARN] Error closing the stream
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[DEBUG] Skipping unreserve on removed node: {nodeID}
org.apache.hadoop.hdfs.DFSInputStream:tryReadZeroCopy(int,java.util.EnumSet):[DEBUG] Unable to perform a zero-copy read from offset {curPos} of {src}; {length63} bytes left in block. blockPos={blockPos}; curPos={curPos}; curEnd={curEnd}
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:getListingFileReader(org.apache.hadoop.conf.Configuration):[ERROR] Couldn't find listing file at: + listingFilePath
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Unable to remove master key X, e
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Save namespace ...
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:putEntities(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[]):[DEBUG] Writing summary log for {} to {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:setChildQueues(java.util.Collection):[DEBUG] setChildQueues: + getChildQueuesToPrint()
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:connectToAzureStorageInSecureMode(java.lang.String,java.lang.String,java.net.URI):[DEBUG] Connecting to Azure storage in Secure Mode
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:updateDemand():[DEBUG] The updated demand for FSParentQueueName is UpdatedDemand; the max is MaxShare
org.apache.hadoop.mapreduce.lib.db.DBSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[INFO] Splitting using BigDecimalSplitter
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:resumeContainer():[WARN] Could not store container [[container.getContainerId()]] state. The Container has been resumed.
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.processor.VolumeAMSProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing CSI volume processor
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerEndEventOnTimelineServiceV2(org.apache.hadoop.yarn.api.records.ContainerStatus,long):[ERROR] Container end event could not be published for {containerId}
org.apache.hadoop.fs.azurebfs.utils.IdentityHandler:lookupForLocalGroupIdentity(java.lang.String):[DEBUG] Result fetched from TextFileBasedIdentityHandler
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:initInternal(org.apache.hadoop.conf.Configuration):[INFO] Invalid value ... for config ... Resetting it to ...
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] NodeHealthReport
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the application table
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:runDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] ContainerId={container.getContainerId()}, docker volume output for {dockerVolumeCommand}: {output}
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] ImageServlet allowing checkpointer: + remoteUser
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:deployUnbonded():[DEBUG] No delegation tokens present: using direct authentication
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyToFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,long,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet,org.apache.hadoop.fs.FileChecksum):[DEBUG] Appending to existing file
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:loadPlugIns(org.apache.hadoop.conf.Configuration):[WARN] Error loading classloader
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] MsiTokenProvider initialized
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor$AzureFileSystemThreadRunnable:run():[ERROR] {} operation failed for file {}
org.apache.hadoop.fs.s3a.S3AFileSystem:close():[DEBUG] Filesystem {} is closed
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeAttributesProvider(org.apache.hadoop.conf.Configuration):[DEBUG] Distributed Node Attributes is enabled with provider class as : ConfigurationNodeAttributesProvider
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Relation of units: + unitsRelation
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doAppLogAggregationPostCleanUp():[DEBUG] Cleaning up {} files
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Finished one round, will wait for [scheduleInterval] for next round
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] AM resource request: exceeds maximum AM resource allowed, dump state
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[DEBUG] Scanning intermediate dir {{absPath}}
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:registerAndInitializeHeartbeat():[INFO] Successfully registered for federation subcluster: {}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent):[WARN] Shuffle failure
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:scanBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[DEBUG] start scanning block {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:autoCreateLeafQueue(org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[INFO] Attempting to auto-create leaf queue
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateAndResolveService(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration):[INFO] Adding component {} from external {}
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovering CA Certificate and Private Key
org.apache.hadoop.mapred.lib.InputSampler$RandomSampler:getSample(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.mapred.JobConf):[DEBUG] seed: + seed
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=GETXATTRS target=path
org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]):[ERROR] args length condition failed, terminating.
org.apache.hadoop.portmap.RpcProgramPortmap:getport(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR):[DEBUG] Portmap GETPORT key= + key + + mapping
org.apache.hadoop.mapred.MapReduceChildJVM:getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId):[INFO] New instance created
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getCachedStore(org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,java.util.List):[DEBUG] try refresh cache {groupId} {appLogs.getAppId()}
org.apache.hadoop.hdfs.server.namenode.Checkpointer:doCheckpoint():[INFO] Checkpoint completed in {[monotonicNow() - startTime] / 1000} seconds. New Image Size: {imageSize}
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:initializePipeline(java.lang.String):[INFO] Initializing request processing pipeline for user: {}
org.apache.hadoop.tools.dynamometer.Client:setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[]):[INFO] Using resource {} directly from current location: {}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:writeFile(org.apache.hadoop.fs.Path,byte[]):[DEBUG] Cleaning up with logger
org.apache.hadoop.yarn.webapp.Controller:renderJSON(java.lang.Object):[DEBUG] org.apache.hadoop.yarn.webapp.Controller: renderJSON called with ${object}
org.apache.hadoop.yarn.client.api.YarnClient:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext):[DEBUG] Submission successful
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor$AzureFileSystemThreadRunnable:run():[ERROR] Encountered Exception for {} operation for file {}
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[]):[DEBUG] Executing command {MarkerTool.MARKERS}
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] Ignoring: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when writing command to temp file, e
org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object):[ERROR] IOStatistics: {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:copyEmptyDirectoryMarkers(java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.impl.DirMarkerTracker):[DEBUG] Copying markers from {dirMarkerTracker}
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getPrediction(java.lang.String):[DEBUG] Predict resource requests for pipelineId: {}.
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object):[WARN] Invalid RPC call version + ver
org.apache.hadoop.hdfs.server.namenode.NNStorage:determineClusterId():[WARN] this sd not available:
org.apache.hadoop.hdfs.qjournal.server.Journal:scanStorageForLatestEdits():[WARN] Latest log has no transactions. moving it aside and looking for previous log ; journal id:
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest):[WARN] Policy for queue: {} does not exist.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] RMStateStore state change from {oldState} to {getRMStateStoreState()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:computeQueueManagementChanges(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue):[DEBUG] {} uses {} millisecond to run
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short):[DEBUG] Sequential invocation on src
org.apache.hadoop.registry.server.services.DeleteCompletionCallback:processResult(org.apache.curator.framework.CuratorFramework,org.apache.curator.framework.api.CuratorEvent):[DEBUG] Delete event {}
org.apache.hadoop.nfs.nfs3.Nfs3Interface:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] File attributes set successfully
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:getApplicationOwner():[INFO] Cleaning up ownerScanner
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfSlowDiskParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}, property, newVal
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:build():[TRACE] {}: returning new remote block reader using UNIX domain socket on {}
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$MD5Filter:accept(java.lang.Object):[WARN] Exception occurred
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=GETACLSTATUS target=path
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getSchedulerConfigurationVersion(javax.servlet.http.HttpServletRequest):[ERROR] Exception thrown when fetching configuration version.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:close():[DEBUG] Closing Abfs: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean,java.util.List,java.util.Queue):[DEBUG] Starting to add to replicas map
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String):[DEBUG] Node {ret} is excluded, continuing.
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:writeFile(org.apache.hadoop.fs.Path,byte[]):[INFO] File system output stream created
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:readDirAsUser(java.lang.String,org.apache.hadoop.fs.Path):[ERROR] ListAsUser for {} returned with exit code: {}
org.apache.hadoop.service.AbstractService:stop():[DEBUG] Ignoring re-entrant call to stop()
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String):[INFO] Token renewal for identifier: ...
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortAllSinglePendingCommits(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[DEBUG] Aborting all pending commit filess under {pendingDir} (recursive={recursive})
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop():[WARN] Stop interrupted
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String):[INFO] {message}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithFailureToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container metrics ended running container
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:populateHeaders(java.util.List,java.lang.String,java.lang.String,int,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,boolean,java.util.Map):[DEBUG] shuffle for jobId reducer reduce length contentLength
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[INFO] Connected to + host
org.apache.hadoop.hdfs.server.namenode.CacheManager:removeCachePool(java.lang.String):[INFO] removeCachePool of + poolName + successful.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeApp(java.lang.String,boolean,java.util.Set):[DEBUG] Removing info for app: appId at: appIdRemovePath and its attempts
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String):[DEBUG] Cancelling token:{} with canceler:{}.
org.apache.hadoop.yarn.server.timeline.LogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[INFO] Parsed {} entities from {} in {} msec
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:rejoinElection():[INFO] Fail to re-join election.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] dfsClient is null
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:processResponse(byte[],int,int):[WARN] IO/Network error: method getMaskedUrl() ex.getMessage()
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:shellToContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ShellContainerCommand):[DEBUG] Websocket exception: + e.getMessage()
org.apache.hadoop.mapred.LocalJobRunner$Job:shuffleError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String):[ERROR] shuffleError: {message} from task: {taskId}
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[WARN] SecondaryNameNode principal not considered, ...
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:checkTrustAndSend(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL encryption trust check: localHostTrusted = {}, remoteHostTrusted = {}
org.apache.hadoop.hdfs.ViewDistributedFileSystem:appendFile(org.apache.hadoop.fs.Path):[WARN] Failed to resolve the path as mount path
org.apache.hadoop.yarn.server.webapp.ContainerBlock:render():[ERROR] Failed to read the container {containerid}.
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startTimer():[WARN] {prefix} metrics system timer already started!
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:findControllerInMtab(java.lang.String,java.util.Map):[WARN] Skipping inaccessible cgroup mount point {e.getKey()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[INFO] <getQueuePath() + ": added new child queue: " + newChildQueue>
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Copying fromPath to toPath
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.event.Event):[INFO] Scheduling Log Deletion for application: <appId> , with delay of <deleteDelaySeconds> seconds
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:isBlockReplicatedOk(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,java.lang.Boolean,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor$BlockStats):[TRACE] Removing unknown block {}
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$MetricsBase:render():[INFO] Generated metrics table
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:listPaths(org.apache.hadoop.hdfs.DFSClient,java.lang.String,byte[]):[INFO] Cookie couldn’t be found: {}, do listing from beginning
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadDelegationTokenFromNode(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState,java.lang.String):[WARN] Content of path is broken.
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:bindToDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Using delegation token {}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:scanBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[WARN] I/O error while finding block {} on volume {}
org.apache.hadoop.hdfs.server.datanode.DataNode:startDataNode(java.util.List,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[INFO] Starting DataNode with maxLockedMemory = {dnConf.maxLockedMemory}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] To-release container={} is in final state
org.apache.hadoop.fs.s3a.S3AUtils:initConnectionSettings(org.apache.hadoop.conf.Configuration,com.amazonaws.ClientConfiguration):[DEBUG] Signer override = {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] DN {dn.getDatanodeUuid()} has no lease to remove.
org.apache.hadoop.hdfs.server.namenode.FSNDNCacheOp:addCacheDirective(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.CacheManager,org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet,boolean):[INFO] Adding cache directive info
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum():[DEBUG] Thread interrupted while performing token counter increment
org.apache.hadoop.util.Shell:joinThread(java.lang.Thread):[WARN] Interrupted while joining on: {t}
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Endpoint {epr} is not the default; parsing
org.apache.hadoop.registry.client.binding.RegistryUtils:statChildren(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String):[DEBUG] stat failed on {}: moved? {}
org.apache.hadoop.fs.cosn.CosNOutputStream:close():[INFO] The outputStream for key: [{}] has been uploaded.
org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[WARN] Unable to execute {ps1script}
org.apache.hadoop.fs.s3a.impl.V2Migration:v1GetObjectMetadataCalled():[WARN] getObjectMetadata() called. This operation and it's response will be changed as part of upgrading S3A to AWS SDK V2
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor:run():[ERROR] Exception while checking heartbeat
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] jwtToken failed validation: + jwtToken.serialize()
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:createEditsSyncDir():[INFO] editsSyncDir + " directory already exists."
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore$EntityDeletionThread:run():[INFO] Deletion thread received interrupt, exiting
org.apache.hadoop.hdfs.server.datanode.DataXceiver:run():[DEBUG] Cached {} closing after {} ops...
org.apache.hadoop.util.LogAdapter:error(java.lang.String):[ERROR] log.error(msg)
org.apache.hadoop.yarn.applications.distributedshell.Client:main(java.lang.String[]):[ERROR] Error running Client
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult):[WARN] Volume {} detected as being unhealthy, reference.getVolume()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:shutdown():[WARN] AsyncDiskService has already shut down.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Directory: [ + path + ] already exists.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(java.lang.Object,java.lang.Object):[INFO] Killing container
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:call():[INFO] Completed re-encrypting one batch of {} edeks from KMS, time consumed: {}, start: {}., result, batch.size(), kmsSW.stop(), batch.getFirstFilePath()
org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData):[WARN] Service did not shut down in time
org.apache.hadoop.fs.s3a.audit.AuditIntegration:createAndStartAuditManager(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.statistics.impl.IOStatisticsStore):[DEBUG] Started Audit Manager {}
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:reencryptEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String):[INFO] Zone {}({}) is submitted for re-encryption., zoneName, inode.getId()
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[WARN] BR lease 0x{} is not valid for DN {}, because the lease has expired.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery():[INFO] Not attempting to recover. The shuffle key is invalid for recovery.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[DEBUG] {}:{}, this, msg
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[WARN] 'userid' was not found in application tags
org.apache.hadoop.hdfs.DistributedFileSystem:getTrashRoots(boolean):[WARN] Cannot get all encrypted trash roots
org.apache.hadoop.lib.service.scheduler.SchedulerService:destroy():[DEBUG] Waiting for scheduler to shutdown
org.apache.hadoop.yarn.webapp.WebApps$Builder:start(org.apache.hadoop.yarn.webapp.WebApp,org.eclipse.jetty.webapp.WebAppContext):[INFO] Web app + name + started at + httpServer.getConnectorAddress(0).getPort()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[]):[DEBUG] Deferring removal of stale storage {} with {} blocks
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor:run():[INFO] Processing Event [event] for Container [containerId]
org.apache.hadoop.hdfs.server.datanode.BPOfferService:verifyAndSetNamespaceInfo(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Acknowledging ACTIVE Namenode during handshake
org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[DEBUG] History proxy cancelled
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[INFO] Recovering task + taskId + from prior app attempt, status was + taskInfo.getTaskStatus()
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:getEstimator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext):[ERROR] Can't make a speculation runtime estimator
org.apache.hadoop.mapreduce.JobSubmitter:submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster):[DEBUG] Creating splits at submitJobDir
org.apache.hadoop.hdfs.server.namenode.FSImage:saveLegacyOIVImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.util.Canceler):[DEBUG] Saving namespace context
org.apache.hadoop.mapreduce.JobSubmitter:readTokensFromFiles(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials):[INFO] loading user's secret keys from + tokensFileName
org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Unable to get key for {} from credential providers. {}, accountName, ioe, ioe
org.apache.hadoop.fs.s3a.S3AFileSystem:createRequestFactory():[WARN] Unknown storage class property STORAGE_CLASS
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Vmem usage in bytes <VMEM_USAGE>
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:refreshScheduler(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationDefinition,java.lang.String):[INFO] Created reservation {0} synchronously., reservationId
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean):[INFO] nodes are empty for write pipeline of + block
org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object):[DEBUG] RpcKind = rpcKind Protocol Name = protocolName version = version ProtocolImpl = protocolImpl.getClass().getName() protocolClass = protocolClass.getName()
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] Stopping service {}, with appId = {}
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:balanceVolumeSet(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[DEBUG] Step : {}
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl$FSAction:runWithRetries():[INFO] Exception while executing a FS operation.
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[TRACE] op=
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getCpuUsagePercent():[DEBUG] CPU Comparison: + procfsUsage + + cgroupUsage
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[ERROR] Failed to create service {} :
org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService:updateToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long):[DEBUG] Updating token in Leveldb state store
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable):[DEBUG] Done
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Node after allocation + nm.getNodeID() + resource = + node.getUnallocatedResource()
org.apache.hadoop.hdfs.DFSInputStream:fetchAndCheckLocatedBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlocks):[DEBUG] newInfo = {}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Killed: countHere
org.apache.hadoop.hdfs.KeyProviderCache:createKeyProviderURI(org.apache.hadoop.conf.Configuration):[ERROR] KeyProvider URI string is invalid [providerUriStr]!!
org.apache.hadoop.yarn.service.client.ServiceClient:printLocalResources(java.util.Map):[DEBUG] {}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.InternalScanner):[DEBUG] preFlush store = [store details based on code logic]
org.apache.hadoop.hdfs.DeadNodeDetector:probeCallBack(org.apache.hadoop.hdfs.DeadNodeDetector$Probe,boolean):[DEBUG] Probe datanode: {} result: {}, type: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerCleanedupAfterKillToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container state transitioned successfully
org.apache.hadoop.mapreduce.task.reduce.Fetcher:run():[DEBUG] Shuffling from host
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[INFO] rpcClient.invokeSequential executed
org.apache.hadoop.hdfs.server.namenode.JournalSet:mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String):[ERROR] Error: status failed for too many journals
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] namenodes = <namenodes>
org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig):[INFO] Allow All Origins: + allowAllOrigins
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceStop():[ERROR] Error stopping proxy web server
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[INFO] STATE* UnderReplicatedBlocks has {} blocks
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(java.lang.Object,java.lang.Object):[INFO] [INIT COMPONENT + component.getName() + ]: + event.getDesired() + instances.
org.apache.hadoop.hdfs.server.namenode.NNStorage:attemptRestoreRemovedStorage():[INFO] NNStorage.attemptRestoreRemovedStorage: check removed(failed) storage. removedStorages size = {}
org.apache.hadoop.tools.util.ProducerConsumer:put(org.apache.hadoop.tools.util.WorkRequest):[ERROR] Could not put workRequest into inputQueue. Retrying...
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:dump(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] MOUNT NULLOP : client: {client}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getAllApplications():[ERROR] History information of application "+" appId +" is not included into the result due to the exception
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:renewDelegationToken(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken,javax.servlet.http.HttpServletRequest,org.apache.hadoop.security.UserGroupInformation):[INFO] Renew delegation token request failed
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:writeDocument(java.lang.Object,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType):[DEBUG] Upserting document under collection : {} with entity type : {} under Database {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeReferenceSection(java.io.InputStream):[INFO] Loaded [counter] inode references
org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map):[ERROR] Can't close BufferedReader of command result
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:getEntity(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineDataToRetrieve):[DEBUG] NoOpTimelineReader is configured. Response to all the read requests would be empty
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] Preempting .. container(s)
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:isMultiDestDirectory(java.lang.String):[DEBUG] The destination {} is a symlink.
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceStart():[INFO] HTTP server started on address
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StartContainerTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown from onContainerStarted for Container {containerId}
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop():[INFO] History service stopped
org.apache.hadoop.hdfs.DFSClient:primitiveMkdir(java.lang.String,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] {}: masked={}, src, absPermission
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:readReplicasFromCache(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Replica Cache file: path has gone stale
org.apache.hadoop.crypto.key.kms.server.KMS:reencryptEncryptedKeys(java.lang.String,java.util.List):[TRACE] Exiting reencryptEncryptedKeys method.
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:shutdown():[WARN] Shutdown has been called, but periodic scanner not started
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.LinkedList,org.apache.hadoop.tools.CopyListingFileStatus):[DEBUG] concat: other chunk: 0: + dstfs.getFileStatus(f)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:doPlacement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.BatchedRequests,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithmOutput,java.util.List,java.util.List,java.util.Map):[WARN] No nodes available for placement at the moment !!
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:authorizeInternal(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Attempt to authorize path
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:getCommitterFactory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] No scheme-specific factory defined in {}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:packetSentInTime():[DEBUG] A packet was last sent {}ms ago.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:onDevicesAllocated(java.util.Set,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.YarnRuntimeType):[INFO] Nvidia Docker v2 assigned GPU: [minorNumbers]
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:updateApplicationPriority(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationPriorityRequest):[WARN] Failure in updating application priority
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:stop():[WARN] Edit log tailer thread exited with an exception
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameTo(java.lang.String,java.lang.String,boolean):[INFO] Audit log event: rename (options=[…]) src dst
org.apache.hadoop.yarn.webapp.Dispatcher:service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] dev mode restart requested
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Write lock released
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveErasureCodingPolicy(java.lang.String,boolean):[DEBUG] RPC ids logged with op
org.apache.hadoop.fs.azure.SimpleKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Unable to get key for accountName from credential providers. ioe, ioe
org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService:removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Token master key removed from leveldb state store
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.hdfs.server.datanode.DataNode:removeVolumes(java.util.Collection,boolean):[INFO] Deactivating volumes (clear failure=true/false): {storageLocations}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Updated reserved container X on node Y for application Z
org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean):[WARN] null file argument.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container):[INFO] Finding containerReq for allocated container:
org.apache.hadoop.registry.cli.RegistryCli:resolve(java.lang.String[]):[ERROR] Exception analyzed and printed
org.apache.hadoop.mapred.QueueConfigurationParser:parseResource(org.w3c.dom.Element):[INFO] Bad configuration no queues defined
org.apache.hadoop.hdfs.DFSClient:createWrappedOutputStream(org.apache.hadoop.hdfs.DFSOutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long):[DEBUG] Decrypted EDEK for file: {}, output stream: 0x{}
org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[ERROR] BUG: Method {} was unable to be found on any of the underlying proxies for {}
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration):[INFO] Filter added
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File[],boolean):[ERROR] In-progress edits file + f + has improperly formatted transaction ID
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEvent):[ERROR] Unknown event type on UpdateContainer: {event.getType()}
org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[]):[DEBUG] Metrics Config: {mc}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:updateMetricsForDeactivatedNode(org.apache.hadoop.yarn.api.records.NodeState,org.apache.hadoop.yarn.api.records.NodeState):[WARN] Unexpected final state
org.apache.hadoop.hdfs.server.datanode.DataNode:startPlugins(org.apache.hadoop.conf.Configuration):[ERROR] Unable to load DataNode plugins. Specified list of plugins: {}
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] {}: the DfsClientShmManager has been closed.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[INFO] Successfully cached one replica:{key} into persistent memory, [cached path={cachePath}, length={length}]
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop():[INFO] The background thread stopped.
org.apache.hadoop.hdfs.PeerCache:getInternal(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean):[WARN] got IOException closing stale peer {peer}, which is {ageMs} ms old
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read():[DEBUG] read requested b.length = {} offset = {} len = {}, b.length, off, len
org.apache.hadoop.util.LogAdapter:info(java.lang.String):[INFO] LOGGER.info(msg)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToStandby(boolean):[INFO] Transitioning to standby state
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain:main(java.lang.String[]):[ERROR] FATAL, Error while starting FS configuration conversion, see previous error messages for details!
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Strict memory control enabled: {}
org.apache.hadoop.mapred.Merger$MergeQueue:merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress):[INFO] Down to the last merge-pass, with segments left
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LaunchTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Killing {containerId} due to recovered as killed
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[DEBUG] Block locations retrieved
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:periodicInvoke():[DEBUG] Start to update quota cache.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.getFileStatus path: {}
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket):[TRACE] Hosts:{}, CNs:{} subjectAlts:{}, ie6:{}, strictWithSubDomains{}
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleCommit(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,int):[INFO] Success
org.apache.hadoop.mapred.pipes.Application$PingSocketCleaner:run():[ERROR] PingSocketCleaner exception
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:removeQueue(java.lang.String):[INFO] Removal of AutoCreatedLeafQueue + queueName + has succeeded
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[DEBUG] Attempting to read aggregated logs using TFileController
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeApp(java.lang.String):[DEBUG] Removing info for app: {} at: {} and its attempts.
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:doChooseVolume(java.util.List,long,java.lang.String):[DEBUG] All volumes are within the configured free space balance threshold. Selecting volume for write of block size replicaSize
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Error launching job , Output path already exists :
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[DEBUG] Checking file
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:unregister():[ERROR] Exception while unregistering , are
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[INFO] Got node report from ASM
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[DEBUG] Given mode: {} is invalid
org.apache.hadoop.streaming.PipeMapRed:addEnvironment(java.util.Properties,java.lang.String):[INFO] Skip env entry: <dynamic_value>
org.apache.hadoop.fs.s3a.commit.files.SuccessData:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] Reading success data from {}
org.apache.hadoop.crypto.key.kms.server.KMS:handleEncryptedKeyOp(java.lang.String,java.lang.String,java.util.Map):[TRACE] Exiting handleEncryptedKeyOp method.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} + maxCount: {} client: {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:run():[INFO] Dumper got Throwable. dumpFilePath: {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:populateTokenSequenceNo(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest,org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] Token sequence no received from heartbeat request: ... System credentials for apps size: ...
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:shouldRetry(java.io.IOException,int,java.lang.String):[WARN] Original exception is
org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader():[DEBUG] Using plugin jars: {}
org.apache.hadoop.mapred.JobACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList):[DEBUG] checkAccess job acls, jobOwner: jobOwner jobacl: jobOperation.toString() user: callerUGI.getShortUserName()
org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call):[INFO] ${Thread.currentThread().getName() + ", call " + call}, ${e}
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:toResponse(java.lang.Throwable):[WARN] INTERNAL_SERVER_ERROR
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[ERROR] Received container status for unknown container:
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[WARN] couldn't find app + appId + while processing FINISH_CONTAINERS event
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:init(org.apache.hadoop.conf.Configuration):[INFO] Reading NUMA topology using configurations.
org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler:handle(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException):[DEBUG] Volume {volume}: block {block} is no longer in the dataset.
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler$PingChecker:checkRunning(long):[DEBUG] Task AttemptID: <entry.getKey().toString()> task timeout set: <taskTimeOut / 1000>s, taskTimedOut: <taskTimedOut>; task stuck timeout set: <taskStuckTimeOut / 1000>s, taskStuck: <taskStuck>
org.apache.hadoop.yarn.state.SingleArcTransition:transition(java.lang.Object,java.lang.Object):[INFO] App completely done transition executed.
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getDestinationForPath(java.lang.String):[DEBUG] Ordered locations following {} are {}, order, mountTableResult
org.apache.hadoop.tools.SimpleCopyListing:writeToFileListing(java.util.List,org.apache.hadoop.io.SequenceFile$Writer):[DEBUG] REL PATH: {}, FULL PATH: {}
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Applying UMask
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List):[DEBUG] Queueing {} entries
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path):[INFO] [{}] {}
org.apache.hadoop.fs.store.DataBlocks$DataBlock:close():[DEBUG] Closed {}
org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker:schedule(org.apache.hadoop.hdfs.server.datanode.checker.Checkable,java.lang.Object):[INFO] Scheduling a check for {}, target
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:waitForAllPartUploads():[WARN] Interrupted partUpload, ie
org.apache.hadoop.tools.DistCh:run(java.lang.String[]):[INFO] ops= + ops
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[DEBUG] Succeeded to start Container {}
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocate(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Set,java.util.Set,int):[INFO] Opportunistic allocation requested for [priority={}, allocationRequestId={}, num_containers={}, capability={}] allocated = {}
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:shouldRun(java.lang.String,java.lang.String):[INFO] Missing location for the node health check script "{}".
org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedExceptionAction):[DEBUG] PrivilegedActionException as: {}
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[INFO] Retrieved job using jobID
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:start():[DEBUG] Started entry writer {}
org.apache.hadoop.registry.server.dns.RegistryDNS:generateReply(org.xbill.DNS.Message,java.net.Socket):[DEBUG] returning null
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[WARN] [COMPONENT {}] Current health {}% has been below health threshold of {}% for {} secs (threshold window = {} secs)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAclEntries(java.lang.String,java.util.List):[ERROR] Access control exception in removeAclEntries
org.apache.hadoop.mapred.Merger$MergeQueue:merge(java.lang.Class,java.lang.Class,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress):[INFO] Merging intermediate segments
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeApplicationStateInternal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Removing state for app + appId + and + appState.attempts.size() + attempts + at + appKey
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand):[DEBUG] using cgroup parent: {cGroupPath}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:executeStage(org.apache.commons.lang3.tuple.Triple):[INFO] {}: Files committed: {}. Total size {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[WARN] Event {event} sent to absent container {event.getContainerID()}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot remove record {recordToRemovePath}
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache:getDfsClient(java.lang.String,int):[ERROR] Failed to create DFSClient for user: {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:chooseStorage4Block(org.apache.hadoop.fs.StorageType,long):[DEBUG] The node {} does not have enough {} space (required={}, scheduled={}, remaining={}).
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:uploadLogsForContainers(boolean):[DEBUG] No pending container in this cycle
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:evictOldDBs():[INFO] Evicting {getName()} DBs scheduled for eviction
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo():[DEBUG] Native call failed
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.Options$Rename[]):[DEBUG] DIR* NameSystem.renameTo: with options - src to dst
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[DEBUG] executing a request outside an audit span ...
org.apache.hadoop.examples.terasort.TeraSort:run(java.lang.String[]):[INFO] done
org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer:startLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,int):[INFO] Journal node retrieved or created
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:refreshToken():[DEBUG] CustomTokenProvider Access token fetch failed with retry count {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getNumberOfDatanodes(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[INFO] Read unlock issued for getNumberOfDatanodes
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetQuota(java.lang.String,long,long):[INFO] Namespace quota set
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:pauseContainer():[INFO] Pausing the container [containerIdStr]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:init():[WARN] RuntimeException of NvidiaDockerV1CommandPlugin init:, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[DEBUG] Current ProcessTree list: [ p.getPID() list ]
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:getEntryToEvict():[TRACE] openFileMap size:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Queue Management Change event cannot be applied for parent queue
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:executeStage(java.util.Collection):[DEBUG] {}: Created {} directories
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovered + numTokens + RM delegation tokens
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleReportedIncreasedContainers(java.util.List):[INFO] Container + containerId + already scheduled for + cleanup, no further processing
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:verifyTimeStamp(org.apache.hadoop.hdfs.server.diskbalancer.planner.NodePlan):[ERROR] Disk Balancer - Plan was generated more than {planValidity} ago
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:recoverLease(java.lang.String,java.lang.String):[INFO] Lease recovery started for src by clientName from clientMachine
org.apache.hadoop.hdfs.server.namenode.NameNode:stopHttpServer():[ERROR] Exception while stopping httpserver
org.apache.hadoop.util.Progress:set(float):[DEBUG] Illegal progress value found, progress is Float.NaN. Progress will be changed to 0
org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker:check(org.apache.hadoop.conf.Configuration,java.util.Collection):[WARN] StorageLocation {location} detected as failed.
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:dumpLocalResources():[DEBUG] {} resources:
org.apache.hadoop.security.authentication.client.Authenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[INFO] DelegationToken authentication attempted
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_RACK_FAULT_TOLERANT_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than 0.5 so datanodes with more used percent will receive more block allocations.
org.apache.hadoop.hdfs.server.datanode.DataXceiver:checkAccess(java.io.OutputStream,boolean,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.datatransfer.Op,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode,org.apache.hadoop.fs.StorageType[],java.lang.String[]):[WARN] Block token verification failed: op={}, remoteAddress={}, message={}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId):[DEBUG] MemoryMappableBlockLoader used for block loading
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int):[TRACE] Entering generateEncryptedKeys method.
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:resumeForTesting():[INFO] Resuming re-encrypt handler for testing.
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture):[DEBUG] Waiting for task completion
org.apache.hadoop.mapreduce.util.ProcessTree:sendSignal(java.lang.String,int,java.lang.String):[INFO] Sending signal to all members of process group ${pid}: ${signalName}. Exit code ${shexec.getExitCode()}
org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[]):[DEBUG] Could not locate file {fname}
org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration):[INFO] Renewed {{token.getKind()}}:{{token.getService()}} until {{formatDate(result)}}
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:setConf(org.apache.hadoop.conf.Configuration):[WARN] The value of {preference_fraction_key} is less than 0.5 so volumes with less available disk space will receive more block allocations
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:pickOpportunisticContainersToReclaimResources(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] There are no sufficient resources to start guaranteed [{}] at the moment. Opportunistic containers are in the process of being killed to make room.
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stopContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.api.ContainerType,int):[WARN] Log aggregation is not initialized for + containerId + , did it fail to start?
org.apache.hadoop.mapred.ShuffleHandler:startStore(org.apache.hadoop.fs.Path):[INFO] Creating state database at + dbfile
org.apache.hadoop.hdfs.server.datanode.DataNode:deleteBlockPool(java.lang.String,boolean):[INFO] deleteBlockPool command received for block pool {}, force={}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path):[DEBUG] Failed to list uploads under {}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] Got container status for DATANODE:
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:sendRequestsToResourceManagers(java.util.Map):[INFO] Allocating asynchronously to home sub-cluster
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:createAllDirectories(java.util.Collection):[INFO] Time to prepare directories {humanTime(d.toMillis())}
org.apache.hadoop.hdfs.server.blockmanagement.ErasureCodingWork:createReplicationWork(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo):[DEBUG] Add replication task from source {} to target {} for EC block {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:prepareForLaunch(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[INFO] Container + containerId + not launched as + cleanup already called
org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline:createRestartMetaStream():[WARN] Failed to delete restart meta file: [restartMeta.getPath()]
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:readResultsWithTimestamps(org.apache.hadoop.hbase.client.Result,byte[],byte[],org.apache.hadoop.yarn.server.timelineservice.storage.common.KeyConverter,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,boolean):[ERROR] Illegal column found, skipping this column.
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:doUnregistration():[INFO] Waiting for application to be successfully unregistered.
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:chooseFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[ERROR] Cannot get node mapping when resolving {} at {} from {}, path, loc, clientAddr
org.apache.hadoop.hdfs.server.datanode.DataNode:getStorageLocations(org.apache.hadoop.conf.Configuration):[ERROR] Failed to initialize storage directory {}.Exception details: {}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkAllVolumes(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi):[WARN] checkAllVolumesAsync - no volumes can be referenced
org.apache.hadoop.hdfs.server.datanode.LocalReplica:truncateBlock(long):[INFO] truncateBlock: blockFile= + blockFile + , metaFile= + metaFile + , oldlen= + oldlen + , newlen= + newlen
org.apache.hadoop.mapred.MapTask:runOldMapper(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.split.JobSplit$TaskSplitIndex,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter):[INFO] numReduceTasks: x
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] No version file in
org.apache.hadoop.service.LoggingStateChangeListener:stateChanged(org.apache.hadoop.service.Service):[INFO] Entry to state {service.getServiceState()} for {service.getName()}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean):[WARN] BLOCK* removeDatanode: {node} does not exist
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteObjects(com.amazonaws.services.s3.model.DeleteObjectsRequest):[DEBUG] {}: "{}" - {}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[ERROR] StoragePolicySatisfier thread received runtime exception.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:tryCreatingHistoryDirs(boolean):[INFO] Waiting for FileSystem at {intermediateDoneDirPath.toUri().getAuthority()} to be available
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handleExcludeNodeList(boolean,int):[INFO] Recommission node with state DECOMMISSIONING
org.apache.hadoop.hdfs.server.namenode.FSDirectory:addEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields,org.apache.hadoop.hdfs.server.namenode.XAttrFeature):[WARN] Error parsing protocol buffer of EZ XAttr [xattr.getName()] dir:[inode.getFullPathName()]
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:getCommitterFactory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[INFO] No output committer factory defined, defaulting to FileOutputCommitterFactory
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:createNewApplication(javax.servlet.http.HttpServletRequest):[DEBUG] Generating new application ID
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getBlockReports(java.lang.String):[INFO] Generating block reports
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Getting list of all Jobs.
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[DEBUG] Removing token at {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[]):[INFO] Invoke single method executed
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortMultipartCommit(java.lang.String,java.lang.String):[DEBUG] Aborting commit ID %s to path %s
org.apache.hadoop.hdfs.server.balancer.Balancer$Cli:parse(java.lang.String[]):[INFO] Will run the balancer even during an ongoing HDFS upgrade
org.apache.hadoop.crypto.key.kms.server.KMS:deleteKey(java.lang.String):[DEBUG] Deleting key with name {}., name
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:deleteRMStateStore(org.apache.hadoop.conf.Configuration):[INFO] Deleting ResourceManager state store...
org.apache.hadoop.util.DurationInfo:close():[INFO] {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfigurationProvider:revertToOldConfig(org.apache.hadoop.conf.Configuration):[DEBUG] Reverting to old configuration
org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails):[WARN] Slow RPC : {} took {} {} to process from client {}, the processing detail is {}
org.apache.hadoop.hdfs.server.datanode.BlockSender:getPartialChunkChecksumForFinalized(org.apache.hadoop.hdfs.server.datanode.FinalizedReplica):[WARN] meta file {finalized.getMetaFile()} is missing!
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:runLoop(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[TRACE] {}: no block pools are ready to scan yet. Waiting {} ms., this, timeout
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:internalUpdateAttributesOnNodes(java.util.Map,org.apache.hadoop.yarn.server.api.protocolrecords.AttributeMappingOperationType,java.util.Map,java.lang.String):[DEBUG] {}
org.apache.hadoop.util.SysInfoLinux:readProcStatFile():[WARN] Error reading the stream {exception details}
org.apache.hadoop.fs.s3a.S3AUtils:createAWSCredentialProviderSet(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] For URI {}, using credentials {}
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[TRACE] Delegation token found: {}
org.apache.hadoop.mapred.gridmix.StressJobFactory:checkLoadAndGetSlotsToBackfill():[WARN] Ignoring blacklisted job: + id
org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite):[DEBUG] New instance created
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:serviceStop():[WARN] Interrupted while waiting for deletion thread to complete, closing db now
org.apache.hadoop.fs.s3a.S3AFileSystem:bindAWSClient(java.net.URI,boolean):[DEBUG] Using delegation tokens
org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Saver:serializeDirDiffList(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.util.List,java.io.OutputStream):[ERROR] Name 'elementName' is repeated in the 'deleted' difflist of directory 'dirFullPath', INodeId=dir.getId()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[WARN] Default network: ... is not in the set of allowed networks: ... Please check configuration
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[INFO] Block recovery attempt for + block + rejected, as the + previous attempt times out in + timeoutIn + seconds.
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[INFO] Trying method + (++i) + / + methods.size() + : + method
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean):[ERROR] Allocated empty container + rmContainer.getContainerId()
org.apache.hadoop.fs.s3a.S3AInputStream:read(long,byte[],int,int):[ERROR] EOFException encountered
org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector:close(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Closing output
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNNStartedTimeInMillis():[DEBUG] Failed to get the router startup time
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:skip(long):[TRACE] skip(n={}, block={}, filename={}): discarded {} bytes from dataBuf and advanced dataPos by {}
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:updateTimelineCollectorContext(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow version: {}, flowVersion
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber:run():[DEBUG] Namenode is in safemode, skipping scrubbing of corrupted lazy-persist files.
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:nextOpImpl(boolean):[ERROR] caught exception initializing this
org.apache.hadoop.hdfs.qjournal.client.QuorumCall:getQuorumTimeoutIncreaseMillis(long,int):[INFO] Pause detected while waiting for QuorumCall response; increasing timeout threshold by pause time of {pauseTime} ms.
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntityByTime(byte[],java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl):[ERROR] IOException encountered in seek
org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation):[WARN] Couldn't setup connection for <username> to <remoteId>
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:scanForLogs():[DEBUG] Scanner skips for unknown dir/file {statAttempt.getPath()}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] updating the following ReservationRequest: ...
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart():[DEBUG] Starting AsyncCallQueue.Processor + daemon
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:rename(java.lang.String,java.lang.String):[ERROR] Rename object unsuccessfully. source cos key: [srcKey], dest COS key: [dstKey], exception: [Exception e]
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:reconcile():[DEBUG] reconcile start DirectoryScanning
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[INFO] Node update recording started
org.apache.hadoop.tools.rumen.HistoryEventEmitter:maybeParseCounters(java.lang.String):[WARN] The counter string, "{counters}" is badly formatted.
org.apache.hadoop.mapred.YARNRunner:warnForJavaLibPath(java.lang.String,java.lang.String,java.lang.String,java.lang.String):[WARN] Usage of -Djava.library.path in + javaConf + can cause + programs to no longer function if hadoop native libraries + are used. These values should be set as part of the + LD_LIBRARY_PATH in the + component + JVM env using + envConf + config settings.
org.apache.hadoop.fs.s3a.S3AFileSystem:bindAWSClient(java.net.URI,boolean):[DEBUG] Using existing delegation token
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:reInitializeContainer(org.apache.hadoop.yarn.api.protocolrecords.ReInitializeContainerRequest):[INFO] Error when parsing local resource URI for upgrade of Container [ + containerId + ]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Acquired read lock
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:executeStage(java.lang.Object):[DEBUG] {}: Created {} directories
org.apache.hadoop.hdfs.server.namenode.FSEditLog:selectInputStreams(long,long,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean):[ERROR] Exception while selecting input streams
org.apache.hadoop.tools.dynamometer.ApplicationMaster$LaunchContainerRunnable:run():[INFO] Setting up container launch context for containerid=${container.getId} isNameNode=${isNameNodeLauncher}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:load():[TRACE] load({}, {}): loaded iterator {} from {}: {}, storageID, bpid, name, file.getAbsoluteFile(), WRITER.writeValueAsString(state)
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor$MonitorThread:run():[DEBUG] {} health check succeeded, assuming storage is up
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[INFO] Setting owner name and group
org.apache.hadoop.hdfs.server.namenode.FSEditLog:printStatistics(boolean):[INFO] Number of transactions: <numTransactions> Total time for transactions(ms): <totalTimeTransactions> Number of transactions batched in Syncs: <numTransactionsBatchedInSync.longValue()> Number of syncs: <editLogStream.getNumSync()> SyncTimes(ms): <journalSet.getSyncTimes()>
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:createRbw(org.apache.hadoop.fs.StorageType,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,boolean):[WARN] Insufficient space for placing the block on a transient volume, fall back to persistent storage
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean):[WARN] Ignored {} nodes while loading key cache.
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:intPositiveOption(org.apache.hadoop.conf.Configuration,java.lang.String,int):[WARN] key + is configured to + v + , will use default value: + defVal
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean,long):[DEBUG] logRpcIds executed
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:verifyJournalRequest(org.apache.hadoop.hdfs.server.protocol.JournalInfo):[WARN] Invalid clusterId in journal request - expected [journalInfo.getClusterId()] actual [namesystem.getClusterId()]
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$SendEntity:run():[DEBUG] Number of timeline entities being sent in batch: {}
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean):[DEBUG] flushing segment 0
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager:getTopUsersForMetric(long,java.lang.String,org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$RollingWindowMap):[DEBUG] gc window of metric: {} userName: {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:createStreamPair(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherOption,java.io.OutputStream,java.io.InputStream,boolean):[DEBUG] Creating IOStreamPair of CryptoInputStream and CryptoOutputStream.
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Get cpu usage <CPU_USAGE>
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddErasureCodingPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy,boolean):[DEBUG] logRpcIds called
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Configuring job jar
org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive():[INFO] But old node has our own data, so don't need to fence it.
org.apache.hadoop.yarn.service.component.Component:requestContainers(long):[INFO] Submitting container request : {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor:run():[WARN] NameNode low on available disk space. Entering safe mode.
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:getDelegationToken(org.apache.hadoop.io.Text):[DEBUG] Generate delegation token with renewer
org.apache.hadoop.mapred.MapTask:createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter):[WARN] Unable to initialize MapOutputCollector + clazz.getName() + " (" + remainingCollectors + " more collector(s) to try)"
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]):[DEBUG] key: attr
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:serviceDependencySatisfied(org.apache.hadoop.yarn.service.api.records.Service):[INFO] Service dependency is not satisified.
org.apache.hadoop.hdfs.DFSOutputStream:flushInternalWithoutWaitingAck():[DEBUG] DFSClient check open
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet):[DEBUG] AzureBlobFileSystem.setXAttr path: {}
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] Remote address for request is: {httpReq.getRemoteAddr()}
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceStop():[WARN] Interrupted Exception while stopping
org.apache.hadoop.hdfs.server.datanode.DataXceiver:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[]):[INFO] Receiving {} src: {} dest: {}
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unknown event arrived at OpportunisticContainerAllocatorAMService: {event}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Invalid argument, data size is less than count in request
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:failDestinationExists(org.apache.hadoop.fs.Path,java.lang.String):[ERROR] Failing commit by job {...} to write to existing output path {...}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getClientRMProxyForSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[ERROR] Unable to create the interface to reach the SubCluster
org.apache.hadoop.hdfs.server.namenode.FSEditLog:getJournalClass(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] No class configured for {uriScheme}, {key} is empty
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[DEBUG] PID: ...
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:shouldDeleteLogDir(org.apache.hadoop.fs.FileStatus,long,org.apache.hadoop.fs.FileSystem):[DEBUG] Error reading the contents of {dir.getPath()}, IOException logged
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:generateOverview(org.apache.hadoop.yarn.api.records.ApplicationAttemptReport,java.util.Collection,org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo,java.lang.String):[INFO] Application Attempt Overview
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[INFO] Task failed {}
org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory:buildAmazonS3Client(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters):[WARN] List inconsistency is no longer emulated; only throttling and read errors
org.apache.hadoop.yarn.server.resourcemanager.AdminService:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[INFO] User X successfully called transitionToActive
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkCommit(org.apache.hadoop.hdfs.DFSClient,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean):[ERROR] Got stream error during data sync
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing reservation allocation. reservationEvent.getReservationIdName()
org.apache.hadoop.yarn.service.client.ServiceClient:flexComponents(java.lang.String,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[INFO] [COMPONENT <entry.getKey()>]: number of containers changed from <entry.getValue()> to <componentCounts.get(entry.getKey())>
org.apache.hadoop.tools.OptionsParser:parse(java.lang.String[]):[WARN] -f is a deprecated option. Ignoring.
org.apache.hadoop.mapreduce.security.TokenCache:loadTokens(java.lang.String,org.apache.hadoop.mapred.JobConf):[DEBUG] load job token from a file
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[WARN] If other tasks/jobs are writing to {}, this action may cause them to fail
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] <PROCESS_TREE_DUMP>
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameservices():[ERROR] Cannot retrieve nameservices for JMX: {}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver$ExpireChecker:run():[DEBUG] [key:ip] Expired after expiryIntervalMs / 1000 secs
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:processXml():[DEBUG] Loading <fsimage>.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[DEBUG] Checking operation (inside try block)
org.apache.hadoop.tools.DistCh$ChangeInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int):[INFO] numSplits= + numSplits + , splits.size()= + splits.size()
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:failDestinationExists(org.apache.hadoop.fs.Path,java.lang.String):[INFO] Partial Directory listing
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:processResponseQueue():[INFO] Application {} goes to finish.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:run():[DEBUG] DatanodeAdminMonitorV2 is running.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:markContainerToNonKillable(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] Container removed from killable list
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:doPreUpgrade():[ERROR] Failed to move aside pre-upgrade storage in image directory
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:startFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.permission.PermissionStatus,java.lang.String,java.lang.String,java.util.EnumSet,boolean,short,long,org.apache.hadoop.fs.FileEncryptionInfo,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean,java.lang.String,java.lang.String,boolean):[DEBUG] DIR* NameSystem.startFile: added src inode id holder
org.apache.hadoop.yarn.webapp.Router:lookupRoute(org.apache.hadoop.yarn.webapp.WebApp$HTTP,java.lang.String):[DEBUG] prefix match2 for {}: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap):[TRACE] storageTypes={}
org.apache.hadoop.nfs.NfsExports:getInstance(org.apache.hadoop.conf.Configuration):[ERROR] Invalid NFS Exports provided:
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.PlanningAlgorithm:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[ERROR] Planning failed: No valid allocation found
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown():[DEBUG] Redundant shutdown
org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration):[DEBUG] Stream closed
org.apache.hadoop.hdfs.server.datanode.DataNode:refreshVolumes(java.lang.String):[INFO] Successfully added volume: {volume}
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] Setting keystore location to
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:run():[DEBUG] Sending Heartbeat to RM. AskList:{}
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:parseTokenFromStream(java.io.InputStream,boolean):[DEBUG] Expiry based on expires_on: {expiresOnInSecs}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Token added
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Filesystem {} is using delegation tokens of kind {}, getCanonicalUri(), tokenBindingName
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String):[INFO] Token cancellation requested for identifier: + formatTokenId(id)
org.apache.hadoop.tools.dynamometer.Client:launchAndMonitorWorkloadDriver(java.util.Properties):[ERROR] Exception encountered while running workload job
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext,java.lang.String):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:innerCommitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Failed to save task commit data to {}
org.apache.hadoop.yarn.event.EventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Interrupted. Trying to exit gracefully.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[WARN] Node exists exception
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:namenodeHeartbeat(org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatRequest):[DEBUG] Updating NN registration: {} -> {}
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Service initialization with configuration.
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getTask(org.apache.hadoop.mapred.JvmContext):[INFO] JVM with ID: + jvmId + asking for task before AM launch registered. Given null task
org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List):[DEBUG] Exceptions occurred: + ioe
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Cannot rename the root directory of a filesystem.
org.apache.hadoop.hdfs.web.URLConnectionFactory:openConnection(java.net.URL):[DEBUG] open URL connection
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:getEntity(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineDataToRetrieve):[INFO] Cannot find entity {id:context.getEntityId() , type:context.getEntityType()}. Will send HTTP 404 in response.
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadReservationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovered {numReservations} reservations
org.apache.hadoop.streaming.PipeMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[INFO] [getContext(), IOException]
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService$FileSystemCachePurger:run():[WARN] Error while purging filesystem, ex.toString()
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: {event.getClass()}
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier):[ERROR] Unable to remove token [tokenId.getSequenceNumber()], e
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List):[INFO] Scanned {} INode directories to build namespace.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerBecomeReadyTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] Incrementing containers ready
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:retrieveBlock(java.lang.String,long,long):[ERROR] Retrieving COS key: [{}] with byteRangeStart: [{}] occurs an exception: [{}].
org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem):[WARN] path + " does not have jars in it. It will be ignored."
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[ERROR] Failed to create file:{fileToCreate} at fallback : {linkedFallbackFs.getUri()}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:loadVolumes(java.lang.String[]):[ERROR] Failed to parse persistent memory volume
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$RedundancyMonitor:run():[ERROR] RedundancyMonitor thread received Runtime exception.
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap):[DEBUG] Caught exception was: ...
org.apache.hadoop.hdfs.tools.DFSck:listCorruptFileBlocks(java.lang.String,java.lang.String):[INFO] The filesystem under path 'dir' has numCorrupt CORRUPT files
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaBinaryHelper):[INFO] Trying to discover GPU information ...
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches():[ERROR] Error updating cache for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:checkInterfaceCompatibility(java.lang.Class,java.lang.Class):[INFO] {expectedClass.getSimpleName()} compatibility is ok.
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureEndpoint(com.amazonaws.services.s3.AmazonS3Builder,com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration):[DEBUG] SDK_REGION_CHAIN_IN_USE
org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node):[INFO] Adding a new node:
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[DEBUG] saveNameSystemSection completed
org.apache.hadoop.fs.s3a.impl.RenameOperation:recursiveDirectoryRename():[DEBUG] Deleting fake directory marker at destination {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Deleting ClusterNode [{rmNode.getNodeID()}] with queue wait time [{currentNode.queueWaitTime}] and wait queue length [{currentNode.queueLength}]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map):[INFO] Found: {ipFilePath}
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[]):[ERROR] Invalid syntax
org.apache.hadoop.security.KDiag:execute():[DEBUG] Logging in
org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader():[DEBUG] Parsing URL for {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Can't remove lease for unknown datanode {dn.getDatanodeUuid()}
org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String):[WARN] Can't find group name for gid + gid + . Use default group name + unknown
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Scanning service definitions for user {}.
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,java.lang.String,int,java.lang.String):[ERROR] prefix: [%s], delimiter: [%s], maxListingLength: [%d], priorLastKey: [%s]. List objects occur an exception: [%s].
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$ConnectionCreator:run():[ERROR] Fatal error caught by connection creator
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStart():[INFO] initialApps.size() + apps recorded as active at this time
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas):[DEBUG] invalidateCorruptReplicas error in deleting bad block {} on {}
org.apache.hadoop.hdfs.util.ECPolicyLoader:loadPolicies(org.w3c.dom.Element,java.util.Map):[WARN] Repetitive policies in EC policy configuration file: {policy.toString()}
org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData):[WARN] Repeated interrupt: escalating to a JVM halt
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getLogs(java.lang.String,java.lang.String,java.lang.String,java.lang.String):[DEBUG] Can not find the container:{} in this node.
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Found tarball at: {destinationFile.getAbsolutePath()}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:activate(org.apache.hadoop.conf.Configuration):[WARN] Deprecated configuration key {} will be ignored.
org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object):[ERROR] RPC.stopProxy called on non proxy: class=
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:close():[DEBUG] Not closing {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:selectInputStreams(long,long):[ERROR] Exception while selecting input streams
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Application + applicationAttemptId + has already been + stopped!
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:refreshNamenodes(org.apache.hadoop.conf.Configuration):[WARN] Unable to get NameNode addresses.
org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String):[WARN] Jetty request log can only be enabled using Log4j
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:appAttemptStartContainer(org.apache.hadoop.yarn.security.NMTokenIdentifier):[DEBUG] NMToken key updated for application attempt : identifier.getApplicationAttemptId().toString()
org.apache.hadoop.hdfs.qjournal.server.Journal:doRollback():[DEBUG] storage.refreshStorage called
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Nonpositive count in invalid READDIR request: {count}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:setPathProperties(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] Executing operation with tracing
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:loadPools(java.io.DataInput):[DEBUG] Setting total cache pools
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Initializing AzureBlobFileSystem for {}
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementAttemptedItems$BlocksStorageMovementAttemptMonitor:run():[WARN] BlocksStorageMovementAttemptMonitor thread received exception and exiting.
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean):[DEBUG] Delete Successful for : {}
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:handle(org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEvent):[INFO] Notify AM launcher launched:[amContainer.getId()]
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:addKeys(org.apache.hadoop.hdfs.security.token.block.ExportedBlockKeys):[INFO] Setting block keys
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.LinkedList,org.apache.hadoop.tools.CopyListingFileStatus):[DEBUG] concat + targetFile + allChunkSize+ + allChunkPaths.size()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* addBlock: block {} on node {} size {} does not belong to any file
org.apache.hadoop.yarn.webapp.Router:load(java.lang.Class,java.lang.String):[WARN] found a {} but it's not a {}
org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails):[DEBUG] addResponseTime for call: {} priority: {} queueTime: {} processingTime: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAllPendingDNMessages():[INFO] Processing {} messages from DataNodes that were previously queued during standby state
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage$Arguments):[INFO] Summary of summaryInfo.manifestCount manifests loaded in manifestDir: summaryInfo
org.apache.hadoop.hdfs.DFSClient:removeNodeFromDeadNodeDetector(org.apache.hadoop.hdfs.DFSInputStream,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] DeadNode detection is not enabled or given block {} is null, skip to remove node.
org.apache.hadoop.hdfs.DFSInputStream:refetchLocations(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[WARN] DFS chooseDataNode: got #{failures + 1} IOException, will wait for {waitTime} msec.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:shutdown():[WARN] FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getNumberOfDatanodes(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Performing getDatanodeListForReport
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter:saveNextReplica():[DEBUG] LazyWriter: Start persisting RamDisk block: block pool Id: {block.getBlockPoolId()} block id: {block.getBlockId()} on target volume {targetVolume}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSyncAll():[INFO] Done logSyncAll lastWrittenTxId= + lastWrittenTxId + lastSyncedTxid= + synctxid + mostRecentTxid= + txid
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:canDelete(java.lang.String):[INFO] The specified queue: + queueName + does not exist!
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:isAvailable():[WARN] Failed to get Operating System name. + Exception
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] 
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:doNext():[DEBUG] currentIndex [currentIndex value] + next + [next value] + [range value]
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] Service {} is stopped.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:fileSystemExists():[DEBUG] AzureBlobFileSystem.fileSystemExists uri: {}
org.apache.hadoop.mapred.gridmix.StressJobFactory:checkLoadAndGetSlotsToBackfill():[DEBUG] System.currentTimeMillis() + " [JobLoad] Overloaded is " + Boolean.TRUE.toString() + " NumJobsBackfill is " + loadStatus.getJobLoad()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Retrieved file status: null
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS FSSTAT fileHandle: {} client: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:updateDfsUsageConfig(java.lang.Long,java.lang.Long,java.lang.Class):[DEBUG] Checking if dfsUsage is an instance of CachingGetSpaceUsed
org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]):[INFO] Already in election. Not re-connecting.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] To-release container={}, for a reserved container, is in final state
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[INFO] RELATES_TO field filter added
org.apache.hadoop.mapred.TaskLog:getLogFileDetail(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskLog$LogName,boolean):[ERROR] Index file for the log of taskid doesn't exist.
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:doRefreshNamenodes(java.util.Map,java.util.Map):[INFO] Starting BPOfferServices for nameservices: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getRuntimeSpec(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Null DeviceRuntimeSpec value got from [devicePlugin.getClass()] for container: [containerId], please check plugin logic
org.apache.hadoop.tools.dynamometer.Client:run():[INFO] Got Cluster metric info from ASM, numNodeManagers={}
org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean:isSecurityEnabled():[DEBUG] Checking if security is enabled
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(java.lang.Object):[ERROR] Unexpected health check result {} for volume {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:isContainerOutOfLimit(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not parse %s in %s
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline service address: + getTimelineServiceAddress()
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:dump():[DEBUG] commit block list with {} blocks for blob {}
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:loadStorageDirectory(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Block pool storage directory for location ... does not exist
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] 'Authorization' does not start with 'Basic' : {authorization}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeFile(java.lang.String,java.io.File,byte[]):[INFO] Store file from input stream. COS key: [{}], length: [{}].
org.apache.hadoop.mapred.ClientServiceDelegate:invoke(java.lang.String,java.lang.Class,java.lang.Object):[WARN] ClientServiceDelegate invoke call interrupted
org.apache.hadoop.hdfs.DFSClient:reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[INFO] Found corruption while reading + file + . Error repairing corrupt blocks. Bad blocks remain.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[ERROR] Can't handle this event at current state: Current: [] eventType: [], container: []
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor$AzureFileSystemThreadRunnable:run():[DEBUG] Time taken to process {} files count for {} operation: {} ms
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$CancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] {} nothing to cancel
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Renaming directory
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:checkDiskScheduler():[WARN] Couldn't read {PARTITIONS_FILE}; can't determine disk scheduler type
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext):[WARN] {} is not active, returning terminated error
org.apache.hadoop.resourceestimator.translator.impl.BaseLogParser:parseStream(java.io.InputStream):[ERROR] Input validation fails, please specify with valid input parameters.
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] Getting splits for input paths
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache:getUserGroupInformation(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Created ugi: {} for username: {}
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[TRACE] No valid proxies left
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:writeChannelCommit(io.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int):[DEBUG] Commit done: x
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop():[INFO] Service {} unregistered with RM, with attemptId = {}, diagnostics = {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:commitQueueManagementChanges(java.util.List):[DEBUG] Queue is already active. Skipping activation : {}
org.apache.hadoop.hdfs.server.datanode.DataStorage:doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Rolling back storage directory sd.getRoot(). target LV = DATANODE_LAYOUT_VERSION; target CTime = nsInfo.getCTime()
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] DynamicInputFormat: Getting splits for job: + jobContext.getJobID()
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[DEBUG] getLogAggregationContext
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent):[DEBUG] {} reported unusable, eventNode
org.apache.hadoop.fs.s3a.impl.RenameOperation:removeSourceObjects(java.util.List):[DEBUG] {} {}
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogTypeWithoutNodeId(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[ERROR] containerLogNotFound
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File):[ERROR] Edits file + f + has improperly formatted + transaction ID
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:validateSpillIndexFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[WARN] validateSpillIndexFileCB.. could not retrieve indexFile.. Path: {}
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:loadRuleMap(java.lang.String):[DEBUG] Got no rules - will disallow anyone access
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Storing info for app: {appId}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Can't get path for dirHandle: {}
org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory):[INFO] Reset exclude protocol list: {}
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:dispatch():[WARN] Failed to move + this, e
org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] jsp requested
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL):[INFO] Missing header hash for appid
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:updateReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationUpdateRequest):[ERROR] Unable to update reservation: [reservationId]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:containerIncreasedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container):[INFO] Unknown application + containerId.getApplicationAttemptId().getApplicationId() + increased container + containerId + on node: + node
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] Please specify the path for setting the storage policy.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.
org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean):[ERROR] Unable to get service state for {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getBlockLocations(java.lang.String,java.lang.String,long,long):[WARN] Failed to update the access time of <src>
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] *BLOCK* NameNode.processIncrementalBlockReport: from {} receiving: {}, received: {}, deleted: {}
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] {msg}
org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String):[INFO] Set {} to '{}'{}, name, redactor.redact(name, value), source == null ? : from + source
org.apache.hadoop.yarn.server.resourcemanager.AdminService:logAndWrapException(java.lang.Exception,java.lang.String,java.lang.String,java.lang.String):[WARN] Exception + msg
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads():[ERROR] Could not stop KeyCache
org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation):[WARN] Exception encountered while connecting to the server {}
org.apache.hadoop.yarn.service.client.ServiceClient:checkPermissions(org.apache.hadoop.fs.Path):[ERROR] Parent directory {} of {} tarball location {} does not have world read/execute permission
org.apache.hadoop.mapred.QueueConfigurationParser:parseResource(org.w3c.dom.Element):[WARN] Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid.
org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[INFO] Enabling multicast for Ganglia with TTL ...
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logEnableErasureCodingPolicy(java.lang.String,boolean):[DEBUG] Logging RPC IDs
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(java.lang.Object,java.lang.Object):[WARN] Failed to parse resource-request
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getTask(org.apache.hadoop.mapred.JvmContext):[INFO] JVM with ID: + jvmId + given task: + task.getTaskID()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean):[TRACE] UC block {} insufficiently-replicated since numLive ({}) < minR ({})
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processChosenExcessRedundancy(java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[DEBUG] BLOCK* chooseExcessRedundancies: ({}, {}) is added to invalidated blocks set
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:parseGpuDevicesFromAutoDiscoveredGpuInfo():[ERROR] YarnConfiguration.NM_GPU_ALLOWED_DEVICES is set to YarnConfiguration.AUTOMATICALLY_DISCOVER_GPU_DEVICES, however automatically discovering GPU information failed, please check NodeManager log for more details, as an alternative, admin can specify YarnConfiguration.NM_GPU_ALLOWED_DEVICES manually to enable GPU isolation.
org.apache.hadoop.fs.s3a.S3AUtils:createAWSCredentialProvider(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.URI):[DEBUG] Credential provider class is {}
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:getSplits(org.apache.hadoop.conf.Configuration,int,long):[DEBUG] Average bytes per map: + nBytesPerSplit + , Number of maps: + numSplits + , total size: + totalSizeBytes
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer):[TRACE] {}: createNewShm: created {}
org.apache.hadoop.fs.s3a.S3AFileSystem:incrementPutCompletedStatistics(boolean,long):[DEBUG] PUT completed success={}; {} bytes
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[INFO] Bytes Read: bytes
org.apache.hadoop.mapred.BackupStore:write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer):[INFO] Written to file cache
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS READDIRPLUS fileHandle: {} cookie: {} dirCount: {} maxCount: {} client: {}
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getBlock(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,byte[]):[TRACE] Done read from Cache for {} position {} length {}, stream.getPath(), position, bytesRead
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus):[DEBUG] Container Status: id={}, status={}
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getDelegationToken():[DEBUG] Message placeholder
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:heartbeatCheck():[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Received + event
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:refreshUserToGroupsMappings():[INFO] Refreshing all user-to-groups mappings. Requested by user: (getRemoteUser().getShortUserName result)
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateAppCollectorsMap(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[INFO] Update collector information for application {appId} with new address: {collectorData.getCollectorAddr()} timestamp: {collectorData.getRMIdentifier()}, {collectorData.getVersion()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:run():[ERROR] Error: Shutting down
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:checkVersion():[INFO] Loaded NM state version info + loadedVersion
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollEditLog():[DEBUG] rollEditLog
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:publishConfigsOnJobSubmittedEvent(org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId):[ERROR] Exception while publishing configs on JOB_SUBMITTED Event for the job : {jobId}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:recoverRbwImpl(org.apache.hadoop.hdfs.server.datanode.ReplicaInPipeline,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long):[INFO] Resetting bytesOnDisk to match blockDataLength (={}) for replica {}
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteWithoutCloseCheck(org.apache.hadoop.fs.Path,boolean):[DEBUG] Failed to create fake dir above {}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[INFO] updating RMDelegation token with sequence number: sequence_number_value
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:deleteFileWithRetries(org.apache.hadoop.fs.FileContext,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.Path):[INFO] Deleting the path
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initDriver():[ERROR] Cannot initialize the ZK connection, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:writeConfigVersion(long):[INFO] Failed to write config version at {configVersionFile}, {e}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter):[DEBUG] AzureBlobFileSystem.listStatusIterator path : {}
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long):[ERROR] Unable to update token {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.String):[ERROR] Cannot remove record {recordToRemovePath}
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int):[WARN] waiting to get block: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Superuser privileges verified
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:stop(boolean):[INFO] Stopping StoragePolicySatisfier.
org.apache.hadoop.streaming.StreamJob:parseArgv():[WARN] -cacheFile option is deprecated, please use -files instead.
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:addContainerRequest(org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest):[WARN] ContainerRequest has duplicate racks: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[WARN] Ignoring attempt to recover existing resource + rsrc
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:getPossiblyCompressedOutputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Retrieved file system
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCacheDirectives():[DEBUG] Directive {}: ignoring non-directive, non-file inode {}
org.apache.hadoop.yarn.server.nodemanager.DeletionService:recover(org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$RecoveredDeletionServiceState):[ERROR] Unable to locate dependency task for deletion task
org.apache.hadoop.net.DNS:getIPs(java.lang.String):[WARN] I/O error finding interface {strInterface}
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:skipIfInRange():[WARN] Skipping index {next}-{range.getEndIndex()}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptSucceededTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] Issuing kill to other attempt <attempt.getID()>
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration):[INFO] Successfully loaded & initialized native-bzip2 library + libname
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveCurrentTokens(java.io.DataOutputStream,java.lang.String):[INFO] End step DELEGATION_TOKENS
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS LOOKUP fileId: {} name: {} does not exist
org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo):[ERROR] Error message with object array
org.apache.hadoop.hdfs.server.datanode.DataNode:triggerBlockReport(org.apache.hadoop.hdfs.client.BlockReportOptions):[DEBUG] Checking superuser privilege
org.apache.hadoop.hdfs.qjournal.server.JNStorage:getOrCreatePaxosDir():[INFO] Creating paxos dir: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getErasureCodingPolicies():[DEBUG] Successful completion of operation 'getErasureCodingPolicies'
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:tryLock():[ERROR] Failed to acquire lock on {}. If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[INFO] Got Cluster metric info from ASM, numNodeManagers=
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleReduceContainerRequest(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[INFO] <diagMsg>
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[INFO] Begin saveInodes and Snapshots
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadProxyCAManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Couldn't find Proxy CA data
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:getLocalIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Unable to get Local hostname and ip for {}
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:createDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,java.lang.String):[DEBUG] createDir: {} perm:{} owner:{}, dirPath, perms, owner
org.apache.hadoop.hdfs.server.namenode.FSImage:loadFSImage(java.io.File,org.apache.hadoop.io.MD5Hash,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean):[INFO] Loaded image for txid + txId + from + curFile
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:buildNamespace(java.io.InputStream,java.util.List):[INFO] Scanned {} INode directories to build namespace.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[TRACE] startDecommission: Node {} in {}, nothing to do.
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[ERROR] Error getting HTTP response from the timeline server.
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[INFO] Read in time
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:resetCGroupParameters():[WARN] Error in cleanup, ex
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:containerAssigned(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor$ContainerRequest):[DEBUG] Assigned container (allocated) to task assigned.attemptID on node allocated.getNodeId().toString()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator:calculateResToObtainByPartitionForLeafQueues(java.util.Set,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Queue={} partition={} resource-to-obtain={}
org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] GET
org.apache.hadoop.nfs.NfsExports$ExactMatch:isIncluded(java.lang.String,java.lang.String):[DEBUG] ExactMatcher ' + ipOrHost + ', denying client ' + address + ', ' + hostname
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Caught exception while adding replicas from ... Will throw later.
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:getDelegationToken(org.apache.hadoop.io.Text):[WARN] trying to get DT with no secret manager running
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[INFO] Storing RMDTMasterKey.
org.apache.hadoop.yarn.server.router.webapp.AppsBlock:render():[INFO] Cannot add application {}: {}
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:nextOp():[INFO] Next operation retrieved from file input stream
org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread:run():[WARN] CleanupThread:Unable to delete path {context.fullPath}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$SafeModeMonitor:run():[INFO] NameNode is being shutdown, exit SafeModeMonitor thread
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing {} of type {}, event.getContainerId(), event.getType()
org.apache.hadoop.yarn.service.ServiceScheduler:terminateServiceIfAllComponentsFinished():[INFO] All component finished, exiting Service Master... , final status=Succeeded
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:rebootNodeStatusUpdaterAndRegisterWithRM():[INFO] Currently being shutdown. Aborting reboot
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:initializePreMountedCGroupController(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController):[INFO] Yarn control group does not exist. Creating ...
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[ERROR] Cannot check app home subcluster for appId
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] isValidRequestor is comparing to valid requestor: ...
org.apache.hadoop.hdfs.server.datanode.LocalReplica:truncateBlock(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,java.io.File,long,long,org.apache.hadoop.hdfs.server.datanode.FileIoProvider):[INFO] truncateBlock: blockFile=<blockFile>, metaFile=<metaFile>, oldlen=<oldlen>, newlen=<newlen>
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int):[DEBUG] BLOCK* NameSystem.LowRedundancyBlock.remove: Removing block {} from priority queue {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] Start loading edits file {edits.getName()} maxTxnsToRead = {maxTxnsToRead} {LogThrottlingHelper.getLogSuppressionMessage(preLogAction)}
org.apache.hadoop.crypto.key.kms.server.KMS:rolloverKey(java.lang.String,java.util.Map):[DEBUG] Rolling key with name {}., name
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:cleanupDeprecatedFinishedApps():[WARN] cleanup keys with prefix + FINISHED_APPS_KEY_PREFIX + from leveldb failed
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[WARN] Exception caught, ignoring node:{}
org.apache.hadoop.ipc.Server$Listener:run():[INFO] <placeholder>: starting
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl):[DEBUG] LazyWriter: Finish persisting RamDisk block: block pool Id: bpId block id: blockId to block file savedFiles[1] and meta file savedFiles[0] on target volume targetVolume
org.apache.hadoop.hdfs.server.namenode.LeaseManager:removeLease(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile):[WARN] Removing non-existent lease! holder={} src={}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logUpdateBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean):[DEBUG] logEdit
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:setSessionTimeZone(org.apache.hadoop.conf.Configuration,java.sql.Connection):[WARN] Setting default time zone: GMT
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSMkdirs operation executed
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getRootDir():[WARN] The root directory is not available, using {}
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] Initialized KeyProvider
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:get(org.apache.hadoop.yarn.server.timeline.EntityIdentifier):[ERROR] {e.getMessage()}
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[ERROR] File is not a regular file
org.apache.hadoop.hdfs.StripeReader:readStripe():[DEBUG] Read task returned: {result}, for stripe {alignedStripe}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:allocateDevices(java.util.Set,int,java.util.Map):[ERROR] Failed to do topology scheduling. Skip to use basic scheduling
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[TRACE] Updating {} for re-encryption.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] couldn't find application + appID + while processing + FINISH_APPS event. The ResourceManager allocated resources + for this application to the NodeManager but no active + containers were found to process.
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getRemoteAppLogDir(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Entering getRemoteAppLogDir
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:runDiagnose(java.lang.String,int):[WARN] Failed to execute binary diagnose, exception message: e.getMessage(), output: output, continue ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:removeTmpConfigFile():[INFO] delete temp configuration file: + tempConfigPath
org.apache.hadoop.hdfs.DFSClient:createWrappedInputStream(org.apache.hadoop.hdfs.DFSInputStream):[INFO] No encryption info; returning direct stream
org.apache.hadoop.fs.azurebfs.services.AbfsLease:acquireLease(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Attempting to acquire lease on {}, retry {}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:nextInternal(java.util.List,org.apache.hadoop.hbase.regionserver.ScannerContext):[DEBUG] emitted cells. + addedCnt + for + this.action + rowKey= + FlowRunRowKey.parseRowKey(CellUtil.cloneRow(cells.get(0)))
org.apache.hadoop.tools.dynamometer.Client:setupRemoteResource(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.tools.dynamometer.DynoResource,java.util.Map,java.lang.String[]):[INFO] Uploading resource {} from {} to {}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processCheckpoints(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker):[DEBUG] Removed re-encryption tracker for zone ... because it completed with ... tasks.
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Error when publishing entity
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[INFO] AppNameMappingPlacementRule applied
org.apache.hadoop.mapreduce.v2.hs.JobHistory$MoveIntermediateToDoneRunnable:run():[ERROR] Error while scanning intermediate done dir , e
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:completeReencryption(org.apache.hadoop.hdfs.server.namenode.INode):[INFO] Re-encryption completed on zone {}. Re-encrypted {} files, failures encountered: {}.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:tryToCreateExternalBlockReader():[WARN] Failed to construct new object of type
org.apache.hadoop.fs.azure.NativeAzureFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[ERROR] Error message
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.net.URI,java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[WARN] Error closing the HTTP response's inputstream.
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:analyseBlocksStorageMovementsAndAssignToDN(org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy):[DEBUG] BlockMovingInfo: {}
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getDelegationToken(org.apache.hadoop.security.UserGroupInformation):[DEBUG] {} token found in cache : {}
org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod():[ERROR] Cannot access method {} with types {} from {}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:handleMirrorOutError(java.io.IOException):[INFO] datanode.getDNRegistrationForBP(bpid):Exception writing block to mirror mirrorAddr
org.apache.hadoop.fs.azurebfs.commit.ResilientCommitByRename:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Verifying existence of source and it's a file
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:init(org.apache.hadoop.conf.Configuration):[INFO] Reading NUMA topology using 'numactl --hardware' command.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Storing RMDelegationToken and SequenceNumber , e
org.apache.hadoop.mapred.MapOutputCollector:collect(java.lang.Object,java.lang.Object,int):[INFO] Collection by DirectMapOutputCollector started
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[INFO] Giving handle (fileHandle: {fileHandle} file URI: {URI}) to client for export {path}
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS:600000
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Removing RMDelegationToken and SequenceNumber , e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[ERROR] Calling allocate on previous or removed or non existent application attempt ...
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:pauseContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] {} doesn't support pausing., container.getContainerId()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:checkAndUpdate(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo):[WARN] Resolve Duplicate Replicas
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path):[WARN] Can't create(mkdir) trash directory: ...
org.apache.hadoop.util.Shell:checkIsBashSupported():[WARN] Interrupted, unable to determine if bash is supported
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[ERROR] FATAL, Shutting down the resource manager.
org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run():[WARN] Trash caught: {exception}. Skipping {trashRoot.getPath()}.
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:dumpData(java.io.FileOutputStream,java.io.RandomAccessFile):[DEBUG] After dump, new dumpFileOffset: dumpFileOffset
org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:run():[DEBUG] DataNode$DataTransfer: close-ack={closeAck}
org.apache.hadoop.hdfs.DataStreamer:getPinnings(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]):[DEBUG] {} was chosen by name node (favored={}).
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration):[INFO] Start checkpoint for address
org.apache.hadoop.hdfs.server.balancer.Balancer:logUtilizationCollections():[DEBUG] logUtilizationCollection("underutilized", underUtilized)
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads():[ERROR] Could not stop Key Id Counter
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[WARN] Transitioning the resource manager to standby.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:bootstrap(java.lang.String,int,int):[INFO] NM recovery is not enabled. We'll wipe tc state before proceeding.
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$AddNodeTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[INFO] Node transitioned to RUNNING
org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int):[INFO] Trying to re-establish ZK session
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Finish information is missing for container
org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider:toResponse(java.lang.Exception):[WARN] User {} request {} {} caused exception.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] Setting node count for blacklist to {}
org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo():[DEBUG] Interrupted while waiting to retry, e
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:main(java.lang.String[]):[INFO] Initializing Client
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Num completed Tasks: ...
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:execute(org.apache.commons.cli.CommandLine):[DEBUG] Processing Plan Command.
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:checkAcls(java.lang.String):[INFO] HS Admin: + method + invoked by user + user.getShortUserName()
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:readFile(java.lang.String,java.lang.String):[INFO] Added entry to result set
org.apache.hadoop.hdfs.server.datanode.DataNode:notifyNamenodeReceivedBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String,java.lang.String,boolean):[ERROR] Cannot find BPOfferService for reporting block received + for bpid={}
org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String):[INFO] Error getting groups for {user}: {e.getMessage()}
org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse):[DEBUG] Listing beans for + qry
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:doWork():[ERROR] Throwable Exception in doCheckpoint
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[DEBUG] Stack trace
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[DEBUG] Activating span from request
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[DEBUG] Duplicate initApp detected for application
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processCheckpoints(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker):[DEBUG] Updating re-encryption checkpoint with completed task. last: ..., size:...
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:checkSafeMode():[DEBUG] STATE* Safe mode extension entered.
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:submitPlan(java.lang.String,long,java.lang.String,java.lang.String,boolean):[ERROR] Disk Balancer - Executing another plan, submitPlan failed.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Application + appId + " is stopped and can't be moved!"
org.apache.hadoop.yarn.service.client.ServiceClient:getStatus(java.lang.String):[INFO] Service {} does not have an application ID
org.apache.hadoop.ha.ZKFailoverController:becomeActive():[INFO] Trying to make localTarget active...
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:checkLocalDir(java.lang.String):[WARN] "Could not carry out resource dir checks for " + localDir + ", which was marked as good"
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createSystemMetricsPublisher():[INFO] system metrics publisher with the timeline service V2 is configured
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Cannot rollback resource for container + containerId + . The application that the container + belongs to does not exist.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource):[INFO] Initializing [Queue Path Details With Associated Attributes]
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:constructTaskAttemptCompletionEvents():[WARN] Cannot construct TACEStatus from TaskAttemptState: [%s] for taskAttemptId: [%s]. Defaulting to KILLED
org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy:chooseVolume(int,java.util.List,long):[WARN] The volume[...] with the available space (=... B) is less than the block size (=... B).
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] Verification result: {verificationFailed ? "FAILED" : "PASSED"}
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Waiting for application {applicationId} to be killed.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getCachedStore(org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,java.util.List):[WARN] AppLogs for groupId {groupId} is set to null!
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[DEBUG] Succesfully shutdown executor service
org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized():[INFO] Initialized cache for UID to User mapping with a cache timeout of cacheTimeout / 1000 + seconds.
org.apache.hadoop.yarn.applications.distributedshell.Client:uploadFile(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String,java.lang.String):[INFO] Uploading file: [fileSrcPath] to [dst]
org.apache.hadoop.mapred.gridmix.Statistics$StatCollector:run():[ERROR] Statistics Error while waiting for other threads to get ready
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[WARN] delete returned false for path: [{}]
org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler:handle(org.apache.hadoop.yarn.event.Event):[WARN] AsyncDispatcher thread interrupted, e
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[INFO] Session expired. Entering neutral mode and rejoining...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean):[TRACE] UC block {} sufficiently-replicated since numLive ({}) >= minR ({})
org.apache.hadoop.hdfs.DFSUtilClient:isLocalAddress(java.net.InetSocketAddress):[TRACE] Address {targetAddr} is {local ? "" : "not"} local
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[INFO] Storage policy satisfier is already disabled, mode:{} so ignoring change mode event.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getAllocatedDevices(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] "Get allocation from deviceMappingManager: {}, {} for container: {}" with args allocated, resourceName, containerId
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshSuperUserGroupsConfiguration():[ERROR] Refresh super user groups configuration failed for proxy.getAddress()
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:uploadBlockAsync(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock,java.lang.Boolean):[DEBUG] Queueing upload of {} for upload {}
org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto):[DEBUG] Sending sasl message + message
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStop():[WARN] Gave up waiting for the cleaner task to shutdown.
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:createAndRegisterNewUAM(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String):[INFO] Received new application ID {} from RM
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:retrieve(java.lang.String):[DEBUG] Retrieve COS key:[{}]. range start:[{}].
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:isBlockReplicatedOk(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,java.lang.Boolean,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor$BlockStats):[WARN] File {} is not under construction. Skipping add to low redundancy open files!
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:run(int):[DEBUG] Lease renewer daemon for [clientsString] with renew id [id] executed
org.apache.hadoop.yarn.server.timelineservice.storage.SchemaCreator:createTimelineSchema(java.lang.String[]):[INFO] Starting DocumentStore schema creation
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[DEBUG] Realm: miniKdc.getRealm()
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:findCounter(java.lang.String):[WARN] Counter already exists
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition:transition(java.lang.Object,java.lang.Object):[INFO] task.commitAttempt already given a go for committing the task output, so killing attemptID
org.apache.hadoop.hdfs.HAUtil:getNameNodeId(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Suffix IDs retrieved successfully
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getPoliciesConfigurations(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPoliciesConfigurationsRequest):[ERROR] Unable to obtain the policy information for all the queues.
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:doSaslHandshake(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler,org.apache.hadoop.security.token.Token):[DEBUG] Handshake secret is null, sending without handshake secret.
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[DEBUG] Block token with {id} doesn't have the correct token password
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] drop FINISH_APPS event to + appID + because + container + container.getContainerId() + is recovering
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext,java.lang.String):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEvent):[INFO] Scheduling Log Deletion for application: + appId + , with delay of + this.deleteDelaySeconds + seconds
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:copyBlockFiles(long,long,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File,boolean,int,org.apache.hadoop.conf.Configuration):[DEBUG] Copied {srcReplica.getBlockURI()} to {dstFile}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:throttle():[DEBUG] Re-encryption updater throttling expect: {}, actual: {}, throttleTimerAll:{}
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectMetadataRequest,java.lang.String):[DEBUG] Unable to restrict HEAD request to etag; will check later
org.apache.hadoop.tools.dynamometer.Client:monitorInfraApplication():[INFO] Infra app was killed; exiting from client.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Void):[INFO] {}: directory {} contained {} file(s); data size {}, getName(), taskAttemptDir, fileCount, fileDataSize
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[INFO] Failed to allocate container for application on node because this allocation violates the placement constraint.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:checkDockerVolumeCreated(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] All docker volumes in the system, command={}, dockerVolumeInspectCommand
org.apache.hadoop.yarn.server.timeline.LogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[INFO] Log {} appears to be corrupted. Skip.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest):[DEBUG] Checking access privileges
org.apache.hadoop.minikdc.MiniKdc:delete(java.io.File):[WARN] WARNING: cannot delete file + f.getAbsolutePath()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:refreshLoadedJobCache():[WARN] Failed to execute refreshLoadedJobCache: JobHistory service is not started
org.apache.hadoop.fs.s3a.S3AUtils:initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration):[WARN] Proxy host set without port. Using HTTP default 80
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:getAllowedLocalityLevelByTime(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,long,long,long):[DEBUG] Init the lastScheduledContainer time, priority: {}, time: {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[INFO] Begin saveErasureCodingSection
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[INFO] Sending signal x to container y
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:toString():[TRACE] Logging enabled, fetching clients string
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest):[DEBUG] AFTER decResourceRequest: allocationRequestId= + req.getAllocationRequestId() + priority= + priority.getPriority() + resourceName= + resourceName + numContainers= + resourceRequestInfo.remoteRequest.getNumContainers() + #asks= + ask.size()
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder):[DEBUG] Snapshotted source + sa.name()
org.apache.hadoop.yarn.service.webapp.ApiServer:upgradeService(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {} version {} upgrade initialized
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[DEBUG] Normalizing source path
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] FS for {} is {}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Token service set
org.apache.hadoop.yarn.util.FSDownload:call():[DEBUG] File has been downloaded to {} from {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType):[DEBUG] Added priority=
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:putAll(java.util.List,boolean,boolean):[ERROR] Attempt to insert record {} that already exists
org.apache.hadoop.tools.dynamometer.Client:run():[INFO] Max virtual cores capability of resources in this cluster {}
org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path):[ERROR] FileNotFoundException occurred
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:removeZone(java.lang.Long):[DEBUG] Removing re-encryption status of zone {}
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Downloading tarball from: {downloadURL} to {destinationFile.getAbsolutePath()}
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:refreshToken():[DEBUG] AADToken: refreshing token from MSI
org.apache.hadoop.hdfs.server.federation.router.Router:createLocalNamenodeHeartbeatService():[ERROR] Cannot find namenode id for local {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:serviceStop():[INFO] this.getName() + " waiting for pending aggregation during exit"
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFolderContentsToDelete(org.apache.hadoop.fs.azure.FileMetadata,java.util.ArrayList):[ERROR] Authorization check failed. Files or folders under {} will not be processed for deletion.
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Reserved Memory: memoryHere
org.apache.hadoop.registry.server.dns.RegistryDNSServer:main(java.lang.String[]):[INFO] Starting shutdown message
org.apache.hadoop.hdfs.server.federation.router.RouterStoragePolicy:satisfyStoragePolicy(java.lang.String):[DEBUG] Invoked sequential method
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:loadVolumes(java.lang.String[]):[INFO] Added persistent memory - {} with size={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visit(java.lang.String):[ERROR] Found ResourceRequest for a non-existent node/rack named + resourceName
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fromHandle fileId: {}
org.apache.hadoop.hdfs.tools.DFSHAAdmin:transitionToObserver(org.apache.commons.cli.CommandLine):[INFO] Usage printed
org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupList(java.lang.String):[WARN] Potential performance problem: getGroups(user= + user + ) + took + deltaMs + milliseconds.
org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] POST
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext):[WARN] InterruptedException executing command: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendFinishedEvents():[INFO] Handling application container finished event
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:clearTrash():[INFO] Cleared trash for storage directory {}
org.apache.hadoop.tools.SimpleCopyListing$FileStatusProcessor:processItem(org.apache.hadoop.tools.util.WorkRequest):[ERROR] FileNotFoundException exception in listStatus: {fnf.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:validateResourceRequests(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue):[WARN] Queue {queue name} cannot handle resource request because it has zero available amount of resource for a requested resource type, so the resource request is ignored! Requested resources: {capability}, maximum queue resources: {maxShare}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:chooseTargetTypeInSameNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.EnumMap,java.util.List):[DEBUG] Datanode:{} storage type:{} doesn’t have sufficient space:{} to move the target block size:{}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recoverContainer(org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$RecoveredContainerState):[INFO] Adding {containerId} to recently stopped containers
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[INFO] Begin saveNameSystemSection
org.apache.hadoop.crypto.key.kms.server.KMSACLs:setKeyACLs(org.apache.hadoop.conf.Configuration):[WARN] Invalid key name '{}'
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing RMDelegationToken and SequenceNumber
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Cleaner set to delete logs older than {} seconds
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[DEBUG] Request for appInfo of unknown attempt {}
org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress):[WARN] AUTHZ_FAILED_FOR ...
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] {} is set, will be used to run proxy.
org.apache.hadoop.fs.s3a.S3AFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path):[DEBUG] listLocatedStatus({}, {})
org.apache.hadoop.mapred.LocalContainerLauncher:renameMapOutputForReduce(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,org.apache.hadoop.mapred.MapOutputFile):[DEBUG] Renaming map output file for task attempt + mapId.toString() + from original location + mapOut.toString() + to destination + reduceIn.toString()
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:newConnection(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Class):[ERROR] Unsupported protocol for connection to NameNode: ...
org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run():[INFO] Starting reconfiguration task.
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getInterceptorForSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[ERROR] The interceptor for SubCluster {} does not exist in the cache.
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:init(org.apache.hadoop.conf.Configuration):[INFO] Initialized connection pool to the Federation StateStore database at address: {url}
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] ACLs not supported on at least one file system: , {e}
org.apache.hadoop.hdfs.server.datanode.BlockScanner$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Periodic block scanner is not running
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[DEBUG] Reading next element
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[WARN] Exception when sending signal to container y: e
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogScanner:run():[DEBUG] Scanned {} active applications
org.apache.hadoop.hdfs.DistributedFileSystem:getSnapshotDiffReportInternal(java.lang.String,java.lang.String,java.lang.String):[WARN] Falling back to getSnapshotDiffReport {e.getMessage()}
org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:aggregateSkylines(java.util.Map,int):[ERROR] Solver requires job resource skyline history for at least {} runs, but it only receives history info for {} runs.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Active services created and initialized
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:removeLocalizedResource(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.fs.Path):[DEBUG] Removing local resource at {}
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:launch(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent):[INFO] Shuffle port returned by ContainerManager for {taskAttemptID} : {port}
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop():[WARN] The InMemorySCMStore was interrupted while shutting down the app check task.
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStart():[INFO] Getting the active app list to initialize the in-memory scm store
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:squashCGroupOperations(java.util.List):[WARN] Unsupported operation type: + op.getOperationType()
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:removeAnyContainerTokenIfExpired():[ERROR] Unable to remove token for container
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Error when reading history file of application + appId, e
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Storing RMDelegationToken and SequenceNumber
org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean):[DEBUG] executing [{}], untarCommand
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:cleanup(org.slf4j.Logger,java.io.Closeable):[DEBUG] Exception in closing {closeable}, {e}
org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile():[WARN] Error closing the stream /proc/diskstats
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkSubClusterState(org.apache.hadoop.yarn.server.federation.store.records.SubClusterState):[WARN] Missing SubCluster State information. Please try again by specifying SubCluster State information.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[INFO] This is an earlier submitted application: [applicationId]
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int):[INFO] Exits the main loop.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:createFilesystem(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] createFilesystem for filesystem: {}
org.apache.hadoop.fs.s3a.impl.V2Migration:v1S3ClientRequested():[WARN] getAmazonS3ClientForTesting() will be removed as part of upgrading S3A to AWS SDK V2
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Available Nodes: countHere
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:putEntities(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[]):[DEBUG] Writing summary log for {} to {}
org.apache.hadoop.hdfs.server.federation.router.RouterPermissionChecker:checkSuperuserPrivilege():[ERROR] Cannot get the remote user name
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger):[INFO] valueName: value
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:tailFile(org.apache.hadoop.fs.Path,long,long):[DEBUG] Read fully completed
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:add(org.apache.hadoop.net.Node):[DEBUG] adding node {}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:checkDir(java.lang.String,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result):[DEBUG] Checked files in directory
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean):[DEBUG] Failed to create {} at fallback fs : {}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:setTimerForTokenRenewal(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[INFO] Renew {token} in {expiresIn} ms, appId = {token.referringAppIds}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:parseOutput(java.lang.String):[ERROR] Unknown format of script output! Skipping this line
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token):[DEBUG] Connecting to url {} with token {} as {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: MIN_RESOURCES
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:addPrimaryFilter(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity,byte[],int):[WARN] Error while decoding {name}, {exception}
org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String):[DEBUG] Size of protoMap for {rpcKind} = {getProtocolImplMap(rpcKind).size()}
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:closeBlock():[DEBUG] block[{}]: closeBlock(), index
org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[INFO] Executing {ps1script}
org.apache.hadoop.mapred.MapTask:closeQuietly(org.apache.hadoop.mapreduce.RecordWriter,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Ignoring exception during close for + c, ie
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:writeTmpConfig(org.apache.hadoop.conf.Configuration):[INFO] write temp capacity configuration successfully, schedulerConfigFile= + tempSchedulerConfigPath
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:discardOldEntities(long):[INFO] Deleted 0 entities of type entityType
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS READLINK fileHandle: {handle.dumpFileHandle()} client: {remoteAddress}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[INFO] Block analysis status:{} for the file id:{}. So, Cleaning up the Xattrs.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:killContainer():[WARN] Found no running containers to kill in order to release memory
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:auditLogKillEvent(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] Successfully logged kill audit event.
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[DEBUG] Delegation token encoded
org.apache.hadoop.fs.s3a.WriteOperations:abortMultipartUploadsUnderPath(java.lang.String):[INFO] Attempting to abort multipart uploads under path
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:createNonExistentDirs(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.permission.FsPermission):[WARN] Unable to create directory {dir} error {e.getMessage()}, removing from the list of valid directories.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuResourcePlugin:getNMResourceInfo():[ERROR] Error message from YarnException
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[ERROR] Error reattaching UAM to {} for {}
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] Token generated with type {}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:getEntityTypes(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext):[DEBUG] Storage is up
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:disableBlockPoolId(java.lang.String):[TRACE] {}: disabling scanning on block pool {}, this, bpid
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing RMDTMasterKey.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:logLineFromTasksFile(java.io.File):[WARN] Failed to read cgroup tasks file.
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:removeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier):[DEBUG] Removing token
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyToFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,long,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet,org.apache.hadoop.fs.FileChecksum):[DEBUG] Getting buffer size
org.apache.hadoop.ipc.Server$RpcCall:run():[INFO] Thread.currentThread().getName() + ": skipped " + this
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] this + beginning handshake with NN
org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int):[WARN] Total Nodes in scope : {} are less than Available Nodes : {}, totalInScopeNodes, availableNodes
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:render():[INFO] Unknown application with id rendered
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection):[WARN] Total Nodes in scope : {} are less than Available Nodes : {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Trying to move container={} to node={}
org.apache.hadoop.examples.terasort.TeraSort:run(java.lang.String[]):[INFO] starting
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits):[DEBUG] User {} in queue {} will exceed limit - consumed: {} limit: {}
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] get valid queue mapping from app name config: {newMappings.toString()}, override: {overrideWithQueueMappings}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:addCGroupParentIfRequired(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand):[DEBUG] cGroupsHandler is null. cgroups are not in use. nothing to do.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:writeCredentials(org.apache.hadoop.fs.Path):[DEBUG] Credentials list in {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] AdminService and EmbeddedElector configured for HA
org.apache.hadoop.hdfs.server.common.Util:stringAsURI(java.lang.String):[INFO] Assuming 'file' scheme for path + s + in configuration.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:pauseContainer():[INFO] [message]
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovery message with NONE state
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:addResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent):[DEBUG] Skip downloading resource: {key} since it is locked by other threads
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,boolean,java.util.EnumMap):[WARN] Failed to place enough replicas, still in need of X to reach Y (unavailableStorages=...,storagePolicy=...,newBlock=...) e.getMessage()
org.apache.hadoop.hdfs.util.ECPolicyLoader:loadPolicy(org.w3c.dom.Element,java.util.Map):[WARN] Invalid tagName: + tagName
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement):[INFO] Set entitlement for AutoCreatedLeafQueue + inQueue + to + queue.getCapacity() + request was (+entitlement.getCapacity()+)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handleExcludeNodeList(boolean,int):[INFO] Gracefully decommission node with state not DECOMMISSIONED/DECOMMISSIONING
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:reportLocalError(java.io.IOException):[ERROR] Shuffle failed : local error on this node
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:convertCapacitySchedulerXml(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[INFO] Ignoring the conversion of placement rules
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateAndResolveService(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration):[INFO] Merging external component {} from external {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Successfully recovered + count + out of + appStates.size() + applications
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getLogs(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean):[INFO] Get log file
org.apache.hadoop.mapred.ShuffleHandler:addJobToken(org.apache.hadoop.mapred.JobID,java.lang.String,org.apache.hadoop.security.token.Token):[INFO] Added token for + jobId.toString()
org.apache.hadoop.mapreduce.task.reduce.Fetcher:checkTimeoutOrRetry(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.IOException):[WARN] Shuffle output from host.getHostName() failed, retry it., ioe
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:createTokenIdentifier(java.util.Optional,org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[WARN] Forwarding existing session credentials to {} -duration unknown
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] No files to commit
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$DecommissioningNodeTransition:transition(java.lang.Object,java.lang.Object):[INFO] NODE_ID is already DECOMMISSIONING
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetReplication(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,short):[DEBUG] Increasing replication from {} to {} for {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setUserLimit(java.lang.String,int):[DEBUG] here setUserLimit: queuePrefix={}, userLimit={}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] logUpdateMasterKey started
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {COMMITTER_NAME_MAGIC}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Total Memory: memoryHere
org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode():[DEBUG] Ensuring existence of + prefixPath
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:initAuxService(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord,org.apache.hadoop.conf.Configuration,boolean):[INFO] Initialized auxiliary service [service_name]
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:doCopy(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet):[INFO] Copying {source.getPath()} to {target}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,long):[ERROR] Error in updating persisted RMDelegationToken with sequence number: (actual sequence number retrieved from id.getSequenceNumber())
org.apache.hadoop.registry.server.services.RegistryAdminService:verifyRealmValidity():[DEBUG] Started Registry operations in realm {}
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean):[WARN] Couldn't delete checkpoint: + dir + Ignoring.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container):[DEBUG] allocate: applicationAttemptId=... container=... host=... type=...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:startDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Starting decommission of {} {} with {} blocks
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[INFO] Accepted application {applicationId} from user {user}.
org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text):[DEBUG] selected by alias={} token={}, service, token
org.apache.hadoop.tools.DistCpOptions:getUniquePaths(java.util.List):[INFO] Path: {} added multiple times, ignoring the redundant entry.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest):[INFO] Authorized user
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:clean():[DEBUG] Sorting directories
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenSingleCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String,boolean):[DEBUG] Response {httpResponseCode}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:cleanupRegistryAndCompHdfsDir(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] <componentId>: Failed to delete directory
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:initScheduler(org.apache.hadoop.conf.Configuration):[WARN] FairSchedulerConfiguration.UPDATE_INTERVAL_MS is invalid, so using default value
org.apache.hadoop.hdfs.server.federation.router.security.token.DistributedSQLCounter:selectCounterValue():[DEBUG] Select counter statement: {query_placeholder}
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent):[ERROR] Can't handle this event at current state
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation):[DEBUG] Block blobs with compaction directories: {}
org.apache.hadoop.yarn.server.AMRMClientRelayer:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Override allocate responseId from ... to ...
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:discardOldEntities(long):[INFO] Discarded + totalCount + entities for timestamp + timestamp + and earlier in + (t2 - t1) / 1000.0 + seconds
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Authentication exception: [EXCEPTION_MESSAGE]
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:processFileInode(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo):[DEBUG] File {} skipped re-encryption because edek's key version name is not changed.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:initExisting():[INFO] currCacheSize * 100.0 / maxCacheSize + % of cache is loaded.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:cacheBlock(long,java.lang.String,java.lang.String,long,long,java.util.concurrent.Executor):[DEBUG] Block with id {}, pool {} already exists in the FsDatasetCache with state {}
org.apache.hadoop.yarn.service.client.ServiceClient:addJarResource(java.lang.String,java.util.Map):[WARN] Property {} has a value {}, but is not a valid file
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],java.util.Set,int,java.lang.String):[INFO] Operation category WRITE checked
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveCachePool(java.lang.String,boolean):[INFO] Edited log entry
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeLoggedTask(org.apache.hadoop.tools.rumen.LoggedTask):[WARN] Task + task.getTaskID() + has nulll TaskType
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[DEBUG] Verifying and creating remote log directory
org.apache.hadoop.ha.HealthMonitor:tryConnect():[WARN] Could not connect to local service at targetToMonitor: e.getMessage()
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:writeAMRMTokenForUAM(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.token.Token):[ERROR] Failed writing AMRMToken to registry for subcluster subClusterId, e
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandomWithStorageTypeTwoTrial(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType):[DEBUG] First trial failed, node has no type {}, making second trial carrying this type
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getSuffix(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Path {prefix} is not a prefix of the path {fullPath}
org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer:initServer(java.lang.String):[INFO] WebImageViewer started. Listening on + address.toString() + . Press Ctrl+C to stop the viewer.
org.apache.hadoop.yarn.service.ServiceManager:finalizeUpgrade(boolean):[WARN] Unable to delete upgrade definition for service {} version {}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser:run():[TRACE] {}: released {}, this, slot
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:onUpdateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.UpdateContainerSchedulerEvent):[WARN] Could not update resources on continer update of {containerId}, {ex}
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:sendContainerRequest():[DEBUG] Application {} sends out request for {} failed reducers.
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Call: {methodName} took {callTime}ms
org.apache.hadoop.hdfs.server.datanode.DataStorage:doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Rollback of sd.getRoot() is complete
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver:getSubclusterInfo(org.apache.hadoop.hdfs.server.federation.store.MembershipStore):[ERROR] Cannot get stats info for {}: {}.
org.apache.hadoop.hdfs.DFSUtil:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] Setting password to null since IOException is caught when getting password
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute():[INFO] {keyName} has been successfully deleted.
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskStriped:truncatePartialBlock(java.util.List,long):[WARN] Failed to updateBlock (newblock=, datanode=r.id)
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] PrimaryGroup rule: parent rule found: {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignMapsWithLocality(java.util.List):[DEBUG] Assigned based on rack match [rack]
org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive():[INFO] Checking for any old active which needs to be fenced...
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishApplicationAttemptEventOnTimelineServiceV2(org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$DSEvent):[ERROR] App Attempt start/end event could not be published for [appAttemptID], [exception]
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:processBlocksInternal(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Iterator,java.util.List,boolean):[DEBUG] Yielded lock during decommission/maintenance check
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogCleaner:run():[ERROR] Error cleaning files, e
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.lang.String):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber:clearCorruptLazyPersistFiles():[INFO] Cannot find block info for block [block]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager:parseQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager$QueueHook):[INFO] Initialized queue: + fullQueueName
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Initializing SSL Context to channel mode OpenSSL
org.apache.hadoop.fs.s3a.S3AUtils:initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration):[ERROR] Proxy error: proxyUsername or proxyPassword set without the other.
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[INFO] Got response from RM for container ask, allocatedCnt= + allocatedContainers.size()
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[INFO] Launching shell command on a new container, containerId=yarnContainerId, yarnShellId=yarnShellId, containerNode=nodeHost:nodePort, containerNodeURI=nodeHttpAddress, containerResourceMemory=memorySize, containerResourceVirtualCores=virtualCores
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog):[ERROR] Download of Edit Log file for Syncing failed. Deleting temp file: {tmpEditsFile}
org.apache.hadoop.yarn.server.nodemanager.NodeManager:stopRecoveryStore():[INFO] Removing state store at + recoveryRoot + due to decommission
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForWrite(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics,boolean,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] openFileForWrite filesystem:{} path:{} overwrite:{}, client.getFileSystem(), path, overwrite
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createSystemMetricsPublisher():[INFO] TimelineServicePublisher is not configured
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:rename(java.lang.String,java.lang.String):[ERROR] Cannot rename {} to {}, src, dst, e
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[WARN] Found null for clusterId. Not proceeding with writing to hbase
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerBecomeReadyTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] Container state set to READY
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long):[WARN] OOM was not resolved in X ms
org.apache.hadoop.security.UserGroupInformation:print():[DEBUG] User: [getUserName()]
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[DEBUG] scanning file: {{fs.getPath()}}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getQueueInfo(org.apache.hadoop.yarn.api.protocolrecords.GetQueueInfoRequest):[INFO] Failed to getQueueInfo for [queueName], [ioe]
org.apache.hadoop.registry.client.impl.zk.CuratorService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Creating Registry with root {}
org.apache.hadoop.yarn.service.ContainerFailureTracker:incNodeFailure(java.lang.String):[INFO] [COMPONENT {}]: Failed {} times on this host, blacklisted {}. Current list of blacklisted nodes: {}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:blockIdCK(java.lang.String):[WARN] Error in looking up block
org.apache.hadoop.util.HostsFileReader:refresh():[INFO] Refreshing hosts (include/exclude) list
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS LOOKUP dir fileHandle: {dumpFileHandle} name: {fileName} client: {remoteAddress}
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchUserService(java.util.Map):[WARN] Error while closing serviceClient for user {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[ERROR] Unable to update the application {}
org.apache.hadoop.fs.s3a.WriteOperationHelper:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String):[ERROR] Failure of S3 Select request against {}
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryDiffListFactory:init(int,int,org.slf4j.Logger):[INFO] SkipList is enabled with skipInterval= + skipInterval + , maxLevels= + maxLevels
org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress):[WARN] Failed to perform reverse lookup: {}
org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String):[INFO] addJerseyResourcePackage: packageName={packageName}, pathSpec={pathSpec}
org.apache.hadoop.io.Writable:write(java.io.DataOutput):[DEBUG] Starting write operation
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:loadSinglePendingCommits(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[WARN] Failed to load commit file {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] List of errors:
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:allowSnapshot(java.lang.String):[INFO] Snapshot allowed for path
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:processTaskEntries(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ReencryptionTask):[DEBUG] INode {} doesn't exist, skipping re-encrypt.
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Group names
org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL client callback: setting realm: + rc.getDefaultText()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCache:loadCache(boolean):[INFO] MembeshipStoreImpl cache loaded
org.apache.hadoop.yarn.service.ServiceMaster:serviceStart():[INFO] Starting service as user
org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String):[WARN] Could not make + path + in local directories from + dirsProp
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[WARN] Can not load log meta from the log file:
org.apache.hadoop.ha.ZKFailoverController:initZK():[DEBUG] No filesystem found for the hdfs scheme
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:delete(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] delete filesystem: {} path: {} recursive: {}
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:get(org.apache.hadoop.yarn.server.timeline.EntityIdentifier):[ERROR] GenericObjectMapper cannot read key from key {entityId.toString()} into an object. Read aborted!
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:loadPools(java.io.DataInput):[DEBUG] End loading cache pools
org.apache.hadoop.tools.mapred.lib.DynamicInputChunk:release():[ERROR] Unable to release chunk at path: + chunkFilePath
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[INFO] Modifying permissions to ...
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:updateState():[WARN] Cannot register namenode <NamenodeStatusReport>
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.volume.csi.event.VolumeEvent):[WARN] Can't handle this event at current state: Current: [ + oldState + ], eventType: [ + event.getType() + ], volumeId: [ + volumeId + ]
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver:getSubclusterInfo(org.apache.hadoop.hdfs.server.federation.store.MembershipStore):[ERROR] Cannot get Namenodes from the State Store.
org.apache.hadoop.yarn.service.component.Component:requestContainers(long):[INFO] Requesting for {} container(s), componentSpec.getName(), count
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long):[DEBUG] Ignoring negative increment value {} for counter {}
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] KeyStore loaded successfully from '%s' since '%s' was corrupted !!
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map):[WARN] Requested IP file not found
org.apache.hadoop.hdfs.DistributedFileSystem$PartialListingIterator:hasNext():[TRACE] Got batchedListing: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[DEBUG] Interrupting Event Handling thread
org.apache.hadoop.mapred.nativetask.HadoopPlatform:init():[INFO] Hadoop platform inited
org.apache.hadoop.streaming.StreamJob:parseArgv():[WARN] -cacheArchive option is deprecated, please use -archives instead.
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage,boolean):[WARN] Cannot access storage directory {}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:resolveHost(java.lang.String):[WARN] Failed to resolve address: ${src}. Continuing to use the same.
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:serviceDependencySatisfied(org.apache.hadoop.yarn.service.api.records.Service):[INFO] Service dependency is not satisfied for service: {} state: {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State):[INFO] Aborting Job {} in state {}
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:getLatestEditsFiles():[DEBUG] Name checkpoint time is newer than edits, not loading edits.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:loadPlugIns(org.apache.hadoop.conf.Configuration):[INFO] Load plugin class {}
org.apache.hadoop.yarn.service.ServiceScheduler:recoverComponents(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Found {} containers from ZK registry: {}
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:writeAppendBlobCurrentBufferToService():[INFO] writeCurrentBufferToService started
org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String):[DEBUG] Handling deprecation for (String)item
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:chooseCommitterFactory(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Committer option is {COMMITTER_NAME_PARTITIONED}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource):[INFO] Available resource value after conversion: + availableResourceValue
org.apache.hadoop.http.HttpServer2$Builder:build():[INFO] New instance created
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:createWorkPlan(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.diskbalancer.planner.Step):[WARN] Disk Balancer - Source and destination volumes are same: [sourceVolUuid]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem$RecoverLeaseOp,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,java.lang.String,boolean):[INFO] startFile: recover + lease + , src= + src + client + clientName
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest):[WARN] Unable to create a new ApplicationId in SubCluster
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:findSourceAndTargetToMove(java.util.List,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.List,java.util.List,java.util.EnumMap,org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy,java.util.List):[WARN] Failed to choose target datanode for the required storage types {}, block:{}, existing storage type:{}
org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit):[INFO] Possible loss of precision converting {vStr}{vUnit.suffix()} to {returnUnit} for {name}
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:saveDirectives(java.io.DataOutputStream,java.lang.String):[DEBUG] Begin step SAVING_CHECKPOINT
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int):[DEBUG] Change concurrent thread count to {} from {}
org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders():[ERROR] The mapping provider, <name> does not have a valid class
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[ERROR] onContainerStarted received unknown container ID: + containerId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[WARN] Application {applicationAttemptId.getApplicationId()} cannot be found in scheduler.
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:getEntities(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineEntityFilters,org.apache.hadoop.yarn.server.timelineservice.reader.TimelineDataToRetrieve):[DEBUG] NoOpTimelineReader is configured. Response to all the read requests would be empty
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unable to update container resource in store
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:getTrafficControlBandwidthHandler(org.apache.hadoop.conf.Configuration):[INFO] Creating new traffic control bandwidth handler.
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager:snapshot(long):[DEBUG] iterating in reported metrics, size={} values={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:recoverAssignedGpus(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Starting recovery of GpuDevice for {}.
org.apache.hadoop.hdfs.DataStreamer:waitAndQueuePacket(org.apache.hadoop.hdfs.DFSPacket):[DEBUG] Closed channel exception
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:buildLaunchOp(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand):[DEBUG] Launching container with cmd: {command}
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[DEBUG] rpcServer.isInvokeConcurrent returned FALSE
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:run():[INFO] {} exiting.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Returning; either check access is not enabled or the account used is not namespace enabled
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute):[DEBUG] Incoming requestAttribute:{} matches with node:{}
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]):[ERROR] Unable to start failover controller. Parent znode does not exist.\nRun with -formatZK flag to initialize ZooKeeper.
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[INFO] Removing "{existingRecord}"
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[INFO] Update the SubCluster to {} for application {} in the StateStore
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:bootstrap(java.lang.String,int,int):[WARN] Unable to create directory: +tmpDirPath
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:writeDocument(org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.TimelineDocument,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType):[DEBUG] Upserting document under collection : {} with entity type : {} under Database {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getSnapshotDiffReport(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Checking NN startup
org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] IPC_FCQ_DECAYSCHEDULER_PERIOD_KEY is deprecated. Please use IPC_SCHEDULER_DECAYSCHEDULER_PERIOD_KEY
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:checkAccess(org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore$ApplicationReportExt):[INFO] User does not have privilege to see this application
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:replaceLabelsOnNodes(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeToLabelsEntryList,javax.servlet.http.HttpServletRequest):[INFO] Replacing labels on nodes
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getLowRedundancyBlocks():[DEBUG] Failed to get number of blocks under replicated
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean):[INFO] logAuditEvent(true, "createSymlink", link, target, auditStat)
org.apache.hadoop.yarn.security.YarnAuthorizationProvider:getInstance(org.apache.hadoop.conf.Configuration):[INFO] authorizerClass.getName() + " is instantiated."
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:cancelAllActiveFutures():[DEBUG] Cancelling futures
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveErasureCodingPolicy(java.lang.String,boolean):[INFO] Edit operation logged
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Found volume name for GPU:{}
org.apache.hadoop.net.DNS:resolveLocalHostIPAddress():[ERROR] Unable to determine local loopback address of 'localhost' -this system's network configuration is unsupported
org.apache.hadoop.metrics2.util.MetricsCache$RecordCache:removeEldestEntry(java.util.Map$Entry):[WARN] Metrics cache overflow at {size()} for {eldest}
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches(boolean):[INFO] Skipping State Store cache update, driver is not ready.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] this : registerSlot slotIdx : allocatedSlots= + allocatedSlots + StringUtils.getStackTrace(Thread.currentThread())
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:readEntityTypes(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[WARN] Failed to add type ... to the result set because there is a duplicated copy.
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onNodesUpdated(java.util.List):[WARN] Nodes updated info: (detailed node information)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getApplicationAttempts(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Failed to fetch application attempts from ATS v2
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean,long):[DEBUG] logEdit executed
org.apache.hadoop.hdfs.server.federation.resolver.order.HashFirstResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[DEBUG] Only using the first part of the path: {} -> {}, path, trimmedPath
org.apache.hadoop.mapred.gridmix.Statistics:add(org.apache.hadoop.mapred.gridmix.Statistics$JobStats):[ERROR] [Statistics] Missing entry for job + job.getJob().getJobID()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] removing master key with keyID
org.apache.hadoop.hdfs.protocol.ClientProtocol:renewDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Renewing delegation token
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[INFO] capacity scheduler file max version = ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Got exception while pausing container: + StringUtils.stringifyException(e)
org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager:createPassword(org.apache.hadoop.yarn.security.NMTokenIdentifier):[DEBUG] creating password for {applicationAttemptId} for user {applicationSubmitter} to run on NM {nodeId}
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun():[INFO] The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getErasureCodingPolicy(java.lang.String):[INFO] Finished executing getErasureCodingPolicy for path src
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectory():[DEBUG] Building listing using iterator mode for %s
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$StatusUpdateWhenHealthyTransition:transition(java.lang.Object,java.lang.Object):[INFO] Node {rmNode.nodeId} reported UNHEALTHY with details: {remoteNodeHealthStatus.getHealthReport()}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForRead(org.apache.hadoop.fs.Path,java.util.Optional,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] openFileForRead filesystem: {} path: {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[DEBUG] Add new write to the list with nextOffset {} and requested offset={}
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object):[WARN] Failed to register MBean "{name}", e
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:moveToDone():[DEBUG] Application {} is done, trying to move to done dir {}, appId, doneAppPath
org.apache.hadoop.fs.cosn.CosNOutputStream:close():[INFO] The output stream has been close, and begin to upload the last block: [{}].
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] FinishAM finished with isUnregistered = {} in {} ms for {}
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close():[INFO] Deleted {numFilesDeleted} cache files
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[DEBUG] RBW replica created
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Recovering container
org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock(org.apache.hadoop.hdfs.server.datanode.BPOfferService,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[WARN] {msg}
org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception):[DEBUG] Displaying error: message
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:initializeProcessTrees(java.util.Map$Entry):[INFO] {}'s ip = {}, and hostname = {}
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable):[TRACE] {}: loading {}
org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception):[DEBUG] Exception while invoking call #callId proxyDescriptor.getProxyInfo().getString(method.getName()). Not retrying because retryInfo.action.reason
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]):[ERROR] Unable to start failover controller. Unable to connect to ZooKeeper quorum at zkQuorum. Please check the configured value for ZK_QUORUM_KEY and ensure that ZooKeeper is running.
org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean):[WARN] I/O error finding interface {}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl):[DEBUG] startJobs: parent= + path + child= + oldJobIDString
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Invalid eventtype + event.getType() + . Ignoring!
org.apache.hadoop.yarn.server.webapp.LogWebService:init():[INFO] Initialized LogWeService with clusterid " + defaultClusterid + " for URI: " + base
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:getStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage):[INFO] Provided storage transitioning to state State.NORMAL
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Has kerberos credentials: %b
org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils:parseCredentials(org.apache.hadoop.yarn.api.records.ContainerLaunchContext):[DEBUG] {}={}
org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String):[DEBUG] Error while create symlink linkname to target. Exception: e
org.apache.hadoop.yarn.client.api.async.NMClientAsync$AbstractCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStopped called
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:execute(org.apache.commons.cli.CommandLine):[INFO] Executing "Cancel plan" command.
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:lostFoundInit(org.apache.hadoop.hdfs.DFSClient):[WARN] Cannot use /lost+found : a regular file with this name exists.
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[INFO] storing RMDelegation token with sequence number: + identifier.getSequenceNumber()
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] NodeManager Version:
org.apache.hadoop.crypto.key.kms.server.KMSConfiguration:initLogging():[WARN] Logging with INFO level to standard output
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$LaunchContainerRunnable:run():[INFO] Setting up container launch container for containerid= + container.getId() + with shellid= + shellId
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:processSummationMajorCompaction(java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.common.NumericValueConverter,long):[DEBUG] In processSummationMajorCompaction, will drop cells older than {} CurrentColumnCells size={}
org.apache.hadoop.fs.s3a.Invoker:ignoreIOExceptions(org.slf4j.Logger,java.lang.String,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[DEBUG] Operation executed with detailed action and path
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] RegistryOperationsService added
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String):[DEBUG] ACL not found for queue access-type {} for queue {}
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] doSecureLogin
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:multipartCopy(java.lang.String,long,java.lang.String):[DEBUG] completeMultipartUploadResult.getETag()
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Source listing completed in {0}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:deleteReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[WARN] Failed to delete block file for replica + replicaToDelete
org.apache.hadoop.hdfs.server.federation.router.RouterCacheAdmin:listCachePools(java.lang.String):[INFO] Namespaces retrieved
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getInitialCachedResources(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[INFO] A total of initialCachedEntries.size() files are now mapped
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:splitAllocateRequest(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Splitting resource requests and updating subClusters
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:traverseDirectoryMultiThreaded():[ERROR] Giving up on {} after {} retries.
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Auxiliary services manifest is enabled, but no manifest file is specified in the configuration.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:executeStage(java.lang.Void):[INFO] {}: Directory count = {}; maximum depth {}, getName(), dirCount, depth
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:createNewApplication(javax.servlet.http.HttpServletRequest):[ERROR] Fail to create a new application.
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] GridMix is configured to use a compression ratio of ... for the map output data.
org.apache.hadoop.yarn.appcatalog.application.YarnServiceClient:stopApp(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Error in stopping application: IOException
org.apache.hadoop.hdfs.server.federation.router.RouterCacheAdmin:listCachePools(java.lang.String):[DEBUG] RPC operation check executed
org.apache.hadoop.hdfs.KeyProviderCache:createKeyProviderURI(org.apache.hadoop.conf.Configuration):[ERROR] Could not find uri with key [CommonConfigurationKeysPublic.HADOOP_SECURITY_KEY_PROVIDER_PATH] to create a keyProvider !!
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerStartedTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] Could not get container creation time, using current time
org.apache.hadoop.mapred.uploader.FrameworkUploader:checkSymlink(java.io.File):[INFO] Ignoring same directory link %s to %s
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:getContainerStatusAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId):[WARN] Exception when scheduling the event of querying the status of Container {containerId}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllSchemas(org.apache.hadoop.conf.Configuration,boolean):[INFO] Successfully created HBase schema.
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:startSpill():[INFO] bufstart= + bufstart + ; bufend= + bufmark + ; bufvoid= + bufvoid
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[WARN] {}: Failed to rename manifests to {}
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:loadFullHistoryData(boolean,org.apache.hadoop.fs.Path):[INFO] TaskInfo loaded
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Fetching application details
org.apache.hadoop.util.Progress:set(float):[DEBUG] Illegal progress value found, progress is Float.POSITIVE_INFINITY. Progress will be changed to 1
org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService:stat(java.lang.String):[DEBUG] Stat {} => {}, path, status
org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getHostPortString(java.net.InetSocketAddress):[ERROR] Failed to get local host name
org.apache.hadoop.yarn.service.webapp.ApiServer:getVersion():[INFO] {version}
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.LogHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEvent):[INFO] Handling log event in LogAggregationService
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl$MonitoringThread:run():[WARN] org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl$MonitoringThread is interrupted. Exiting.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error updating appAttempt: + attemptState.getAttemptId(), e
org.apache.hadoop.hdfs.server.common.sps.BlockStorageMovementTracker:run():[ERROR] Exception while moving block replica to target storage type
org.apache.hadoop.hdfs.server.common.Util:stringAsURI(java.lang.String):[ERROR] Syntax error in URI + s + . Please check hdfs configuration.
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer$TxnBuffer:dumpRemainingEditLogs():[WARN] Unable to dump remaining operations, remaining raw bytes: " + Hex.encodeHexString(remainingRawEdits), ioe
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:clean():[DEBUG] Scanning directory for history files
org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration):[INFO] Copying file
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:bulkDeleteRetried(com.amazonaws.services.s3.model.DeleteObjectsRequest,java.lang.Exception):[WARN] Bulk delete operation interrupted: {ex.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getIncrementAllocation():[WARN] Configuration overridingKey=get(overridingKey) is overriding the RM_SCHEDULER_INCREMENT_ALLOCATION_MB=get(RM_SCHEDULER_INCREMENT_ALLOCATION_MB) property
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:run():[INFO] Processing the event UNKNOWN
org.apache.hadoop.hdfs.server.balancer.Dispatcher:dispatchBlockMoves():[INFO] Total bytes (blocks) moved in this iteration {} ({})
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] Configuration reloaded from store
org.apache.hadoop.examples.DBCountPageView:run(java.lang.String[]):[DEBUG] Configuring job settings
org.apache.hadoop.fs.s3a.S3AFileSystem:abortMultipartUpload(java.lang.String,java.lang.String):[DEBUG] Aborting multipart upload {} to {} initiated by {} on {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$UserLogDir:scanIfNeeded(org.apache.hadoop.fs.FileStatus):[DEBUG] Scan not needed of + fs.getPath()
org.apache.hadoop.fs.s3a.select.SelectInputStream:seek(long):[DEBUG] Forward seek by reading {} bytes
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:validate(java.util.Set):[ERROR] Invalid Node Label(s) from Provider : ...
org.apache.hadoop.hdfs.server.namenode.JournalSet:finalizeLogSegment(long,long):[INFO] Stream closed
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:loadVolumes(java.lang.String[]):[ERROR] Bad persistent memory volume:
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:setConfig(java.lang.Object):[DEBUG] Logging setup debug
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long):[WARN] After resync, position is ...
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:rollEditLog():[INFO] Roll Edit Log from [Server.getRemoteAddress()]
org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat):[INFO] Writing znode {} to indicate that the local node is the most recent active..., zkBreadCrumbPath
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:processResponseQueue():[ERROR] Application {} has one container killed ({}).
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: USER_MAX_APPS_DEFAULT
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:activateSavedReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica):[INFO] Moved <metaFile> to <targetMetaFile>
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:beforeExecution(com.amazonaws.AmazonWebServiceRequest):[DEBUG] executing a request outside an audit span ..., ex
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext):[INFO] ContainerId=..., docker exec output for ...: ...
org.apache.hadoop.mapred.gridmix.GridmixJob:scaleConfigParameter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,long):[DEBUG] For the job configuration parameter ' + jobValueKey + ' and the cluster configuration parameter ' + clusterValueKey + ', the original job's configuration value is scaled from ' + originalJobValue + ' to ' + simulatedJobValue + ' using the default (unit) value of ' + originalClusterDefaultValue + ' for the original cluster and ' + simulatedClusterDefaultValue + ' for the simulated cluster.
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:copySucceeded(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MapHost,long,long,long,org.apache.hadoop.mapreduce.task.reduce.MapOutput):[WARN] Aborting already-finished MapOutput for TaskAttemptID
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceStart():[INFO] In memory ZK started at {}
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:createNewApplication(javax.servlet.http.HttpServletRequest):[DEBUG] getNewApplication try #{i} on SubCluster {subClusterId}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminMonitorBase:setConf(org.apache.hadoop.conf.Configuration):[ERROR] {} is set to an invalid value, it must be zero or greater. Defaulting to {}, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES_DEFAULT
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:cancelReencryptEncryptionZone(org.apache.hadoop.hdfs.server.namenode.INodesInPath):[INFO] Cancelled zone {}({}) for re-encryption.
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:run():[WARN] This cycle terminating immediately because 'shouldRun' has been deactivated
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] {error}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[ERROR] Initialization failed for this because
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:newSocketSend(java.net.Socket,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] Sending client SASL negotiation
org.apache.hadoop.yarn.service.client.ServiceClient:cleanUpRegistryPath(java.lang.String,java.lang.String):[DEBUG] getRegistryClient exists check passed
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:dispatchBlocks():[INFO] Failed to find a pending move for + noMoveInterval + ms. Skipping + this
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] The AMRMToken has been rolled-over. Send new AMRMToken back to application
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStop():[INFO] Waiting for executor to terminate
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:closeWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId):[ERROR] Error closing writer for JobID:
org.apache.hadoop.hdfs.server.namenode.FSImage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] Reading + editIn + expecting start txid # + (lastAppliedTxId + 1) + logSuppressed
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state for + this.taskId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Container + container + of + finished application + appId + completed with event + event
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocate(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Set,java.util.Set,int):[INFO] Not allocating more containers as max allocations per AM heartbeat {} has reached
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:registerApplicationMaster(java.lang.String,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Registering UAM id {} for application {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[INFO] Resource increase requests : + increaseRequests
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onRequestsRejected(java.util.List):[INFO] Scheduling Request {req} has been rejected. Reason {reason}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest):[INFO] Container started successfully
org.apache.hadoop.registry.server.dns.RegistryDNS:getReverseZoneName(java.lang.String):[WARN] Unable to convert {} to DNS name
org.apache.hadoop.fs.s3a.impl.MkdirOperation:execute():[DEBUG] {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS READDIR fileHandle: {handle.dumpFileHandle()} cookie: {cookie} count: {count} client: {remoteAddress}
org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Cancelling delegation token {} with url:{}, as:{}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[WARN] Could not find a target for file {src} with favored node {favoredNode}
org.apache.hadoop.hdfs.server.datanode.DataNode:handleBadBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException,boolean):[WARN] report bad block {} failed
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeUpgrade():[ERROR] IOException during finalizing upgrade
org.apache.hadoop.lib.service.scheduler.SchedulerService:init():[DEBUG] Scheduler started
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Checksum JSON location response
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[DEBUG] getEntity: Found nothing
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:setupContainerAskForRM():[INFO] Requested container ask: + request.toString()
org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run():[WARN] Exception encountered while running the renewal command for {}
org.apache.hadoop.yarn.service.utils.SliderFileSystem:deleteComponentsVersionDirIfEmpty(java.lang.String):[INFO] deleted public resource dir {}, publicResourceDir
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] SystemMetricsPublisher created and set
org.apache.hadoop.fs.cosn.CosNFileSystem:mkdir(org.apache.hadoop.fs.Path):[DEBUG] Make directory: [{}] in COS.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas):[DEBUG] BLOCK* invalidateBlocks: {b} on {dn} listed for deletion.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[WARN] Resource + rsrc + localized without a location
org.apache.hadoop.tools.mapred.CopyCommitter:commitData(org.apache.hadoop.conf.Configuration):[ERROR] Unable to commit data to finalDir
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureEndpoint(com.amazonaws.services.s3.AmazonS3Builder,com.amazonaws.client.builder.AwsClientBuilder$EndpointConfiguration):[DEBUG] Using default endpoint; setting region to {}, region
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:mapWorkflowPriorityForApp(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,java.lang.String,org.apache.hadoop.yarn.api.records.Priority):[INFO] Application mapping log details
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.registry.server.dns.RegistryDNS:getReverseZoneName(org.apache.commons.net.util.SubnetUtils,java.lang.String):[WARN] Unable to convert {} to DNS name
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getPoliciesConfigurations(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPoliciesConfigurationsRequest):[INFO] Success state store call
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[DEBUG] Watcher event type: ...
org.apache.hadoop.hdfs.server.namenode.CacheManager:validateExpiryTime(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,long):[TRACE] Validating directive {} pool maxRelativeExpiryTime {}
org.apache.hadoop.fs.cosn.BufferPool:returnBuffer(org.apache.hadoop.fs.cosn.ByteBufferWrapper):[ERROR] Return the buffer to buffer pool failed.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattrInternal(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.nfs.nfs3.request.SetAttr3,boolean):[DEBUG] set atime: {} mtime: {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] writeAuditLog
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[ERROR] All of your job[s] have the same submit time. Please just use your input file.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[WARN] forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:getMultipartSizeProperty(org.apache.hadoop.conf.Configuration,java.lang.String,long):[WARN] oss: {} capped to ~2.14GB(maximum allowed size with current output mechanism)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown():[WARN] Error stopping the metrics system
org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider:isTokenAboutToExpire():[DEBUG] AADToken: no token. Returning expiring=true
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedWriter:initTargetStreams():[WARN] {e.getMessage()}
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:release(byte[]):[DEBUG] recycle: array.length=
org.apache.hadoop.hdfs.StripeReader:readStripe():[ERROR] Read request interrupted
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Falling back to authHandler.getClass() (req=request)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:handle(org.apache.hadoop.yarn.event.Event):[INFO] Skipping localization request for recently cleaned localizer locId resource: req.getResource()
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status):[INFO] Sending an out of band ack of type + ackStatus
org.apache.hadoop.hdfs.server.namenode.Checkpointer:run():[ERROR] Throwable Exception in doCheckpoint:
org.apache.hadoop.mapreduce.v2.hs.JobHistory$HistoryCleaner:run():[INFO] History Cleaner complete
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:trimEvictionMaps():[TRACE] : trimEvictionMaps is purging <replica_description><stack_trace>
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:doWork():[ERROR] Exception in doCheckpoint
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:putObject():[DEBUG] Executing regular upload for {}
org.apache.hadoop.hdfs.KeyProviderCache:get(org.apache.hadoop.conf.Configuration,java.net.URI):[ERROR] Could not create KeyProvider for DFSClient !!
org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit):[INFO] Possible loss of precision converting vStr with vUnit suffix to returnUnit for name
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:populateResourceCapability(org.apache.hadoop.mapreduce.v2.api.records.TaskType):[INFO] Resource capability of task type {} is set to {}
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:deleteAppDirLogs(long,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.yarn.api.ApplicationClientProtocol,org.apache.hadoop.fs.FileStatus):[INFO] Deleting aggregated logs in + appDir.getPath()
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] System.err.println(AdminHelper.prettifyException(e))
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSuccessCheckOp(org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,long,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Returning success response from append blob idempotency code
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String):[WARN] Fatal disk error on + dnName + : + msg
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean):[DEBUG] Key with path [ + nodeCreatePath + ] already exists.. Updating !!
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Unknown application: + appId + released container + container.getId() + on node: + node + with event: + event
org.apache.hadoop.fs.s3a.S3AUtils:patchSecurityCredentialProviders(org.apache.hadoop.conf.Configuration):[DEBUG] Setting {} to {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] OUTCOME: FAILURE, Reservation ID: reservationId.toString(), Contract: contract.toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[ERROR] {shexec.getOutput}
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] entry.getKey() + "\\t" + am.getQueue() + "\\t" + am.getAMType() + "\\t" + am.getDuration() + "\\t" + am.getNumTasks()
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$UpdateNodeResourceWhenUnusableTransition:transition(java.lang.Object,java.lang.Object):[WARN] Try to update resource on a + rmNode.getState().toString() + node: + rmNode.toString()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logModifyCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean):[INFO] logEdit
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:isResourcesAvailable(org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Memory available: ... , CPUs available: ..., requested: ...
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteObject(java.lang.String):[DEBUG] deleting %s
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List):[INFO] Checksum calculation succeeded on block file
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeSection(java.io.InputStream):[DEBUG] Finished sorting inodes
org.apache.hadoop.hdfs.server.federation.router.RouterCacheAdmin:listCachePools(java.lang.String):[INFO] Concurrent invocation executed
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startStandbyServices(org.apache.hadoop.conf.Configuration,boolean):[INFO] Starting services required for standby state
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:createNewReservation():[ERROR] Unable to create new reservation from RM web service
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] count=count, return byte[array.length]
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List,java.lang.String):[WARN] Target queue + targetQueue + (cs.isAmbiguous(targetQueue) ? is ambiguous while trying to move : does not exist while trying to move ) + app.getApplicationId
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int):[DEBUG] Changing meta file offset of block {b} from {oldPos} to {newPos}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[WARN] The SchedulingRequest has requested more than 1 allocation, but only 1 will be attempted !!
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:breakLease(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] lease path: {}
org.apache.hadoop.tools.HadoopArchiveLogs:filterAppsByAggregatedStatus():[INFO] Skipping appId due to aggregation status being aggStatus
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:readFile(java.lang.String,java.lang.String):[INFO] Parsing entry: str
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Event [event.getType()] not handled, because previousFailedAttempt is null
org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object):[WARN] Failed to set scheduling priority for client
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionUpgradeComponents(java.lang.String,java.util.List):[ERROR] Failed to upgrade components: {Exception e}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:populateHeaders(java.util.List,java.lang.String,java.lang.String,int,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,boolean,java.util.Map):[TRACE] shuffle for jobId reducer reduce length contentLength mappers: mapIds
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] FilterFs listStatus call invoked
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:uncacheBlock(java.lang.String,long):[DEBUG] {} is anchored, and can't be uncached now. Scheduling it for uncaching in {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:parseGpuDevicesFromAutoDiscoveredGpuInfo():[DEBUG] Found {} GPU devices
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[DEBUG] Validating mount table record
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration):[INFO] Using pure-Java version of bzip2 library
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[INFO] Standalone MiniKdc Running
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue):[INFO] Reinitialized queue management policy for parent queue {parentQueue.getQueuePath()} with leaf queue template capacities : [{leafQueueTemplate.getQueueCapacities()}]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Scanning block pool {bpid} on volume {v}...
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getSnapshottableDirListing():[DEBUG] Audit log false for operation listSnapshottableDirectory
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[DEBUG] Stream {} aborted: {}; remaining={}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToActive():[INFO] Transitioned to active state
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:signalProcess(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Sending signal to pid {} as user {} for container {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] IP file path: {ipFilePath}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:getPrivilegedOperationExecutionCommand(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation):[DEBUG] Privileged Execution Command Array: ...
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:blockCompaction():[ERROR] Storage exception encountered during block compaction phase: {key} Storage Exception: {ex} Error Code: {ex.getErrorCode()}
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getDelegationToken(org.apache.hadoop.security.UserGroupInformation):[DEBUG] UGI Information: {}
org.apache.hadoop.yarn.service.client.ApiServiceClient:initiateUpgrade(java.lang.String,java.lang.String,boolean):[ERROR] Failed to upgrade application:
org.apache.hadoop.mapred.pipes.PipesReducer:close():[INFO] got done
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:init(org.apache.hadoop.conf.Configuration):[INFO] Initializing RollingLevelDB for [name]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startRollingUpgrade():[DEBUG] Log start rolling upgrade
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List):[ERROR] Please specify the policy name.
org.apache.hadoop.security.authentication.client.Authenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[INFO] Kerberos authentication attempted
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:isEnabled():[INFO] Node Resource monitoring interval is <=0. org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is disabled.
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenUsingRefreshToken(java.lang.String,java.lang.String,java.lang.String):[DEBUG] AADToken: starting to fetch token using refresh token for client ID <clientId>
org.apache.hadoop.tools.mapred.CopyMapper:map(org.apache.hadoop.io.Text,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Copying path to target
org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException):[WARN] Interrupted while trying for connection
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet):[TRACE] Failed to place enough replicas, still in need of ...
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:scanActiveLogs():[WARN] Ignoring unexpected file in active directory {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renewDelegationToken(org.apache.hadoop.security.token.Token):[ERROR] Access control exception during renew delegation token
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[ERROR] Received event
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:run():[DEBUG] Deleting path: {} as user {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:registerNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Can't register DN {} because it is already registered., dn.getDatanodeUuid()
org.apache.hadoop.fs.adl.AdlFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[INFO] No valid ADL SDK timeout configured: using SDK default.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:convertCapacitySchedulerXml(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[INFO] Converting placement rules
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:waitForAllPartUploads():[WARN] Interrupted partUpload
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.commons.logging.Log):[WARN] failed to register any UNIX signal loggers: , t
org.apache.hadoop.hdfs.server.namenode.JournalSet:finalizeLogSegment(long,long):[DEBUG] Active JournalAndStream detected
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Wrote {bytes} bytes to {outputName}.java
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:rejectApplicationWithMessage(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] {msg}
org.apache.hadoop.yarn.server.nodemanager.NodeManager:initAndStartNodeManager(org.apache.hadoop.conf.Configuration,boolean):[ERROR] Failing NodeManager start since we're on a Unix-based system but bash doesn't seem to be available.
org.apache.hadoop.ha.ZKFailoverController:becomeStandby():[ERROR] Couldn't transition [localTarget] to standby state
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError():[WARN] Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.
org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node):[INFO] Removing a node: + NodeBase.getPath(node)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] KeyStore loaded successfully !!
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:cleanupOnClose():[DEBUG] Statistics: {statistics values}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$FlushTimerTask:run():[DEBUG] In flush timer task
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List):[DEBUG] Ignoring cache report from {} because {} = false. number of blocks: {}
org.apache.hadoop.hdfs.server.namenode.BackupNode:stop(boolean):[ERROR] Failed to report to name-node., e
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop():[INFO] Service state changed to {}
org.apache.hadoop.yarn.server.webapp.AppBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Failed to read the application
org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:getMethod():[ERROR] Cannot get method {} with types {} from {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeCachePool(java.lang.String,boolean):[WARN] Failure: removeCachePool, {poolName: {poolName}}, false
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:updateHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[INFO] Successfully updateHistory resource skylines for {}.
org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker:processErasureCodingTasks(java.util.Collection):[WARN] No missing internal block. Skip reconstruction for task:{task info}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:run():[WARN] DatanodeAdminMonitor caught exception when processing node., e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:dumpSchedulerState():[DEBUG] FairScheduler state: Cluster Capacity: ... Allocations: ... Availability: ... Demand: ...
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:processOneManifest(org.apache.hadoop.fs.FileStatus):[DEBUG] {}: task attempt {} added {} directories
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean):[DEBUG] Local filesystem path determined
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric name + name + was emitted with a null value.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:isAvailable():[INFO] CGroupsResourceCalculator requires enabling CGroups cpu and memory
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory():[INFO] Failed to process fileInfo for job: {{found.getJobId()}}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable):[DEBUG] Opening file: {} for append
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:middleStep():[DEBUG] NodeManager {} releases a container ({})
org.apache.hadoop.fs.cosn.BufferPool:createDir(java.lang.String):[WARN] Set the buffer dir: [{}]'s permission [writable, readable, executable] failed.
org.apache.hadoop.registry.server.dns.RegistryDNS:addAnswer(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int,int,int):[DEBUG] finding record
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor:run():[WARN] Skipping next heartbeat scan due to excessive pause
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] Got container status for NAMENODE:
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:storeFileWithRetry(java.lang.String,java.io.InputStream,byte[],long):[ERROR] Store file failed. COS key: [%s], exception: [%s]
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getNextSPSPath():[DEBUG] SPS service mode is {}, so external SPS service is not allowed to fetch the path Ids
org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget):[ERROR] Couldn't fence old active + target, e
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:removeSPSHint(long):[INFO] SPS hint already removed for the inodeId:{}. Ignoring exception:{}
org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]):[INFO] Can't send invalid block
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer:go():[ERROR] image loading failed at offset {tracker.getPos()}
org.apache.hadoop.contrib.utils.join.DataJoinJob:createDataJoinJob(java.lang.String[]):[INFO] Using TextOutputFormat: ${args[7]}
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainerLogs(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,boolean,boolean):[INFO] getContainerLogsInfo
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS READLINK fileHandle: {} client: {}
org.apache.hadoop.mapred.JobConf:checkAndWarnDeprecation():[WARN] JobConf.deprecatedString(JobConf.MAPRED_REDUCE_TASK_ULIMIT)
org.apache.hadoop.util.SysInfoLinux:readProcNetInfoFile():[WARN] Error reading the stream
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ScheduleTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Using blacklist for AM: additions({}) and removals({})
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:getServiceDefinition(org.apache.hadoop.fs.Path):[INFO] Error while loading service definition from FS: {}
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:validatePaths(java.lang.String[]):[WARN] e.getMessage()
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[INFO] Network latency avoided with append operation
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Sending the cached reply to retransmitted request {}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleContainerStatus(java.util.List):[INFO] Container {containerId} belongs to an application that is already killed, no further processing
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:close():[WARN] Failed to delete {idPath}
org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.Object,java.lang.Object):[DEBUG] GroupCacheLoader - reload (async).
org.apache.hadoop.hdfs.server.mover.Mover$Processor:processPath(java.lang.String,org.apache.hadoop.hdfs.server.mover.Mover$Result):[WARN] Failed to list directory ... Ignore the directory and continue.
org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy:killContainer(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionContainer):[INFO] Evicting [task details]
org.apache.hadoop.yarn.service.utils.CoreFileSystem:copyLocalFileToHdfs(java.io.File,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Copying file {} to {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing Constraint Placement Processor:
org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration):[ERROR] DtFetcher for service 'service' does not require a token. Check your configuration. Note: security may be disabled or there may be two DtFetcher providers for the same service designation.
org.apache.hadoop.mapred.SortedRanges:remove(org.apache.hadoop.mapred.SortedRanges$Range):[DEBUG] previousRange
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntityTypes():[DEBUG] Attempting to clean up iterator
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[DEBUG] maxTxnsToRead = 1 actual edits read = 0
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:preallocate():[DEBUG] Preallocated + total + bytes at the end of + the edit log (offset + oldSize + )
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] No keytab exists: <keytab>
org.apache.hadoop.fs.s3a.auth.STSClientFactory$STSClient:requestSessionCredentials(long,java.util.concurrent.TimeUnit):[DEBUG] Requesting session token of duration {}
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersion(java.lang.String):[TRACE] Exiting getKeyVersion method.
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:logProxyException(java.lang.Exception,java.lang.String):[WARN] Invocation returned exception on [{}], proxyInfo, ex
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:getCgroupsBlkioResourceHandler(org.apache.hadoop.conf.Configuration):[DEBUG] Creating new cgroups blkio handler
org.apache.hadoop.tools.mapred.CopyCommitter:commitData(org.apache.hadoop.conf.Configuration):[INFO] Data committed successfully to finalDir
org.apache.hadoop.crypto.key.kms.server.KMSAudit:getAuditLoggerClasses(org.apache.hadoop.conf.Configuration):[INFO] No audit logger configured, using default.
org.apache.hadoop.mapred.FileInputFormat:listStatus(org.apache.hadoop.mapred.JobConf):[INFO] Total input files to process : {result.length}
org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer:start():[INFO] Starting InMemoryLevelDBAliasMapServer on {}
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Token not set, looking for delegation token. Creds:{}, size:{}
org.apache.hadoop.fs.s3a.WriteOperations:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String):[DEBUG] Request execution in progress
org.apache.hadoop.tools.mapred.CopyCommitter:cleanup(org.apache.hadoop.conf.Configuration):[INFO] Cleaning up temporary work folder: +
org.apache.hadoop.yarn.server.resourcemanager.placement.RejectPlacementRule:setConfig(java.lang.Object):[DEBUG] RejectPlacementRule instantiated
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[INFO] Adding credentials
org.apache.hadoop.fs.s3a.impl.DeleteOperation:deleteDirectoryTree(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Deleting final batch of listed files
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersion(java.lang.String):[TRACE] Entering getKeyVersion method.
org.apache.hadoop.yarn.service.client.ApiServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] YarnClient created
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState):[WARN] Couldn't find application + applicationId
org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory:getCluster(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] scheme : {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanodeListForReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] getDatanodeListForReport with includedNodes = ..., excludedNodes = ..., foundNodes = ..., nodes = ...
org.apache.hadoop.crypto.key.kms.server.KMS:reencryptEncryptedKeys(java.lang.String,java.util.List):[WARN] Payload size {} too big for reencryptEncryptedKeys from user {}.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getApplications(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest):[INFO] logSuccess called for GET_APPLICATIONS_REQUEST by user
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity):[ERROR] Seems like client has been removed before the entity + could be published for + entity
org.apache.hadoop.hdfs.server.datanode.LocalReplica:breakHardLinksIfNeeded():[INFO] Breaking hardlink for {linkCount}x-linked block {this}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:tryIsolateDevices(org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager$DeviceAllocation,java.lang.String):[WARN] Could not update cgroup for container
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$PutEventThread:run():[DEBUG] Creating SendEntity task in PutEventThread
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameTo(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.Options$Rename[]):[WARN] logAuditEvent(false, "rename (options=" + Arrays.toString(options) + ")", src, dst, null)
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getOwnerForPath(org.apache.hadoop.fs.Path):[ERROR] Could not retrieve owner information for path -
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper:call():[DEBUG] Add current thread to localizingThreads
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:get(java.lang.Class):[ERROR] Cannot get data for {} at {}, cleaning corrupted data
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:shouldDefer():[DEBUG] Uncaching {} now that it is no longer in use by any clients.
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Done recovering task ...
org.apache.hadoop.hdfs.DFSOutputStream:addBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String[],java.util.EnumSet):[WARN] NotReplicatedYetException sleeping src retries left retries
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Operation unchecked
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] # nodes = {}, # racks = {}, capacity of each node {}.
org.apache.hadoop.fs.s3a.S3AInputStream:unbuffer():[DEBUG] Switching to seek policy Random after unbuffer() invoked
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:chooseDatanode(org.apache.hadoop.hdfs.server.namenode.NameNode,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus):[INFO] New instance created
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:getPreemptionCandidatesOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean):[DEBUG] Skip selecting AM container on host={} AM container={}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter():[INFO] Now scanning bpid {} on volume {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkFaultTolerantRetry(org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.String,java.io.IOException,org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,java.util.List):[ERROR] Cannot invoke {} for {} in {}: {}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:serviceStart():[DEBUG] Configuring job jar
org.apache.hadoop.yarn.client.util.YarnClientUtils:generateToken(java.lang.String):[DEBUG] The user credential is {}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render():[ERROR] Unable to locate any logs for container
org.apache.hadoop.tools.CopyFilter:getCopyFilter(org.apache.hadoop.conf.Configuration):[ERROR] Class instantiation error: {filtersClassName}
org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client):[DEBUG] stopping actual client because no more references remain: [ClientInstance]
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String):[WARN] ApplicationMaster is out of sync with ResourceManager, hence resyncing.
org.apache.hadoop.tools.dynamometer.Client:addFileToZipRecursively(java.io.File,java.io.File,java.util.zip.ZipOutputStream):[WARN] Skipping file; it is a symlink with a nonexistent target: {file}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEvent):[INFO] Got exception while resuming container: {exception}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAclEntries(java.lang.String,java.util.List):[INFO] Audit log updated
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine):[ERROR] Failover failed: <exception_message>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore:checkVersion():[INFO] Storing configuration store version info {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state: Current: [{}], eventType: [{}], container: [{}]
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] YarnClient created
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doAppLogAggregationPostCleanUp():[WARN] Log dir {} is in an unsupported file system
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:startLogSegment(long,int):[WARN] Unable to start log segment txid at currentInProgress: e.getLocalizedMessage()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:sigKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Cannot kill container {containerId} pid -{pid}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation,java.io.File,java.util.Map,boolean,boolean):[DEBUG] Executing privileged operation
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:reInitializeContainer(org.apache.hadoop.yarn.api.protocolrecords.ReInitializeContainerRequest):[DEBUG] {} requested reinit, containerId
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$ContainerDoneTransition:transition(java.lang.Object,java.lang.Object):[WARN] Removing unknown + containerEvent.getContainerID() + from application + app.toString()
org.apache.hadoop.yarn.webapp.Router:lookupRoute(org.apache.hadoop.yarn.webapp.WebApp$HTTP,java.lang.String):[DEBUG] prefix match for {}: {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getTimelineStoresForRead(java.lang.String,java.lang.String,java.util.List):[DEBUG] plugin {} returns a non-null value on query {}
org.apache.hadoop.yarn.service.utils.CoreFileSystem:verifyDirectoryWriteAccess(org.apache.hadoop.fs.Path):[WARN] Failed to create file {}: {}
org.apache.hadoop.hdfs.server.namenode.FSImage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[DEBUG] About to load edits:\n [STREAM_DESC]
org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler:allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest):[DEBUG] Forwarding allocate request to the Distributed Scheduler Service on YARN RM
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlock(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.hdfs.protocol.LocatedBlock,java.io.OutputStream):[INFO] Failed to connect to [targetAddr]: [IOException message]
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[INFO] Conflict Resolution mode is {conflictResolution}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Unexpected Event.. [REINITIALIZE_CONTAINER]
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Available Memory: memoryHere
org.apache.hadoop.mapred.gridmix.JobMonitor:submissionFailed(org.apache.hadoop.mapred.gridmix.Statistics$JobStats):[INFO] Job submission failed notification for job + jobID
org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean):[DEBUG] Failure during shutdown: {} , failure, failure
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:processSpeculatorEvent(org.apache.hadoop.mapreduce.v2.app.speculate.SpeculatorEvent):[INFO] JOB_CREATE + event.getJobID()
org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class):[WARN] Credential Provider URI is invalid. {}
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:shutdown():[WARN] Exception while closing CheckpointStorage
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:createTemporary(org.apache.hadoop.fs.StorageType,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,boolean):[WARN] Unable to stop existing writer for block + b + after + writerStopMs + miniseconds.
org.apache.hadoop.hdfs.DistributedFileSystem:getTrashRoot(org.apache.hadoop.fs.Path):[WARN] Exception while checking whether encryption zone is supported
org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream):[INFO] Refreshing hosts (include/exclude) list
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Unable to parse the JWT token
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Added node + nodeManager.getNodeAddress() + clusterResource: + clusterResource
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:execute(org.apache.commons.cli.CommandLine):[WARN] Skipping date check on this plan. This could mean we are executing an old plan and may not be the right plan for this data node.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord):[WARN] Error sending metrics to Graphite
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] Application ID doesn't exist for service {}
org.apache.hadoop.yarn.server.timeline.security.TimelineDelgationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Token renewal interval obtained
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys():[DEBUG] Creating Connection
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Not a symlink, fileId: {}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:invokeCallback():[WARN] Unexpected exception
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:findLostContainers(int,java.util.List):[WARN] Container [containerId] was running but not reported from [nodeId]
org.apache.hadoop.yarn.server.utils.BuilderUtils:newContainerToken(org.apache.hadoop.yarn.api.records.NodeId,byte[],org.apache.hadoop.yarn.security.ContainerTokenIdentifier):[DEBUG] Creating socket address for host
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:extractAndActivateSpanFromRequest(com.amazonaws.HandlerContextAware):[DEBUG] NOT_A_WRAPPED_SPAN + : {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Reported block {} on {} size {} replicaState = {}
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Total VCores allocated for Containers
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:processEventForNewTimelineService(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent,org.apache.hadoop.mapreduce.v2.api.records.JobId,long):[WARN] EventType: UnrecognizedTask cannot be recognized and handled by timeline service
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:doCheckpoint():[INFO] A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid {}. Skipping...
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore$AppCheckTask:run():[WARN] Exception while checking the app status; will leave the entry in the list.
org.apache.hadoop.tools.util.DistCpUtils:toCopyListingFileStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,int):[DEBUG] add file/dir + clfs
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:call():[INFO] Recovered container ${containerId} succeeded
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerStartEventOnTimelineServiceV2(org.apache.hadoop.yarn.api.records.Container,long):[ERROR] Container start event could not be published for {container.getId().toString()}
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable:run():[WARN] Allocated thread interrupted. Returning.
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersion(java.lang.String):[DEBUG] Exception in getKeyVersion.
org.apache.hadoop.mapred.pipes.DownwardProtocol:start():[DEBUG] Starting communication
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI):[DEBUG] backing jks path initialized to ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:schedule(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:initRetryCache(org.apache.hadoop.conf.Configuration):[INFO] Retry cache will use 0.2 of total heap and retry cache entry expiry time is 600000 millis
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] User rule: parent rule found: {parentRule.getName()}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:checkAndUpdate(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo):[WARN] Deleted a metadata file without a block + diskMetaFile.getAbsolutePath()
org.apache.hadoop.util.AsyncDiskService:shutdown():[INFO] Shutting down all AsyncDiskService threads...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Can't get path for fileId: {}
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:execute(org.apache.commons.cli.CommandLine):[INFO] Executing "query plan" command.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:updateProcessTree():[DEBUG] Swap cgroups monitoring is not compiled into the kernel {}
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:scheduleOldDBsForEviction():[INFO] Scheduling ${getName()} DBs older than ${fdf.format(evictionThreshold)} for eviction
org.apache.hadoop.hdfs.DFSInputStream:refetchLocations(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[INFO] No node available for {blockInfo}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Error retrieving tokenInfo [sequenceNumber] from ZK
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=GETXATTRS target=path
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappedBlock:close():[INFO] Successfully uncached one replica:{} from persistent memory, [cached path={}, length={}]
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceStart():[INFO] Starting Local Zookeeper service
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection):[ERROR] BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}
org.apache.hadoop.hdfs.server.datanode.DataXceiver:copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[INFO] opCopyBlock ... received exception ...
org.apache.hadoop.mapred.uploader.FrameworkUploader:run():[INFO] Uploaded + target
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] NameNode container stopped: [ContainerId]
org.apache.hadoop.yarn.service.client.ServiceClient:addJarResource(java.lang.String,java.util.Map):[INFO] Uploading all dependency jars to HDFS. For faster submission of apps, set config property {} to the dependency tarball location. Dependency tarball can be uploaded to any HDFS path directly or by using command: yarn app -{} [<Destination Folder>]
org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text):[DEBUG] selected by service={} token={}, service, token
org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration):[INFO] CORS filter not enabled. Please set + key + to 'true' to enable it
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFileInfo(java.lang.String,boolean,boolean,boolean):[INFO] logAuditEvent
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onShutdownRequest():[INFO] Shutdown request received. Processing since keep_containers_across_application_attempts is disabled
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:registerMultiNodePolicyNames(boolean,java.util.Set):[INFO] MultiNode scheduling is '{multiNodePlacementEnabled}', and configured policies are {StringUtils.join(policySpecs.iterator(), ",")}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:isMultiDestDirectory(java.lang.String):[DEBUG] The destination {} doesn't exist.
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:flush():[INFO] kvbuffer is null. Skipping flush.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setReplication(java.lang.String,short):[INFO] Audit Event: setReplication
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest):[INFO] RMAuditLogger.logSuccess
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:getContainerStatuses():[DEBUG] Sending out {} container statuses: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:readUnlock():[INFO] \tNumber of suppressed read-lock reports: {} \n\tLongest read-lock held at {} for {}ms via {}
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[WARN] Invocation returned exception: ... on [...]
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor$MonitorThread:run():[DEBUG] Running Timeline Storage monitor
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing {} of type {}
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:createTracker(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.s3a.statistics.PutTrackerStatistics):[WARN] File being created has a "magic" path, but the filesystem has magic file support disabled: {}, path
org.apache.hadoop.hdfs.server.datanode.DataNode:createInterDataNodeProtocolProxy(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.conf.Configuration,int,boolean):[DEBUG] Connecting to datanode {} addr={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] requested {deviceCount} Intel FPGA(s)
org.apache.hadoop.hdfs.server.federation.router.RouterCacheAdmin:listCachePools(java.lang.String):[DEBUG] Retrieval of results initiated
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:removeApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String):[INFO] Application removed - appId: + application.getApplicationId() + user: + application.getUser() + queue: + getQueuePath() + #user-pending-applications: + user.getPendingApplications() + #user-active-applications: + user.getActiveApplications() + #queue-pending-applications: + getNumPendingApplications() + #queue-active-applications: + getNumActiveApplications
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] capacityConfigType is updated as '{}' for queue {}
org.apache.hadoop.hdfs.server.balancer.Balancer:runOneIteration():[INFO] Will move {StringUtils.byteDesc(bytesBeingMoved)} in this iteration for {nnc.toString()}
org.apache.hadoop.yarn.security.NMTokenIdentifier:write(java.io.DataOutput):[DEBUG] Writing NMTokenIdentifier to RPC layer: {}, this
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List):[DEBUG] {} corruption detected! Child nodes are missing.
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[INFO] Running old API mapper
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getTrashRoot(org.apache.hadoop.fs.Path):[WARN] Cannot find trash root of path
org.apache.hadoop.yarn.YarnUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] FATAL, Thread [thread] threw an Error. Shutting down now...
org.apache.hadoop.mapred.ShuffleHandler:checkVersion():[INFO] Loaded state DB schema version info
org.apache.hadoop.yarn.service.component.Component:handle(org.apache.hadoop.yarn.event.Event):[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Compressed input; cannot compute number of records in the split
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNodesImpl(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[ERROR] Cannot get {} nodes, Router in safe mode
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:analyzeFileState(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[]):[INFO] BLOCK* allocateBlock: caught retry for allocation of a new block in /* src from code */. Returning previously allocated block /* lastBlockInFile from code */
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Fetching InputFormat map
org.apache.hadoop.yarn.util.resource.Resources:subtractFrom(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[WARN] Resource is missing: {exception_message}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:computeQueueManagementChanges():[DEBUG] Parent queue = ..., nodeLabel = ..., absCapacity = ..., leafQueueAbsoluteCapacity = ..., deactivatedCapacity = ..., absChildActivatedCapacity = ..., availableCapacity = ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(java.lang.Object,java.lang.Object):[INFO] Container is localizing: ...
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:cleanup():[WARN] Failed to remove application staging directory, e
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:cleanupRegistryAndCompHdfsDir(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] <componentId>: Failed to delete component instance dir: <compInstanceDir>
org.apache.hadoop.hdfs.server.namenode.NameNode:startHttpServer(org.apache.hadoop.conf.Configuration):[INFO] HTTP Server started
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the finish data of container [containerId]
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:initializeNewPlans(org.apache.hadoop.conf.Configuration):[INFO] Refreshing Reservation system
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[WARN] Authentication exception: {ex.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initializing MultiNodeSorter= + policySpec.getPolicyName() + , with sorting interval= + policySpec.getSortingInterval()
org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String):[DEBUG] Deprecation logged for name
org.apache.hadoop.yarn.util.resource.ResourceUtils:getAllocation(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,long):[DEBUG] Mandatory Resource '{}' is not configured in resource-types config file. Setting allocation specified using '{}'
org.apache.hadoop.util.SysInfoLinux:readProcStatFile():[WARN] Error closing the stream {stream details}
org.apache.hadoop.mapred.LocalJobRunner:setupChildMapredLocalDirs(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JobConf):[DEBUG] LOCAL_DIR for child : {childMapredLocalDir}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:copyBlocks(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] No block pools found on volume. volume : {}. Exiting.
org.apache.hadoop.yarn.service.provider.ProviderUtils:createConfigFileAndAddLocalResource(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,java.util.Map,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams):[INFO] {instance.getCompInstanceId()} version {compLaunchContextgetServiceVersion()} : Creating Public Resource dir on hdfs: {compPublicResourceDir}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:getHdfsImageToHashReader():[DEBUG] Did not load hdfs image to hash file, file doesn't exist
org.apache.hadoop.hdfs.DFSStripedOutputStream:waitCreatingStreamers(java.util.Set):[INFO] close the slow stream
org.apache.hadoop.fs.s3a.S3AUtils:initProxySupport(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.ClientConfiguration):[WARN] Proxy host set without port. Using HTTPS default 443
org.apache.hadoop.hdfs.DFSOutputStream:addBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String[],java.util.EnumSet):[WARN] Caught exception
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getNamespaceEditsDirs(org.apache.hadoop.conf.Configuration,boolean):[WARN] Edits URI ... listed multiple times in ... Ignoring duplicates.
org.apache.hadoop.yarn.service.client.ServiceClient:updateLifetime(java.lang.String,long):[INFO] Updating lifetime of an service: serviceName = {serviceName}, appId = {appId}, lifetime = {lifetime}
org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[DEBUG] Initializing the GangliaSink for Ganglia metrics.
org.apache.hadoop.fs.azurebfs.services.AbfsClient:append(java.lang.String,byte[],org.apache.hadoop.fs.azurebfs.contracts.services.AppendRequestParameters,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] User error, retrying without 100 continue enabled for the given path {}
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[TRACE] Found no rules for user
org.apache.hadoop.yarn.service.client.ServiceClient:actionDependency(java.lang.String,boolean):[ERROR] Got exception creating tarball and uploading to HDFS
org.apache.hadoop.fs.cosn.CosNFileReadTask:run():[WARN] Exception occurs when retrieve the block range start: {start} end: {end}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:run():[WARN] Unexpected error trying to delete/move block
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:doneApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Skip killing {container.getContainerId()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:updateCGroupParam(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String,java.lang.String,java.lang.String):[DEBUG] updateCGroupParam for path: {cGroupParamPath} with value {value}
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask:run():[WARN] Error while checking local directories: , t
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadState():[INFO] Loading history server state from <rootStatePath>
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:abortMultipartUpload(java.lang.String,java.lang.String):[INFO] Abort the multipart upload. COS key: [{}], upload id: [{}].
org.apache.hadoop.hdfs.server.datanode.DataNode:getVolumeReport():[WARN] DataNode volume info not available.
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[INFO] Application {applicationName} mapping [{queueName}] to [{mappedQueue.getQueue()}] override {overrideWithQueueMappings}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:updateAppsRunnability(java.util.List,int):[ERROR] Waiting app [appSched] expected to be in usersNonRunnableApps, but was not. This should never happen.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[]):[INFO] commitBlockSynchronization(oldBlock=...
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PolicyInvoker:run():[ERROR] Exception raised while executing preemption checker, skip this run..., exception=
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:calculateEffectiveResourcesAndCapacity(java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Updating effective min resource for queue:childQueuePath as effMinResource=effectiveMinResource and Updating effective max resource as effMaxResource=effectiveMaxResource
org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling():[WARN] {}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:forceKillApplication(org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest):[INFO] forceKillApplication applicationId on SubCluster subClusterId
org.apache.hadoop.hdfs.server.namenode.BackupImage:waitUntilNamespaceFrozen():[INFO] Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:commit(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}
org.apache.hadoop.hdfs.qjournal.server.Journal:doRollback():[DEBUG] IOUtils.cleanupWithLogger called
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable:run():[ERROR] Error communicating with RM: + e.getMessage(), e
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceStop():[ERROR] Failed to shutdown ScheduledExecutorService
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processReduceAttemptLine(org.apache.hadoop.tools.rumen.ParsedLine):[ERROR] HadoopLogsAnalyzer.processReduceAttemptLine: bad numerical format, at line...
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:loadManifest(org.apache.hadoop.conf.Configuration,boolean):[INFO] Manifest file + manifest + doesn't exist, stopping + auxiliary services
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_KEY is invalid, Current value is balancedSpaceTolerance, Default value DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_TOLERANCE_DEFAULT will be used instead.
org.apache.hadoop.hdfs.server.namenode.JournalSet:selectInputStreams(java.util.Collection,long,boolean,boolean):[WARN] Unable to determine input streams from + jas.getManager() + . Skipping.
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeSection(java.io.InputStream):[INFO] Loading {numInodes} inodes.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:processor():[DEBUG] Adding block of {} entries
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean):[DEBUG] DIR* Namesystem.delete: {iip.getPath()} is removed
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Checked safe mode status
org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int):[ERROR] BUG: Found lastValidNode {} but not nth valid node. parentNode={}, excludedScopeNode={}, excludedNodes={}, totalInScopeNodes={}, availableNodes={}, nthValidToReturn={}, lastValidNode, parentNode, excludedScopeNode, excludedNodes, totalInScopeNodes, availableNodes, nthValidToReturn
org.apache.hadoop.mapred.gridmix.JobSubmitter:add(java.lang.Object):[INFO] Total number of queued jobs:
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionStop(java.lang.String):[ERROR] Fail to stop application:
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterConfig,org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] Failed to save summary to {}, {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:updateNodeResource(org.apache.hadoop.yarn.server.api.protocolrecords.UpdateNodeResourceRequest):[INFO] AdminService: updateNodeResource successful
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[DEBUG] Found ... files
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:printDryRunResults():[INFO] Number of errors: {noOfErrors}
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[INFO] Unauthenticated access audited
org.apache.hadoop.oncrpc.RpcProgram:register(org.apache.hadoop.portmap.PortmapMapping,boolean):[ERROR] Registration failure with host:port, portmap entry: mapEntry
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] setOwner filesystem: {client.getFileSystem()} path: {path} owner: {owner} group: {group}
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:savePools(java.io.DataOutputStream,java.lang.String):[INFO] End step for saving cache pools
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getOffSwitchPerHeartbeatLimit():[WARN] OFFSWITCH_PER_HEARTBEAT_LIMIT + "(" + limit + ") < 1. Using 1.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:handleRejectedRequests(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Following requests of [{}] exhausted all retry attempts trying to schedule on placed node: {}
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized federation membership service.
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Cluster Metrics Initialized
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStart():[INFO] Epoch set for Federation: + epoch
org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.Object):[DEBUG] GroupCacheLoader - load.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:killContainer():[WARN] container %s killed by elastic cgroups OOM handler.
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:start():[DEBUG] Starting HttpServer
org.apache.hadoop.fs.s3a.S3AFileSystem:bindAWSClient(java.net.URI,boolean):[DEBUG] No delegation token for this instance
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:close():[DEBUG] Closing {}
org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator:setRandomTextDataGeneratorListSize(org.apache.hadoop.conf.Configuration,int):[DEBUG] Random text data generator is configured to use a dictionary with {listSize} words
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] post-assignContainers
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadProxyCAManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Couldn't find Proxy CA data
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskStriped:recover():[DEBUG] Recovering block " + block + ", length=" + block.getNumBytes() + ", safeLength=" + safeLength + ", syncList=" + syncBlocks
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:reencryptEncryptionZone(long):[INFO] Cannot re-encrypt directory with id {} because it's not a directory.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:stopDecommission(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[TRACE] stopDecommission: Node {} in {}, nothing to do., node, node.getAdminState()
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader:load(java.io.File):[INFO] Loaded FSImage in {} seconds.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[INFO] Intel aocl program {ipPath} to {aclName} successfully
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:initRetryCache(org.apache.hadoop.conf.Configuration):[INFO] Retry cache on namenode is disabled
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[DEBUG] Recording source-path: {sourceStatus.getPath()} for copy.
org.apache.hadoop.yarn.service.webapp.ApiServer:updateService(javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.yarn.service.api.records.Service):[INFO] PUT: updateService for app = {} with data = {} user = {}, appName, updateServiceData, ugi
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.CopyObjectRequest,java.lang.String):[DEBUG] Unable to restrict HEAD request to etag; will check later
org.apache.hadoop.yarn.client.AMRMClientUtils:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.token.Token):[INFO] Creating RMProxy to RM {} for protocol {} for user {}
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[TRACE] program + procedure # + call.getProcedure()
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:checkAccess(org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[ERROR] Block token with id doesn't have the correct token password
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRename(java.lang.String,java.lang.String,long,boolean):[DEBUG] Edit log recorded
org.apache.hadoop.hdfs.DFSInputStream:actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[WARN] fetchBlockByteRange(). Got a checksum exception for [src] at [block]:[pos] from [datanode]
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:copyBlocks(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[DEBUG] Moved block with size {} from {} to {}
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:processResponseQueue():[DEBUG] Application {} has one reducer killed ({})
org.apache.hadoop.yarn.server.AMRMClientRelayer:shutdown():[WARN] Shutdown called twice for AMRMClientRelayer for RM + this.rmId
org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String):[ERROR] Cannot create PowerShell script
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore$EntityDeletionThread:run():[ERROR] Exception occurred: {IOException message}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] attemptId + TaskAttempt Transitioned from + oldState + to + getInternalState()
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:copy(java.lang.String,java.lang.String):[ERROR] Copy object unsuccessfully. source COS key: %s, dest COS key: %s, exception: %s
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Arguments):[DEBUG] Cleanup of directory with
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:loadFullHistoryData(boolean,org.apache.hadoop.fs.Path):[WARN] Could not parse history file [ + historyFileAbsolute + ] (Exception Details)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:createAuxService(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord,org.apache.hadoop.conf.Configuration,boolean):[INFO] The aux service: {sName} is using the custom classloader with classpath {destFiles}
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:moveBlock(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.Socket,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[DEBUG] Pinned block can't be moved, so skipping block
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[INFO] kvstart = {kvstart}; length = {maxRec}
org.apache.hadoop.resourceestimator.service.ShutdownHook:run():[ERROR] HttpServer fails to shut down!
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStopContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] onStopContainerError received unknown containerID: + containerId
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:processResponseQueue():[DEBUG] Application {} has one streamer finished ({})
org.apache.hadoop.hdfs.qjournal.server.JournalNode:main(java.lang.String[]):[ERROR] Failed to start journalnode.
org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap():[DEBUG] Updated map for non-Mac
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] FATAL, Shutting down the resource manager.
org.apache.hadoop.hdfs.DFSInputStream:hedgedFetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[DEBUG] Failed getting node for hedged read: {}
org.apache.hadoop.yarn.service.client.ServiceClient:printLocalResources(java.util.Map):[DEBUG] Added LocalResource for localization:
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[DEBUG] Successfully cached {}. We are now caching {} bytes in total.
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:getHAServiceState(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo):[DEBUG] NameNode {} threw StandbyException when fetching HAState
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Temporary redirect for checksum
org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:nextKeyValue():[DEBUG] taskId + ": RecordReader is null. No records to be read."
org.apache.hadoop.mapred.LocalDistributedCacheManager:setup(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobID):[INFO] Localized resourcePath as path
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] UGI instance = %s
org.apache.hadoop.yarn.server.resourcemanager.RMInfo:register():[WARN] Error registering RMInfo MBean
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:parsePreviousJobHistory():[INFO] Read completed tasks from history
org.apache.hadoop.tools.HadoopArchiveLogsRunner:run(java.lang.String[]):[INFO] Running as user
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:run():[INFO] Deleted block
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KillOnNewTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container kill event processed
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {handle.getFileId()}
org.apache.hadoop.hdfs.web.resources.ExceptionHandler:toResponse(java.lang.Exception):[WARN] INTERNAL_SERVER_ERROR
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:reopen(long):[DEBUG] Aborting old stream to open at pos ...
org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,java.util.Map):[DEBUG] Found existing ${servletMappings[i].getServletName()} servlet at path ${pathSpec}; will replace mapping with ${sh.getName()} servlet
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:dump():[INFO] Create dump file: {}
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:recover():[INFO] Block recovery for block {block} succeeded
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop():[INFO] prefix + metrics system stopped.
org.apache.hadoop.fs.azure.security.JsonUtils:parse(java.lang.String):[DEBUG] JSON Parsing exception: {} while parsing {}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$UpdateContainerResourceTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown from onContainerResourceUpdated for Container
org.apache.hadoop.yarn.server.webapp.WebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] Privileged action performed
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:getUAMToken():[WARN] AMRMToken not found in the application report for application: {}
org.apache.hadoop.yarn.service.client.ServiceClient:getStatus(java.lang.String):[INFO] Service {} is at {} state
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Temporary redirect to URI
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:toDevice(java.nio.file.Path,org.apache.commons.lang3.mutable.MutableInt):[INFO] Device syspath: {sysPath}
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:getErasureCodingPolicyName(org.apache.hadoop.hdfs.server.namenode.INode):[WARN] Encountered error getting ec policy for inode path
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:addHandlersFromConfiguredResourcePlugins(java.util.List,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[WARN] Plugin manager was null while trying to add ResourceHandlers from configuration!
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:main(java.lang.String[]):[INFO] Application Master completed successfully. exiting
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent):[ERROR] Ignoring invalid eventtype + event.getType()
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Recovering task for upgrading scenario, moving files from ... to ...
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,java.lang.String,boolean):[DEBUG] Enabling path style access!
org.apache.hadoop.yarn.service.webapp.ApiServer:startService(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Successfully started service + appName
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short):[INFO] Checking operation category WRITE
org.apache.hadoop.hdfs.server.datanode.DataXceiver:stopWriter():[INFO] Stopped the writer: {}
org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List):[DEBUG] Use {selectedAuthType.getMethod()} authentication for protocol {protocol.getSimpleName()}
org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec:parse(java.lang.String):[INFO] Parsed constraint Empty
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Percentage of invalid ops: ...
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[WARN] rmApp.getApplicationId() + final state ( + appState + ) was recorded, but + appAttempt.applicationAttemptId + final state ( + appAttempt.recoveredFinalState + ) was not recorded.
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[INFO] removing RMDelegation token with sequence number: {ident.getSequenceNumber()}
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:put(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl$ResourceRequestInfo):[DEBUG] Added resourceName={}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:delete(java.lang.String):[ERROR] Delete key: [{}] occurs an exception: [{}].
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnclosedSegment(long):[INFO] Beginning recovery of unclosed segment starting at txid + segmentTxId
org.apache.hadoop.fs.s3a.S3AUtils:createAwsConf(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Signer override for {}} = {}, awsServiceIdentifier, signerOverride
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocateContainersInternal(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$AllocationParams,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$ContainerIdGenerator,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.Map,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$EnrichedResourceRequest,int):[INFO] Allocated [containerId] as opportunistic at location [location]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] movedContainer container=[container details] containerState=[state details] resource=[resource details] queueMoveIn=[this] usedCapacity=[capacity details] absoluteUsedCapacity=[absoluteCapacity details] used=[used details] cluster=[cluster details]
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Deleting the temporary directory of '{attemptId}': '{taskAttemptPath}'
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:addReplicaToReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean):[WARN] Failed to delete restart meta file: {restartMeta.getPath()}
org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:run():[WARN] DataNode{bpReg}:Failed to transfer {block} to {target} got
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: RESERVATION_SYSTEM
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:refreshSuperUserGroupsConfiguration():[INFO] HSAuditLogger log success
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List):[TRACE] Cache report from datanode {} has block {}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:deleteDestinationPaths(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Deleting Job attempt Path
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[INFO] Container {} pid file not set. Returning terminated error
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersReceivedFromPreviousAttempts(java.util.List):[INFO] Not waiting to recover container {}, releasing
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:handleInteraction(org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter$HttpInteraction):[TRACE] Rejecting interaction; no rule found
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:getTrackingUri(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.yarn.server.webproxy.AppReportFetcher$AppReportSource):[DEBUG] Original tracking url is '{}'. Redirecting to AHS app page
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:shouldAllocOrReserveNewContainer(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] needsContainers: app.#re-reserve= + application.getReReservations(schedulerKey) + reserved= + reservedContainers + nodeFactor= + nodeFactor + minAllocFactor= + application.getCSLeafQueue().getMinimumAllocationFactor() + starvation= + starvation
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:findNodeToUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] unreserving for app: + getApplicationId() + on nodeId: + idToUnreserve + in order to replace reserved application and place it on node: + node.getNodeID() + needing: + minimumUnreservedResource
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Reinstating vectored read operation for path {}, pathStr
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser:run():[DEBUG] End parsing summary logs.
org.apache.hadoop.tools.dynamometer.Client:launchAndMonitorWorkloadDriver(java.util.Properties):[INFO] Launching workload job using input path: + workloadInputPath
org.apache.hadoop.http.HttpServer2:stop():[ERROR] Error while stopping listener for webapp + webAppContext.getDisplayName(), e
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:run():[WARN] Re-encryption updater thread exception.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator:showRequests():[DEBUG] Print human-readable requests to LOG debug.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[]):[ERROR] Illegal values in env for shell script path
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor$HeartbeatCallBack:callback(java.lang.Object):[WARN] notifyOfResponse for policy failed for sub-cluster + subClusterId, e
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:getDirectoryPolicy(org.apache.hadoop.conf.Configuration,java.util.function.Predicate):[INFO] Directory markers will be kept
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:executeStage(java.lang.Object):[INFO] {}: Executing Manifest Job Commit with manifests in {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:toDevice(java.nio.file.Path,org.apache.commons.lang3.mutable.MutableInt):[INFO] Checking device file: {p}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rename(java.lang.String,java.lang.String):[DEBUG] *DIR* NameNode.rename: src to dst
org.apache.hadoop.hdfs.server.datanode.StorageLocation:makeBlockPoolDir(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Skipping creating directory for block pool {blockPoolID} for PROVIDED storage location {this}
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:updateAppAttemptKey(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.security.MasterKeyData):[ERROR] Unable to store master key for application + attempt
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[DEBUG] Connecting to target address.
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateExpiration(com.nimbusds.jwt.SignedJWT):[WARN] JWT expiration date validation failed.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:assignGpus(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Interrupted while waiting for available GPU
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandomWithStorageTypeTwoTrial(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType):[DEBUG] No node to choose.
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkGetACLS(java.lang.String):[DEBUG] GetACLS {}
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:createBlobClient(com.microsoft.azure.storage.CloudStorageAccount):[ERROR] createBlobClient is an invalid operation in SAS Key Mode
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:startThreads():[ERROR] Error rebuilding local cache for zkDelegationTokens
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Waiting scan completion
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path):[INFO] Saved output of task '${attemptId}' to ${committedTaskPath}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState, getAppAttemptState(), event.getType())
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Error launching job , Invalid job conf :
org.apache.hadoop.yarn.applications.distributedshell.Client:sendStopSignal():[INFO] done stopping Client
org.apache.hadoop.fs.azurebfs.security.AbfsTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[DEBUG] Renewing the delegation token
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:registerNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Registered DN {} ({})., dn.getDatanodeUuid(), dn.getXferAddr()
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[WARN] No rule was found for user '{}'
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfigurePropertyImpl(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}
org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path):[DEBUG] Attempting symlink resolution
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:register():[ERROR] Exception while registering
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[INFO] Request #getBlocks to Standby NameNode success. remoteAddress: {}
org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] {caption} UGI: {ugi}
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[ERROR] The owner of the posted timeline entities is not set
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:convertTemporaryToRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo):[DEBUG] Checksum loaded and data length set
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean):[DEBUG] mkdirs of {}={}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getMissingLogSegments(java.util.List,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto,org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy):[WARN] Journal at ... has no edit logs
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes):[ERROR] Can't close stream for fileHandle: <dumpedHandle>, <exception>
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Finalizing upgrade of storage directory + sd.getRoot()
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[INFO] Storage policy satisfier is already in mode:{}, so ignoring change mode event.
org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse):[ERROR] Problem while trying to process JMX query: + qry + with MBean + oname
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[INFO] Verified that the service is down.
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[INFO] FSPlacementRule applied
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getJson():[TRACE] Retrieval of slow peer reports as json string is disabled. To enable it, please enable config {}.
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] size of input data to be generated specified using -generate option should be nonnegative.\n
org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing AMS Processing chain. Root Processor=[this.head.getClass().getName()].
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:increaseContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container):[WARN] Exception when scheduling the event of increasing resource of Container <container_id>
org.apache.hadoop.resourceestimator.skylinestore.validator.SkylineStoreValidator:validate(org.apache.hadoop.resourceestimator.common.api.RecurrenceId):[ERROR] Resource allocation for {pipelineId} is null.
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:addPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy):[INFO] A policy with same schema and cell size already exists
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processTaskLine(org.apache.hadoop.tools.rumen.ParsedLine):[ERROR] A task status you don't know about is "unknownStatus".
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] No source etag; unable to probe for the operation's success
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:clearRollingUpgradeMarkers(java.util.List):[INFO] Deleting {}
org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String):[TRACE] TRACE
org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator):[INFO] Iterating through items
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String):[DEBUG] Connecting to url {} with token {} as {}
org.apache.hadoop.yarn.service.webapp.ApiServer:createService(javax.servlet.http.HttpServletRequest,org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Failed to create service {}
org.apache.hadoop.hdfs.server.balancer.Balancer:init(java.util.List):[INFO] dn + [ + t + ] has utilization= + utilization + >= average= + average + but it is not specified as a source; skipping it.
org.apache.hadoop.hdfs.HAUtilClient:cloneDelegationTokenForLogicalUri(org.apache.hadoop.security.UserGroupInformation,java.net.URI,java.util.Collection):[DEBUG] No HA service delegation token found for logical URI + haUri
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[DEBUG] Unable to de-serialize block token identifier for user={userId}, block={block}, access mode={mode}
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getDatanodesSubcluster():[ERROR] Cannot access the Router RPC server
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close():[ERROR] Forcing CleanerThreadPool to shutdown!
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getNodeResources(org.apache.hadoop.conf.Configuration):[DEBUG] Node resource information map is {}
org.apache.hadoop.fs.Globber:doGlob():[DEBUG] No matches found and there was no wildcard in the path {}
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:listDirRegistry(org.apache.hadoop.registry.client.api.RegistryOperations,org.apache.hadoop.security.UserGroupInformation,java.lang.String,boolean):[ERROR] Registry list key " + key + " failed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:checkIfNodeBlackListed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey):[DEBUG] Skipped App Activity recorded for blacklisted node
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.mapred.pipes.Application:waitForAuthentication():[DEBUG] Waiting for authentication response
org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor:run():[ERROR] Error in handling event type
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerQueued(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] storeContainerQueued: containerId={}
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] Trying to recover task from ...
org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater:run():[ERROR] Failed to set keys
org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner:run():[INFO] Resource usage matcher thread started.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:getNodeIdToUnreserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator):[DEBUG] unreserving node with reservation size: {} in order to allocate container with size: {}, reservedResource, resourceNeedUnreserve
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler):[INFO] Sent total: ...
org.apache.hadoop.yarn.server.webapp.WrappedLogMetaRequest:getContainerLogMetas():[DEBUG] Setting App ID: {applicationId}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumExpiredNamenodes():[ERROR] Cannot retrieve numExpiredNamenodes for JMX: {IOException.getMessage()}
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker:getJson():[DEBUG] Failed to serialize statistics
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:doWork():[ERROR] Merging failed X times.
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Setting client token master key
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:upsertDocument(org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.TimelineDocument):[DEBUG] Successfully wrote doc with id : {} and type : {} under Database : {}
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getNextBlockToRead():[TRACE] ReadBufferWorker picked file {buffer.getStream().getPath()} for offset {buffer.getOffset()}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:delete(org.apache.hadoop.fs.Path,boolean):[TRACE] Log message related to the deletion process
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object):[WARN] Failed to register MBean \ + name + \, e
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: [event.getClass()]
org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler:registerApplicationMasterForDistributedScheduling(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Forwarding registration request to the Distributed Scheduler Service on YARN RM
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:render():[DEBUG] Bad request: requires Application ID
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminMonitorBase:getUnhealthyNodesToRequeue(java.util.List,int):[WARN] {} limit has been reached, re-queueing {} nodes which are dead while in Decommission In Progress., DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_MAX_CONCURRENT_TRACKED_NODES, numUnhealthyNodesToRequeue
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Submitting application to RM
org.apache.hadoop.hdfs.server.namenode.NameNode:stopAtException(java.lang.Exception):[WARN] Encountered exception when handling exception (${e.getMessage()}):, ${ex}
org.apache.hadoop.hdfs.server.namenode.FSImage:loadFSImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] Loaded image for txid + txId + from + curFile
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL from user
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:uploadPart(java.io.File,java.lang.String,java.lang.String,int):[DEBUG] Failed to upload {file.getPath()}, try again., {e}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean):[DEBUG] Updating non existent Key path [ + nodeCreatePath + ].. Adding new !!
org.apache.hadoop.ipc.WritableRpcEngine$Server:log(java.lang.String):[INFO] LOG.INFO: value
org.apache.hadoop.ipc.Server$Connection:buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[]):[DEBUG] Will send state token of size null from saslServer.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean):[DEBUG] {applicationAttemptId} is recovering. Skipping notifying ATTEMPT_ADDED
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:removeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] removeContainerPaused: containerId={}
org.apache.hadoop.tools.mapred.lib.DynamicInputChunk:assignTo(org.apache.hadoop.mapreduce.TaskID):[WARN] chunkFilePath could not be assigned to taskId
org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException):[INFO] Logging exit info
org.apache.hadoop.hdfs.qjournal.server.Journal:updateLastPromisedEpoch(long):[INFO] Updating lastPromisedEpoch from <lastPromisedEpoch value> to <newEpoch> for client <RemoteIp> ; journal id: <journalId>
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object):[DEBUG] Not decrementing resource as ResourceRequestInfo with priority={} resourceName={} executionType={} capability={} is not present in request table
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:launchAM(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] AM process exited with value: {exitCode}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Creating file: {f}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerEndEvent(org.apache.hadoop.yarn.client.api.TimelineClient,org.apache.hadoop.yarn.api.records.ContainerStatus,java.lang.String,org.apache.hadoop.security.UserGroupInformation):[ERROR] Container end event could not be published for ...
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getEditLogManifest(long):[INFO] Checking operation category
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean):[ERROR] Could not initialize shared edits dir, ioe
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.listStatus path: {f.toString()}
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:shutDown():[WARN] Got interrupted while joining + getName(), ie
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:removeInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource):[INFO] Request to remove more resources than what is available
org.apache.hadoop.yarn.service.client.ServiceClient:initiateUpgrade(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Service {} upgrade to version {} failed because {}
org.apache.hadoop.yarn.service.ServiceMaster:checkAndUpdateServiceState(org.apache.hadoop.yarn.service.ServiceScheduler):[INFO] Service state changed from {} -> {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:processor():[DEBUG] interrupted
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close():[INFO] getStats()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanodeDescriptor(java.lang.String):[INFO] Choosing random node after resolving network location
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Pmem enforcement enabled
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler$LogDeleterRunnable:run():[ERROR] Error removing log deletion state
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:innerClose():[DEBUG] Block[{}]: Buffer file {} exists —close upload stream
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:isFile(org.apache.hadoop.fs.Path):[TRACE] {}: isFile('{}'), getName(), path
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectRpcInputStreams(java.util.Collection,long,boolean):[INFO] Selected loggers with >= maxAllowedTxns transactions starting from lowest txn ID
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:launchContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext):[INFO] Json Generation Exception
org.apache.hadoop.registry.server.services.RegistryAdminService:createRootRegistryPaths():[INFO] System ACLs {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onShutdownRequest():[INFO] Shutdown request received. Ignoring since keep_containers_across_application_attempts is enabled
org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI):[DEBUG] Namenode domain name will be resolved with {dnr.getClass().getName()}
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:submitUnmanagedApp(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Submitting unmanaged application {}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,long):[ERROR] Error in storing RMDelegationToken with sequence number
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:init(org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] Bootstrapping resource handler chain: {}
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:logCurrentHadoopUser():[WARN] Failed to get current user, {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long):[WARN] After resync, the position, {} is not greater than the previous position {}. Skipping remainder of this log.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:run():[DEBUG] Deferred uncaching of {} completed. usedBytes = {}
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long):[DEBUG] mode.toString() + " Loaded TrustStore: " + truststoreLocation
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] The file system initialized uri scheme is matching with the given target uri scheme. So, the target file system instances will not be cached. To cache fs instances, please set fs.viewfs.enable.inner.cache to true. The target uri is: uri
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context):[INFO] FieldSelectionHelper.specToString(fieldSeparator, reduceOutputKeyValueSpec, allReduceValueFieldsFrom, reduceOutputKeyFieldList, reduceOutputValueFieldList)
org.apache.hadoop.lib.server.Server:initConfig():[DEBUG] Loading site configuration from [{siteFile}]
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Executing command {MarkerTool.MARKERS}
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,org.slf4j.Logger):[TRACE] ${method} invoked by user ${user.getShortUserName()}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getCleanException(java.io.IOException):[ERROR] Could not create exception {ioeClass.getSimpleName()}, {ReflectiveOperationException}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent):[INFO] Got APPLICATION_INIT for service [event.getServiceID()]
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] App: {} can't handle this event at current state
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeLabelsProvider(org.apache.hadoop.conf.Configuration):[DEBUG] Distributed Node Labels is enabled with provider class as : {ConfigurationNodeLabelsProvider}
org.apache.hadoop.fs.s3a.S3AInputStream:onReadFailure(java.io.IOException,boolean):[DEBUG] Got exception while trying to read from stream {}, client: {} object: {}, trying to recover:
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:shellToContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ShellContainerCommand):[ERROR] Fail to shell to container: + t.getMessage()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(byte[],int,int):[ERROR] Encountered Storage Exception for write on Blob : {} Exception details: {} Error Code : {}
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewToken(org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier,long):[DEBUG] Storing token {sequence number}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntity(java.lang.String,java.lang.String,java.lang.Long,java.util.EnumSet,org.iq80.leveldb.DBIterator,byte[],int):[WARN] Found unexpected column for entity...
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:stopApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Error removing AMRMProxy application context for {applicationId}, e=""
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:readReplicasFromCache(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Replica Cache file: path doesn't exist
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read(byte[],int,int):[ERROR] Encountered Storage Exception for read on Blob : {key} Exception details: {e} Error Code : {((StorageException) innerException).getErrorCode()}
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] ResourceHandlerChain.postComplete failed for containerId: {}. Exception: , containerId, e
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:sendMap(org.apache.hadoop.mapred.ShuffleHandler$ReduceContext):[ERROR] Shuffle error :[exception]
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$DecommissioningNodeTransition:transition(java.lang.Object,java.lang.Object):[INFO] Update NODE_ID DecommissioningTimeout to be TIMEOUT_PLACEHOLDER
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits):[DEBUG] User {} has been removed!, userName
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read(long,byte[],int,int):[ERROR] Encountered Storage Exception for read on Blob : {} Exception details: {} Error Code : {}
org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile():[WARN] Couldn't read [procfsCpuFile]; can't determine cpu info
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllSchemas(org.apache.hadoop.conf.Configuration,boolean):[WARN] Schema creation finished with the following exceptions
org.apache.hadoop.fs.s3a.WriteOperationHelper:newUploadPartRequest(java.lang.String,java.lang.String,int,long,java.io.InputStream,java.io.File,java.lang.Long):[INFO] upload part request
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:maybeInitSTS():[DEBUG] Creating STS client for {getDescription()}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[WARN] Job init failed, e
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Store RMDT master key with key id: {delegationKey.getKeyId()}. Currently rmDTMasterKeyState size: {rmDTMasterKeyState.size()}
org.apache.hadoop.hdfs.ClientContext:printConfWarningIfNeeded(org.apache.hadoop.hdfs.client.impl.DfsClientConf):[WARN] Existing client context ' + name + ' does not match + requested configuration. Existing: + existing + , Requested: + requested
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider:deleteWithExistsCheck(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File):[WARN] Failed to delete file {f}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder,java.lang.String):[TRACE] Using cached SAS token.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] allocate: post-update applicationAttemptId= ...
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration):[WARN] Failed to load/initialize native-bzip2 library + libname + , will use pure-Java version
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:extractAndActivateSpanFromRequest(com.amazonaws.HandlerContextAware):[DEBUG] No audit span attached to request {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadRMAppState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Done loading applications from FS state store
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveAllKeys(java.io.DataOutputStream,java.lang.String):[DEBUG] Key written and counter incremented
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:innerCommitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Saving work of {} to {}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String):[WARN] Failed to parse \" + s + \", which is the index \" + i + \" element in \" + originalString + \"
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:waitExecutorTerminated(java.util.concurrent.ExecutorService):[ERROR] Interrupted waiting for executor terminated.
org.apache.hadoop.fs.s3a.S3AUtils:closeAutocloseables(org.slf4j.Logger,java.lang.AutoCloseable[]):[DEBUG] Closing {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[ERROR] Wrong behavior during the insertion of SubCluster {}
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:allocateContainersInternal(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$AllocationParams,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$ContainerIdGenerator,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.Map,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerAllocator$EnrichedResourceRequest,int):[INFO] No nodes currently available to allocate OPPORTUNISTIC containers.
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:postWrite(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerContext):[INFO] PostWrite processing with IndexedFileController
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:readEntityTypes(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[DEBUG] New entity type discovered: ...
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:failApplicationAttempt(org.apache.hadoop.yarn.api.protocolrecords.FailApplicationAttemptRequest):[INFO] Success log for completed application state
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:init(org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] Resource handler chain enabled = {}
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Service {serviceName} is being gracefully stopped...
org.apache.hadoop.hdfs.server.common.JspHelper:getUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] getUGI is returning: + ugi.getShortUserName()
org.apache.hadoop.hdfs.server.namenode.NameNode:copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration):[TRACE] copying op: {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$NameSectionProcessor:process():[DEBUG] NS_INFO writing header: {<content>}
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:run():[INFO] Error cleaning master , ie
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:recoverUnfinalizedSegments():[INFO] Recovering unfinalized segments in currentDir
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[INFO] Updating info for attempt: {appAttemptId} at: {nodeCreatePath}
org.apache.hadoop.examples.QuasiMonteCarlo:estimatePi(int,long,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Starting Job
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String):[DEBUG] Creating token with ugi:{}, renewer:{}, service:{}.
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown():[INFO] metrics system shutdown complete.
org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration):[WARN] strategy not supported by BuiltInZlibDeflater.
org.apache.hadoop.yarn.server.resourcemanager.AdminService:updateNodeResource(org.apache.hadoop.yarn.server.api.protocolrecords.UpdateNodeResourceRequest):[WARN] Resource update get failed on an unrecognized node: {nodeId}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$IBRTaskHandler:run():[INFO] Starting IBR Task Handler.
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List):[DEBUG] *BLOCK* NameNode.cacheReport: from ...
org.apache.hadoop.mapreduce.counters.AbstractCounters:getGroup(java.lang.String):[WARN] Group + groupName + is deprecated. Use + newGroupName + instead
org.apache.hadoop.hdfs.server.namenode.Checkpointer:initialize(org.apache.hadoop.conf.Configuration):[INFO] Transactions count is : + checkpointConf.getTxnCount() + , to trigger checkpoint
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:mkdirs(org.apache.hadoop.fs.Path,boolean):[TRACE] {}: mkdirs('{}')
org.apache.hadoop.fs.cosn.CosNFileSystem:copyDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[WARN] interrupted when wait copies to finish
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumApplicationLifetime(java.lang.String):[ERROR] Ambiguous queue reference: + queueName + please use full queue path instead.
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:parseStaleDataNodeList(java.lang.String,int,org.slf4j.Logger):[DEBUG] Queueing Datanode <currentNodeAddr> for block report; numBlocks = numBlocks
org.apache.hadoop.hdfs.server.datanode.DataXceiver:sendOOB():[INFO] Sending OOB to peer: {peer}
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[INFO] Disabling StoragePolicySatisfier, mode:{}
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:report(java.lang.String,java.lang.String):[DEBUG] a metric is reported: cmd: {} user: {}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:createOutputCommitter(org.apache.hadoop.conf.Configuration):[INFO] OutputCommitter set in config <config_value>
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS WRITE fileHandle: {} offset: {} length: {} stableHow: {} xid: {} client: {}
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String):[DEBUG] Found implementation of {}: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Write operation checked
org.apache.hadoop.hdfs.util.ByteArrayManager$FixedLengthManager:allocate():[DEBUG] : wait ...
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseEvenlyFromRemainingRacks(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,int,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy$NotEnoughReplicasException):[TRACE] New Excluded nodes: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set):[ERROR] Unable to retrieve apps from ClientRMService
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[DEBUG] Adding source dir for traverse: {sourceStatus.getPath()}
org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class):[DEBUG] Filesystem based provider excluded from provider path due to recursive dependency: {}
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Got Cluster metric info from ASM, numNodeManagers={}
org.apache.hadoop.yarn.service.ServiceScheduler:recoverComponents(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Received {} containers from previous attempt.
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:activateNode(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.Resource):[ERROR] This shouldn't happen, cannot get host in nodeCollection associated to the node being activated
org.apache.hadoop.hdfs.tools.DFSck:listCorruptFileBlocks(java.lang.String,java.lang.String):[INFO] line
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:deleteLocalDir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.yarn.server.nodemanager.DeletionService,java.lang.String):[INFO] usercache path : [path]
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[DEBUG] saveInodes and Snapshots completed
org.apache.hadoop.ha.HealthMonitor:enterState(org.apache.hadoop.ha.HealthMonitor$State):[INFO] Entering state {}
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] Deleted {} objects
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:handleInteraction(org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter$HttpInteraction):[TRACE] Looking for delegation token to identify user
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:release(byte[]):[DEBUG] recycle: array.length=, freeQueueSize=
org.apache.hadoop.mapreduce.lib.db.FloatSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[WARN] may result in an incomplete import.
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:enableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.EnableNameserviceRequest):[ERROR] Cannot enable {}, it was not disabled
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:readResultsWithTimestamps(org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix,org.apache.hadoop.yarn.server.timelineservice.storage.common.KeyConverter):[ERROR] Illegal column found, skipping this column.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:assignDevices(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Container : [containerId] is waiting for free [resourceName] devices.
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String):[ERROR] Error getting domain
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadRMDelegationTokenState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Unknown child node with name {} under {}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] {}: upload file count: {}
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] delete: Path is a directory: {}
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:unregisterSlot(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId):[TRACE] unregisterSlot: ShortCircuitRegistry is not enabled.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:breakLease(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.breakLease path: {}
org.apache.hadoop.mapred.pipes.PipesReducer:startApplication(org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[INFO] starting application
org.apache.hadoop.mapred.uploader.FrameworkUploader:addJar(java.io.File):[ERROR] Duplicate jar/path/to/jar
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg$AppLevelAggregator:aggregate():[ERROR] Error aggregating timeline metrics
org.apache.hadoop.yarn.service.client.ServiceClient:actionDependency(java.lang.String,boolean):[INFO] To let apps use this tarball, in yarn-site set config property {} to {}
org.apache.hadoop.fs.azure.security.JsonUtils:parse(java.lang.String):[ERROR] Internal Server Error was encountered while making a request
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:blockIdCK(java.lang.String):[WARN] Fsck on blockId
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean):[WARN] Task cleanup failed for attempt
org.apache.hadoop.mapred.TaskAttemptListenerImpl:preempted(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[INFO] Preempted state update from taskAttemptID.toString()
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:listStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] listStatus filesystem: {client.getFileSystem()} path: {path}, startFrom: {startFrom}
org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider:getProxyInternal():[ERROR] Unable to create proxy to the ResourceManager
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:scheduleReconstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int):[DEBUG] Block {} cannot be reconstructed due to shortage of source datanodes
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:setTimelineCollectorInfo(org.apache.hadoop.yarn.api.records.CollectorInfo):[WARN] Not setting collector info as it is null.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Problem connecting to server: + nnAddr
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[DEBUG] Adding event to entity
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:cleanLogs(org.apache.hadoop.fs.Path,long):[TRACE] Directory list obtained
org.apache.hadoop.hdfs.DFSStripedOutputStream:logCorruptBlocks():[WARN] Block group <bgIndex> failed to write <corruptBlockCount> blocks. It's at high risk of losing data.
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow name: {parts[1]}
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:close():[DEBUG] Token cancel failed: {ioe}
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:markZoneForRetry(java.lang.Long):[INFO] Zone {} will retry re-encryption
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Number of tracked deleted directories {0}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:pruneStorageMap(org.apache.hadoop.hdfs.server.protocol.StorageReport[]):[DEBUG] Number of storages reported in heartbeat={}; Number of storages in storageMap={}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] Starting TimelineClient
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal():[INFO] Executing 'hadoop archives'
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent):[ERROR] Shuffle error: , cause
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:computeIgnoreBlacklisting():[INFO] Ignore blacklisting set to false. Known: <clusterNmCount>, Blacklisted: <blacklistedNodeCount>, <val>%
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:internalGetLabeledQueueCapacity(java.lang.String,java.lang.String,java.lang.String,float):[DEBUG] CSConf - getCapacityOfLabel: prefix= + getNodeLabelPrefix(queue, label) + , capacity= + capacity
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[INFO] logEdit called
org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options):[INFO] Key creation with KeyProviderExtension successful
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getSecondaryGroup(java.lang.String):[DEBUG] User {} is not associated with any Secondary Group. Hence it may use the 'default' queue
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadIndexedLogsMeta(org.apache.hadoop.fs.Path,long,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Some debug message
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:sendOOBToPeers():[WARN] Got error when sending OOB message.
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] allocate(arrayLength): count=count, return byte[array.length]
org.apache.hadoop.hdfs.server.datanode.DataNode:notifyNamenodeReceivingBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[ERROR] Cannot find BPOfferService for reporting block receiving for bpid={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[ERROR] Cannot finish application from non-leaf queue:
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[INFO] Cleaning resources
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:getParentRuleElement(org.w3c.dom.Node):[WARN] Rule '{}' has multiple parent rules defined, only the last parent rule will be used
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:cacheBlock(java.lang.String,long):[WARN] Failed to cache block with id + blockId + , pool + bpid + : replica is not finalized; it is in state + info.getState()
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer):[WARN] Checksum error in block + block + from + inAddr + , + specificOffset, ce
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] Starting ApplicationMaster
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] movedContainer queueMoveOut= usedCapacity= absoluteUsedCapacity= used= cluster=
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:computeQueueManagementChanges(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue):[DEBUG] Skipping queue management updates for parent queue {} since configuration for auto creating queues beyond parent's guaranteed capacity is disabled
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:close():[DEBUG] ByteBufferInputStream.close() for {}
org.apache.hadoop.hdfs.util.StripedBlockUtil:getNextCompletedStripedRead(java.util.concurrent.CompletionService,java.util.Map,long):[DEBUG] Exception during striped read task
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentInstanceEventHandler:handle(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[ERROR] instance.getCompInstanceId() + : Error in handling event type + event.getType(), t
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo:getCmdLine(java.lang.String):[WARN] The process vanished in the interim!
org.apache.hadoop.yarn.server.webapp.LogServlet:getLogFile(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String,boolean):[WARN] Could not obtain node HTTP address from provider.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:startTimelineReaderWebApp():[INFO] Instantiating TimelineReaderWebApp at bindAddress
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getSASTokenProvider():[TRACE] Initializing {sasTokenProviderClass.getName()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:getHdfsImageToHashReader():[DEBUG] Did not load hdfs image to hash file, file is null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean):[INFO] updatePipeline( + oldBlock.getLocalBlock() + => + newBlock.getLocalBlock() + ) success
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerLaunched(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] storeContainerLaunched: containerId={}
org.apache.hadoop.hdfs.server.balancer.Balancer:getLongBytes(org.apache.hadoop.conf.Configuration,java.lang.String,long):[INFO] key = v (default=defaultValue)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:createApplicationHistoryStore(org.apache.hadoop.conf.Configuration):[ERROR] Could not instantiate ApplicationHistoryWriter: [storeClass name]
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handle(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[ERROR] : Invalid event {eventType} at {oldState}, {exception}
org.apache.hadoop.mapreduce.JobResourceUploader:disableErasureCodingForPath(org.apache.hadoop.fs.Path):[DEBUG] Ignore disabling erasure coding for path {path} because method disableErasureCodingForPath doesn't exist, probably talking to a lower version HDFS.
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readInternal(long,byte[],int,int,boolean):[DEBUG] read ahead disabled, reading remote
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:recoverTrackerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTracker,org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$LocalResourceTrackerState):[INFO] Deleting in-progress localization for {} at {}
org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String):[INFO] found resource name at url
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Reserved VirtualCores: countHere
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown in handler for event [ + containerEvent.getType() + ] for Container + containerId
org.apache.hadoop.lib.server.Server:init():[INFO] Temp dir: {}
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] copying {sourceCurrStatus} {tmpTarget}
org.apache.hadoop.ha.HealthMonitor:doHealthChecks():[WARN] Service health check failed for {}, targetToMonitor, t
org.apache.hadoop.yarn.client.api.AppAdminClient:actionStop(java.lang.String):[INFO] Service client action stop initiated
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[ERROR] Failed to read the application [appid].
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] KMS unauthenticated access audited
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementDispatcher:init(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithm,int):[INFO] Initializing Constraint Placement Planner:
org.apache.hadoop.hdfs.server.namenode.BackupImage:journal(long,int,byte[]):[TRACE] Got journal, state = {bnState}; firstTxId = {firstTxId}; numTxns = {numTxns}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:allocate(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Try allocate on node
org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String):[DEBUG] Running cmd: {cmd}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS ACCESS fileHandle: {handle.dumpFileHandle()} client: {remoteAddress}
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:createCuratorClient(java.util.Properties):[INFO] Connecting to ZooKeeper without authentication
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initRecordStorage(java.lang.String,java.lang.Class):[ERROR] Cannot create data directory {}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$SetupFailedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Handling new committer job abort event
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:destroy():[INFO] {CompInstanceId} no container is assigned when destroying
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:getNetworkTaggingHandler(org.apache.hadoop.conf.Configuration):[INFO] Creating new network-tagging-handler.
org.apache.hadoop.hdfs.server.namenode.NameNode:reconfHeartbeatRecheckInterval(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed heartbeatRecheckInterval to ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithFailureToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[ERROR] Container metrics failed container
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$CompactionTimerTask:run():[INFO] Full compaction cycle completed in + duration + msec
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:shouldDefer():[INFO] Replica {} still can't be uncached because some clients continue to use it. Will wait for {}
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File[],boolean):[ERROR] Edits file + f + has improperly formatted + transaction ID
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:free():[TRACE] this + ": freed"
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:freeHost(org.apache.hadoop.mapreduce.task.reduce.MapHost):[INFO] host freed by Thread.currentThread().getName() in (Time.monotonicNow() - SHUFFLE_START.get()) ms
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:writeUnlock(java.lang.String,boolean):[INFO] Number of suppressed write-lock reports: {logAction.getCount() - 1} Longest write-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()} Total suppressed write-lock held time: {logAction.getStats(0).getSum() - lockHeldInfo.getIntervalMs()}
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameOperation:removeSrc4OldRename():[WARN] DIR* FSDirRenameOp.unprotectedRenameTo: failed to rename srcIIP.getPath() to dstIIP.getPath() because the source can not be removed
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:silentlyStopSchedulingMonitor(java.lang.String):[INFO] Sucessfully stopped monitor= + mon.getName()
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForNameserviceId(java.lang.String):[ERROR] Cannot locate eligible NNs for {}
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:mark(int):[DEBUG] mark at {}, position()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:checkAccessForGenericEntities(java.util.Set,org.apache.hadoop.security.UserGroupInformation,java.lang.String):[DEBUG] Display entity per user filter enabled
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using ResourceCalculatorPlugin : + this.resourceCalculatorPlugin
org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread:run():[INFO] START REPLAY @ [initTime]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[DEBUG] Getting datanode storage report
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:resumeContainer():[INFO] Resuming the container [containerIdStr]
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[TRACE] Basic auth starting
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:getProxyInternal(boolean):[INFO] Connecting to ... subClusterId ... with protocol ... without a proxy user
org.apache.hadoop.hdfs.server.datanode.DataNode:startPlugins(org.apache.hadoop.conf.Configuration):[INFO] Started plug-in {}
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getHistoryResourceSkyline(java.lang.String,java.lang.String):[DEBUG] Query the skyline store for recurrenceId: {}. + recurrenceId
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersions(java.lang.String):[TRACE] Exiting getKeyVersions method.
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCachedBlockMap():[DEBUG] Block {}: cannot be found in block manager and hence skipped from calculation for node {}.
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logDisableErasureCodingPolicy(java.lang.String,boolean):[DEBUG] logRpcIds operation
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationStartData):[INFO] Start information of application X is written
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAll():[INFO] SuperUserGroups configuration refreshed
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:postWrite(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerContext):[INFO] PostWrite processing with TFileController
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:readFile(java.lang.String,java.lang.String):[INFO] Reading hosts file into set for type and filename
org.apache.hadoop.hdfs.server.namenode.NNStorage:newBlockPoolID():[WARN] Could not find ip address of "default" inteface.
org.apache.hadoop.yarn.service.component.Component:handle(org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [COMPONENT {}] Transitioned from {} to {} on {} event., componentSpec.getName(), oldState, getState(), event.getType()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer):[DEBUG] Failed to accept this proposal because it tries to release an outdated reserved container
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$ResilientCommitByRenameImpl:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] renameFileWithEtag source: {} dest: {} etag {}
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume:setUsed(long):[WARN] Volume usage (%d) is greater than capacity (%d). Setting volume usage to the capacity
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String,org.apache.hadoop.fs.azure.SelfRenewingLease):[ERROR] Unable to free lease on key
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:collectBlocksSummary(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[WARN] Fsck: Block manager is able to process only + processedBlocks + mis-replicated blocks (Total count : + misReplicatedBlocks.size() + ) for path + path
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:executeRenamingOperation(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE):[INFO] getName(): operation raised an exception: exceptionMessage
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Expecting ... records each with a length of ... bytes in the split with an effective size of ... bytes
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:getFullJob(org.apache.hadoop.mapreduce.v2.api.records.JobId):[ERROR] e.getCause().getMessage()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:reportNodeUnusable(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.api.records.NodeState):[DEBUG] Handling NodeRemovedSchedulerEvent
org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB:init(org.apache.hadoop.conf.Configuration):[WARN] Exception in connecting to InMemoryAliasMap at {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:computeQueueManagementChanges():[DEBUG] Activated leaf queues : [...]
org.apache.hadoop.fs.azure.SimpleKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Failure to retrieve storage account key for accountName, e
org.apache.hadoop.crypto.key.kms.server.KMSACLs:hasAccess(org.apache.hadoop.crypto.key.kms.server.KMSACLs$Type,org.apache.hadoop.security.UserGroupInformation):[DEBUG] User: [{}], Type: {} Result: {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post response created for UNSETSTORAGEPOLICY
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf):[INFO] Initializing cache loader: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader
org.apache.hadoop.mapred.ClientServiceDelegate:getProxy():[INFO] Could not get Job info from RM for job
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:waitForProcess(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long):[TRACE] got a relevant read buffer for file {} offset {} buffer idx {}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[WARN] {}: unknown response code {} while attempting to set up short-circuit access. {}. Short-circuit read for DataNode {} is {} based on {}., this, resp.getStatus(), resp.getMessage(), datanode, disableMsg, DFS_DOMAIN_SOCKET_DISABLE_INTERVAL_SECOND_KEY
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue):[INFO] Initialized queue management policy for parent queue <dynamic_value> with leaf queue template capacities: [<dynamic_value>]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing + event.getTaskAttemptID() + of type + event.getType()
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int):[INFO] state = {}
org.apache.hadoop.fs.s3a.auth.STSClientFactory:builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider,java.lang.String,java.lang.String):[DEBUG] STS Endpoint={}; region='{}'
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:execute(org.apache.commons.cli.CommandLine):[DEBUG] Using default data node port : {nodeAddress}
org.apache.hadoop.fs.http.server.HttpFSExceptionProvider:log(javax.ws.rs.core.Response$Status,java.lang.Throwable):[WARN] [{}:{}] response [{}] {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Setting file size is not supported when mkdir: fileName in dirHandle dirHandle
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer,org.apache.hadoop.hdfs.server.federation.store.StateStoreService):[INFO] Registered FederationRPCMBean: {}
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceStop():[DEBUG] Closing Namenode metrics
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addSnapshot(int,java.lang.String,org.apache.hadoop.hdfs.server.namenode.LeaseManager,boolean,int,long):[INFO] Adding snapshot
org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int):[DEBUG] Response={}({}), resetting authToken, conn.getResponseCode(), conn.getResponseMessage()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* removeStoredBlock: {} removed from caching related lists on node {}
org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto):[DEBUG] SASL server context established. Negotiated QoP is...
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set maxRelativeExpiry to {info.getMaxRelativeExpiryMs()}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handle(org.apache.hadoop.yarn.event.Event):[ERROR] [CompInstanceId]: Invalid event [eventType] at [oldState] [Exception]
org.apache.hadoop.hdfs.DataStreamer:transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.security.token.Token):[DEBUG] IOUtils stream closed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Unknown application + applicationAttemptId + has completed!
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] FsPathBooleanRunner is running
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Writing with NoOpTimelineWriterImpl
org.apache.hadoop.hdfs.DataStreamer:transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.security.token.Token):[DEBUG] StreamerStreams initialized
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$RenewalTimerTask:run():[ERROR] Exception renewing token + token + . Not rescheduled
org.apache.hadoop.util.Shell:runCommand():[WARN] Error reading the error stream
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long):[INFO] Setting quota for mount table
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long):[DEBUG] Storing token
org.apache.hadoop.tools.rumen.Folder:run():[ERROR] No more jobs to process in the trace with 'starts-after' set to ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for dir fileId: {}
org.apache.hadoop.tools.DistCh:setup(java.util.List,org.apache.hadoop.fs.Path):[INFO] JOB_DIR_LABEL=/jobdir_path
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:start():[INFO] Storage policy satisfier is disabled
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs):[INFO] BLOCK* processReport: logged info for {} of {} reported.
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] Output Path is null in abortTask()
org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int):[INFO] Detected a loopback TCP socket, disconnecting it
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[ERROR] Interrupted while waiting for application {applicationId} to be killed.
org.apache.hadoop.yarn.webapp.Router:lookupRoute(org.apache.hadoop.yarn.webapp.WebApp$HTTP,java.lang.String):[DEBUG] exact match for {}: {}
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:main(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] Must specify exactly one token file
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[DEBUG] Adding trackID:{} for the file id:{} back to retry queue as none of the blocks found its eligible targets.
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:handleWritingApplicationHistoryEvent(org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationHistoryEvent):[INFO] Stored the finish data of application attempt [applicationAttemptId]
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchServices(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.service.client.ServiceClient,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Service {} submitted with Application ID: {}
org.apache.hadoop.yarn.service.ServiceManager:finalizeUpgrade(boolean):[ERROR] Upgrade did not complete because unable to re-write the service definition
org.apache.hadoop.fs.http.server.HttpFSServer:put(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [{}] permission [{}] override [{}] replication [{}] blockSize [{}] unmaskedpermission [{}]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startRollingUpgradeInternalForNonHA(long):[INFO] Successfully saved namespace for preparing rolling upgrade.
org.apache.hadoop.mapred.TaskLog:getLogFileDetail(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskLog$LogName,boolean):[DEBUG] Retrieved log file details for task
org.apache.hadoop.mapred.BackupStore$BackupRamManager:unreserve(int):[DEBUG] Unreserving: {requestedSize}. Available: {availableSize}
org.apache.hadoop.yarn.service.monitor.ServiceMonitor$ReadinessChecker:run():[INFO] [COMPONENT {}]: Dependencies satisfied, ramping up.
org.apache.hadoop.ipc.Client$Connection:run():[DEBUG] : starting, having connections
org.apache.hadoop.hdfs.server.namenode.NameNode:join():[INFO] Caught interrupted exception
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[DEBUG] Attempting to create application info for the report
org.apache.hadoop.hdfs.server.diskbalancer.connectors.JsonNodeConnector:getNodes():[INFO] Found %d node(s)
org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer:instantiateHistoryProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress):[DEBUG] Connecting to MRHistoryServer at: + hsAddress
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:cacheReport():[DEBUG] Sending cacheReport from service actor: this
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:processDoneFiles(org.apache.hadoop.mapreduce.v2.api.records.JobId):[WARN] No file for job-history with jobId found in cache!
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RollbackContainerTransition:createReInitContext(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[WARN] Container [ + container.getContainerId() + ] + about to be explicitly Rolledback !!
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] Node path detail
org.apache.hadoop.yarn.service.utils.HttpUtil:generateToken(java.lang.String):[DEBUG] Got valid challenge for host {}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempts(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Completed reading history information of all application attempts of application appId
org.apache.hadoop.hdfs.server.datanode.DataNode:checkDiskErrorAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi):[WARN] checkDiskErrorAsync callback got {} failed volumes: {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:startReconfiguration():[DEBUG] startNamenodeReconfiguration
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:reconstruct():[INFO] Decoding to reconstruct targets...
org.apache.hadoop.fs.s3a.S3AUtils:propagateBucketOptions(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Propagating entries under {}
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread:run():[DEBUG] Interrupted while waiting for queue
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:setMetrics(org.apache.hadoop.conf.Configuration):[INFO] Initializing HttpFSServerMetrics
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementDispatcher:collect(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithmOutput):[WARN] Planning Algorithm has rejected for application [{}] the following [{}]
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:isResourceCalculatorAvailable():[INFO] ResourceCalculatorProcessTree is unavailable on this system. {} is disabled.
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateSignature(com.nimbusds.jwt.SignedJWT):[DEBUG] JWT token is in a SIGNED state
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:parseDNFromHostsEntry(java.lang.String):[WARN] Invalid hostname + hostStr + in hosts file
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:call():[INFO] Failed to re-encrypting one batch of {} edeks from KMS, time consumed: {}, start: {}., result, batch.size(), kmsSW.stop(), batch.getFirstFilePath()
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystemUtil:toInMemoryAllocation(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId,org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator):[INFO] Allocations created
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateComponent(org.apache.hadoop.yarn.service.api.records.Component,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[INFO] Validating service resource
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Container):[INFO] Making reservation: node= + node.getNodeName() + app_id= + getApplicationId()
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:updateStateStore():[ERROR] Cannot heartbeat router {}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:nextInternal(java.util.List,org.apache.hadoop.hbase.regionserver.ScannerContext):[DEBUG] emitted no cells for + this.action
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:authenticateWithTlsExtension(java.lang.String,java.lang.String):[DEBUG] Authentication successful for {}
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:applicationMasterFinished(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Application finished, removing password for appAttemptId
org.apache.hadoop.yarn.server.webproxy.ProxyCA:init(java.security.cert.X509Certificate,java.security.PrivateKey):[WARN] Could not verify Certificate, Public Key, and Private Key: regenerating
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Updating RMDelegationToken and SequenceNumber
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:initializePipeline(java.lang.String):[INFO] Request to start an already existing user: {} was received, so ignoring.
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean):[DEBUG] Deleting entity type:{} id:{} primary filter entry {} {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:cleanupExpiredQueues(java.lang.String,boolean,java.util.Set,java.lang.String):[WARN] Exception while trying to expire reservation: {}, {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateFromReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Trying to fulfill reservation for application {} on node: {}, reservedApplication.getApplicationId(), node.getNodeID()
org.apache.hadoop.yarn.csi.client.CsiClient:nodePublishVolume(csi.v0.Csi$NodePublishVolumeRequest):[ERROR] IOException encountered during node publish volume
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:initSecurity():[DEBUG] Auth is SASL user="..." JAAS context="..."
org.apache.hadoop.hdfs.DeadNodeDetector:run():[DEBUG] Current detector state {}, the detected nodes: {}.
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:checkAcls(java.lang.String):[WARN] Couldn't get current user
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int,int):[INFO] Exits the main loop.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation,java.io.File,java.util.Map,boolean,boolean):[WARN] Privileged operation may have issues
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] RPC initialized
org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp:addSymlink(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean):[INFO] addSymlink: failed to add [path]
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint():[WARN] Couldn't delete checkpoint: + dir + Ignoring.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[DEBUG] Log audit event: failed operation addCachePool
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:checkAccessForGenericEntities(java.util.Set,org.apache.hadoop.security.UserGroupInformation,java.lang.String):[INFO] User access checked
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Queuing scan of directory {}
org.apache.hadoop.mapred.YARNRunner:setTokenRenewerConf(org.apache.hadoop.yarn.api.records.ContainerLaunchContext,org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] key ===> value
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] DNS instance created
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:buildLocation(java.lang.String,org.apache.hadoop.hdfs.server.federation.store.records.MountTable):[ERROR] Cannot build location, {path} not a child of {srcPath}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet):[INFO] Invoking method concurrently
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateStarvedApps():[INFO] Subtracting minshare starvation subsumed by fairshare starvation
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[INFO] Initialized container resources
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:run():[DEBUG] Sending Heartbeat to RM. AskList:{} [LOG] LOG.DEBUG: Sending Heartbeat to RM. AskList:{},
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path):[INFO] Moved: '...' to trash at: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption):[WARN] Update resource on node: + node.getNodeName() + with the same resource: + newResource
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Any job larger than [oversizedJob.getMaxTasksAllowed] will not be loaded.
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy,long,long):[DEBUG] Creating RMProxy instance
org.apache.hadoop.mapred.ReduceTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[INFO] Using ShuffleConsumerPlugin: ...
org.apache.hadoop.hdfs.server.datanode.DataXceiver:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[INFO] "Client {} did not send a valid status code after reading. Will close connection.", peer.getRemoteAddressString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute):[DEBUG] skip this node:{} for requestAttribute:{}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Checkpoint renamed from IMAGE_ROLLBACK to IMAGE
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:beginPhase(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase):[DEBUG] Beginning of the phase: {phase}
org.apache.hadoop.hdfs.server.namenode.NameNode:copyEditLogSegmentsToSharedDir(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.util.Collection,org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.conf.Configuration):[DEBUG] ending log segment because of END_LOG_SEGMENT op in {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:getRecoveredMappableBlock(java.io.File,java.lang.String,byte):[INFO] Recovering persistent memory cache for block {}, path = {}, length = {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:render():[DEBUG] Unexpected state filter for inactive RM node
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:warnIfNotExists(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[ERROR] AM is trying to {} a container {} that does not exist. Might happen shortly after NM restart when NM recovery is enabled
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue():[INFO] Key deserialized
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:matchEditLogs(java.io.File):[ERROR] In-progress edits file + f + has improperly formatted transaction ID
org.apache.hadoop.fs.FSInputChecker:read(long,byte[],int,int):[DEBUG] Read loop exited after final operation attempt
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:render():[DEBUG] HtmlBlock.render called
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext,java.lang.String,java.lang.String):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:assignContainersToChildQueues(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Assigned to queue: {queuePath} stats: {childQueueStats} --> {resource}, {type}
org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int):[DEBUG] nthValidToReturn is {}
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[WARN] The value of DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY is less than 0.5 so datanodes with more used percent will receive more block allocations.
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:refresh():[INFO] Following commands registered for host[{}] : {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error storing info for AMRMTokenSecretManager: [e]
org.apache.hadoop.fs.s3a.S3AUtils:getS3EncryptionKey(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Cannot retrieve S3_ENCRYPTION_KEY for bucket {bucket}
org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration):[DEBUG] Configured with port to QOP mapping as: + portPropMapping
org.apache.hadoop.hdfs.server.common.Storage:checkVersionUpgradable(int):[ERROR] *********** Upgrade is not supported from this older version {oldVersion} of storage to the current version. Please upgrade to {LAST_UPGRADABLE_HADOOP_VERSION} or a later version and then upgrade to current version. Old layout version is {oldVersion} and latest layout version this software version can upgrade from is {LAST_UPGRADABLE_LAYOUT_VERSION}. ************
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:publishVolumes():[INFO] publishing volumes
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:computePartialChunkCrc(long,long):[DEBUG] computePartialChunkCrc for block: sizePartialChunk=sizePartialChunk, block offset=blkoff, metafile offset=ckoff
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:readStats():[WARN] Failed to get tc stats
org.apache.hadoop.hdfs.server.namenode.FSImage:doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Can perform rollback for sd
org.apache.hadoop.hdfs.server.datanode.DataStorage:recoverTransitionRead(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption):[ERROR] All specified directories have failed to load.
org.apache.hadoop.tools.dynamometer.Client:run():[INFO] Queue info: queueName={}, queueCurrentCapacity={}, queueMaxCapacity={}, queueApplicationCount={}, queueChildQueueCount={}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:updateMetricsForDeactivatedNode(org.apache.hadoop.yarn.api.records.NodeState,org.apache.hadoop.yarn.api.records.NodeState):[WARN] Unexpected initial state
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger$ArgsBuilder):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.hdfs.server.federation.router.DFSRouter:main(java.lang.String[]):[INFO] Starting up router
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue):[INFO] Queue + q.getName() + not equal to + newq.getName()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:finishResourceLocalization(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto):[DEBUG] Storing localized resource to {}
org.apache.hadoop.yarn.service.client.ServiceClient:getStatus(java.lang.String):[WARN] application ID {} is reported as null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:addVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[INFO] Added new volume: volume.getStorageID()
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean):[DEBUG] Time taken to list {} blobs for delete operation: {} ms
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureJobJar(org.apache.hadoop.conf.Configuration,java.util.Map):[INFO] Job jar is not present. Not adding any jar to the list of resources.
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration):[ERROR] No shared edits directory configured for namespace + nsId + namenode + namenodeId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:createSchedContainerChangeRequests(java.util.List,boolean):[WARN] Error happens when checking increase request, Ignoring.. exception=
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getFileStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] getFileStatus filesystem: {} path: {} isNamespaceEnabled: {}
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentEventHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] MessageFormat.format([COMPONENT {0}]: Error in handling event type {1}, component.getName(), event.getType()), t
org.apache.hadoop.fs.cosn.CosNFileReadTask:run():[ERROR] Exception occurs when retry[{retryPolicy}] to retrieve the block range start: {start}, end:{end}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas):[DEBUG] BLOCK* invalidateBlocks: postponing invalidation of {b} on {dn} because {nr.replicasOnStaleNodes()} replica(s) are located on nodes with potentially out-of-date block reports
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:verifyIfValidPmemVolume(java.io.File):[WARN] Failed to delete test file {testFilePath} from persistent memory
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Node heartbeat + nm.getNodeID() + available resource = + node.getUnallocatedResource()
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:setUsername(com.zaxxer.hikari.HikariDataSource,java.lang.String):[DEBUG] NULL Username specified for Store connection, so ignoring
org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction):[INFO] getLoginUser
org.apache.hadoop.hdfs.server.datanode.DataStorage:removeVolumes(java.util.Collection):[WARN] I/O error attempting to unlock storage directory {}.
org.apache.hadoop.yarn.client.api.async.NMClientAsync$CallbackHandler:onStopContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Exception raised while stopping container
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean):[TRACE] Receipt verification is not enabled on the DataNode. Not verifying slotId
org.apache.hadoop.fs.s3a.S3AInputStream:resetConnection():[INFO] Forcing reset of connection to {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Can't get path for fileId: {handle.getFileId()}
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:doGetUri(java.net.URI,java.lang.String,javax.ws.rs.core.MultivaluedMap):[ERROR] Response from the timeline reader server is ...
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handleComponentInstanceRelaunch(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent,boolean,java.lang.String):[INFO] Publishing component instance status {} {}
org.apache.hadoop.yarn.util.resource.ResourceUtils:addResourcesFileToConf(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Found {}, adding to configuration
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:processProvidedStorageReport():[INFO] Calling process first blk report from storage: {providedStorageInfo}
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:startUpload():[DEBUG] Start datablock[index] upload
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:loadPlugIns(org.apache.hadoop.conf.Configuration):[DEBUG] Trying to load plugin class {}
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:tryLock():[ERROR] It appears that another node {} has already locked the storage directory: {}
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using match all for 'host' and READ_WRITE
org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String):[INFO] Validating paths for files
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.protocol.Block):[INFO] Quota verified for truncate
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger):[WARN] failed to register any UNIX signal loggers: , t
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:close():[WARN] Closing is Interrupted
org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData):[DEBUG] Error changing permissions of {item}, {e}
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Render counters for task
org.apache.hadoop.hdfs.DFSClient:removeNodeFromDeadNodeDetector(org.apache.hadoop.hdfs.DFSInputStream,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[DEBUG] DeadNode detection is not enabled or given block {} is null, skip to remove node.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:prepareForLaunch(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[INFO] Container + containerId + not launched as it has already + been marked for Killing
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] {} Service RPC address: {}
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[INFO] Killed infrastructure app
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[INFO] Application {} user {} mapping [{}] to [{}] override {}, applicationId, user, queueName, mappedQueue.getQueue(), overrideWithQueueMappings
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveCurrentTokens(java.io.DataOutputStream,java.lang.String):[INFO] Step DELEGATION_TOKENS started
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:disableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.DisableNameserviceRequest):[ERROR] Cannot disable {}, it does not exists, nsId
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor):[INFO] Removing CPU constraints for YARN containers.
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager$ConnectionCreator:run():[ERROR] The connection creator was interrupted
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[WARN] Proxy user Authentication exception
org.apache.hadoop.hdfs.server.datanode.BlockScanner:markSuspectBlock(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] Not scanning suspicious block {block} on {storageId}, because the block scanner is disabled.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] All checks pass. Launching privileged container for : [containerId]
org.apache.hadoop.hdfs.server.namenode.NameNode:initialize(org.apache.hadoop.conf.Configuration):[INFO] Clients are to use [nameNodeAddress] to access this namenode/service.
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.lang.String):[INFO] Got an error when resolve hostNames. Falling back to default-rack for all.
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap):[TRACE] Chosen nodes: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:deleteCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String):[DEBUG] deleteCGroup: {}, cGroupPath
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] doSecureLogin starts
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal():[WARN] The created archive " + harName + " is missing or empty.
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenSingleCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String,boolean):[DEBUG] Requesting an OAuth token by {httpMethod} to {authEndpoint}
org.apache.hadoop.hdfs.server.namenode.GlobalStateIdContext:receiveRequestState(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,long):[TRACE] Client State ID= {} and Server State ID= {}
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:attempts():[ERROR] Failed to render attempts page with task type : + $(TASK_TYPE) + for job id : + $(JOB_ID)
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplitRatio(int,int,org.apache.hadoop.conf.Configuration):[WARN] nMaps == 1. Why use DynamicInputFormat?
org.apache.hadoop.hdfs.DFSOutputStream:flushInternalWithoutWaitingAck():[DEBUG] Last queued seqno retrieved
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render():[INFO] The job has a total of {taskCount} tasks. Any job larger than {oversizedJob.getMaxTasksAllowed()} will not be loaded.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:getComputedResourceLimitForAllUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] userLimit is fetched. userLimit={}, userSpecificUserLimit={}, schedulingMode={}, partition={}
org.apache.hadoop.hdfs.server.namenode.FSDirectory:normalizePaths(java.util.Collection,java.lang.String):[ERROR] {} ignoring relative path {}
org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation):[INFO] op.getDebugInfo()
org.apache.hadoop.hdfs.DFSInputStream:reportLostBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[WARN] No live nodes contain block
org.apache.hadoop.yarn.applications.distributedshell.Client:sendStopSignal():[INFO] Stopping yarnClient within the DS Client
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:streamCleanup(org.apache.hadoop.nfs.nfs3.FileHandle,long):[DEBUG] stream can be closed for fileId: {}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:createRequestUrl(java.lang.String):[DEBUG] Unexpected error.
org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Loading the mount-table {} into configuration.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStart():[INFO] Scanning active directory {} every {} seconds
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String):[INFO] Processed URL + url + but flowrun not found (Took + (Time.monotonicNow() - startTime) + ms.)
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadBlockPoolSliceStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[INFO] loadBlockPoolSliceStorage: {} upgrade tasks
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.PlanningAlgorithm:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[INFO] Updated reservation
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptKilledTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[DEBUG] Not generating HistoryFinish event since start event not generated for task: task.getID()
org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Failed to initialize filesystem
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:format(java.io.File,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Formatting block pool {blockpoolID} directory {bpSdir.getCurrentDir()}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean):[DEBUG] Directory Delete encountered: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:addDependentNodesToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set):[WARN] Not able to find datanode {hostname} which has dependency with datanode {chosenNode.getHostName()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:validateQueueManagementChanges(java.util.List):[ERROR] Queue + getQueuePath() + is not an instance of PlanQueue or ManagedParentQueue. + + Ignoring update + queueManagementChanges
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:render():[INFO] Container list rendered
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:putIfAbsent(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[INFO] the collector for + appId + was added
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:changeModeEvent(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[INFO] Failed to change storage policy satisfier as DFS_STORAGE_POLICY_ENABLED_KEY set to {}
org.apache.hadoop.yarn.service.ServiceMaster:removeHdfsDelegationToken(org.apache.hadoop.security.UserGroupInformation):[ERROR] AM is not holding on a keytab in a secure deployment: service will fail when tokens expire
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:writeTmpConfig(org.apache.hadoop.conf.Configuration):[INFO] write temp capacity configuration fail, schedulerConfigFile= + tempSchedulerConfigPath, e
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer:crcPartialBlock():[INFO] Reading fully from block input stream
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Recovering storage directory {} from previous rollback
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:dispatchBlocks():[TRACE] this + blocksToReceive= + blocksToReceive + , scheduledSize= + getScheduledSize() + , srcBlocks#= + srcBlocks.size()
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:removeAppFromRegistry(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Failed removing registry directory key + key
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when writing command to temp file
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteObjects(com.amazonaws.services.s3.model.DeleteObjectsRequest):[DEBUG] Partial failure of delete, {} errors
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initAppAggregator(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[INFO] Creating application directory
org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAclImpl:check(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity):[INFO] Error when verifying access for user ... on the events of the timeline entity ...
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeDirectorySectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String):[INFO] Loading the INodeDirectory section in parallel with {} sub-sections
org.apache.hadoop.security.JniBasedUnixGroupsMapping:logError(int,java.lang.String):[ERROR] error looking up the name of group ... : ...
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getDelegationToken(org.apache.hadoop.security.UserGroupInformation):[DEBUG] Using UGI token: {}
org.apache.hadoop.util.ShutdownHookManager:executeShutdown():[WARN] ShutdownHook 'SimpleName' failed, ex.toString(), ex
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:updateTimelineCollectorContext(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[INFO] Get timeline collector context for + appId
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeOrUpdateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[INFO] Storing SEQUENCE_NUMBER
org.apache.hadoop.hdfs.DFSStripedOutputStream:logCorruptBlocks():[WARN] Block group <bgIndex> failed to write <corruptBlockCount> blocks.
org.apache.hadoop.hdfs.server.namenode.snapshot.FSImageFormatPBSnapshot$Saver:serializeDirDiffList(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.util.List,java.io.OutputStream):[ERROR] Misordered entries in the 'deleted' difflist of directory 'dirFullPath', INodeId=dir.getId(). The full list is 'deletedArray'
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateStarvedApps():[DEBUG] Fetching apps with pending demand
org.apache.hadoop.mapreduce.util.ProcessTree:sendSignal(java.lang.String,int,java.lang.String):[WARN] Error executing shell command ${ioe}
org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String):[INFO] capacity = 2^{exponent} = {c} entries
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] Duplicate files in input path: , {e}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToActive():[INFO] Transitioning to active state
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap):[INFO] Available space rack fault tolerant block placement policy initialized
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Error when reading history file of container
org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList):[DEBUG] Destination file exists: {dst.stat}
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:processArgs(java.util.List,java.io.PrintStream):[DEBUG] Command: LIST, age %d msec, path %s (prefix \"%s\")
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:pauseContainer():[WARN] Could not store container [container.getContainerId()] state. The Container has been paused.
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath):[WARN] DIR* FSDirectory.unprotectedDelete: failed to remove ${iip.getPath()} because the root is not allowed to be deleted
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg$AppLevelAggregator:aggregate():[DEBUG] App-level collector is empty, skip aggregation.
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Forwarding registration request to the real YARN RM
org.apache.hadoop.contrib.utils.join.DataJoinReducerBase:regroup(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.Reporter):[INFO] key: + key.toString() + this.largestNumOfValues: + this.largestNumOfValues
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[DEBUG] With principals: Arrays.asList(principals)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromHadoopCommon(java.util.function.Function,java.lang.String):[INFO] $HADOOP_COMMON_HOME is not set
org.apache.hadoop.fs.s3a.Invoker:retryUntranslated(java.lang.String,boolean,org.apache.hadoop.fs.s3a.Invoker$Retried,org.apache.hadoop.util.functional.CallableRaisingIOE):[DEBUG] retry #{}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:copyBlocks(org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] Disk Balancer - Unable to find source volume: {}
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[TRACE] Returned false due to null rempteIp
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[ERROR] Event not handled because previousFailedAttempt is null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:parseOutput(java.lang.String):[WARN] Skipping device {device} because it's not healthy
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:validate(java.util.Set):[ERROR] Invalid node attribute(s) from Provider: + e.getMessage()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:assignGpus(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not get valid GPU device for container 'some_container' as some other containers might not releasing GPUs.
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:normalize(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[ERROR] Memory cannot be allocated in increments of zero. Assuming MB increment size. Please ensure the scheduler configuration is correct.
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:requestNewShm(java.lang.String,org.apache.hadoop.hdfs.net.DomainPeer):[WARN] this + : error requesting short-circuit shared memory + access: + error
org.apache.hadoop.security.KDiag:execute():[WARN] The default cluster security is insecure
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:addDigestACL(org.apache.zookeeper.data.ACL):[DEBUG] Added ACL {}, aclToString(acl)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter:handle(com.sun.jersey.api.client.ClientRequest):[ERROR] Jersey retry failed! Message: IOException
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:isPluginDuplicate(java.util.Map,java.lang.String):[WARN] Ignoring duplicate Resource plugin definition: {resourceName}
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getINodeWithLeases(org.apache.hadoop.hdfs.server.namenode.INodeDirectory):[INFO] Took {} ms to collect {} open files with leases {}, (endTimeMs - startTimeMs), iipSet.size(), ((ancestorDir != null) ? " under " + ancestorDir.getFullPathName() : ".")
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:writeAuditLog(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] App succeeded with state: KILLED
org.apache.hadoop.examples.terasort.TeraGen$RangeInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] Generating + totalRows + using + numSplits
org.apache.hadoop.fs.s3a.S3AFileSystem:innerListStatus(org.apache.hadoop.fs.Path):[DEBUG] Adding: rd (not a dir): {path}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[INFO] logAuditEvent - success
org.apache.hadoop.yarn.client.api.async.NMClientAsync$AbstractCallbackHandler:onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus):[DEBUG] Processing status
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Running: countHere
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[INFO] Original source {item}
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:heartbeatCheck():[INFO] New instance created
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:createSnapshot(java.lang.String,java.lang.String):[INFO] Sequential invocation executed
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:updateConfigIfNeeded():[INFO] Capacity Scheduler configuration changed, updated preemption properties to:...
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Created DelegationTokenManager {}
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:checkQueue(java.lang.String):[WARN] Missing Queue. Please try again by specifying a Queue.
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String):[WARN] Got unexpected exception trying to acquire lease on
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Directory + prevDir + does not exist.
org.apache.hadoop.hdfs.qjournal.server.Journal:acceptRecovery(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL):[INFO] Accepted recovery for segment
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] DatanodeCommand action: DNA_ACCESSKEYUPDATE
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:getHdfsImageToHashReader():[DEBUG] Did not load hdfs image to hash file, file is unmodified
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:persistNewBlock(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile):[DEBUG] persistNewBlock: {path} with new block {file.getLastBlock().toString()}, current total block count is {file.getBlocks().length}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStartContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to start DataNode Container {containerId}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unable to remove application + appAttemptRemovedEvent.getApplicationAttemptID(), ie
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID):[INFO] fetcher# + id + - MergeManager returned Status.WAIT ...
org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:nextKeyValue():[DEBUG] taskId + ": Current chunk exhausted. Attempting to pick up new one."
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateAndResolveService(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration):[INFO] Marking {} for removal
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getListing(java.lang.String,byte[],boolean):[INFO] Audit event for listStatus operation allowed on src
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:enterSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.EnterSafeModeRequest):[INFO] STATE* Safe mode is ON.\nIt was turned on manually. Use "hdfs dfsrouteradmin -safemode leave" to turn safe mode off.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:printChildQueues():[DEBUG] printChildQueues - queue: [QueuePath] child-queues: [ChildQueuesToPrint]
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[DEBUG] Outputting container log
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:main(java.lang.String[]):[INFO] Logging initialized
org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver:run():[INFO] Cancelled image saving for + sd.getRoot() + : + snce.getMessage()
org.apache.hadoop.hdfs.server.sps.ExternalSPSFilePathCollector:checkProcessingQueuesFree():[DEBUG] Waiting for storageMovementNeeded queue to be free!
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:loadPolicyConfiguration(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade):[WARN] Failed to get policy from FederationFacade with queue {queue}: {e.getMessage()}
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] Unable to determine the max transaction ID seen by
org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute():[INFO] Rolling key version from KeyProvider: provider
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:updateResourceMappings(java.util.Map,java.util.Map):[INFO] Resource mappings updated
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,java.util.List,boolean):[DEBUG] Aborting %s uploads
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:removeVolumes(java.util.Collection,boolean):[INFO] Checking removing StorageLocation {location} with id {uuid}
org.apache.hadoop.mapred.gridmix.LoadJob$ResourceUsageMatcherRunner:run():[INFO] Exception while running the resource-usage-emulation matcher + thread! Exiting.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:launchContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext):[INFO] Launch container failed:
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:inspectDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[WARN] Found image file at but storage directory is not configured to contain images.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Error in handling event type [event.getType()] for applicationAttempt [appAttemptId] with [previousFailedAttempt]
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run():[WARN] Lost job
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceStop():[DEBUG] Shutting down Router metrics
org.apache.hadoop.hdfs.server.common.Util:stringCollectionAsURIs(java.util.Collection):[ERROR] Error while processing URI: {name}
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[DEBUG] ComponentHealthThresholdMonitor run method
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:selectInputStreams(java.util.Collection,long,boolean,boolean):[DEBUG] this: selecting input streams starting at {fromTxId} {(inProgressOk ? (inProgress ok) : (excluding inProgress) )} from among {elfs.size()} candidate file(s)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[ERROR] Error in handling event type for applicationAttempt
org.apache.hadoop.hdfs.server.namenode.NameNode:doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration):[INFO] starting recovery...
org.apache.hadoop.hdfs.DFSUtilClient:getAddressesForNameserviceId(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String[]):[WARN] Namenode for {} remains unresolved for ID {}. Check your hdfs-site.xml file to ensure namenodes are configured properly.
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[]):[DEBUG] Created keytab: keytabFile
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:serviceStart():[INFO] ContainerManager bound to <!-- initialAddress value -->
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow:getSum(long):[DEBUG] Sum: + sum Bucket: updateTime: timeStr (bucketTime) isStale stale at time
org.apache.hadoop.fs.s3a.s3guard.S3Guard:checkNoS3Guard(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Ignoring S3Guard store option of NULL_METADATA_STORE -no longer needed Origin {origin}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumStaleDataNodes():[DEBUG] Failed to get number of stale nodes
org.apache.hadoop.examples.dancing.DistributedPentomino:createInputDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.examples.dancing.Pentomino,int):[INFO] File length retrieved
org.apache.hadoop.ipc.Server$Connection:authorizeConnection():[INFO] Connection from {this} for protocol {connectionContext.getProtocol()} is unauthorized for user {user}
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSFileStatus operation executed
org.apache.hadoop.hdfs.DFSOutputStream:completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[INFO] Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning false hdfsTimeout 0
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:putTimelineDataInJSONFile(java.lang.String,java.lang.String):[INFO] Timeline entities are successfully put
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String):[INFO] Error report from Unknown DataNode: ...
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:getEntryToEvict():[DEBUG] idlest stream's idle time:
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:disableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.DisableNameserviceRequest):[INFO] Nameservice {} disabled successfully.
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:serviceStop():[ERROR] Failed to kill unmanaged application master
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator:load(java.io.File,boolean):[DEBUG] Loading using Protobuf Loader
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Scan for users on {}
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List):[INFO] Publication of num splits completed
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[INFO] Usage text displayed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:writeTmpConfig(org.apache.hadoop.conf.Configuration):[INFO] write temp configuration to fileSystem took + (Time.monotonicNow() - start) + ms
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:retryableRequest(java.lang.String[],java.lang.String,java.util.List,java.lang.String):[ERROR] Encountered error while making remote call to {urls} retried {retry} time(s).
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Resource Info Retrieved
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo():[TRACE] #{} processRetryInfo: waitTime={}, getCallId(), waitTime
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] No Output found for {attemptId}
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAllContainersLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[DEBUG] readAggregatedLogs
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$Callback):[DEBUG] Cannot schedule check on null volume
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:createMapTasks(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,long,org.apache.hadoop.mapreduce.split.JobSplit$TaskSplitMetaInfo[]):[INFO] Input size for job + job.jobId + = + inputLength + . Number of splits = + splits.length
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression):[INFO] Image file {} of size {} bytes saved in {} seconds {}
org.apache.hadoop.yarn.applications.distributedshell.Client:prepareTimelineDomain():[INFO] Put the timeline domain: TimelineUtils.dumpTimelineRecordtoJSON(domain)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Redirecting with URI
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[INFO] logMsg
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:endPhase(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase):[DEBUG] End of the phase: {}
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long):[INFO] Handling event publish
org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode():[INFO] Recursively deleting + znodeWorkingDir + from ZK...
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStart():[INFO] Scheduled the shared cache cleaner task to run every + periodInMinutes + minutes.
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:checkVersion():[INFO] Storing timeline store version info
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery():[INFO] Not attempting to recover. Intermediate spill encryption is enabled.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:cleanAll():[TRACE] openFileMap size:[Actual size]
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:callCOSClientWithRetry(java.lang.Object):[ERROR] Call cos sdk failed, retryIndex: [max / max], call method: putObject, exception: ...
org.apache.hadoop.ha.ActiveStandbyElector:enterNeutralMode():[DEBUG] Entering neutral mode for {}
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:leaveSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.LeaveSafeModeRequest):[ERROR] Unable to leave safemode.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:schedulePlacedRequests(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Unsuccessful allocation attempt [placementAttempt] for [sReqClone]
org.apache.hadoop.mapred.uploader.FrameworkUploader:addJar(java.io.File):[INFO] Blacklisted /path/to/jar
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] No sync response, expect an async response for request XID={}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=LISTXATTRS target=path
org.apache.hadoop.hdfs.qjournal.server.JournalNode:start():[ERROR] Failed to start JournalNode.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] allocateResource log message if debug enabled
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[INFO] Removing CPU constraints for YARN containers.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getAclStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] getAclStatus filesystem: {} path: {}
org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion):[INFO] EagerKeyGeneratorKeyProviderCryptoExtension Decryption successful
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map):[INFO] MRConfig.LOCAL_DIR for uber task
org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[WARN] Exception while trying to get password for alias {}:
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger:merge(java.util.List):[INFO] {reduceId} Finished merging {inputs.size()} map output files on disk of total-size {approxOutputSize}. Local output file is {outputPath} of size {localFS.getFileStatus(outputPath).getLen()}
org.apache.hadoop.yarn.service.webapp.ApiServer:flexService(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {appName} is successfully flexed.
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Temporary redirect with URI
org.apache.hadoop.yarn.util.Times:elapsed(long,long):[WARN] Current time + current + is ahead of started time + started
org.apache.hadoop.yarn.sls.SLSRunner:waitForNodesRunning():[INFO] SLSRunner takes {} ms to launch all nodes.
org.apache.hadoop.hdfs.qjournal.server.Journal:finalizeLogSegment(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long):[INFO] Validating log segment {elf.getFile()} about to be finalized ; journal id: {journalId}
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:doChooseVolume(java.util.List,long,java.lang.String):[DEBUG] Volumes are imbalanced. Selecting volume from low available space volumes for write of block size replicaSize
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:close():[ERROR] Interrupted freeing leases
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:serviceStop():[WARN] Unable to stop service {service.getName()}
org.apache.hadoop.yarn.service.client.ServiceClient:actionDestroy(java.lang.String):[INFO] Successfully deleted service dir for + serviceName + : + appDir
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:stopActiveServices():[INFO] Stopping services started for active state
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:containerFailedOnHost(java.lang.String):[DEBUG] Host <hostName> is already blacklisted.
org.apache.hadoop.yarn.applications.distributedshell.Client:sendStopSignal():[INFO] Sending stop Signal to Client
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:run():[INFO] canceling the task attempt
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.files.SuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] Report already exists: {st}
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceStop():[INFO] Initiating final putEntities, remaining entities left in entityQueue: {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:completeActiveCopies(java.lang.String):[DEBUG] Waiting for {activeCopies.size()} active copies to complete: {reason}
org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path):[DEBUG] running merge pass
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleLaunchForLaunchType(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext,org.apache.hadoop.yarn.api.ApplicationConstants$ContainerLaunchType):[INFO] Container was marked as inactive. Returning terminated error
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[ERROR] Error storing AMRMProxy application context entry for {this.attemptId}
org.apache.hadoop.tools.mapred.CopyCommitter:preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration):[INFO] About to preserve attributes: + attrSymbols
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getAllSlowDataNodes():[DEBUG] {} is disabled. Try enabling it first to capture slow peer outliers.
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:checkServiceDependencySatisified(org.apache.hadoop.yarn.service.api.records.Service):[INFO] Waiting for service dependencies.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] User {user} does not have permission to submit {applicationId} to queue {queueName}.
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] {} Web address: {}
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[WARN] [STRESS] Check failed!, ioe
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:addDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] DatanodeManager.addDatanode: node {node} is added to datanodeMap.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$NMContainerPolicyUtils:appendSecurityFlags(java.util.List,java.util.Map,java.nio.file.Path,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$SandboxMode):[WARN] The container will run without the java security manager due to an unsupported container command. The command will be permitted to run in Sandbox permissive mode: command
org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor:run():[ERROR] Returning, interrupted :
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean):[DEBUG] Deleting entity type:{} id:{} from invisible reverse related entity entry of type:{} id:{}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:createNewApplication():[ERROR] Unable to create new app from RM web service
org.apache.hadoop.security.KDiag:run(java.lang.String[]):[INFO] Loading resource {resource_value}
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:registerSlot(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,boolean):[TRACE] this: registered blockId with slot slotId (isCached=false)
org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec:parse(java.lang.String):[INFO] Parsed constraint: {}
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] ChRootedFs listStatus call invoked
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[DEBUG] Log types added
org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String):[INFO] Setting the includes file to [includesFile]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfigValidator:validateQueueHierarchy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] Converting the leaf queue
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:getPluginsFromConfig(org.apache.hadoop.conf.Configuration):[INFO] Found Resource plugins from configuration: [plugin1, plugin2]
org.apache.hadoop.yarn.webapp.Router:load(java.lang.Class,java.lang.String):[DEBUG] trying: {}
org.apache.hadoop.hdfs.DFSUtil:getJournalNodeAddresses(org.apache.hadoop.conf.Configuration):[ERROR] The conf property DFS_NAMENODE_SHARED_EDITS_DIR_KEY is not set properly with correct journal node uri
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set):[DEBUG] getEntityTimelines type={} ids={}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Not a symlink, fileId: {handle.getFileId()}
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue:logErrorMessage(java.io.File,java.lang.Exception):[ERROR] Error aggregating log file. Log file : someFilePath. Some exception message.
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID):[INFO] localfetcher# + id + about to shuffle output
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:initializeClient(java.net.URI,java.lang.String,java.lang.String,boolean):[TRACE] Fetching SharedKey credentials
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:run():[ERROR] Exception during DirectoryScanner execution - will continue next cycle
org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int):[DEBUG] Chosen node {} from first random, ret
org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean):[INFO] {message}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest):[INFO] Reservation creation failed
org.apache.hadoop.lib.server.Server:init():[DEBUG] Loading services
org.apache.hadoop.yarn.service.client.ServiceClient:addCredentials(org.apache.hadoop.yarn.api.records.ContainerLaunchContext,org.apache.hadoop.yarn.service.api.records.Service):[DEBUG] Got DT: {}
org.apache.hadoop.tools.mapred.CopyCommitter:cleanup(org.apache.hadoop.conf.Configuration):[ERROR] Exception encountered , ignore
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:evictOldDBs():[WARN] Failed to evict old db {path}
org.apache.hadoop.yarn.service.webapp.ApiServer:deleteService(javax.servlet.http.HttpServletRequest,java.lang.String):[ERROR] Fail to stop service: {}
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:handle(org.apache.hadoop.oncrpc.RpcDeniedReply):[INFO] Portmap mapping registration succeeded
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[ERROR] Cannot warm up EDEKs.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest):[INFO] Reservation created successfully
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports):[DEBUG] Pending replication tasks: X erasure-coded tasks: Y
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Rebooted Nodes: countHere
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator:getCSAssignmentFromAllocateResult(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Reserved container application={application.getApplicationId()} resource={allocatedResource} queue={appInfo.getQueueName()} cluster={clusterResource}
org.apache.hadoop.lib.server.Server:loadServices():[DEBUG] Replacing service [serviceInterface] implementation [serviceImplementation]
org.apache.hadoop.fs.FileSystem:loadFileSystems():[WARN] Cannot load filesystem:
org.apache.hadoop.hdfs.util.MD5FileUtils:readStoredMd5(java.io.File):[ERROR] Error reading md5 file at {md5File}
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext):[INFO] {} was deactivated
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:processSpeculatorEvent(org.apache.hadoop.mapreduce.v2.app.speculate.SpeculatorEvent):[INFO] ATTEMPT_START + event.getTaskID()
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:getApplicationAttemptEntities(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.util.Map,long,java.lang.String):[INFO] Fields parameter adjusted
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getRouters():[ERROR] Cannot get State Store versions
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[DEBUG] Redirecting to URI
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:purgeOldLegacyOIVImages(java.lang.String,long):[WARN] Invalid file name. Skipping {fileName}
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand):[DEBUG] Sending signal to pid a as user b for container y
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceStop():[WARN] reloadThread fails to join.
org.apache.hadoop.mapreduce.lib.db.FloatSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[WARN] Generating splits for a floating-point index column. Due to the
org.apache.hadoop.yarn.client.cli.TopCLI:setTerminalHeight():[WARN] Couldn't determine terminal height, setting to 24
org.apache.hadoop.streaming.PipeMapRed$MRErrorThread:incrCounter(java.lang.String):[WARN] Cannot parse counter line: + line
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:genImpl(java.lang.Class,java.lang.String,int):[INFO] Generating class {}<T>
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:replacePartitions(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit):[DEBUG] {}: removing partition path to be replaced:
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] Appending SAS token to query
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[INFO] Recovering Transition Read
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Wrong RPC AUTH flavor, {} is not AUTH_SYS or RPCSEC_GSS.
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:load(java.lang.String):[DEBUG] Loading section INODE_REFERENCE length: {s.getLength()}
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] userResolver.getClass() needs target user list. Use -users option.\n
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:register():[INFO] queue: + queue
org.apache.hadoop.fs.s3a.S3AInputStream:readCombinedRangeAndUpdateChildren(org.apache.hadoop.fs.impl.CombinedFileRange,java.util.function.IntFunction):[DEBUG] Exception while reading a range {} from path {}
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementAttemptedItems$BlocksStorageMovementAttemptMonitor:run():[INFO] BlocksStorageMovementAttemptMonitor thread is interrupted.
org.apache.hadoop.tools.util.ProducerConsumer$Worker:run():[DEBUG] Worker thread was interrupted while processing an item, or putting into outputQueue. Retrying...
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Proxy user Authentication exception
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:getReader(java.lang.String):[ERROR] Cannot open read stream for {}
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceStop():[INFO] Stopped federation membership heartbeat
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:getPluginsFromConfig(org.apache.hadoop.conf.Configuration):[INFO] No Resource plugins found from configuration!
org.apache.hadoop.yarn.service.provider.ProviderUtils:resolveHadoopXmlTemplateAndSaveOnHdfs(org.apache.hadoop.fs.FileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.ConfigFile,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.service.ServiceContext):[INFO] Reading config from: configFile.getSrcFile(), writing to: remoteFile
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for fileId: {}
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementFactory:getPlacementRule(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Creating PlacementRule implementation: + ruleClass
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:removeApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String):[ERROR] Given app to remove does not exist in queue
org.apache.hadoop.hdfs.server.namenode.FSEditLog:getEditLogManifest(long):[DEBUG] getEditLogManifest
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute():[DEBUG] nothing to commit for {key}
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[ERROR] DeleteAsUser for {} returned with exit code: {}
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path):[DEBUG] Deleting the temporary directory of '${attemptId}': '${taskAttemptPath}'
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:handleShutdownOrResyncCommand(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[WARN] Received SHUTDOWN signal from Resourcemanager as part of heartbeat, hence shutting down.
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:publishCompressedDataStatistics(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long):[INFO] Total size of compressed input data : [compressedDataSize]
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getCachedStore(org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,java.util.List):[WARN] AppLogs for group id {groupId} is null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] No valid image-tag-to-hash files
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:startThreads():[INFO] Watcher for tokens is disabled in this secret manager
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[WARN] Failed to cache + key + : checksum verification failed.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[ERROR] Exception while computing policy changes for leaf queue : + getQueuePath(), ye
org.apache.hadoop.mapred.JobQueueClient:displayQueueAclsInfoForCurrentUser():[INFO] Queue acls for user : <shortUserName>
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getAllJobs(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Fetching job details
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:isFileExist(long):[WARN] Exception while getting file is for the given path:{}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:run():[WARN] MarkedDeleteBlockScrubber encountered an exception during the block deletion process, the deletion of the block will retry in {} millisecond.
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:initCsiAdaptorCache(java.util.Map,org.apache.hadoop.conf.Configuration):[INFO] Found csi-driver-adaptor socket address: + addr
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:initPlugin(org.apache.hadoop.conf.Configuration):[WARN] Failed to find FPGA discoverer executable from system environment ALTERAOCLSDKROOT_NAME, please check your environment!
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] attemptID + given a go for committing the task output.
org.apache.hadoop.examples.Sort:run(java.lang.String[]):[INFO] The job took seconds seconds.
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger):[WARN] Unable to fetch valueName; retried retryCount times / waited time ms
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption):[INFO] Update resource on node: {} from: {}, to: {} in {} ms
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:cancelPlanUsingHash(java.lang.String,java.lang.String):[ERROR] Cancelling plan on {} failed. Result: {}, Message: {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:checkAppNumCompletedLimit():[INFO] Max number of completed apps kept in state store met: maxCompletedAppsInStateStore = {maxCompletedAppsInStateStore}, removing app {removeApp.getApplicationId()} from state store.
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:retrieveNamespaceInfo():[WARN] Problem connecting to server: nnAddr
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader:newConnectedPeer(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.InetSocketAddress,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID):[ERROR] Cleanup with logger due to failure
org.apache.hadoop.hdfs.server.datanode.DataNode:checkBlockToken(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[DEBUG] BlockTokenIdentifier id: {}
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:fixGlobalQuota(org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,org.apache.hadoop.fs.QuotaUsage):[INFO] Fix Quota src={} dst={} oldQuota={}/{} newQuota={}/{}, location.getSrc(), location, remoteQuota.getQuota(), remoteQuota.getSpaceQuota(), gQuota.getQuota(), gQuota.getSpaceQuota()
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Balance succeed!
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onFailure(java.lang.Throwable):[WARN] Exception running disk checks against volume + reference.getVolume(), exception
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:nextTcpPeer():[TRACE] nextTcpPeer: failed to create newConnectedPeer connected to {}
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer():[TRACE] Unsafe comparer selected for byte unaligned system architecture
org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto):[INFO] AUTH success for ... from ...
org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map):[INFO] Added global filter ' + name + ' (class= + classname + )
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:handleFairSchedulerConfig(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterParams,org.apache.hadoop.conf.Configuration):[INFO] Using FAIR_SCHEDULER_XML defined in YARN_SITE_XML by key: FairSchedulerConfiguration.ALLOCATION_FILE
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:getSubClusterForNode(java.lang.String):[ERROR] Failed to resolve sub-cluster for node {}, skipping this node
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:readVersion():[DEBUG] Loaded <version> with onDiskVersion= + onDiskVersion + , layoutVersion= + layoutVersion + .
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getAclStatus(java.lang.String):[INFO] logAuditEvent(true, getAclStatus, src)
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:pauseForTesting():[INFO] Pausing re-encrypt handler for testing.
org.apache.hadoop.mapred.Merger$MergeQueue:merge(java.lang.Class,java.lang.Class,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress):[INFO] Merging N sorted segments
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:createRMNodeLabelsMappingProvider(org.apache.hadoop.conf.Configuration):[DEBUG] RM Node labels mapping provider class is : nodeLabelsMappingProvider.getClass()
org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int):[INFO] Detected a loopback TCP socket, disconnecting it
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCacheDirectives():[DEBUG] Directive {}: No inode found at {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:shutdownBlockPool(java.lang.String):[INFO] Removing block pool + bpid
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state, e
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeSequential(java.util.List,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.Class,java.lang.Object):[ERROR] Unexpected exception {} proxying {} to {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:receivedNewWriteInternal(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider):[DEBUG] UNSTABLE write request, send response for offset: OFFSET_PLACEHOLDER
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskAttemptFetchFailureTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Too many fetch-failures for output of task attempt: {mapId} ... raising fetch failure to map
org.apache.hadoop.mapred.BackupStore$FileCache:write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer):[DEBUG] ID: [segmentList.size()] WRITE TO DISK
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[ERROR] Invalid arguments: , {e}
org.apache.hadoop.yarn.client.cli.TopCLI:getRMStartTime():[ERROR] Could not fetch RM start time
org.apache.hadoop.mapred.QueueConfigurationParser:loadResource(java.io.InputStream):[INFO] Failed to set setXIncludeAware(true) for parser + docBuilderFactory + NAME_SEPARATOR + e
org.apache.hadoop.streaming.PipeMapRed:addJobConfToEnvironment(org.apache.hadoop.mapred.JobConf,java.util.Properties):[WARN] Environment variable truncated to fit system limits.
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:getCommitterFactory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[INFO] Using schema-specific factory for {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo):[WARN] Waiting to remove IN_INTERMEDIATE state histories (e.g. firstInIntermediateKey) from JobListCache because it is not in done yet. Total count is inIntermediateCount.
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:initializePlan(java.lang.String):[INFO] Initialized plan {} based on reservable queue {}
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration,boolean,boolean):[WARN] Encountered exception during format
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:deleteRoot():[DEBUG] Deleting root content
org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Submitted Class Name: <b> + logName + </b><br />
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:writeDistCacheFilesList():[ERROR] Missing ${fileCount} distributed cache files under the directory\n${distCachePath}\nthat are needed for gridmix to emulate distributed cache load. Either use -generate\noption to generate distributed cache data along with input data OR disable\ndistributed cache emulation by configuring '${DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE}' to false.
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:initAndStartAppMaster(org.apache.hadoop.mapreduce.v2.app.MRAppMaster,org.apache.hadoop.mapred.JobConf,java.lang.String):[INFO] Executing with tokens: {}
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:createTimelineClient(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Unable to create timeline client for app + appId, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[INFO] Application {applicationId} submitted by user {user} rejected by placement rules.
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:handleShutdownOrResyncCommand(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[WARN] Message from ResourceManager: + response.getDiagnosticsMessage()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:updateAppsRunnability(java.util.List,int):[INFO] {} is now runnable in {}
org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Token kind is {} and the token's service name is {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setErasureCodingPolicy(java.lang.String,java.lang.String,boolean):[INFO] Audit event for operation: setErasureCodingPolicy on srcArg
org.apache.hadoop.fs.s3a.S3AFileSystem:toString():[DEBUG] Appending instrumentation info
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$WintuilsProcessStubExecutor:validateResult():[WARN] output.toString()
org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object):[INFO] available bytes: <bytes>
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:publishCompressedDataStatistics(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long):[INFO] Gridmix is configured to use compressed input data.
org.apache.hadoop.fs.azure.BlockBlobAppendStream:writeBlockRequestInternal(java.lang.String,java.nio.ByteBuffer,boolean):[DEBUG] upload block finished for {} ms. block {}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[ERROR] Application {appName} with appId {appId} failed to be submitted.
org.apache.hadoop.examples.pi.DistSum$Machine:compute(org.apache.hadoop.examples.pi.math.Summation,org.apache.hadoop.mapreduce.TaskInputOutputContext):[INFO] result=...
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getApps(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set,java.util.Set,java.lang.String,java.util.Set):[INFO] Interceptor chain obtained
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[DEBUG] FilterList created for get is - {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:createDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] createDirectory filesystem: {} path: {} permission: {} umask: {} isNamespaceEnabled: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processFirstBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs):[DEBUG] Initial report of block {} on {} size {} replicaState = {}
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:cleanOldLogs(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.security.UserGroupInformation):[ERROR] Failed to delete path
org.apache.hadoop.hdfs.DFSUtil:httpServerTemplateForNNAndJN(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String,java.lang.String,java.lang.String):[INFO] Filter initializers set : {initializers}
org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread:run():[INFO] START SERIAL @ ...
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:schedule(org.apache.hadoop.yarn.server.resourcemanager.volume.csi.provisioner.VolumeProvisioningTask,int):[INFO] Scheduling provision volume task (with delay + delaySecond + s), + handling + volumeProvisioningTask.getVolumes().size() + volume provisioning
org.apache.hadoop.hdfs.tools.DiskBalancerCLI:main(java.lang.String[]):[ERROR] Exception thrown while running DiskBalancerCLI.
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:checkVersion():[INFO] Storing timeline state store version info {getCurrentVersion()}
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:shouldRetry(java.io.IOException,int,java.lang.String):[ERROR] Re-throwing API exception, no more retries
org.apache.hadoop.ha.ActiveStandbyElector:becomeStandby():[DEBUG] Becoming standby for {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing {applicationID} of type {event.getType()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Cannot find to-be-moved container's application={}
org.apache.hadoop.yarn.service.webapp.ApiServer:decommissionInstances(org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {appName} has successfully decommissioned instances.
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long):[DEBUG] mode.toString() + " KeyStore: " + keystoreLocation
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:getNumLiveDataNodes():[WARN] Exception while getting number of live datanodes.
org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean):[TRACE] Auth method is not set, yield from setting auth fallback.
org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive():[INFO] Old node exists: {}
org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text):[DEBUG] Added {}:{} into tokenKindMap
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[INFO] Insert into the StateStore the application: {} in SubCluster: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:logLineFromTasksFile(java.io.File):[DEBUG] First line in cgroup tasks file: {} {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.event.Event):[WARN] Move Application has failed: e.getMessage()
org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object):[WARN] reference to %s lost during creation, key
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:reset():[DEBUG] reset
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode:activateDelay(long):[INFO] this activateDelay {delta / 1000.0} seconds
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[INFO] All EC block group status: OK
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:lookupForLocalUserIdentity(java.lang.String):[ERROR] Error while parsing the line, returning empty string
org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String):[DEBUG] GroupCacheLoader - load.
org.apache.hadoop.mapred.MapTask:createSortingCollector(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Task$TaskReporter):[DEBUG] Trying map output collector class: + subclazz.getName()
org.apache.hadoop.mapred.IndexCache:getIndexInformation(java.lang.String,int,org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] IndexCache HIT: MapId + mapId + found
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:bootstrap(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration):[INFO] YARN containers restricted to yarnProcessors cores
org.apache.hadoop.fs.cosn.CosNFileSystem:createParent(org.apache.hadoop.fs.Path):[DEBUG] Store a empty file in COS failed., e
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:removeResourceFromCacheFileSystem(org.apache.hadoop.fs.Path):[INFO] Deleting {path.toString()}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:removeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID):[WARN] BLOCK* removeDatanode: node does not exist
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEvent):[INFO] Got exception while pausing container: {exception}
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:extractChecksumProperties(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto,org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,int):[WARN] Current bytesPerCRC={} doesn’t match next bpc={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getOrCreateQueueFromPlacementContext(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext,boolean):[ERROR] Could not auto-create leaf queue due to :
org.apache.hadoop.tools.rumen.Folder:run():[DEBUG] Considering jobs with submit time greater than ... Skipped ... jobs.<br>
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler:call():[TRACE] Scanner volume report: {result}
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Total Pmem allocated for Container
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String):[DEBUG] Getting new token from {}, renewer:{}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$AddNodeTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[DEBUG] Inform scheduler of node transition
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:scriptBasedInit(java.util.function.Function,java.lang.String[]):[INFO] Script not found under /sbin/DevicePluginScript/, falling back to default search directories
org.apache.hadoop.hdfs.DFSInputStream:blockSeekTo(long):[WARN] Failed to connect to ... for file ... for block ..., add to deadNodes and continue.
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Average bytes per map: ..., Number of maps: ..., total size: ...
org.apache.hadoop.hdfs.server.namenode.NameNode:startCommonServices(org.apache.hadoop.conf.Configuration):[INFO] getRole() + " RPC up at: " + getNameNodeAddress()
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialBinding:requestSessionCredentials(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String,int,org.apache.hadoop.fs.s3a.Invoker):[ERROR] Region must be provided when requesting session credentials.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[DEBUG] NameNode safe mode check completed
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:upsertDocument(org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType,org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.TimelineDocument):[WARN] There was a conflict while upserting, hence retrying...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for dir fileId: {}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntityTypes(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.hdfs.DFSOutputStream:flushOrSync(boolean,java.util.EnumSet):[WARN] Unable to persist blocks in hflush for
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersions(java.lang.String):[TRACE] Entering getKeyVersions method.
org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator):[DEBUG] RemoteIterator Statistics: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:loadFromDisk(org.apache.hadoop.conf.Configuration):[WARN] Encountered exception loading fsimage
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[DEBUG] Appending key-value pair
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:runLocalization(java.net.InetSocketAddress):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[DEBUG] Try to commit allocation proposal={}
org.apache.hadoop.hdfs.server.datanode.DataNode:handleDiskError(java.lang.String,int):[WARN] DataNode.handleDiskError on: [{}] Keep Running: {}
org.apache.hadoop.yarn.service.ServiceScheduler:syncSysFs(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Fail to sync service spec: {e}
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] Too few arguments to Gridmix.\n
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleInitApplicationResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application):[INFO] Application resources initialized for user: {userName}
org.apache.hadoop.hdfs.qjournal.server.Journal:syncLog(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto,java.net.URL):[WARN] Failed to delete temporary file [tmpFile]
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore$AppCheckTask:run():[INFO] Looking into [number] apps to see if they are still active.
org.apache.hadoop.mapred.pipes.BinaryProtocol$UplinkReaderThread:run():[WARN] Message received before authentication is complete. Ignoring
org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration):[DEBUG] Reinit compressor with new compression configuration
org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path):[DEBUG] listStatus({}) failed; returning empty array
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:main(java.lang.String[]):[ERROR] DFSZKFailOverController exiting due to earlier exception: t
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:updateReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[ERROR] The specified Reservation with ID {} does not exist in the plan
org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String):[DEBUG] Missing ip list file : + fileName
org.apache.hadoop.resourceestimator.skylinestore.validator.SkylineStoreValidator:validate(java.lang.String):[ERROR] Resource allocation for {pipelineId} is null.
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] Unknown option specified.\n
org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE):[INFO] Operation completed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:removeQueue(java.lang.String):[INFO] Removing queue: + queueName
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:run():[INFO] Executing re-encrypt commands on zone {}. Current zones:{}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:commitSubSection(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder,org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName):[DEBUG] Saving a subsection for {}
org.apache.hadoop.conf.Configuration:logDeprecation(java.lang.String):[INFO] message
org.apache.hadoop.fs.s3a.S3AFileSystem:incrementPutStartStatistics(long):[DEBUG] PUT start {} bytes
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:getStorageIDToVolumeBasePathMap():[ERROR] Disk Balancer - Internal Error.
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate):[TRACE] Hosts:{}, CNs:{} subjectAlts:{}, ie6:{}, strictWithSubDomains{}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the entity table
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getNode(java.util.Collection,java.lang.String):[WARN] Failed to get node report
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Error in removing master key with KeyID:
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Physical memory check enabled: {}
org.apache.hadoop.fs.azure.PageBlobOutputStream:close():[DEBUG] Closing page blob output stream.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$LogMonitorThread:run():[WARN] Uncaught exception in ContainerMemoryManager while monitoring log usage for containerId
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:startTimelineClient(org.apache.hadoop.conf.Configuration):[INFO] Timeline service V1 client is enabled
org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext:isSchedulerReadyForAllocatingContainers():[INFO] Skip allocating containers. Scheduler is waiting for recovery.
org.apache.hadoop.mapred.gridmix.Statistics:add(java.lang.Object):[ERROR] [Statistics] Missing entry for job + job.getJob().getJobID()
org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String):[WARN] Start op not found: {endKind}({blockNumber})
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:deleteTempFile(org.apache.hadoop.fs.Path):[DEBUG] Exception received while deleting temp files
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] Using kerberos user: {user}
org.apache.hadoop.mapred.ClientServiceDelegate:getTaskCompletionEvents(org.apache.hadoop.mapreduce.JobID,int,int):[INFO] Invoking getTaskAttemptCompletionEvents
org.apache.hadoop.yarn.service.ClientAMService:flexComponents(org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto):[INFO] Flexing component {} to {}
org.apache.hadoop.hdfs.qjournal.server.Journal:getSegmentInfo(long):[INFO] Edit log file + elf + appears to be empty. + Moving it aside... + ; journal id: + journalId
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[DEBUG] Delegation token retrieved
org.apache.hadoop.hdfs.server.datanode.DataStorage:linkBlocks(java.io.File,java.io.File,int,org.apache.hadoop.fs.HardLink,org.apache.hadoop.conf.Configuration):[INFO] Start linking block files from {} to {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor:run():[WARN] NameNode low on available disk space. Already in safe mode.
org.apache.hadoop.fs.azurebfs.services.AbfsLease:free():[DEBUG] Freeing lease: path {}, lease id {}
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkAllVolumes(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi):[WARN] checkAllVolumes timed out after {} ms maxAllowedTimeForCheckMs
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean):[DEBUG] BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for {} to add as corrupt on {} by {} {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:addReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation,boolean):[ERROR] The specified Reservation with ID ... already exists
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo,org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy):[DEBUG] blocks = {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post response created for TRUNCATE
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:stateChanged(org.apache.hadoop.service.Service):[INFO] Service + service.getName() + changed state: + service.getServiceState()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:newInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.namenode.NNStorage,java.util.List):[INFO] Edit logging is async: true/false
org.apache.hadoop.yarn.service.client.ApiServiceClient:getRMWebAddress():[INFO] Fail to connect to: {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] SASL server doing encrypted handshake for peer = {}, datanodeId = {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:initiateCsiClients(org.apache.hadoop.conf.Configuration):[INFO] Initializing a csi-adaptor-client for csi-adaptor {}, csi-driver {}
org.apache.hadoop.hdfs.server.datanode.LocalReplica:breakHardlinks(java.io.File,org.apache.hadoop.hdfs.protocol.Block):[INFO] detachFile failed to delete temporary file
org.apache.hadoop.fs.s3a.WriteOperationHelper:commitUpload(java.lang.String,java.lang.String,java.util.List,long):[DEBUG] Completing multipart upload {} with {} parts
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:dispatchBlocks():[WARN] Exception while getting reportedBlock list
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:parse():[INFO] Got an error parsing job-history file, ignoring incomplete events.
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:load(java.io.File):[INFO] Loading image file curFile of size curFile.length() bytes loaded in elapsedTime seconds.
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:sendLaunchedEvents():[INFO] TaskAttempt: [${attemptId}] using containerId: [${container.getId()} on NM: [${StringInterner.weakIntern(container.getNodeId().toString())}]
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int):[DEBUG] this + : + caller + : closing fd + fd + at the request of the handler.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendFinishedEvents():[INFO] Container stop monitoring
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Checking and enabling app aggregators
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:demoteOldEvictableMmaped(long):[TRACE] demoteOldEvictable: demoting <replica>: <rationale>: <stackTrace>
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:verifySoftwareVersion(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[INFO] Reported DataNode version 'dnVersion' of DN dnReg does not match NameNode version 'nnVersion'. Note: This is normal during a rolling upgrade.
org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry):[DEBUG] Logging enabled
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[ERROR] App attempt: + appAttemptID + can't handle this event at current state, e
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTTransition:transition(java.lang.Object,java.lang.Object):[INFO] Storing RMDelegationToken and SequenceNumber
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:lastStep():[INFO] Application {} is shutting down.
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:verifyECWithTopology(int,int,int,int,java.lang.String):[DEBUG] %d DataNodes are required for the erasure coding policies: %s. The number of DataNodes is only %d.
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy):[INFO] Retry policy created
org.apache.hadoop.mapred.Merger$MergeQueue:next():[INFO] Progress updated
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:refresh():[ERROR] No commands found for line [{}]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute):[DEBUG] Incoming requestAttribute:{} is not present in {}, skip such node.
org.apache.hadoop.metrics2.sink.GraphiteSink:flush():[WARN] Error flushing metrics to Graphite
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:reAttachUAMAndMergeRegisterResponse(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse,org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] registryClient is null, skip attaching existing UAM if any
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createLocatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.LocatedBlockBuilder,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[WARN] Inconsistent number of corrupt replicas for {} + blockMap has {} but corrupt replicas map has {}, blk, numCorruptNodes, numCorruptReplicas
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[WARN] Ignored {expanded}. It is not a directory
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:init(org.apache.hadoop.conf.Configuration):[INFO] Initializing the GreedyReservationAgent to favor "early" (left) allocations (controlled by parameter: FAVOR_EARLY_ALLOCATION)
org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Fetching Mapper type map
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + TimelineReaderWebServicesUtils.getUserName(callerUGI)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] No excess replica can be found. excessTypes: {}. moreThanOne: {}. exactlyOne: {}.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:computeReconstructionWorkForBlocks(java.util.List):[DEBUG] BLOCK* ask {} to replicate {} to {}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery():[INFO] Not attempting to recover. Recovery is not supported by committer.getClass(). Use an OutputCommitter that supports recovery.
org.apache.hadoop.yarn.server.timeline.util.LeveldbUtils:loadOrRepairLevelDb(org.fusesource.leveldbjni.JniDBFactory,org.apache.hadoop.fs.Path,org.iq80.leveldb.Options):[WARN] Incurred exception while loading LevelDb database. Backing up at [computed dbBackupPath from dbPath]
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:init(int,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.sls.SLSRunner,long,long,java.lang.String,java.lang.String,boolean,java.lang.String,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map,java.util.Map):[INFO] Added new job with {} streams, running for {}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getRouters():[ERROR] Cannot get Routers JSON from the State Store
org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int):[INFO] Adding new operation of type PREFETCH
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[WARN] Exception, e
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:waitForAllPartUploads():[DEBUG] Waiting for {} uploads to complete
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:checkAcls(java.lang.String):[INFO] SCM Admin: method invoked by user
org.apache.hadoop.fs.adl.AdlFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[WARN] Got exception when getting Hadoop user name. Set the user name to 'hadoop'.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAclEntries(java.lang.String,java.util.List):[DEBUG] Starting removeAclEntries
org.apache.hadoop.hdfs.DFSInputStream:getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[DEBUG] Connecting to datanode {dnAddr}
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.flowrun.FlowRunDocument:aggregateMetrics(java.util.Map):[DEBUG] No incoming metric to aggregate for : {metricId}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduleStats:log(java.lang.String):[INFO] msgPrefix + PendingReds: + numPendingReduces + ScheduledMaps: + numScheduledMaps + ScheduledReds: + numScheduledReduces + AssignedMaps: + numAssignedMaps + AssignedReds: + numAssignedReduces + CompletedMaps: + numCompletedMaps + CompletedReds: + numCompletedReduces + ContAlloc: + numContainersAllocated + ContRel: + numContainersReleased + HostLocal: + hostLocalAssigned + RackLocal: + rackLocalAssigned
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:writeAMRMTokenForUAM(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.token.Token):[DEBUG] Same amrmToken received from subClusterId, skip writing registry for appId
org.apache.hadoop.tools.dynamometer.Client:monitorInfraApplication():[INFO] Track the application at: ...
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition$TriggerScheduledFuture:run():[INFO] Sending event + toSend + to + job.getID()
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockWriter:init():[DEBUG] Closing streams upon failure.
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(java.lang.Object):[DEBUG] Volume {} is {}.
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[INFO] bufstart = {bufstart}; bufvoid = {bufvoid}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Number of processors <NUM_PROCESSORS>
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:loadUUIDFromLogFile(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[DEBUG] Attempting to load UUID from log file
org.apache.hadoop.hdfs.server.namenode.NNStorage:writeTransactionIdFileToStorage(long,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeDirType):[WARN] writeTransactionIdToStorage failed on {}
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:run():[INFO] AMHeartbeatRequestHandler thread for {} is exiting
org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI):[DEBUG] the local file does not exist.
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkTimestamp(long):[WARN] Invalid timestamp information. Please try again by specifying valid Timestamp Information.
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String):[INFO] Connecting to + host + ...
org.apache.hadoop.yarn.server.util.timeline.TimelineServerUtils:setTimelineFilters(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Set):[INFO] Filter initializers set for timeline service: + actualInitializers
org.apache.hadoop.yarn.service.utils.ServiceUtils:putAllJars(java.util.Map,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path,java.lang.String,java.lang.String):[DEBUG] Loading all dependencies from {}
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext,java.lang.String,java.lang.String):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] %s
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getBlockReaderLocal():[DEBUG] {}: failed to get ShortCircuitReplica. Cannot construct BlockReaderLocal via {}, this, pathInfo.getPath()
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] NodeManager configured with {} physical memory allocated to containers, which is more than 80% of the total physical memory available ({}). Thrashing might happen.
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[TRACE] Acquiring write lock to replay edit log
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:compileReport(java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler):[INFO] Compiling report for volume: ProvidedVolumeImpl; bpid: someBpid
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:initDefaultNameService(org.apache.hadoop.conf.Configuration):[WARN] Default name service is disabled.
org.apache.hadoop.hdfs.NameNodeProxiesClient:createProxyWithLossyRetryHandler(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,int,java.util.concurrent.atomic.AtomicBoolean):[DEBUG] buildTokenServiceForLogicalUri
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[DEBUG] Running task cleanup task
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Checking and resuming update operation
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:openDatabase(org.apache.hadoop.conf.Configuration):[INFO] Using state database at {storeRoot} for recovery
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:containerStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerStartData):[ERROR] Error when writing start information of container + containerStart.getContainerId(), e
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:createDefaultReservationQueue(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue,java.lang.String):[WARN] Exception while trying to create default reservation queue for + plan: {}, planQueueName, e
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS READ fileHandle: {} offset: {} count: {} client: {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String):[DEBUG] Creating {} requires creating parent {}, src, parent
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:removeLabelsFromNode(java.util.Map):[ERROR] NODE_LABELS_NOT_ENABLED_ERR
org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean):[DEBUG] parsing input stream + is
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:createTimelineWriter(org.apache.hadoop.conf.Configuration):[INFO] Using TimelineWriter: + timelineWriterClassName
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:modifyAclEntries(java.lang.String,java.util.List):[INFO] logAuditEvent(false, "modifyAclEntries", src)
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:serviceStop():[INFO] Interrupted while joining on delayed removal thread.
org.apache.hadoop.yarn.event.AsyncDispatcher:dispatch(org.apache.hadoop.yarn.event.Event):[DEBUG] Dispatching the event {}.{}, event.getClass().getName(), event
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:autoCreateLeafQueue(org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] SchedulerDynamicEditException: Parent queue not specified
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter$SortingThread:run():[ERROR] Exception raised while executing multinode sorter, skip this run..., exception=
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onIncreaseContainerResourceError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[INFO] onIncreaseContainerResourceError: {}, {}
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponentInstance(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.service.api.records.Container):[INFO] PUT: update component instance {} for component = {} + service = {} user = {}
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.protocol.Block):[INFO] Modification time set for file
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:runLocalization(java.net.InetSocketAddress):[INFO] New instance created
org.apache.hadoop.hdfs.tools.DFSAdmin:getVolumeReport(java.lang.String[],int):[INFO] Active Volumes : X
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:deleteSavedFiles():[WARN] Failed to delete meta file
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long):[DEBUG] Updating token {token_sequence_number}
org.apache.hadoop.hdfs.server.datanode.ErrorReportAction:reportTo(org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB,org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration):[INFO] trySendErrorReport encountered RemoteException errorMessage: {errorMessage} errorCode: {errorCode}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot remove "{existingRecord}"
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] Failed to choose remote rack (location = ~ + localMachine.getNetworkLocation() + ), fallback to local rack
org.apache.hadoop.hdfs.server.namenode.TreePath:toFile(org.apache.hadoop.hdfs.server.namenode.UGIResolver,org.apache.hadoop.hdfs.server.namenode.BlockResolver,org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Writer):[WARN] "Exact path handle not supported by filesystem "
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.CentralizedOpportunisticContainerAllocator:allocateContainers(org.apache.hadoop.yarn.api.records.ResourceBlacklistRequest,java.util.List,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,long,java.lang.String):[INFO] Not allocating more containers as we have reached max allocations per AM heartbeat {}
org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext:acquire(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Acquiring pre-assigned chunk: + acquiredFilePath
org.apache.hadoop.mapred.YARNRunner:setupLocalResources(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Creating setup context, jobSubmitDir url is ...
org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab():[DEBUG] Initiating logout for {}
org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class):[DEBUG] Configuring client
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:setup(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl):[WARN] Shuffle secret key missing from job credentials. Using job token secret as shuffle secret.
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:write(int):[DEBUG] writing more data than block has capacity -triggering upload
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:main(java.lang.String[]):[INFO] Startup shutdown message
org.apache.hadoop.hdfs.DFSOutputStream:completeFile():[INFO] Could not complete {...} retrying...
org.apache.hadoop.yarn.service.monitor.ServiceMonitor$ReadinessChecker:run():[INFO] Readiness check succeeded for {}: {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:concat(java.lang.String,java.lang.String[]):[DEBUG] *DIR* NameNode.concat: src path {} to target path {}
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:write(java.io.DataOutput):[DEBUG] Writing ContainerTokenIdentifier to RPC layer: {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSync():[DEBUG] logSync {edit}
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:downloadCheckpointFiles(java.net.URL,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature,org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest):[INFO] Image has not changed. Will not download image.
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerResourceIncreased(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] onContainerResourceIncreased: {}, {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:run():[INFO] Localizer failed for + localizerId, exception
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:init():[INFO] YarnConfiguration.NVIDIA_DOCKER_PLUGIN_V1_ENDPOINT set to empty, skip init ..
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext):[DEBUG] *BLOCK* NameNode.blockReport: from {nodeReg}, reports.length={reports.length}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:handleDTRenewerAppRecoverEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerAppRecoverEvent):[WARN] Unable to add the application to the delegation token renewer on recovery., t
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:cacheReport():[DEBUG] CacheReport of block(s) took createCost msecs to generate and sendCost msecs for RPC and NN processing
org.apache.hadoop.yarn.server.resourcemanager.DBManager:initDatabase(java.io.File,org.iq80.leveldb.Options,java.util.function.Consumer):[INFO] Creating configuration version/database at {}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$FSAction:runWithRetries():[INFO] Exception while executing an FS operation.
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Executing command {SelectTool.NAME}
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:isLeader():[INFO] rmId + failed to transition to active, giving up leadership, e
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshAdminAcls(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshAdminAclsRequest):[INFO] Successfully executed refreshAdminAcls
org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress):[INFO] AUTHZ_SUCCESSFUL_FOR ...
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:verifyAndCreateRemoteLogDir():[WARN] Remote Root Log Dir [ + remoteRootLogDir + ] already exist, but with incorrect permissions. Expected: [ + TLDIR_PERMISSIONS + ], Found: [ + perms + ]. The cluster may have problems with multiple users.
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[INFO] Path already exists, no action taken
org.apache.hadoop.ha.ZKFailoverController:becomeStandby():[INFO] Successfully transitioned [localTarget] to standby state
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanupContainersOnNMResync():[INFO] Done waiting for containers to be killed. Still alive: {containers.keySet()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:allocate(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] There are no available cpus: + resource.getVirtualCores() + in numa nodes for + containerId
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric was emitted with no name.
org.apache.hadoop.util.JvmPauseMonitor$Monitor:run():[INFO] Starting JVM pause monitor
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSFileChecksum operation executed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Scheduler pool size [schedPSize]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:printReferenceTraceInfo(java.lang.String):[TRACE] Reference count: + op + this +: + this.reference.getReferenceCount()
org.apache.hadoop.yarn.server.resourcemanager.AdminService:checkForDecommissioningNodes(org.apache.hadoop.yarn.server.api.protocolrecords.CheckForDecommissioningNodesRequest):[INFO] RMAuditLogger.logSuccess
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendFinishedEvents():[DEBUG] Cloning and getting container status
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[DEBUG] Operation unsetErasureCodingPolicy started
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {userName}
org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:waitForZKConnectionEvent(int):[ERROR] Connection timed out: couldn't connect to ZooKeeper in {} milliseconds
org.apache.hadoop.security.Groups:getUserToGroupsMappingService():[DEBUG] Creating new Groups object
org.apache.hadoop.yarn.util.resource.ResourceUtils:addResourcesFileToConf(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Unable to find ' + resourceFile + '.
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadDataStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[INFO] loadDataStorage: {} upgrade tasks
org.apache.hadoop.conf.Configuration:checkForOverride(java.util.Properties,java.lang.String,java.lang.String,java.lang.String):[WARN] name:an attempt to override final parameter: attr; Ignoring.
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotal():[DEBUG] Failed to Get total capacity
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Path {nodeUpdatePath} for {appId} didn't exist. Creating a new znode to update the application state.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[INFO] Submitting application to RouterWebServices
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:requestNewHdfsDelegationTokenIfNeeded(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[INFO] Token= ( + dttr + ) is expiring, request new token.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[INFO] Skipping monitoring container containerId because memory usage is not available.
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextDestroyed(javax.servlet.ServletContextEvent):[INFO] KMS Stopped
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntityTypes():[DEBUG] Adding entity type to list
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeJMXValue(java.lang.String,java.lang.String,java.lang.String,double,double,boolean,java.util.Properties,java.util.function.Supplier,org.slf4j.Logger):[INFO] valueName = value; threshold action; done waiting after time ms.
org.apache.hadoop.security.authentication.examples.RequestLoggerFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] xResponse.getResponseInfo().toString()
org.apache.hadoop.yarn.util.DockerClientConfigHandler:writeDockerCredentialsToPath(java.io.File,org.apache.hadoop.security.Credentials):[DEBUG] Prepared token for write: {}
org.apache.hadoop.mapreduce.JobResourceUploader:uploadFiles(org.apache.hadoop.mapreduce.Job,java.util.Collection,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,short,java.util.Map,java.util.Map):[DEBUG] Uploading files
org.apache.hadoop.hdfs.server.namenode.LeaseManager:checkLeases(java.util.Collection):[DEBUG] Lease recovery for inode {} is complete. File closed
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext):[WARN] Exception from container-launch with container ID: {} and exit code: {}
org.apache.hadoop.util.JsonSerialization:fromResource(java.lang.String):[ERROR] Exception while parsing json resource {}
org.apache.hadoop.hdfs.server.federation.store.MountTableStore:updateCacheAllRouters():[ERROR] Cannot refresh mount table: state store not available
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] assignContainers: node=... #applications=...
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:mkdir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Explicitly setting permissions to : + fsp.toShort() + , + fsp
org.apache.hadoop.fs.aliyun.oss.OSSListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Prefix: {}
org.apache.hadoop.hdfs.server.namenode.CacheManager:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] addCachePool of {info} successful.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanodeDescriptor(java.lang.String):[INFO] Using random node as fallback
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean):[WARN] Unexpected item in trash: + dir + . Ignoring.
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillNewTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[DEBUG] Not generating HistoryFinish event since start event not generated for task: ${task.getID()}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:initializeReplQueues():[INFO] initializing replication queues
org.apache.hadoop.lib.server.Server:init():[INFO] Server [{}] started!, status [{}]
org.apache.hadoop.mapreduce.task.reduce.Fetcher:copyMapOutput(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.DataInputStream,java.util.Set,boolean):[DEBUG] header: mapId, len: compressedLength, decomp len: decompressedLength
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Task final state is not FAILED or KILLED:
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceStop():[INFO] AsyncDispatcher is draining to stop, ignoring any new events.
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateToken(com.nimbusds.jwt.SignedJWT):[WARN] Signature could not be verified
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getSASTokenProvider():[TRACE] {sasTokenProviderClass.getName()} init complete
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:initializeStatusRetriever(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent,long):[INFO] {} retrieve status after {}
org.apache.hadoop.yarn.service.client.ServiceClient:addKeytabResourceIfSecure(org.apache.hadoop.yarn.service.utils.SliderFileSystem,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[WARN] No Kerberos principal name specified for service.getName()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String):[DEBUG] Added priority= + priority
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Finish information is missing for application + appId
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:getInitializedCGroupsHandler(org.apache.hadoop.conf.Configuration):[DEBUG] Value of CGroupsHandler is: {}
org.apache.hadoop.hdfs.DistributedFileSystem$PartialListingIterator:hasNext():[TRACE] No more elements
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[WARN] Balancer already running as a long-service!
org.apache.hadoop.fs.s3a.S3AFileSystem:initiateMultipartUpload(com.amazonaws.services.s3.model.InitiateMultipartUploadRequest):[DEBUG] Initiate multipart upload to {}
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Destination directory exists; conflict policy permits this
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:logErrorMessage(java.io.File,java.lang.Exception):[ERROR] Error aggregating log file. Log file : [logFile]. [Exception message]
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Finished submitting vectored read to threadpool on path {} for ranges {}, pathStr, ranges
org.apache.hadoop.hdfs.server.namenode.FSImage:deleteCancelledCheckpoint(long):[WARN] Unable to delete cancelled checkpoint in <sd>
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setStoragePolicy(java.lang.String,java.lang.String):[ERROR] Audit event failed due to AccessControlException
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.service.utils.CoreFileSystem:createWithPermissions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] mkdir {dir} with perms {clusterPerms}
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[ERROR] Interrupted on sleep while exiting.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:create(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator,org.apache.hadoop.util.Waitable):[INFO] {}: could not load {} due to InvalidToken exception.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromTcp():[WARN] I/O error constructing remote block reader.
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:updateLogAggregationStatus(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.LogAggregationStatus,long,java.lang.String,boolean):[WARN] The log aggregation is disabled. No need to update the log aggregation status
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:dumpSchedulerLogs(java.lang.String,javax.servlet.http.HttpServletRequest):[DEBUG] dumpSchedulerLogs
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:setTimeout(java.net.HttpURLConnection):[INFO] Image Transfer timeout configured to + timeout + milliseconds
org.apache.hadoop.ha.ZKFailoverController:becomeActive():[INFO] Successfully transitioned localTarget to active state
org.apache.hadoop.hdfs.server.mover.Mover$Cli:run(java.lang.String[]):[DEBUG] Entering run method
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:listEncryptionZones(long):[DEBUG] Audit log for operation: listEncryptionZones, success: true
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:updateProcessTree():[WARN] Failed to parse + pid, e
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:init():[INFO] Using FileSystemAccess simple/pseudo authentication, principal {}
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:getExpiry(java.lang.String):[ERROR] Error decoding se query parameter ({}) from SAS.
org.apache.hadoop.yarn.webapp.view.InfoBlock:render():[INFO] Processing item
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[INFO] Skipping monitoring container containerId since CPU usage is not yet available.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:getContainerStatus(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor,org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] Container Status: {dockerContainerStatus.getName()} ContainerId: {containerId}
org.apache.hadoop.fs.FSInputChecker:read(long,byte[],int,int):[INFO] Read aborted due to zero length
org.apache.hadoop.mapred.QueueManager:refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher):[INFO] Queue configuration is refreshed successfully.
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:throttle():[DEBUG] Re-encryption handler throttling expect: {}, actual: {}, throttleTimerAll:{}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:addExternalShuffleProviders(org.apache.hadoop.conf.Configuration,java.util.Map):[INFO] Adding ShuffleProvider Service: {shuffleProvider} to serviceData
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Temporary redirect with URI for file checksum
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:rollEdits():[INFO] Concurrent invocation success
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext):[WARN] InterruptedException executing command: , ie
org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils:getIndexInfo(java.lang.String):[WARN] Parsing job history file with partial data encoded into name: [jhFileName]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceAllocator:addFpgaDevices(java.lang.String,java.util.List):[INFO] Added a list of FPGA Devices:
org.apache.hadoop.ha.ActiveStandbyElector:terminateConnection():[WARN] e.toString()
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:removeKeyRegistry(org.apache.hadoop.registry.client.api.RegistryOperations,org.apache.hadoop.security.UserGroupInformation,java.lang.String,boolean,boolean):[ERROR] Registry remove key {key} failed
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher$PutEventThread:run():[INFO] System metrics publisher will put events every ... milliseconds
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[DEBUG] Processing + event.getTaskID() + of type + event.getType()
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Balance failed, error code: + retCode
org.apache.hadoop.fs.s3a.S3AFileSystem:removeKeysS3(java.util.List,boolean):[DEBUG] {} {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:updateDockerRunCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] The device plugin: className returns null device runtime spec value for container: containerId
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[INFO] Application stop event received for stopping AppId: {ApplicationID}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Start information is missing for application attempt
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.SampleContainerLogAggregationPolicy:parseParameters(java.lang.String):[WARN] The format isn't valid. Min threshold falls back to the default value ${DEFAULT_SAMPLE_MIN_THRESHOLD}
org.apache.hadoop.lib.server.Server:initLog():[INFO] Logger for Server class initialized
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:setConfig(java.lang.Object):[INFO] Logging setup info
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] ImageServlet rejecting: + remoteUser
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer():[TRACE] <Throwable>.getMessage()
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:updateApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.UpdateApplicationHomeSubClusterRequest):[ERROR] Application {appId} does not exist
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:triggerActiveLogRoll():[WARN] Unable to trigger a roll of the active NN
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for dir fileId: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error storing info for AMRMTokenSecretManager
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:compileReport(java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler):[WARN] Exception occurred while compiling report
org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing Default AMS Processor
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:clearDirectory(java.lang.String):[INFO] Create new dump directory {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:deleteAsync(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[INFO] Scheduling block for deletion
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:genericRefresh(java.lang.String[],int):[INFO] Failed to get response.
org.apache.hadoop.yarn.server.webproxy.AppReportFetcher:getApplicationReport(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application report fetched from RM
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:allocate(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] Assigning multiple NUMA nodes ( + StringUtils.join(,, assignedNumaNodeInfo.getMemNodes()) + ) for memory, ( + StringUtils.join(,, assignedNumaNodeInfo.getCpuNodes()) + ) for cpus for + containerId
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List):[DEBUG] ignoring enqueue of empty list
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post called for TRUNCATE
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:addAndScheduleAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar):[INFO] Task being rescheduled
org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts():[DEBUG] Start to decay current costs.;
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:checkOperation(org.apache.hadoop.hdfs.server.namenode.NameNode$OperationCategory):[DEBUG] Proxying operation: {}
org.apache.hadoop.hdfs.server.namenode.FSDirectory:copyINodeDefaultAcl(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] {}: no parent default ACL to inherit
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:rmDockerContainerDelayed():[INFO] Deletion service called
org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] No configuration: skipping check for fs.{}.impl
org.apache.hadoop.yarn.server.nodemanager.DeletionService:recordDeletionTaskInStateStore(org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask):[ERROR] Unable to store deletion task
org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[DEBUG] Canceling delegation token {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:getUserNameForPlacement(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager):[DEBUG] Application tag based placement is enabled, checking for 'userid' among the application tags
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=GETACLSTATUS target=path
org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey):[WARN] Exception while changing ops : e
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:sendLifeline():[DEBUG] Sending lifeline with ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceUpdaterImpl:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[WARN] resourceName + plugin failed to discover resource ( null value got).
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Assigning container failed on node '{}' because queue resource usage is larger than MaxShare: {}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Return JSON with location
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage,boolean):[INFO] {} does not exist. Creating ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Skip recovering container for already stopped attempt.
org.apache.hadoop.hdfs.qjournal.server.Journal:doPreUpgrade():[DEBUG] Cleaning up with logger
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:compare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,boolean):[ERROR] A problem was encountered while calculating resource availability that should not occur under normal circumstances. Please report this error to the Hadoop community by opening a JIRA ticket at http://issues.apache.org/jira and including the following information:\n* Exception encountered: <stack_trace>* Cluster resources: <cluster_resources>\n* LHS resource: <lhs_resources>\n* RHS resource: <rhs_resources>
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessSmapMemoryInfo:setMemInfo(java.lang.String,java.lang.String):[ERROR] Error in parsing : {info} : value {value.trim()}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStart():[INFO] Cleaning logs every {} seconds
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:startSyncJournalsDaemon():[WARN] JournalNodeSyncer interrupted
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToStandby(boolean):[INFO] Already in standby state
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Sorry it looks like the job has no counters.
org.apache.hadoop.hdfs.DFSInputStream:checkInterrupted(java.io.IOException):[DEBUG] The reading thread has been interrupted.
org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider:createProxyIfNeeded(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo):[ERROR] Failed to create RPC proxy to NameNode at {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver:serializeINodeDirectorySection(java.io.OutputStream):[ERROR] FSImageFormatPBINode#serializeINodeDirectorySection: Dangling child pointer found. Missing INode in inodeMap: id=; path=; parent=
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] HA configuration set and verified
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal(org.apache.hadoop.fs.Path):[DEBUG] Path {} is a folder.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:call():[WARN] User + user + is not authorized to upload file + localPath.getName()
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceStop():[INFO] Stopping Router RMAdminService
org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider:getCredentials():[ERROR] Failed to get credentials for role {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[DEBUG] Processing {} of type {}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:startLogSegment(long,int):[INFO] Starting log segment at + segmentTxId
org.apache.hadoop.examples.QuasiMonteCarlo:estimatePi(int,long,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Wrote input for Map
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:register():[INFO] maxContainerCapability: + maxContainerCapability
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:removeAll(java.lang.Class):[INFO] Deleting all children under {znode}
org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class):[ERROR] Unable to create proxy to the ResourceManager , {ioe}
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:run():[ERROR] System Error during DirectoryScanner execution - permanently terminating periodic scanner
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:deleteStore():[INFO] Deleting state database at [root_path]
org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl:getRelativeBlobSASUri(java.lang.String,java.lang.String,java.lang.String):[DEBUG] Generating RelativePath SAS Key for relativePath {} inside Container {} inside Storage Account {}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:cleanupStagingDirs():[DEBUG] Cleaning up work path {}
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler):[INFO] Sending fileName: {localfile.getName()}, fileSize: {localfile.length()}.
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:recover():[DEBUG] Block recovery: DataNode: {id} does not have replica for block: {block}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[TRACE] Initiate check for delegation token manager
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Created new instance of AMRMTokenSecretManagerState
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:logConf(org.apache.hadoop.conf.Configuration):[INFO] NNTop conf: + DFSConfigKeys.NNTOP_WINDOWS_MINUTES_KEY + = +
org.apache.hadoop.yarn.server.resourcemanager.RMInfo:register():[INFO] Registered RMInfo MBean
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:initializeNewPlans(org.apache.hadoop.conf.Configuration):[WARN] Exception while trying to refresh reservable queues
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renewDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Token renewed, expiration time updated
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:scanEditLog(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long):[WARN] Caught exception after scanning through ... ops from ... while determining its valid length. Position was ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[INFO] Remove RMDT with sequence number {rmDTIdentifier.getSequenceNumber()}
org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException):[INFO] Retrying connect to server: + server + . Already tried + curRetries + time(s); retry policy is + connectionRetryPolicy
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:readUnlock(java.lang.String):[INFO] Number of suppressed read-lock reports: {numSuppressedWarnings} Longest read-lock held at {Time.formatTime(lockHeldInfo.getStartTimeMs())} for {lockHeldInfo.getIntervalMs()}ms via {lockHeldInfo.getStackTrace()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:pickAndDoSchedule(java.util.Set,java.util.Map,java.util.Set,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,int,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.DevicePluginScheduler):[DEBUG] Try to schedule ... using ...
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Failed to load OpenSSL. Falling back to the JSSE default.
org.apache.hadoop.mapred.BackupStore$FileCache:createSpillFile():[INFO] Created file: + tmp
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateDestination(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode):[WARN] DIR* FSDirectory.unprotectedRenameTo: Rename destination " + dst + " is a directory or file under source " + src
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRunApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Fetching flow run apps for user
org.apache.hadoop.fs.s3a.Listing:getListFilesAssumingDir(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Listing$FileStatusAcceptor,org.apache.hadoop.fs.store.audit.AuditSpan):[DEBUG] Recursive list of all entries under {}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:close():[DEBUG] Submitting metrics when file system closed took {} ms., (System.currentTimeMillis() - startTime)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:isEnabled():[INFO] ResourceCalculatorPlugin is unavailable on this system. org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl is disabled.
org.apache.hadoop.service.CompositeService:serviceStart():[DEBUG] CompositeService: starting services, size=...
org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer):[DEBUG] got # + callId
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:closeAllPeers():[INFO] Closing all peers.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateDemand():[DEBUG] The updated fairshare for + getName() + is + getFairShare()
org.apache.hadoop.util.JvmPauseMonitor$Monitor:run():[INFO] formatMessage(extraSleepTime, gcTimesAfterSleep, gcTimesBeforeSleep)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getReservationACLFromAuditConstant(java.lang.String):[ERROR] Audit Constant ${auditConstant} is not recognized.
org.apache.hadoop.hdfs.protocol.ClientProtocol:fsync(java.lang.String,long,java.lang.String,long):[INFO] fsync called in RouterClientProtocol
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map):[ERROR] FSError from child
org.apache.hadoop.tools.DistCp:run(java.lang.String[]):[INFO] Input Options: {context}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[INFO] this starting to offer service
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render():[ERROR] Error getting logs for
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:addVolume(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Added volume - location, StorageType: storageType
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateWorkForOneNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[DEBUG] BLOCK* {}: ask {} to delete {}, getClass().getSimpleName(), dn, toInvalidate
org.apache.hadoop.fs.s3a.impl.DeleteOperation:asyncDeleteAction(java.util.List):[DEBUG] Deleting of {number} file objects
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkCommitInternal(long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,boolean):[DEBUG] getFlushedOffset={} commitOffset={} nextOffset={}
org.apache.hadoop.fs.store.LogExactlyOnce:error(java.lang.String,java.lang.Object[]):[ERROR] format, args
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext,java.lang.String):[DEBUG] renameAsync filesystem: {} source: {} destination: {}
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkAnyParityStreamerIsHealthy():[DEBUG] Skips encoding and writing parity cells as there are no healthy parity data streamers: streamers
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Cluster Status
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String):[WARN] Fatal disk error on Unknown DataNode: ...
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:reencryptEncryptionZone(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction,boolean):[INFO] logAuditEvent
org.apache.hadoop.hdfs.server.namenode.FSImage:purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile):[WARN] Unable to purge old storage + nnf.getName(), e
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runSubtask(org.apache.hadoop.mapred.Task,org.apache.hadoop.mapreduce.v2.api.records.TaskType,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,int,boolean,java.util.Map):[ERROR] CONTAINER_REMOTE_LAUNCH contains a reduce task
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:getMajorAndMinorNumber(java.lang.String):[DEBUG] stat output:{shexec.getOutput()}
org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute():[INFO] Cannot roll key: keyName within KeyProvider: provider.
org.apache.hadoop.mapred.QueueManager:hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Checking access for the acl + toFullPropertyName(queueName, qACL.getAclName()) + for user + ugi.getShortUserName()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMApps(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovered + numApps + applications and + numAppAttempts + application attempts
org.apache.hadoop.hdfs.server.federation.router.RouterFsck:fsck():[WARN] Now FSCK to DFSRouter is unstable feature. There may be incompatible changes between releases.
org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion):[INFO] LoadBalancingKMSClientProvider Decryption successful
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectInputStreams(java.util.Collection,long,boolean,boolean):[WARN] Encountered exception while tailing edits >= ... via RPC; falling back to streaming.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[INFO] Audit event logged
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStart():[DEBUG] Dispatcher registered
org.apache.hadoop.hdfs.DFSInputStream:blockSeekTo(long):[INFO] Will fetch a new encryption key and retry, encryption key was invalid when connecting to ...
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:run():[INFO] Thread.currentThread().getName() + " was interrupted, exiting"
org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String):[INFO] keyInfo.getWarningMessage(name, source)
org.apache.hadoop.tools.dynamometer.ApplicationMaster$LaunchContainerRunnable:run():[INFO] Starting ${isNameNodeLauncher ? "NAMENODE" : "DATANODE"}; track at: http://${container.getNodeHttpAddress()}/node/containerlogs/${container.getId()}/${launchingUser}/
org.apache.hadoop.yarn.service.client.ServiceClient:actionStartAndGetId(java.lang.String):[INFO] Finalize service {serviceName} upgrade
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:getOverallLimits(float):[WARN] The quota calculated for the cgroup was too low. The minimum value is MIN_PERIOD_US, calculated value is quotaUS. Setting quota to minimum value.
org.apache.hadoop.yarn.server.AMRMClientRelayer:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[WARN] ApplicationMaster is out of sync with RM ... for ..., hence resyncing.
org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String):[WARN] Command 'cmd' failed returnVal with: ec.getMessage()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] BLOCK* block RECEIVING_BLOCK: block {} is received from {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed to reload fair scheduler config file - will use existing allocations.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:cleanupRegistryAndCompHdfsDir(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] <componentId>: Deleted component instance dir: <compInstanceDir>
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:updateContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container):[WARN] Exception when scheduling the event of increasing resource of Container container.getId()
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] User before logged in is: <UserGroupInformation.getCurrentUser()>
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:findControllerInMtab(java.lang.String,java.util.Map):[WARN] Skipping inaccessible cgroup mount point %s
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:reInitializeContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerLaunchContext,boolean):[ERROR] Callback handler does not implement container re-initialize callback methods
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:containerBasedPreemptOrKill(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Trying to use {selector.getClass().getName()} to select preemption candidates
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromTcp():[DEBUG] Closed potentially stale remote peer
org.apache.hadoop.nfs.NfsExports:getMatch(java.lang.String):[DEBUG] Using CIDR match for 'host' and READ_ONLY
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:serviceStop():[INFO] closing the hbase Connection
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initializing NodeSortingService= {getName()}
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$MD5Filter:accept(java.lang.Object):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String):[DEBUG] getAdditionalDatanode: src= + src + , fileId= + fileId + , blk= + blk + , existings= + Arrays.asList(existings) + , excludes= + Arrays.asList(excludes) + , numAdditionalNodes= + numAdditionalNodes + , clientName= + clientName
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handleComponentInstanceRelaunch(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent,boolean,java.lang.String):[INFO] builder.toString()
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext):[WARN] Exception get thrown in job commit, retry (<attempt>) time.
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.ReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] Updated reservation using TryManyReservationAgents
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:expectTagEnd(java.lang.String):[ERROR] Expected tag end event for {expected}, but got tag end event for {tag}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:addApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User):[INFO] Skipping activateApplications for application.getApplicationAttemptId() since cluster resource is none
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Initializing AzureBlobFileSystem for {} complete
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser:run():[TRACE] {}: about to release {}, ShortCircuitCache.this, slot
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:deductUnallocatedResource(org.apache.hadoop.yarn.api.records.Resource):[ERROR] Invalid deduction of null resource for + rmNode.getNodeAddress()
org.apache.hadoop.crypto.key.kms.server.KMS:getKey(java.lang.String):[DEBUG] Exception in getKey.
org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration):[INFO] Cleaning up resources
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int):[DEBUG] No space available. Available: + availableSize + MinSize: + minSize
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:addAMNodeToBlackList(org.apache.hadoop.yarn.api.records.NodeId):[INFO] nodeId + is not added to AM blacklist for + applicationAttemptId + , because it has been removed
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer):[DEBUG] Releasing buffer
org.apache.hadoop.http.HttpServer2:openListeners():[DEBUG] opening listeners: {}
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; no changes.
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initDriver():[INFO] Initializing ZooKeeper connection
org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload():[DEBUG] Start datablock[{}] upload, index
org.apache.hadoop.service.AbstractService:notifyListeners():[WARN] Exception while notifying listeners of {}
org.apache.hadoop.mapred.TaskAttemptListenerImpl$TaskProgressLogPair:resetLog(boolean,float,double,long):[INFO] Progress of TaskAttempt ...
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[WARN] Flow activity document encountered a warning
org.apache.hadoop.yarn.server.api.ServerRMProxy:getRMAddress(org.apache.hadoop.yarn.conf.YarnConfiguration,java.lang.Class):[ERROR] Unsupported protocol found when creating the proxy connection to ResourceManager:
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[DEBUG] Saving MD5 file
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[INFO] Refreshing nodes resources
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo):[ERROR] Error while trying to delete history files that could not be moved to done.
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSection(java.io.InputStream,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[INFO] Successfully loaded {} inodes
org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatisticsImpl:bytesReadFromBuffer(long):[DEBUG] Incremented counter StreamStatisticNames.BYTES_READ_BUFFER by {bytes}
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:updateReencryptionFinish(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.protocol.ZoneReencryptionStatus):[INFO] Re-encryption zone marked as completed
org.apache.hadoop.hdfs.server.namenode.LeaseManager:checkLeases(java.util.Collection):[DEBUG] Started block recovery {} lease {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:unsetStoragePolicy(java.lang.String):[DEBUG] *DIR* NameNode.unsetStoragePolicy for path: {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Unknown file for recovering RMDelegationTokenSecretManager
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processMapAttemptLine(org.apache.hadoop.tools.rumen.ParsedLine):[ERROR] A map attempt status you don't know about is "<status value>".
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponentInstances(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List):[INFO] PUT: upgrade component instances {} for service = {} + user = {}
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:completeExecute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] First execution of REST operation - {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource):[INFO] queueName, capacity=this.queueCapacities.getCapacity(), absoluteCapacity=this.queueCapacities.getAbsoluteCapacity(), maxCapacity=this.queueCapacities.getMaximumCapacity(), absoluteMaxCapacity=this.queueCapacities.getAbsoluteMaximumCapacity(), state=getState(), acls=aclsString, labels=labelStrBuilder, reservationsContinueLooking=reservationsContinueLooking, orderingPolicy=getQueueOrderingPolicyConfigName(), priority=priority
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run():[INFO] Finished requesting datanode containers
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:internalGetLabeledResourceRequirementForQueue(java.lang.String,java.lang.String,java.util.Set,java.lang.String):[DEBUG] CSConf - getAbsolueResourcePerQueue: prefix= + getNodeLabelPrefix(queue, label) + , capacity= + resource
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[DEBUG] Checking superuser privilege
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:removeDefaultAcl(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] removeDefaultAcl filesystem: {} path: {}
org.apache.hadoop.yarn.service.provider.ProviderUtils:addLocalResource(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,java.lang.String,org.apache.hadoop.yarn.api.records.LocalResource,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams):[INFO] Added file for localization: symlink -> localResource.getResource().getFile()
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getTimelineDelegationToken():[WARN] Failed to get delegation token from the timeline server: {e.getMessage()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:storeRetryContext():[WARN] Unable to update finishTimeForRetryAttempts in state store for + containerId
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:sendTaskSucceededEvents():[INFO] Task succeeded with attempt + successfulAttempt
org.apache.hadoop.fs.s3a.audit.AuditIntegration:createAndStartAuditManager(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.statistics.impl.IOStatisticsStore):[DEBUG] auditing is disabled
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[DEBUG] Aborting stream {}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor$HeartbeatCallBack:callback(java.lang.Object):[ERROR] Error storing UAM token as AMRMProxy context entry in NMSS for + attemptId, e
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer$TxnBuffer:dumpRemainingEditLogs():[WARN] The edits buffer is + size() + " bytes long with " + numTxns + " unflushed transactions. Below is the list of unflushed transactions:
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.CentralizedOpportunisticContainerAllocator:allocatePerSchedulerKey(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Set,int):[INFO] Not allocating more containers as max allocations per AM heartbeat {} has reached
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] Found {} existing UAMs for application {} in Yarn Registry
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$NMReportedContainerChangeIsDoneTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent):[WARN] Something wrong happened, container size reported by NM is not expected, ContainerID= + container.getContainerId() + rm-size-resource: + rmContainerResource + nm-size-resource: + nmContainerResource
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectMetadataRequest,java.lang.String):[DEBUG] No version ID to use as a constraint
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Nonpositive dircount in invalid READDIRPLUS request: {dirCount}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor:run():[ERROR] Exception in NameNodeResourceMonitor: { exception details }
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:launchServer(java.lang.String[],org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager,org.apache.hadoop.conf.Configuration):[ERROR] Error starting PerNodeTimelineCollectorServer
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.net.URI,java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[ERROR] Server response:\n
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:update(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int,int,int):[DEBUG] LowRedundancyBlocks.update <block_info> curReplicas <curReplicas> curExpectedReplicas <curExpectedReplicas> oldReplicas <oldReplicas> oldExpectedReplicas <oldExpectedReplicas> curPri <curPri> oldPri <oldPri>
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$StartedAfterUpgradeTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] {} received started but cancellation pending
org.apache.hadoop.security.KDiag:validateShortName():[ERROR] KerberosName(principal) failed: %s\n%s
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doAppLogAggregation():[ERROR] Error during log aggregation
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:initializeAuthHandler(java.lang.String,java.util.Properties):[DEBUG] Initializing Authentication handler of type + authHandlerClassName
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getNumUnderConstructionBlocks():[WARN] Failed to find inode {} in getNumUnderConstructionBlocks().
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogsMeta(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[INFO] Closing log reader
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:init(java.util.Properties):[INFO] {} : {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:reloadAllocations():[INFO] Loading allocation file
org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int):[DEBUG] state = {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:setupSchedulingRequest(org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec):[INFO] Scheduling Request made: {sReq.toString()}
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean):[ERROR] got IOException while trying to validate header of + elf + . Skipping., e
org.apache.hadoop.fs.s3a.S3AFileSystem:getObjectMetadata(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.impl.ChangeTracker,org.apache.hadoop.fs.s3a.Invoker,java.lang.String):[DEBUG] HEAD {key} with change tracker {changeTracker}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:dumpSchedulerState():[DEBUG] ... (dumpState output)
org.apache.hadoop.hdfs.server.datanode.DataNode:initDirectoryScanner(org.apache.hadoop.conf.Configuration):[WARN] Periodic Directory Tree Verification scan is disabled because verification is turned off by configuration
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:getLocalNNThreadDump():[INFO] -- Local NN thread dump --
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(java.lang.Object):[DEBUG] {}: Cleaup of directory {} with {}, getName(), baseDir, args
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] Adding query param: UPN
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean):[DEBUG] <applicationId> is recovering. Skip notifying APP_ACCEPTED
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter:writeHistoryData(org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryDataKey,byte[]):[DEBUG] IOUtils cleanup with LOG
org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger:logAuditSimpleFormat(org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$OpStatus,org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$AuditEvent):[INFO] {}[{}] {}
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Dead node {node} is put in maintenance state immediately.
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[]):[INFO] Rss mem usage in bytes <RSS_USAGE>
org.apache.hadoop.tools.GetUserMappingsProtocol:getGroupsForUser(java.lang.String):[INFO] Getting groups for user via RouterRpcServer
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:loadStateFromRegistry(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application {} does not exist in registry
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:getFileStatus(org.apache.hadoop.fs.Path):[TRACE] {}: getFileStatus('{}'), getName(), path
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:removeVolumes(java.util.Collection,boolean):[INFO] Removing StorageLocation {location} with id {uuid} from FsDataset.
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:unregisterApplicationMaster(org.apache.hadoop.yarn.api.records.FinalApplicationStatus,java.lang.String,java.lang.String):[INFO] Interrupted while waiting for application to be removed from RMStateStore.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:updateServiceRecord(org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders,org.apache.hadoop.yarn.api.records.ContainerStatus):[ERROR] Failed to update service record in registry: + containerId +
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:closeBlock():[WARN] delete({}) returned false, bufferFile.getAbsoluteFile()
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetch(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.util.Waitable):[WARN] failed to get ...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:checkIfClusterIsNowMultiRack(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] DN {node} joining cluster has expanded a formerly single-rack cluster to be multi-rack. Not checking for mis-replicated blocks because this NN is not yet processing repl queues.
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:getSpillFileCB(org.apache.hadoop.fs.Path,java.io.InputStream,org.apache.hadoop.conf.Configuration):[DEBUG] getSpillFileCB... Path {}; Pos: {}
org.apache.hadoop.hdfs.server.sps.ExternalSPSFilePathCollector:scanAndCollectFiles(long):[DEBUG] There is no pending items to satisfy the given path inodeId:{}
org.apache.hadoop.yarn.service.utils.HttpUtil:connect(java.lang.String):[DEBUG] Authorization: Negotiate {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=LISTSTATUS target=path
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:fsync(java.lang.String,long,java.lang.String,long):[INFO] BLOCK* fsync: {src} for {clientName}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] In stop, writing event {ev.getType()}
org.apache.hadoop.hdfs.DFSStripedOutputStream:writeChunk(java.nio.ByteBuffer,int,byte[],int,int):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:doFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain):[INFO] This is standby RM. The redirect url is: ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS FSSTAT fileHandle: {} client: {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration):[DEBUG] DataTransferProtocol using SaslPropertiesResolver, configured QOP {} = {}, configured class {} = {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Check assign to queue: + getQueuePath() + nodePartition: + nodePartition + , usedResources: + queueUsage.getUsed(nodePartition) + , clusterResources: + clusterResource + , currentUsedCapacity: + Resources.divide(resourceCalculator, clusterResource, queueUsage.getUsed(nodePartition), labelManager.getResourceByLabel(nodePartition, clusterResource)) + , max-capacity: + queueCapacities.getAbsoluteMaximumCapacity(nodePartition)
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:init(java.lang.String,org.apache.hadoop.mapred.gridmix.JobCreator,boolean):[WARN] Gridmix will not emulate Distributed Cache load because creation of pseudo local file system failed.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String):[DEBUG] Queue {} does not exist, checking parent {}
org.apache.hadoop.yarn.webapp.view.HtmlBlock:render():[DEBUG] Html block rendering initiated
org.apache.hadoop.mapreduce.Cluster:initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration):[INFO] Initializing cluster for Job Tracker=...
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[INFO] Waiting for threadgroup to exit, active threads is {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection):[DEBUG] In memory blockUCState = {ucState}
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveAllKeys(java.io.DataOutputStream,java.lang.String):[INFO] End step SAVING_CHECKPOINT
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Unable to remove application + appAttemptRemovedEvent.getApplicationAttemptID() + ie
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] updating the following ReservationRequest: + contract
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:bindJVMtoJAASFile(java.io.File):[DEBUG] Binding in {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:handle(org.apache.hadoop.yarn.event.Event):[INFO] Initializing application
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean):[INFO] File not found, creating node without parents
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute():[INFO] {keyName} has been successfully deleted. <!-- No log for post-printProviderWritten exception, as exception handling occurs -->
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:setNextDirectoryInputStream():[INFO] File closed: [currentFileName]
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:main(java.lang.String[]):[INFO] systemPropsToLog
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:checkCardinalityAndPending(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,java.util.Optional):[WARN] Failed to query node cardinality: {exception message}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$EntityLogCleaner:run():[INFO] Cleaner interrupted
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close():[ERROR] Interrupted while waiting for SlotReleaserThreadPool to terminate
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renewDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Audit log created for failed operation
org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer):[TRACE] Log trace without throwable
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getLegacyBlockReaderLocal():[DEBUG] {}: can’t construct BlockReaderLocalLegacy because disableLegacyBlockReaderLocal is set.
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:sendOOBResponse(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status):[INFO] Cannot send OOB response + ackStatus + . Responder not running.
org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList):[DEBUG] Processing arguments
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean):[DEBUG] Commit completed without recovery or rate limiting
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:uploadSourceFromFS():[DEBUG] File processed and submitted for upload
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stopAggregators():[WARN] Some logs may not have been aggregated for {appId}
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementContextWithParent(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping,java.lang.String):[WARN] Placement rule specified a parent queue {}, but it does not exist.
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterUtils:fetchColumnsFromFilterList(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList):[INFO] Unexpected filter type
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:checkRpcAdminAccess():[INFO] Allowed RPC access from [ugi] at [Server.getRemoteAddress()]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:initializePreMountedCGroupController(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController):[INFO] Initializing mounted controller ... at ...
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:isParentZnodeSafe(java.lang.String):[ERROR] Invalid data in ZK: + StringUtils.byteToHexString(data)
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreHeartbeat:run():[DEBUG] Sending the heartbeat with capability: {}
org.apache.hadoop.ha.ZKFailoverController:becomeStandby():[INFO] ZK Election indicated that [localTarget] should become standby
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handle(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent):[INFO] Size of event-queue in RMContainerAllocator is {qSize}
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans():[WARN] MBean already initialized!
org.apache.hadoop.yarn.applications.distributedshell.Client:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application has completed successfully. Breaking monitoring loop
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:launchServices(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.service.client.ServiceClient,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Service {} version {} saved.
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String,java.lang.Class):[DEBUG] User {} NN {} is using connection {}
org.apache.hadoop.yarn.server.webapp.LogServlet:getLogFile(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String,boolean):[DEBUG] Exception happened during obtaining NM web address from RM.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:handleContainerExitWithFailure(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.fs.Path,java.lang.StringBuilder):[WARN] Container launch failed : {diagnosticInfo}
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:markZoneStarted(java.lang.Long):[INFO] Zone {} starts re-encryption processing
org.apache.hadoop.ipc.Server$Handler:run():[INFO] Thread.currentThread().getName() caught an exception
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignMapsWithLocality(java.util.List):[DEBUG] Host matched to the request list [host]
org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run():[WARN] No TGT after renewal. Aborting renew thread for {}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:registerSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterRegisterRequest):[INFO] Registered the SubCluster into the StateStore
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing RMDelegationKey_{delegationKey.getKeyId()}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable):[DEBUG] AzureBlobFileSystem.append path: {f.toString()} bufferSize: {bufferSize}
org.apache.hadoop.hdfs.server.namenode.FSImage:saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.hdfs.util.Canceler):[ERROR] Checkpoint failed
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:tryEvict():[TRACE] No buffer eligible for eviction
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String):[WARN] Lock monitoring failed because session was lost
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set owner to {info.getOwnerName()}
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:checkApplicationId(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Missing Application Id. Please try again by specifying an Application Id.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:serviceStart():[INFO] Localizer started on port + server.getPort()
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[ERROR] Error when reading history file of application attempt
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationStartData):[INFO] Opened history file of application X
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:deprecatedConf(org.apache.hadoop.conf.Configuration):[WARN] Configuring \MAPRED_QUEUE_NAMES_KEY\ in mapred-site.xml or hadoop-site.xml is deprecated and will overshadow QUEUE_CONF_FILE_NAME. Remove this property and configure queue hierarchy in QUEUE_CONF_FILE_NAME
org.apache.hadoop.mapred.ReduceTask:runOldReducer(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol,org.apache.hadoop.mapred.Task$TaskReporter,org.apache.hadoop.mapred.RawKeyValueIterator,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class):[DEBUG] Reflecting instance of the reducer class
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshSuperUserGroupsConfiguration():[INFO] Refresh super user groups configuration successful
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics:addAMRuntime(org.apache.hadoop.yarn.api.records.ApplicationId,long,long,long,long):[INFO] e.getMessage()
org.apache.hadoop.fs.cosn.CosNCopyFileTask:run():[WARN] Exception thrown when copy from {} to {}, exception:{}, this.srcKey, this.dstKey, e
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread:run():[DEBUG] Cancelling token [Service_Name]
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[ERROR] Application {appId} does not exist
org.apache.hadoop.tools.HadoopArchiveLogs:checkFilesAndSeedApps(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Skipping [appName] due to existing .har file
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[INFO] stopping existing webapp instance
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.DevicePlugin:getDevices():[INFO] Fetching devices from Nvidia GPU plugin
org.apache.hadoop.hdfs.server.namenode.NNStorage:readAndInspectDirs(java.util.EnumSet,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption):[WARN] Storage directory ... contains no VERSION file. Skipping...
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:internalUpdateLabelsOnNodes(java.util.Map,org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$NodeLabelUpdateOperation):[INFO] op.name() + " labels on nodes:"
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector:putEntitiesAsync(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] putEntitiesAsync(entities={}, callerUgi={})
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:unRegisterNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.UnRegisterNodeManagerRequest):[INFO] Node not found, ignoring the unregister from node id : + nodeId
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Service initialized
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:updateAppDataToStateStore(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,boolean):[ERROR] Statestore update failed for move application ' + app.getApplicationId() + ' to queue ' + queue + ' with below exception: + ex.getMessage()
org.apache.hadoop.yarn.service.utils.SliderFileSystem:deleteComponentsVersionDirIfEmpty(java.lang.String):[INFO] deleted dir {}, path
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:getMapsForHost(org.apache.hadoop.mapreduce.task.reduce.MapHost):[DEBUG] assigned + includedMaps + of + totalSize + to + host + to + Thread.currentThread().getName()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption):[INFO] Node: + nm.getNodeID() + has already been taken out of scheduling. Skip updating its resource
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for dir fileId: {fileId}
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionUpgradeExpress(java.lang.String,java.io.File):[ERROR] Failed to upgrade application: , e
org.apache.hadoop.security.KDiag:main(java.lang.String[]):[ERROR] e.toString()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSCopyFileTask:run():[WARN] Exception thrown when copy from {srcKey} to {dstKey}, exception: {e}
org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration):[INFO] Compressor initialized
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:updateTimelineCollectorContext(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the user in the context: {}, userId
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:deleteCGroup(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController,java.lang.String):[WARN] Unable to delete %s, tried to delete for %d ms
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:expectTag(java.lang.String,boolean):[TRACE] Skipping XMLEvent of type {ev.getEventType()}({ev})
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] Received non-NN/JN request for edits from [remoteHost]
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager$SerializerCompat:saveAllKeys(java.io.DataOutputStream,java.lang.String):[INFO] Set total to size of currentTokens
org.apache.hadoop.fs.GetSpaceUsed$Builder:build():[WARN] Doesn't look like the class [CLASS_NAME] has the needed constructor, [Exception]
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] LastNodeHealthTime
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:getContainerStatusInternal(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.NMTokenIdentifier,java.lang.String):[INFO] Getting container-status for + containerIDStr
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:getCommitterFactory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] Looking for committer factory for path {}
org.apache.hadoop.yarn.service.component.Component:handle(org.apache.hadoop.yarn.service.component.ComponentEvent):[ERROR] [COMPONENT {0}]: Invalid event {1} at {2}, componentSpec.getName(), event.getType(), oldState
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:validate(java.lang.Object):[ERROR] Invalid node attribute(s) from Provider : + e.getMessage()
org.apache.hadoop.hdfs.DFSInputStream:tryReadZeroCopy(int,java.util.EnumSet):[DEBUG] Unable to perform a zero-copy read from offset {curPos} of {src}; 31-bit MappedByteBuffer limit exceeded. blockPos={blockPos}, curEnd={curEnd}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction):[ERROR] Unexpected safe mode action
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] replaying edit log: 1/10 transactions completed. (10%)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] The async write task has no pending writes, fileId: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:storeAttempt():[INFO] Storing attempt: AppId: <placeholder> AttemptId: <placeholder> MasterContainer: <placeholder>
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:removeGlobalCleanerPidFile():[ERROR] Unable to remove the global cleaner pid file! The file may need to be removed manually.
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfigurePropertyImpl(java.lang.String,java.lang.String):[WARN] Exception while sending the block report after refreshing + volumes {} to {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink):[INFO] Registered sink {name}
org.apache.hadoop.hdfs.server.namenode.NNStorage:format(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Formatting storage directory
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode():[DEBUG] from environment variable: [value of System.getenv(MS_INIT_MODE_KEY)]
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[DEBUG] Block analysis status:{} for the file id:{}. Adding to attempt monitor queue for the storage movement attempt finished report.
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Waiting for MissingBlocks to fall below {}...
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:internalAssignGpus(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Trying to assign %d GPUs to container: %s, #AvailableGPUs=%d, #ReleasingGPUs=%d
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:updateBlacklist(java.util.List,java.util.List):[WARN] The same resources appear in both blacklistAdditions and blacklistRemovals in updateBlacklist.
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest):[ERROR] YarnException ex
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:refreshCallQueue():[INFO] Refresh call queue unsuccessfully for {hostport}
org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer:getFilterHandlers(org.apache.hadoop.conf.Configuration):[ERROR] Failed to initialize handler {class}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:hasAccess(org.apache.hadoop.crypto.key.kms.server.KMSACLs$Type,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Checking user [{}] for: {} {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:truncate(java.lang.String,long,java.lang.String,java.lang.String,long):[DEBUG] DIR* NameSystem.truncate: src={} newLength={}
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:renewDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.RenewDelegationTokenRequest):[DEBUG] Checking if the delegation token operation is allowed
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeDirectorySectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String):[INFO] Completed loading all INodeDirectory sub-sections
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[DEBUG] Security login
org.apache.hadoop.hdfs.DFSOutputStream:completeFile():[INFO] Unable to close file because dfsclient was unable to contact the HDFS servers. clientRunning {...} hdfsTimeout {...}
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:newBlockReader(org.apache.hadoop.hdfs.client.impl.DfsClientConf,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long,org.apache.hadoop.fs.StorageType):[DEBUG] New BlockReaderLocalLegacy for file {} of size {} startOffset {} length {} short circuit checksum {}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean):[ERROR] No node in path [ + nodePath + ]
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Using localizerTokenSelector.
org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node):[ERROR] Error: can’t add leaf node {} at depth {} to topology:{}\n
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadKeys(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState):[WARN] Skipping unexpected file in history server token state: {filePath}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:copyFile(java.lang.String,long,java.lang.String):[DEBUG] Exception thrown when copy file
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter():[INFO] Now rescanning bpid {} on volume {}, after more than {} hour(s)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:triggerActiveLogRoll():[WARN] Unable to finish rolling edits in %d ms
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Unhealthy Nodes: countHere
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:dump():[DEBUG] Start dump. Before dump, nonSequentialWriteInMemory == {}
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:commitBeforeRead(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long):[ERROR] Should not get commit return code: {ret.name()}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getAclStatus(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.getAclStatus path: {}
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:deleteHistoryResourceSkyline(java.lang.String,java.lang.String):[INFO] Delete ResourceSkyline for recurrenceId: {}.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:loadDfsUsed():[WARN] mtime not found in file:{}, will proceed with Du for space computation calculation
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:computeQueueManagementChanges(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue):[DEBUG] Updated queue management changes for parent queue {}: [{}]
org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite):[DEBUG] Codec classes obtained
org.apache.hadoop.ipc.Server$RpcCall:run():[DEBUG] "Deferring response for callId: " + this.callId
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:getProxyInternal(boolean):[INFO] Failing over to the ResourceManager for SubClusterId: ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeApplicationAttemptInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[DEBUG] Removing state for attempt {} at {}
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:updateReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation):[ERROR] Unable to replace reservation: {} from plan.
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovered + numKeys + RM delegation token master keys
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS FSINFO fileHandle: {} client: {}
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[WARN] Failure killing {jobName}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long):[INFO] Loaded edits file(s)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:loadToken(org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService$HistoryServerState,org.apache.hadoop.fs.Path,long):[INFO] Cleanup complete
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long):[TRACE] startMaintenance: Node {node} in {node.getAdminState()}, nothing to do.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] postComplete for container: + containerId.toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:checkDockerVolumeCreated(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Docker volume-name= + volumeName + driver-name= + driverName + already exists for container= + container.getContainerId() + , continue...
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlows(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL <url> (Took <latency> ms.)
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateAudiences(com.nimbusds.jwt.SignedJWT):[DEBUG] JWT token audience has been successfully validated
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getWorkOutputPath(org.apache.hadoop.mapreduce.TaskInputOutputContext):[DEBUG] Work path is {}
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:updateStateStore():[DEBUG] Router heartbeat for router {}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using mapred newApiCommitter.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendContainerMonitorStartEvent():[DEBUG] Getting Resource Memory Size
org.apache.hadoop.yarn.server.webapp.WebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] Application report retrieved and AppInfo created
org.apache.hadoop.hdfs.server.datanode.DataStorage:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Finalizing upgrade for storage directory {}.\\n cur LV = {}; cur CTime = {}
org.apache.hadoop.registry.cli.RegistryCli:analyzeException(java.lang.String,java.lang.Exception,java.util.List):[DEBUG] Operation {} on path {} failed with exception {}, operation, pathArg, e, e
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest):[INFO] Instantiated application attempt report
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Completing previous rollback for storage directory {}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,int):[ERROR] prefix: [{}], delimiter: [{}], maxListingLength: [{}], priorLastKey: [{}]. List objects occur an exception: [{}].
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream):[ERROR] Can not find log metadata for any containers on {nodeId}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:finish():[INFO] Exception thrown in thread join:
org.apache.hadoop.mapreduce.task.reduce.Fetcher:openShuffleUrl(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.util.Set,java.net.URL):[WARN] Connection rejected by the host {host}. Will retry later.
org.apache.hadoop.yarn.applications.distributedshell.Client:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Application did not finish. YarnState=?, DSFinalStatus=?. Breaking monitoring loop
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS SYMLINK, target: {symData} link: {linkIdPath} namenodeId: {namenodeId} client: {remoteAddress}
org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile():[WARN] Error reading the stream [IOException]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:appendFile(java.lang.String,java.lang.String,java.lang.String,java.util.EnumSet,boolean):[INFO] Audit event logged
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[INFO] Application {appId} already submitted on SubCluster {subClusterId}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Speculator event handled
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short):[INFO] Checking invocation result
org.apache.hadoop.fs.cosn.ByteBufferWrapper:munmap(java.nio.MappedByteBuffer):[TRACE] CleanerUtil.UNMAP_NOT_SUPPORTED_REASON
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:normalizeNodeLabelExpressionInRequest(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.QueueInfo):[DEBUG] Setting default node label expression : {queueInfo.getDefaultNodeLabelExpression()}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getMissingLogSegments(java.util.List,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto,org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy):[ERROR] Aborting current sync attempt.
org.apache.hadoop.yarn.service.client.ServiceClient:initiateUpgrade(java.lang.String,java.lang.String,boolean):[ERROR] Service {} upgrade to version {} failed because {}
org.apache.hadoop.hdfs.protocol.ClientProtocol:delete(java.lang.String,boolean):[ERROR] Cannot delete non-empty directory without recursive flag
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getReportsForAllDataNodes():[TRACE] Retrieval of slow peer report for all nodes is disabled. To enable it, please enable config {}.
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:start(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode):[INFO] Starting {} StoragePolicySatisfier.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing {} of type {}
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:saveDirectives(java.io.DataOutputStream,java.lang.String):[INFO] Total size set
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:getLatestImages():[ERROR] Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshSuperUserGroupsConfiguration():[INFO] Refresh super user groups configuration successful for proxy.getAddress()
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:bindToFileSystem(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] No resilient commit support under path {}
org.apache.hadoop.hdfs.server.datanode.DataNode:initStorage(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Setting up storage: nsid={};bpid={};lv={};nsInfo={};dnuuid={}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromTcp():[TRACE] {}: trying to create a remote block reader from a TCP socket
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[INFO] Phase initialized for map (100%)
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCachedBlockMap():[DEBUG] Block {}: removing from PENDING_CACHED for node {} because it cannot fit in remaining cache size {}.
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:logCurrentHadoopUser():[INFO] Current user = {}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[DEBUG] {}: Saving _SUCCESS file to {}
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createUserCacheDirs(java.util.List,java.lang.String):[INFO] Initializing user {}
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateRenameSource(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.util.List):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename source {getPath} is not found.
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleLaunchForLaunchType(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext,org.apache.hadoop.yarn.api.ApplicationConstants$ContainerLaunchType):[ERROR] ResourceHandlerChain.preStart() failed!
org.apache.hadoop.yarn.service.utils.ServiceUtils:findContainingJar(java.lang.Class):[INFO] could not locate JAR containing {} URL={}
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:run():[WARN] [COMPONENT {}] Current health {}% is below health threshold of {}% for {} secs (threshold window = {} secs)
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String,java.lang.String[],java.lang.String,boolean,boolean,org.apache.hadoop.hdfs.server.federation.resolver.order.DestinationOrder,org.apache.hadoop.hdfs.tools.federation.RouterAdmin$ACLEntity):[INFO] Mount point added successfully
org.apache.hadoop.ipc.Client$Connection:run():[WARN] Unexpected error reading responses on connection
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[WARN] BR lease 0x{} is not valid for DN {}, because the DN is not in the pending set.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:reencryptEncryptionZoneInt(org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction,boolean):[INFO] Re-encryption using key version
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:safetyDance():[ERROR] failed to load misc.Unsafe, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[WARN] logWarningWhenAuxServiceThrowExceptions during APPLICATION_INIT
org.apache.hadoop.fs.store.LogExactlyOnce:warn(java.lang.String,java.lang.Object[]):[WARN] log.WARN:format,args
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[WARN] Found jobId {toClose} to have not been closed. Will close
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest):[ERROR] Unable to get the application report for {ApplicationId} to SubCluster {SubClusterId}
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context):[INFO] Spec output bytes w/o records. Using input record count
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DiagnosticInformationUpdater:transition(java.lang.Object,java.lang.Object):[INFO] Diagnostics report from taskAttempt.attemptId: diagEvent.getDiagnosticInfo()
org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration):[ERROR] DtFetcher for service 'service' does not allow aliasing. Cannot apply alias 'alias'. Drop alias flag to get token for this service.
org.apache.hadoop.mapred.Merger$MergeQueue:merge(java.lang.Class,java.lang.Class,int,org.apache.hadoop.fs.Path,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress):[INFO] Down to the last merge-pass, with N segments left of total size: M bytes
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:rollEdits():[DEBUG] Checking operation WRITE
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgradeExpress(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] Service {} express upgrade to version {} failed because {}
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Sorry, no counters for nonexistent job
org.apache.hadoop.lib.server.Server:initLog():[WARN] Log4j [{}] configuration file not found, using default configuration from classpath
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:processSingleResource(org.apache.hadoop.fs.FileStatus):[INFO] Found a renamed directory that was left undeleted at + path.toString() + . Deleting.
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:checksumBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock):[DEBUG] Got access token error in response to OP_BLOCK_CHECKSUM
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:selectTokenFromFSOwner():[INFO] Token lookup completed
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown():[DEBUG] refCount= + refCount
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:rmDockerContainerDelayed():[DEBUG] Docker container deletion initiated
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[DEBUG] Finished events sent
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:completeContainerLaunch():[DEBUG] Completed setting up container command {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType):[INFO] Audit log: Operation successful
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:sigKill(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Terminating container {containerId} Sending SIGKILL to -{pid}
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEvent):[ERROR] Unable to locate user for + appId
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:runDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when executing command, command={dockerVolumeCommand}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:createNewQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue,java.util.List):[ERROR] Can't create queue ' + queueName + ', the child scheduling policy is not allowed by parent queue!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:waitVolumeRemoved(int,java.util.concurrent.locks.Condition):[DEBUG] Waiting for volume reference to be released.
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path):[INFO] Changing the permissions for inputPath {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo,java.lang.String):[WARN] Error closing the stream
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:assignDevices(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not get valid [resourceName] device for container '[containerId]' as some other containers might not releasing them.
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Completed reading history information of container
org.apache.hadoop.service.CompositeService:serviceStop():[DEBUG] getName() + ": stopping services, size=" + numOfServicesToStop
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition:transition(java.lang.Object,java.lang.Object):[WARN] Job init failed
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Job not successful!
org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean):[DEBUG] Initiating logout for {username}
org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader:readDataChecksum(java.io.DataInputStream,java.lang.Object):[WARN] Unexpected meta-file version for + name + : version in file is + header.getVersion() + but expected version is + VERSION
org.apache.hadoop.fs.s3a.WriteOperationHelper:operationRetried(java.lang.String,java.lang.Exception,int,boolean):[INFO] {}: Retried {}: {}
org.apache.hadoop.hdfs.DataStreamer:createSocketForPipeline(org.apache.hadoop.hdfs.protocol.DatanodeInfo,int,org.apache.hadoop.hdfs.DFSClient):[DEBUG] Connecting to datanode {}
org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget):[WARN] "Unable to gracefully make {} standby (unable to connect)"
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:createNewApplication(javax.servlet.http.HttpServletRequest):[DEBUG] Getting interceptor chain
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$LaunchContainerRunnable:run():[ERROR] Not able to add suffix (.bat/.sh) to the shell script filename
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:createTokenIdentifier(java.util.Optional,org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text):[ERROR] Cannot issue delegation tokens because the credential providers listed in ${DELEGATION_TOKEN_CREDENTIALS_PROVIDER} are returning session tokens
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[TRACE] Block {}: added to PENDING_CACHED on DataNode {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:sendLaunchEvent():[INFO] Recovering previously paused container
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler:call():[TRACE] Scanner volume report: {null}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,java.util.List,boolean):[INFO] {}: no pending commits to abort
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[DEBUG] Emitting metric + name +, type + type +, value + value +, slope + gSlope.name() + from hostname + getHostName()
org.apache.hadoop.yarn.service.ServiceScheduler:buildInstance(org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.conf.Configuration):[INFO] Timeline v2 is enabled.
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,long):[ERROR] Unable to store token
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] removeAclEntries filesystem: {} path: {} aclSpec: {}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Completed: countHere
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:startResourceEstimatorApp():[DEBUG] Building HTTP server
org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Bypassing cache to create filesystem {}
org.apache.hadoop.yarn.csi.client.CsiClient:nodePublishVolume(csi.v0.Csi$NodePublishVolumeRequest):[INFO] Node publish volume request initiated
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanupContainersOnNMResync():[INFO] Containers still running on ON_NODEMANAGER_RESYNC : {containers.keySet()}
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:getDirectoryPolicy(org.apache.hadoop.conf.Configuration,java.util.function.Predicate):[INFO] Directory markers will be kept on authoritative paths
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recoverContainer(org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$RecoveredContainerState):[INFO] Recovering {containerId} in state {rcs.getStatus()} with exit code {rcs.getExitCode()}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[DEBUG] {}: attempt path is {}
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget):[WARN] Fencing method + method + was unsuccessful.
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:addResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.client.api.AMRMClient$ContainerRequest,boolean,java.lang.String):[DEBUG] addResourceRequest: applicationId= priority= priority.getPriority() resourceName= resourceName numContainers= resourceRequestInfo.remoteRequest.getNumContainers() #asks= ask.size()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCorruptFilesList():[WARN] Get corrupt file blocks returned error
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshUserToGroupsMappings():[INFO] Refresh user to groups mapping successful
org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String):[INFO] capacity = 2^{exponent} = {c} entries
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:doTailEdits():[DEBUG] Loaded edits starting from txid
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[INFO] No nodes found to update, configuration is up-to-date
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration):[INFO] {}={} min(s), {}={} min(s), {}={}
org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlocks(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[WARN] Reporting bad block
org.apache.hadoop.mapred.nativetask.StatusReportChecker:run():[DEBUG] StatusUpdater thread exiting since it got interrupted
org.apache.hadoop.yarn.applications.distributedshell.Client:run():[ERROR] Unknown resource profile
org.apache.hadoop.tools.HadoopArchiveLogs:filterAppsByAggregatedStatus():[INFO] appId not in the ResourceManager
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:main(java.lang.String[]):[INFO] Startup Shutdown message
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:close():[DEBUG] Closing cache
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[WARN] Fsck: there were errors copying the remains of the corrupted file {fullName} to /lost+found
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List):[DEBUG] Saved INodeReference ids of size {}.
org.apache.hadoop.fs.Globber:doGlob():[DEBUG] Filesystem glob {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[DEBUG] Change nextOffset to {}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:failDestinationExists(org.apache.hadoop.fs.Path,java.lang.String):[INFO] {...}: { dir|file size {...} bytes}
org.apache.hadoop.fs.azurebfs.services.AbfsIoUtils:dumpHeadersToDebugLog(java.lang.String,java.util.Map):[DEBUG] {}={}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:rename(java.lang.String,java.lang.String):[ERROR] Cannot rename {} to {}, src, dst, e
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier):[INFO] Exits the main loop.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$OutOfOrderTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown from onStartContainerError for Container event.getContainerId(), thr
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameserviceAggregatedInt(java.util.function.ToIntFunction):[ERROR] Unable to extract metrics: {IOException message}
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext):[INFO] Build file listing completed.
org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse():[WARN] Detailed error code not set by server on rpc error
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[INFO] Insecure cluster detected
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:pushToZK(byte[],byte[],byte[]):[DEBUG] Unable to push to znode; another server already did it
org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Looking for a token with service {}
org.apache.hadoop.streaming.PipeMapRed:configure(org.apache.hadoop.mapred.JobConf):[INFO] PipeMapRed exec [*argvSplit values*]
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:mountCGroupController(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController):[INFO] Mounting controller controller.getName() at requestedMountPath
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:recover():[INFO] Node label store recover is completed
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:wipeDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] DatanodeManager.wipeDatanode(node): storage key is removed from datanodeMap.
org.apache.hadoop.hdfs.server.datanode.DataXceiver:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String):[WARN] Not able to start
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long):[DEBUG] Error while connecting to namenode
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] listStatus: doing listObjects for directory {key}
org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper():[INFO] New WatcherWithClientRef instance created
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:registerMBean(java.lang.String):[INFO] Registered FSDatasetState MBean
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitShm(java.lang.String):[INFO] cliID: ..., src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: ..., srvID: ..., success: true
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppKilledTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] RMAppImpl audit log kill event
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:run():[INFO] Namesystem is not running, skipping decommissioning/maintenance checks.
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier,int):[INFO] Exits the main loop.
org.apache.hadoop.lib.server.Server:initConfig():[WARN] Default configuration file not available in classpath [{defaultConfig}]
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateSignature(com.nimbusds.jwt.SignedJWT):[DEBUG] JWT token has been successfully verified
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[INFO] No eligible applications to process
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:getTempFile(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Creating temp file: {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] AzureBlobFileSystem.mkdirs path: {} permissions: {}
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveMetadata(java.lang.String):[DEBUG] Retrieving metadata for {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:makeDoneSubdir(org.apache.hadoop.fs.Path):[INFO] Perms after creating (permission) , Expected: (expected_permission)
org.apache.hadoop.lib.server.Server:setService(java.lang.Class):[ERROR] Could not destroy service [{}], {}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[]):[WARN] Error processing datanode Command, ioe
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renamePath(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext,java.lang.String,boolean,boolean):[DEBUG] Retrieved etag of source for rename recovery: {}; isDir={}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFree():[DEBUG] Failed to get remaining capacity
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$InitContainerTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] Killing <containerId> because <app> is in state <appState>
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData:newProxy(org.apache.hadoop.yarn.ipc.YarnRPC,java.lang.String,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Token):[DEBUG] Opening proxy : {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Error in handling event type [event.getType()] for applicationAttempt [appAttemptId]
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logModifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[INFO] logRpcIds invoked
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int):[ERROR] Error retrieving key [...] from ZK
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[DEBUG] Write lock released
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String):[DEBUG] {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Updating RMDelegationToken and SequenceNumber , e
org.apache.hadoop.registry.server.dns.ReverseZoneUtils:getSubnetCountForReverseZones(org.apache.hadoop.conf.Configuration):[ERROR] The supplied range is not a valid integer: Supplied range:
org.apache.hadoop.hdfs.server.datanode.DataXceiver:run():[INFO] Failed to read expected encryption handshake from client at ...
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat):[DEBUG] StatNode result: + rc + for path: + path + connectionState: + zkConnectionState + for + this
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Trying to merge the ranges as they are not disjoint
org.apache.hadoop.mapred.lib.MultithreadedMapRunner:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[DEBUG] Finished dispatching all Mappper.map calls, job {jobName}
org.apache.hadoop.mapreduce.JobSubmitter:submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster):[WARN] Max job attempts set to 1 since encrypted intermediate data spill is enabled
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:uploadFileToPendingCommit(java.io.File,org.apache.hadoop.fs.Path,java.lang.String,long,org.apache.hadoop.util.Progressable):[DEBUG] Upload staged file from {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Container {} transitioned from {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor:runDiagnose(java.lang.String,int):[DEBUG] {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:dynamicallyUpdateAppActivitiesMaxQueueLengthIfNeeded():[INFO] Update max queue length of app activities from {} to {} when multi-node placement enabled.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] getQueuePath() + used= + queueUsage.getUsed() + numContainers= + numContainers + user= + userName + user-resources= + user.getUsed()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:initializeLocalDir(org.apache.hadoop.fs.FileContext,java.lang.String):[WARN] Could not set permissions for local dir {key}, ie
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:addReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation,boolean):[ERROR] Unable to add reservation: ... to plan.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:init(org.apache.hadoop.conf.Configuration):[INFO] Available numa nodes with capacities : %d
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:createAllDirectories(java.util.Collection):[INFO] Preparing {createCount} directory/directories; {parents.size()} parent dirs implicitly created
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[ERROR] [STRESS] Error while submitting the job
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.LogHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEvent):[DEBUG] Handling log event in NonAggregatingLogHandler
org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry):[INFO] Add regex mount point: <src>, target: <target>, interceptor settings: <settings>
org.apache.hadoop.mapred.ClientServiceDelegate:invoke(java.lang.String,java.lang.Class,java.lang.Object):[DEBUG] Failed to contact AM/History for job retrying..
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Copy failed from: fromPath to done location: toPath
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:serviceStop():[INFO] Waiting for deletion thread to complete its current action
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:doSingleWrite(org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx):[DEBUG] do write, fileHandle {} offset: {} length: {} stableHow: {}, handle.dumpFileHandle(), offset, count, stableHow.name()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:doRelaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,int,int):[INFO] Relaunching Container {}. retry interval {} ms
org.apache.hadoop.yarn.server.router.Router:serviceStart():[ERROR] Failed Router login
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short):[DEBUG] Concurrent invocation on src
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Queue x already has n applications from user z, cannot accept submission of application: y
org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] SegmentContainer initialized
org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object):[WARN] IOStatistics: {}
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class):[INFO] Initialized Federation proxy for user: {user}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:save(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSImageCompression):[INFO] Saving image file {} using {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS RENAME from: {}/{} to: {}/{} client: {}
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineClientConnectionRetry:logException(java.lang.Exception,int):[INFO] Exception caught by TimelineClientConnectionRetry, will try {leftRetries} more time(s). Message: {e.getMessage()}
org.apache.hadoop.yarn.server.timeline.security.TimelineDelgationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline delegation token secret manager created
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:handleInteraction(org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter$HttpInteraction):[TRACE] Proceeding with interaction
org.apache.hadoop.hdfs.tools.DFSAdmin:getVolumeReport(java.lang.String[],int):[INFO] \nDatanode Volume Report
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Failed to delete tc rule for classId: + classId
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:saveDelegationToken(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path):[ERROR] Failed to fetch token from {fs.getUri()}
org.apache.hadoop.service.AbstractService:start():[DEBUG] Service {} is started
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:logMutation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore$LogMutation):[INFO] Serialized logMutation
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Unknown apps will be treated as complete after {} seconds
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] allocate: applicationId=applicationAttemptId #ask=ask.size()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Removing token master key at {}
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:newProtoClient(java.lang.Class,org.apache.hadoop.hdfs.server.federation.router.ConnectionPool$ProtoImpl,java.lang.Object):[ERROR] e.getMessage()
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer:handle(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[ERROR] Can't handle this event at current state, e
org.apache.hadoop.tools.dynamometer.ApplicationMaster:cleanup():[INFO] Application completed. Stopping running containers
org.apache.hadoop.yarn.sls.SLSRunner:start():[INFO] Resource Manager started
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:addSpeculativeAttempt(org.apache.hadoop.mapreduce.v2.api.records.TaskId):[INFO] DefaultSpeculator.addSpeculativeAttempt -- we are speculating + taskID
org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded():[WARN] Expected split length of sysInfo to be 11. Got ...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:moveLazyPersistReplicasToFinalized(java.io.File):[WARN] Failed to move meta file from + metaFile + to + targetMetaFile, e
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:receivedNewWriteInternal(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider):[INFO] Have to change stable write to unstable write: STABLE_HOW_PLACEHOLDER
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Exception while loading allocation file:
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[DEBUG] Closing stream
org.apache.hadoop.hdfs.server.namenode.FSDirectory:normalizePaths(java.util.Collection,java.lang.String):[ERROR] {} ignoring reserved path {}
org.apache.hadoop.mapred.Mapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter):[DEBUG] Entering map method
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop():[INFO] closing the flow run table
org.apache.hadoop.mapred.YARNRunner:setTokenRenewerConf(org.apache.hadoop.yarn.api.records.ContainerLaunchContext,org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Send configurations that match regex expression: + regex + , total number of configs: + count + , total size : + dob.getLength() + bytes.
org.apache.hadoop.crypto.key.kms.server.KMS:createKey(java.util.Map):[TRACE] Exiting createKey Method.
org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler:handle(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException):[WARN] Reporting bad {block} on {volume}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:computeIgnoreBlacklisting():[INFO] KnownNode Count at 0. Not computing ignoreBlacklisting
org.apache.hadoop.tools.rumen.ParsedTask:dumpParsedTask():[INFO] ... (l.getLayers() + ";" + l.toString()) ...
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:activate(org.apache.hadoop.conf.Configuration):[WARN] Please update your configuration to use {} instead.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long):[DEBUG] Watchdog interrupted
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:getAppPriority(javax.servlet.http.HttpServletRequest,java.lang.String):[DEBUG] Retrieving priority for app: {appId}
org.apache.hadoop.mapred.MapReduceChildJVM:getVMCommand(java.net.InetSocketAddress,org.apache.hadoop.mapred.Task,org.apache.hadoop.mapred.JVMId):[DEBUG] Configuring job jar
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] 'Authorization' does not start with 'Negotiate' : {authorization}
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set):[INFO] Got application report from ASM for, appId=..., appAttemptId=..., clientToAMToken=..., appDiagnostics=..., appMasterHost=..., appQueue=..., appMasterRpcPort=..., appStartTime=..., yarnAppState=..., distributedFinalState=..., appTrackingUrl=..., appUser=...
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Task attempt finished event logged
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getContentSummary(java.lang.String):[ERROR] Cannot get content summary for mount {}: {}
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:handleRedirect(java.lang.String,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[ERROR] REDIRECT: sending redirect to redirect
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSTrashRoot operation executed
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:preemptOrkillSelectedContainerAfterWait(java.util.Map,long):[DEBUG] Send to scheduler: in app= + appAttemptId + #containers-to-be-preemptionCandidates= + e.getValue().size()
org.apache.hadoop.hdfs.tools.DFSck:listCorruptFileBlocks(java.lang.String,java.lang.String):[INFO] Failed to open path 'dir': Permission denied
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:doCopy(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet):[INFO] Completed writing {target} ({bytesRead} bytes)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:storeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing master key
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:createBlockIfNeeded():[ERROR] Number of partitions in stream exceeds limit for S3: + Constants.MAX_MULTIPART_COUNT + write may fail.
org.apache.hadoop.hdfs.NameNodeProxiesClient:createHAProxy(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider):[DEBUG] getNNAddressCheckLogical
org.apache.hadoop.mapred.BackupStore:write(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.DataInputBuffer):[INFO] Written to memory cache
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Completed reading history information of application + appId
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:callCOSClientWithRetry(java.lang.Object):[ERROR] Call cos sdk failed, retryIndex: [max / max], call method: uploadPart, exception: ...
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Authenticating with dt param: delegationParam
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$CancelUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] {} pending cancellation
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext):[WARN] An exception occurred during the cleanup of localizer job
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:becomeActive():[ERROR] RM could not transition to Active
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:getAllowedLocalityLevelByTime(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,long,long,long):[TRACE] Waiting time: {waitTime} ms, nodeLocalityDelay time: {nodeLocalityDelayMs} ms, change allowedLocality from NODE_LOCAL to RACK_LOCAL, priority: {}, app attempt id: {}
org.apache.hadoop.mapred.uploader.FrameworkUploader:getSmallestReplicatedBlockCount():[INFO] Replication counts offset:%d blocks:%d
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] modifyAclEntries filesystem: {} path: {} aclSpec: {}
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[INFO] Phase initialized for map (67%) and sort (33%)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:leaveSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.LeaveSafeModeRequest):[INFO] STATE* Safe mode is OFF.\n + It was turned off manually.
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:forceKillApplication(org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest):[ERROR] No response when attempting to kill the application applicationId to SubCluster subClusterIdId
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]):[ERROR] Fencing is not configured for localTarget.\nYou must configure a fencing method before using automatic failover.
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:chooseFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[ERROR] Cannot get local namespace for {}, clientAddr
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Incremented write operations count
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] User after logged in is: <UserGroupInformation.getCurrentUser()>
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String):[INFO] Error report from + dnName + : + msg
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[WARN] Got IOException {}; returned false
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult):[WARN] Couldn’t delete State Store record {}: {}
org.apache.hadoop.http.HttpServer2:bindListener(org.eclipse.jetty.server.ServerConnector):[INFO] Jetty bound to port
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:renew():[DEBUG] Lease renewed for client {}
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:execute(java.lang.Runnable):[DEBUG] Current active thread number: + executor.getActiveCount() + queue size: + executor.getQueue().size() + scheduled task number: + executor.getTaskCount()
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[ERROR] Can't handle this event at current state, {e}
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:writeAppendBlobCurrentBufferToService():[INFO] Upload successful
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:loadAMRMProxyState():[WARN] Unknown key ..., remove and move on
org.apache.hadoop.yarn.webapp.WebApps$Builder:start(org.apache.hadoop.yarn.webapp.WebApp):[INFO] Web app + name + started at + httpServer.getConnectorAddress(0).getPort()
org.apache.hadoop.fs.http.client.HttpFSFileSystem:getTrashRoot(org.apache.hadoop.fs.Path):[WARN] Cannot find trash root of + fullPath, ex
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,boolean,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey):[DEBUG] AM resource request: + amAsk.getPerAllocationResource() + exceeds maximum AM resource allowed, + getQueue().dumpState()
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.event.Event):[WARN] Unexpected volume event received, event type is EVENT_TYPE, but the volumeId is null.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[DEBUG] Allocation proposal accepted={}
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceStart():[DEBUG] ZooKeeper config:\n
org.apache.hadoop.util.ThreadUtil:joinUninterruptibly(java.lang.Thread):[WARN] interrupted while sleeping
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:getDecodedPlacementSpec(java.lang.String):[INFO] Decode placement spec: + decodedSpec
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber:run():[INFO] LazyPersistFileScrubber was interrupted, exiting
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean):[DEBUG] passing over + elf + because it ends at + elf.lastTxId + , but we only care about transactions + as new as + fromTxId
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:doRefreshNamenodes(java.util.Map,java.util.Map):[INFO] Stopping BPOfferServices for nameservices: ...
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskAttemptFetchFailureTransition:transition(java.lang.Object,java.lang.Object):[INFO] Too many fetch-failures for output of task attempt: {mapId} ... raising fetch failure to map
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector:preemptFromLeastStarvedApp(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map):[DEBUG] Skipping container: {} with resource:{} as UserLimit for user:{} with resource usage: {} is going under UL
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:resolve(org.apache.hadoop.mapred.TaskCompletionEvent):[INFO] Ignoring output of failed map TIP: '<eventTaskId>'
org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[]):[WARN] Cannot locate configuration: tried {files}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onContainerStatusReceived(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerStatus):[INFO] Promoting container {} to {}
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int):[ERROR] Exception in generateEncryptedKeys:
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failure during secure login
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter:handle(com.sun.jersey.api.client.ClientRequest):[DEBUG] Jersey retry operation initialized
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:requestNextBatch():[DEBUG] All entries in batch were filtered...continuing
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String):[ERROR] Unable to free lease on
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader:getSelectQuery():[ERROR] Could not find the clause substitution token ...
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest):[ERROR] User doesn't have permissions to MODIFY_APP
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:serviceStop():[INFO] Skipping cleaning up the staging dir. assuming AM will be retried.
org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String):[INFO] name not found
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:loadPolicyConfiguration(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade):[INFO] No policy configured for default queue {} in StateStore, fallback to local config
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:transitionToStandby(boolean):[INFO] Transitioned to standby state
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:list(java.lang.String,java.lang.String,int,java.lang.String):[DEBUG] List objects. prefix: [{}], delimiter: [{}], maxListLength: [{}], priorLastKey: [{}].
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStart():[INFO] Recovery started
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:doRelaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,int,int):[INFO] Relaunching Container {}. remaining retry attempts(after relaunch) {}, retry interval {} ms
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:validateJobDef(org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer$JobDefinition):[WARN] Error converting old JobDefinition format
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[WARN] No Output path found for {attemptId}
org.apache.hadoop.fs.s3a.WriteOperationHelper:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String):[DEBUG] SelectBinding toString: {}
org.apache.hadoop.tools.rumen.Folder:initialize(java.lang.String[]):[WARN] This run effectively has a -seed of randomSeed
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(java.lang.Object):[DEBUG] Pushing record
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[INFO] {}: uploading from staging directory to S3 {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:createNewReservation(javax.servlet.http.HttpServletRequest):[ERROR] Unable to create new reservation from RM web service
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:updateActiveUsersResourceUsage(java.lang.String):[DEBUG] User ' + userName + ' has become active. Hence move user to active list. Active users size = + activeUsersSet.size() + Non-active users size = + nonActiveUsersSet.size() + Total Resource usage for active users= + totalResUsageForActiveUsers.getAllUsed() + . Total Resource usage for non-active users= + totalResUsageForNonActiveUsers.getAllUsed()
org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable):[WARN] Caught exception in thread [current thread name] + : [t]
org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils:authorizeRequest():[WARN] Got exception while looking for AMRMToken for user
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:preCommitJob(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit):[INFO] {}: removed output path to be replaced: {}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:moveToDone():[INFO] No file for job-history with + jobId + found in cache!
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:umntall(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[DEBUG] MOUNT UMNTALL : client: + client
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:nodeHeartbeat(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[INFO] Received duplicate heartbeat from node + rmNode.getNodeAddress() + responseId= + remoteNodeStatus.getResponseId()
org.apache.hadoop.hdfs.server.datanode.DataNode:handleDiskError(java.lang.String,int):[WARN] DataNode is shutting down due to failed volumes: [{}]
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:pauseForTesting():[INFO] Pausing re-encrypt updater for testing.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:markContainerForPreemption(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] {}: appAttempt:{} container:{}, SchedulerEventType.MARK_CONTAINER_FOR_PREEMPTION, aid, cont
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String):[DEBUG] Storing state for reservation {reservationIdName} plan {planName} at {key}
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getApps(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set,java.util.Set,java.lang.String,java.util.Set):[INFO] Applications fetched
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Queue Management Change event cannot be applied for parent queue : + parentQueue.getQueuePath()
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Error: invalid data in znode
org.apache.hadoop.mapred.QueueConfigurationParser:parseResource(org.w3c.dom.Element):[WARN] Configuring <ACLS_ENABLED_TAG> flag in <QueueManager.QUEUE_CONF_FILE_NAME> is not valid. <INFO> Bad configuration no queues defined
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:setTimelineDelegationToken(org.apache.hadoop.yarn.api.records.Token,java.lang.String):[WARN] Timeline token does not have service and timeline service address is not yet set. Not updating the token
org.apache.hadoop.security.CompositeGroupsMapping:getGroups(java.lang.String):[WARN] Unable to get groups for user {} via {} because: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:calculateRollingMonitorInterval(org.apache.hadoop.conf.Configuration):[WARN] {} has been set to {}, which is less than the default minimum value {}. This may impact NodeManager's performance.
org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo():[TRACE] #{} processRetryInfo: retryInfo={}, waitTime={}, callId, retryInfo, waitTime
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:addEstimation(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation):[INFO] Successfully add estimated resource allocation for {}.
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[WARN] Cannot set replication to ... for path: ... on a non-distributed filesystem ...
org.apache.hadoop.util.SysInfo:getNumVCoresUsed():[INFO] Obtained number of VCores used from Windows
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:createSpeculator(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext):[ERROR] Can't make a speculator -- check MRJobConfig.MR_AM_JOB_SPECULATOR
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RecoverTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] Transition started
org.apache.hadoop.hdfs.server.namenode.Checkpointer:rollForwardByApplyingLogs(org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Checkpointer about to load edits from [size] stream(s).
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkDelete(java.lang.String,boolean,org.apache.curator.framework.api.BackgroundCallback):[DEBUG] Deleting {}
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getNode(java.lang.String):[ERROR] Subcluster {} failed to return nodeInfo.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler$AsyncScheduleThread:run():[DEBUG] AsyncScheduleThread[ + getName() + ] is running!
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:updateHeartBeatConfiguration(org.apache.hadoop.conf.Configuration):[INFO] Heartbeat Scaling Configuration: defaultInterval: [nextHeartBeatInterval] minimumInterval: [heartBeatIntervalMin] maximumInterval: [heartBeatIntervalMax] speedupFactor: [heartBeatIntervalSpeedupFactor] slowdownFactor: [heartBeatIntervalSlowdownFactor]
org.apache.hadoop.ha.ZKFailoverController:recheckElectability():[INFO] Quitting master election for ... and marking that fencing is necessary
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:getContainerStatuses():[DEBUG] {} is completing, remove {} from NM context.
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadImageToStorage(java.net.URL,long,org.apache.hadoop.hdfs.server.common.Storage,boolean,boolean):[INFO] Downloaded file {dstFiles.get(0).getName()} size {dstFiles.get(0).length()} bytes.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handleInitContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationRequestEvent):[WARN] {c.getContainerId()} is at {c.getContainerState()} state, do not localize resources.
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$FSAction:runWithRetries():[INFO] Retrying operation on FS. Retry no. x
org.apache.hadoop.hdfs.server.datanode.DataNode:checkDiskError():[DEBUG] checkDiskError encountered no failures
org.apache.hadoop.yarn.service.ServiceManager:handle(org.apache.hadoop.yarn.service.ServiceEvent):[INFO] [SERVICE] Transitioned from {} to {} on {} event.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:updateCompletedContainers(java.util.List,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[DEBUG] Container FINISHED: {}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:cleanup():[ERROR] Failed to close outputstream of dump file {}
org.apache.hadoop.fs.s3a.WriteOperationHelper:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String):[DEBUG] Initiating select call {} {}
org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler:handle(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.io.IOException):[INFO] Volume {volume}: verification failed for {block} because of FileNotFoundException. This may be due to a race with write.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[DEBUG] Total Resource Usage stats in NM by all containers: Virtual Memory= vmemUsageByAllContainers, Physical Memory= pmemByAllContainers, Total CPU usage(% per core)= cpuUsagePercentPerCoreByAllContainers
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[WARN] Ignoring state store operation failure because the resource manager is not configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] RMApplicationHistoryWriter and TimelineCollectorManager set
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:checkVersion():[INFO] Storing NM state version info + getCurrentVersion()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:logNodeIsNotChosen(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason,java.lang.String):[DEBUG] Datanode is not chosen
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:computePartialChunkCrc(long,long):[DEBUG] Read in partial CRC chunk from disk for block
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for fileId: {}
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:triggerActiveLogRoll():[INFO] Triggering log roll on remote NameNode
org.apache.hadoop.yarn.service.client.ServiceClient:loadAppJsonFromLocalFS(java.lang.String,java.lang.String,java.lang.Long,java.lang.String):[INFO] Loading service definition from local FS: {filePath}
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor:start():[INFO] Scheduling {} storage monitor at interval {}, this.storage, monitorInterval
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl$LazyWriter:run():[INFO] LazyWriter was interrupted, exiting
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:doneApplication(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState):[WARN] Couldn't find application + applicationId
org.apache.hadoop.fs.azurebfs.commit.ResilientCommitByRename:commitSingleFileByRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.lang.String):[INFO] Rename operation started
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:run():[INFO] Listener stopped before starting
org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver:run():[ERROR] FSImageSaver cancel checkpoint threw an exception:, e
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Containers Reserved: countHere
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyToFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,long,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet,org.apache.hadoop.fs.FileChecksum):[DEBUG] Calculating UMask
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Copying {sourcePath} to {target}
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:setRollingUpgradeMarkers(java.util.List):[INFO] Created {}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:run():[WARN] A cleaner task is already running. This scheduled cleaner task will do nothing.
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[WARN] Unknown host name: {}. Retrying to resolve the host name...
org.apache.hadoop.hdfs.qjournal.server.Journal:scanStorageForLatestEdits():[INFO] No files in
org.apache.hadoop.tools.SimpleCopyListing:doBuildListingWithSnapshotDiff(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext):[DEBUG] Adding source dir for traverse: sourceStatus.getPath()
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[DEBUG] Running job cleanup task
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:assignContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.PendingAsk,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits):[DEBUG] assignContainers: node=... application=...
org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] getting serverKey: ...
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:setTimelineDelegationToken(org.apache.hadoop.yarn.api.records.Token,java.lang.String):[INFO] Updated timeline delegation token
org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:next():[DEBUG] New listing status: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unknown event arrived at ContainerScheduler: ${event.toString()}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:makeUberDecision(long):[INFO] msg.toString()
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:execute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[ERROR] AzureBlobFileSystemException thrown
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:reportMkDirFailure(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry,java.lang.Exception):[WARN] {}: mkdir failure #{} Failed to create directory "{}": {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Invalid eventtype . Ignoring!
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getBlockInputStreamWithCheckingPmemCache(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[DEBUG] Get InputStream by cache file path.
org.apache.hadoop.crypto.key.kms.server.KMS:invalidateCache(java.lang.String):[TRACE] Entering invalidateCache Method.
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:receivedNewWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.security.IdMappingServiceProvider):[DEBUG] Repeated write request which hasn’t been served: xid={}, drop it.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] Max vcores capability of resources in this cluster
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$SetupFailedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[DEBUG] Job metrics end running
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:addResourceRequest(java.lang.Long,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object,boolean,java.lang.String):[DEBUG] Adding request to ask {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container):[INFO] Replacing FAST_FAIL_MAP container
org.apache.hadoop.yarn.service.utils.CoreFileSystem:copyHdfsFileToLocal(org.apache.hadoop.fs.Path,java.io.File):[INFO] Copying file {} to {}
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:refreshLogRetentionSettings():[INFO] User refreshLogRetentionSettings access
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:main(java.lang.String[]):[ERROR] Failed to parse options
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Sync log completed
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createContainerLogDirs(java.lang.String,java.lang.String,java.util.List,java.lang.String):[WARN] Unable to create the container-log directory : {}, appLogDir, e
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] allocate(arrayLength)
org.apache.hadoop.yarn.service.utils.HttpUtil:generateToken(java.lang.String):[ERROR] Error:
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:populateResourceCapability(org.apache.hadoop.mapreduce.v2.api.records.TaskType):[WARN] Configuration ... is overriding the ... configuration
org.apache.hadoop.yarn.service.utils.SliderFileSystem:deleteComponentDir(java.lang.String,java.lang.String):[DEBUG] deleted public resource dir {}, publicResourceDir
org.apache.hadoop.hdfs.server.datanode.DataXceiver:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[WARN] "Got exception while serving {} to {}: ", block, remoteAddress, ioe
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest):[ERROR] Could not delete reservation: reservationId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy):[ERROR] Queue policy can't be DRF if the parent policy is FairSharePolicy. Choose FairSharePolicy or FifoPolicy for child queues instead. Please note that FifoPolicy is only for leaf queues.
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ContainerId):[WARN] createFailureLog(user, operation, target, description, null, null)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[DEBUG] Creating app attempt entity
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error while removing reservation allocation., e
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:validateInputParam(java.lang.String,java.lang.String):[ERROR] param + " is null"
org.apache.hadoop.hdfs.server.federation.router.Router:serviceStart():[DEBUG] Pause monitor started
org.apache.hadoop.yarn.service.component.Component:requestContainers(long):[INFO] Submitting scheduling request: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device):[WARN] Empty device path provided, try to get device type from major:minor device number
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:removePendingChangeRequests(java.util.List):[DEBUG] RM has confirmed changed resource allocation for container ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:call():[ERROR] Unable to kill the paused container [containerIdStr]
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[DEBUG] Acquiring write lock
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseArguments(java.lang.String[]):[INFO] Target file system not specified. Using default [fs]
org.apache.hadoop.hdfs.server.namenode.FSImage:doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Rolling back storage directory ...
org.apache.hadoop.mapred.uploader.FrameworkUploader:beginUpload():[INFO] Target
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeAddrs():[ERROR] The conf property not set properly, it has been configured with different journalnode values
org.apache.hadoop.mapreduce.lib.partition.InputSampler:writePartitionFile(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.lib.partition.InputSampler$Sampler):[INFO] Using {samples.length} samples
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:waitForProcess(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long):[TRACE] latch done for file {} buffer idx {} length {}
org.apache.hadoop.security.CompositeGroupsMapping:getGroups(java.lang.String):[DEBUG] Stacktrace:
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:retrieveNamespaceInfo():[DEBUG] this received versionRequest response: nsInfo
org.apache.hadoop.hdfs.server.datanode.BPOfferService:updateActorStatesFromHeartbeat(org.apache.hadoop.hdfs.server.datanode.BPServiceActor,org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat):[INFO] Acknowledging ACTIVE Namenode actor
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Job history notified
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Retransmitted request, transaction still in progress {}
org.apache.hadoop.yarn.client.cli.ApplicationCLI:killApplication(java.lang.String):[INFO] Application with id 'appId' doesn't exist in RM.
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int):[DEBUG] Adding thread capacity: {}
org.apache.hadoop.fs.s3a.impl.MkdirOperation:execute():[INFO] mkdirs({}}: Access denied when looking for parent directory {}; skipping checks
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:putIfAbsent(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[INFO] the collector for + appId + already exists!
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:render():[DEBUG] Application's information created
org.apache.hadoop.fs.s3a.impl.RenameOperation:endOfLoopActions():[DEBUG] Waiting for active copies to complete
org.apache.hadoop.mapreduce.JobResourceUploader:useSharedCache(java.net.URI,java.lang.String,java.util.Map,org.apache.hadoop.conf.Configuration,boolean):[WARN] Error trying to contact the shared cache manager, disabling the SCMClient for the rest of this job submission
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:transition(java.lang.Object,java.lang.Object):[INFO] Container Re-init Auto Rolled-Back
org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] IdentityProvider not specified, defaulting to UserIdentityProvider
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:localize(org.apache.hadoop.yarn.api.protocolrecords.ResourceLocalizationRequest):[INFO] Error when parsing local resource URI for
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:createCuratorClient(java.util.Properties):[INFO] Connecting to ZooKeeper with SASL/Kerberos and using 'sasl' ACLs
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] NameNode is ready for use!
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:lastStep():[INFO] AM container is null
org.apache.hadoop.yarn.service.timelineservice.ServiceMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord):[WARN] ServiceTimelinePublisher has stopped. Not publishing any more metrics to ATS.
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.LinkedList,org.apache.hadoop.tools.CopyListingFileStatus):[DEBUG] concat: firstchunk: + dstfs.getFileStatus(firstChunkFile)
org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception):[DEBUG] Displaying error: message with object1, object2
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:addHandlersFromConfiguredResourcePlugins(java.util.List,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[DEBUG] List of plugins of ResourcePluginManager: {}
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementAttemptedItems:matchesReportedBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.fs.StorageType,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] Reported block:{} not found in attempted blocks. Datanode:{}, StorageType:{}
org.apache.hadoop.hdfs.DFSOutputStream:initWritePacketSize():[WARN] Configured write packet exceeds {} bytes as max, + using {} bytes., PacketReceiver.MAX_PACKET_SIZE, PacketReceiver.MAX_PACKET_SIZE
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String):[DEBUG] Using handler for protocol {}
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskContiguous:syncBlock(java.util.List):[INFO] BlockRecoveryWorker: block={} (length={}), isTruncateRecovery={}, syncList={}
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Total Vmem allocated for Containers
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:initializeClient(java.net.URI,java.lang.String,java.lang.String,boolean):[TRACE] Fetching SAS token provider
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$StandByTransitionRunnable:run():[ERROR] FATAL, Failed to transition RM to Standby mode.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Application lifelime monitor interval set to X ms.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType):[DEBUG] assignContainers: node= + node.getRMNode().getNodeAddress() + application= + application.getApplicationId().getId() + priority= + schedulerKey.getPriority().getPriority() + assignableContainers= + assignableContainers + capability= + capability + type= + type
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[DEBUG] Try timeline store {}:{} for the request
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadStringTable(java.io.InputStream):[INFO] Loading {s.getNumEntry()} strings
org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL client callback: setting username: + userName
org.apache.hadoop.hdfs.DFSUtil:getNameServiceUris(org.apache.hadoop.conf.Configuration,java.util.Collection,java.lang.String[]):[WARN] Getting exception while trying to determine if nameservice can use logical URI
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] ACL not found for application {} owned by {}. Using default [{}]
org.apache.hadoop.tools.HadoopArchiveLogsRunner:run(java.lang.String[]):[INFO] Running as loginUser.getShortUserName() but will impersonate user
org.apache.hadoop.hdfs.DataStreamer$LastExceptionInStreamer:check(boolean):[TRACE] Got Exception while checking, + DataStreamer.this, new Throwable(thrown)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] Received null remoteUser while authorizing access to getImage servlet
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:satisfyStoragePolicy(java.lang.String,boolean):[ERROR] Audit failed: satisfyStoragePolicy
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:nodePublishVolume(org.apache.hadoop.yarn.api.protocolrecords.NodePublishVolumeRequest):[DEBUG] Received nodePublishVolume call, request: {}
org.apache.hadoop.crypto.key.kms.server.KMSAudit:initializeAuditLoggers(org.apache.hadoop.conf.Configuration):[INFO] Initializing audit logger {logger.getClass()}
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int):[INFO] Storage directory " + sd.getRoot() + " does not contain previous fs state.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor:run():[INFO] Container [containerId] is already stopped or failed
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getBlockPoolId():[ERROR] Cannot fetch block pool ID metrics: {}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:process():[INFO] Processed + numResources + resource(s) in + durationMs + ms.
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:nodeHeartbeat(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest):[INFO] Disallowed NodeManager nodeId: + nodeId + hostname: + nodeId.getHost()
org.apache.hadoop.yarn.event.EventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[INFO] Very low remaining capacity on {getName()} event queue: {remCapacity}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCacheDirectives():[DEBUG] Directive {}: Failed to resolve path {} ({})
org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[]):[INFO] Bytes array null, written -1
org.apache.hadoop.ipc.Server$Connection:checkDataLength(int):[WARN] Requested data length x is longer than maximum configured RPC length y. RPC came from z
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEvent):[WARN] Got exception while signaling container {containerId} with command {command}
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] OUTCOME: SUCCESS, Reservation ID: + reservationId.toString() + , Contract: + contract.toString()
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [FLEX DOWN COMPONENT {component.getName()}]: scaling down from {before} to {event.getDesired()}
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[DEBUG] Storing state for attempt {} at {}
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] War = {Collections.singletonList(apiServer.getWebAppContext().getWar())}
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyMetaCommand:run(java.util.List):[DEBUG] Checksum type: ...
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:setScriptExecutable(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] setScriptExecutable: {} owner:{}, script, owner
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:serviceStop():[WARN] Scheduler terminated before removing the application collectors
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceInit(org.apache.hadoop.conf.Configuration):[WARN] Failed to reload fair scheduler config file because last modified returned 0. File exists:
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Auth failure: {}, {}
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:createLoggers(org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory):[WARN] Quorum journal URI ' + uri + ' has an even number + of Journal Nodes specified. This is not recommended!
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute():[DEBUG] commit already applied for {key}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS REMOVE dir fileHandle: {} fileName: {} client: {}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Error getting logs for
org.apache.hadoop.hdfs.server.datanode.DataNode:parseChangedVolumes(java.lang.String):[INFO] Deactivation request received for failed volume: {failedStorageLocation}
org.apache.hadoop.security.ssl.SSLFactory:disableExcludedCiphers(javax.net.ssl.SSLEngine):[DEBUG] Disabling cipher suite {}.
org.apache.hadoop.yarn.client.api.YarnClient:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext):[DEBUG] Resubmitting application
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeApplicationStateInternal(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[DEBUG] Application state data size for {} is {}
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy:applyRevisionConstraint(com.amazonaws.services.s3.model.GetObjectRequest,java.lang.String):[DEBUG] Unable to restrict HEAD request to etag; will check later
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystemUtil:toInMemoryAllocation(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId,org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator):[DEBUG] Converting from proto format
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:shutdown():[INFO] Shutting down FederationInterceptor for {attemptId}
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSync(long):[DEBUG] logSync(tx) synctxid={} lastJournalledTxId={} mytxid={}
org.apache.hadoop.mapred.FileInputFormat:listStatus(org.apache.hadoop.mapred.JobConf):[DEBUG] Time taken to get FileStatuses: {sw.now(TimeUnit.MILLISECONDS)}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Failed to get status of path {}
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateAudiences(com.nimbusds.jwt.SignedJWT):[WARN] Unable to parse the JWT token.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:updateContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container):[ERROR] Callback handler does not implement container resource increase callback methods
org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer):[INFO] Log info with throwable
org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[WARN] User + remoteUser + is unauthorized to access the page + request.getRequestURI() + .
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:call():[INFO] Relaunch container with workDir = ..., logDir = ..., nmPrivateContainerScriptPath = ..., nmPrivateTokensPath = ..., pidFilePath = ...
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Symlink target should not be null, fileId: {handle.getFileId()}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Moving + src.toString() + to + target.toString()
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Invocation successful on [{}]
org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String):[WARN] Failed to get groups for user {} (attempt={}/{}) using {}. Exception:
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startTimer():[INFO] Scheduled Metric snapshot period at {period / 1000} second(s).
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:create(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.sharedcachemanager.store.SCMStore,org.apache.hadoop.yarn.server.sharedcachemanager.metrics.CleanerMetrics,java.util.concurrent.locks.Lock):[ERROR] Unable to obtain the filesystem for the cleaner service
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:doRefreshNamenodes(java.util.Map,java.util.Map):[INFO] Refreshing list of NNs for nameservices: ...
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeLabelsProvider(org.apache.hadoop.conf.Configuration):[DEBUG] Distributed Node Labels is enabled with provider class as : {ScriptBasedNodeLabelsProvider}
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent):[ERROR] Shuffle error in populating headers:
org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int):[INFO] Requested by {currentUser} at {remoteAddress} to cede active role.
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystemUtil:toInMemoryAllocation(java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId,org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator):[INFO] InMemoryReservationAllocation initialized
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:addHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[INFO] Successfully addHistory new resource skylines for {recurrenceId}.
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$SetupCompletedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] All tasks scheduled
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:computeReconstructionWorkForBlocks(java.util.List):[DEBUG] BLOCK* neededReconstruction = {} pendingReconstruction = {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:registerMBean(java.lang.String):[WARN] Error registering FSDatasetState MBean
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map):[INFO] removed attempt + attemptID + from the futures to keep track of
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:getSystemCredentialsSentFromRM(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext):[DEBUG] Adding new framework-token for {} for localization: {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Rename operation failed.
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:addTimelineDelegationToken(org.apache.hadoop.yarn.api.records.ContainerLaunchContext):[DEBUG] Add timeline delegation token into credentials: {timelineDelegationToken}
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext):[WARN] ResourceHandlerChain.reacquireContainer failed for containerId: {} Exception:
org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary():[INFO] DataNode ... are congested. Backing off for ... ms
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateSignature(com.nimbusds.jwt.SignedJWT):[DEBUG] JWT token signature is not null
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForNameserviceId(java.lang.String):[ERROR] Cannot get disabled name services
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:placeApplication(org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String,boolean):[WARN] Application placement failed for user
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:initializePipeline(java.lang.String):[INFO] Request to start an already existing user: {} was received, so ignoring.
org.apache.hadoop.fs.azure.NativeAzureFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Destination {} is an already existing file, failing the rename.
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:runWithPrincipal(java.lang.String,byte[],org.apache.commons.codec.binary.Base64,javax.servlet.http.HttpServletResponse):[TRACE] SPNEGO in progress
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[WARN] ContainersMonitorImpl$MonitoringThread is interrupted. Exiting.
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:loadFullHistoryData(boolean,org.apache.hadoop.fs.Path):[INFO] Loading history file: [ + historyFileAbsolute + ]
org.apache.hadoop.mapreduce.InputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[INFO] ComposableInputFormat split executed
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:cleanupBeforeRelaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] {} deleting {}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$OutOfOrderTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown from onStartContainerError for Container event.getContainerId(), thr
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Error in storing master key with KeyID:
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] Recovering data for FederationInterceptor for {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[INFO] replaying edit log: 0/1 transactions completed. (0%)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:reregisterCollectors():[DEBUG] {} : {}@<{}, {}>
org.apache.hadoop.yarn.sls.SLSRunner:start():[INFO] Application Masters started
org.apache.hadoop.fs.s3a.WriteOperations:abortMultipartUploadsUnderPath(java.lang.String):[INFO] Abort operation completed
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl):[INFO] Volume removed successfully.
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter:close():[ERROR] Exception in closing {org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter:close()->org.apache.hadoop.io.file.tfile.TFile$Writer:close()}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:ahsRedirectPath(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp):[DEBUG] Error parsing {} as an ContainerId
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:call():[WARN] The file already exists under + finalPath + . Ignoring this attempt.
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:execute(org.apache.commons.cli.CommandLine):[DEBUG] Processing help Command.
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortAllSinglePendingCommits(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[DEBUG] No files to abort under {pendingDir}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:process():[ERROR] Unable to complete the cleaner task, e1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:checkDockerVolumeCreated(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Couldn't find volume= + volumeName + driver= + driverName + for container= + container.getContainerId() + , please check error message in log to understand why this happens.
org.apache.hadoop.yarn.service.client.ApiServiceClient:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Service added
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[INFO] Deleting absolute path : {}
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys():[DEBUG] Creating URL
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:checkAndDeleteCgroup(java.io.File):[INFO] Parent Cgroups directory {cgf.getPath()} does not exist. Skipping deletion
org.apache.hadoop.hdfs.server.mover.Mover$Processor:processFile(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus,org.apache.hadoop.hdfs.server.mover.Mover$Result):[WARN] Failed to get default policy for + fullPath + e
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:closeFileSystems(org.apache.hadoop.security.UserGroupInformation):[WARN] Failed to close filesystems: , e
org.apache.hadoop.mapreduce.JobResourceUploader:addLog4jToDistributedCache(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path):[DEBUG] Log4j property file exists
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:createSplits(org.apache.hadoop.mapreduce.JobContext,java.util.List):[INFO] Number of map tasks determined
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:updateSchedulingMonitors(org.apache.hadoop.conf.Configuration,boolean):[INFO] SchedulingEditPolicy=... removed, stopping it now ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMDTSecretManagerTokenSequenceNumber(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Cleaning up logger
org.apache.hadoop.yarn.service.client.ServiceClient:addAMEnv():[DEBUG] AM env: \n{}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:listPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean):[INFO] Listing succeeded
org.apache.hadoop.security.UserGroupInformation:print():[DEBUG] [groups[i] + " "]
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:init():[INFO] Connects to Namenode [{}]
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSync(long):[ERROR] Could not sync enough journals to persistent storage due to No journals available to flush. Unsynced transactions: {transactions}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Cannot launch privileged container. Submitting user ([submittingUser]) fails ACL check.
org.apache.hadoop.fs.s3a.WriteOperationHelper:completeMPUwithRetries(java.lang.String,java.lang.String,java.util.List,long,java.util.concurrent.atomic.AtomicInteger,org.apache.hadoop.fs.s3a.impl.PutObjectOptions):[DEBUG] Completing multipart upload {} with {} parts, uploadId, partETags.size()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:unassignGpus(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] Trying to unassign GPU device from container
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextCompletedResource(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String):[DEBUG] Loading completed resource from {}
org.apache.hadoop.lib.servlet.HostnameFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[WARN] Request remote address is NULL
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:deleteFiles(java.util.Set):[INFO] Time to delete files {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String):[DEBUG] MemInfo : key : Value : value
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Creating intermediate history logDir: [... based on conf
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Invocation of {method.getName()} using {current.proxyInfo} was successful
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanUpPreviousJobOutput():[INFO] Finished cleaning up previous job temporary files
org.apache.hadoop.hdfs.server.datanode.DataNode:shutdown():[WARN] Exception when unlocking storage
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:removeZone(long):[INFO] Removing zone {} from re-encryption.
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[WARN] The queried SubCluster: {} does not exist.
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logMkDir(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode):[DEBUG] logEdit called
org.apache.hadoop.yarn.client.ClientRMProxy:getRMAddress(org.apache.hadoop.yarn.conf.YarnConfiguration,java.lang.Class):[ERROR] Unsupported protocol found when creating the proxy connection to ResourceManager: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[ERROR] Unknown localization event:
org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier):[DEBUG] Creating password for {} for user {} to run on NM {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl):[DEBUG] Try timeline store {} for the request
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Proxy user Authentication successful
org.apache.hadoop.hdfs.qjournal.server.Journal:moveTmpSegmentToCurrent(java.io.File,java.io.File,long):[ERROR] {finalFile.getParentFile()} doesn’t exist. Aborting tmp segment move to current directory ; journal id: {journalId}
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:recalculateChecksum(int,long):[DEBUG] Recalculate checksum for the missing/failed block index {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.rename src: {} dst: {}\n
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields:initFields(java.util.Set):[WARN] Invalid deSelects string + literals.trim()
org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket):[DEBUG] Ignoring exception while closing socket
org.apache.hadoop.fs.s3a.S3AInputStream:seekInStream(long,long):[DEBUG] Forward seek on {uri}, of {diff} bytes
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:listReservation(java.lang.String,java.lang.String,long,long,boolean,javax.servlet.http.HttpServletRequest):[INFO] List reservation request failed
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine):[INFO] Failover from <node1> to <node2> successful
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String):[INFO] Service {serviceName} is already in a terminated state {report.getYarnApplicationState()}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:removeFailedDelegationToken(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[ERROR] removing failed delegation token for appid= + applicationIds + ";t=" + t.token.getService()
org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger:logAuditEvent(boolean,java.lang.String,java.net.InetAddress,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.FileStatus):[DEBUG] ------------------- logged event for top service: allowed={boolean}\tugi={userName}\tip={addr}\tcmd={cmd}\tsrc={src}\tdst={dst}\tperm={perm}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String):[WARN] Error parsing smaps line : {line}; {t.getMessage()}
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplitRatio(org.apache.hadoop.conf.Configuration):[WARN] nMaps == 1. Why use DynamicInputFormat?
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:invalidate(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[]):[INFO] Invalidating blocks in dataset
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRunApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[ERROR] Exception occurred while fetching applications
org.apache.hadoop.yarn.util.YarnVersionInfo:main(java.lang.String[]):[DEBUG] version: {}
org.apache.hadoop.yarn.service.client.ServiceClient:deleteZKNode(java.lang.String):[INFO] Deleted zookeeper path: + zkPath
org.apache.hadoop.hdfs.DFSClient:getDelegationToken(org.apache.hadoop.io.Text):[INFO] Created [token details]
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long):[DEBUG] Creating application entity
org.apache.hadoop.yarn.util.resource.ResourceUtils:addResourceTypeInformation(java.lang.String,java.lang.String,java.util.Map):[INFO] Found resource entry [prop]
org.apache.hadoop.hdfs.server.namenode.CacheManager:startMonitorThread():[INFO] Not starting CacheReplicationMonitor as name-node caching is disabled.
org.apache.hadoop.hdfs.server.datanode.LocalReplica:bumpReplicaGS(long):[DEBUG] Renaming oldmeta to newmeta
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:getPossiblyCompressedOutputStream(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[INFO] Compression not required
org.apache.hadoop.hdfs.server.datanode.DataNode:secureMain(java.lang.String[],org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[ERROR] Exception in secureMain
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx$Dumper:run():[INFO] Dumper is interrupted, dumpFilePath = {OpenFileCtx.this.dumpFilePath}
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Completed deletion of files from {0}
org.apache.hadoop.crypto.key.kms.server.KMSACLs:setKMSACLs(org.apache.hadoop.conf.Configuration):[INFO] '{} Blacklist '{}'
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Finalize rolling upgrade logged at time: rollingUpgradeInfo.getFinalizeTime()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:moveToDone():[INFO] No summary file for job: + jobId
org.apache.hadoop.fs.FsUrlConnection:connect():[DEBUG] Connecting to {url}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:checkAndRemovePartialRecord(org.apache.hadoop.fs.Path):[ERROR] incomplete rm state store entry found : + record
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[]):[DEBUG] Configuring job jar
org.apache.hadoop.crypto.key.kms.server.KMS:getKeysMetadata(java.util.List):[TRACE] Exiting getKeysMetadata method.
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:run():[INFO] Error cleaning master , e
org.apache.hadoop.tools.HadoopArchiveLogs:prepareWorkingDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[INFO] Existing Working Dir detected: - FORCE_OPTION specified -> recreating Working Dir
org.apache.hadoop.fs.azure.NativeAzureFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Creating file: {}
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readInternal(long,byte[],int,int,boolean):[DEBUG] read ahead enabled issuing readheads num = ...
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress):[INFO] Looking for process running on port {port}
org.apache.hadoop.tools.HadoopArchiveLogs:runDistributedShell(java.io.File):[INFO] Running Distributed Shell with arguments: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[WARN] Failed to parse resource-request
org.apache.hadoop.registry.server.dns.RegistryDNS:createPrimaryQuery(org.xbill.DNS.Message):[INFO] Received query {}. Forwarding query {}, name, qualifiedName
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:startWebApp():[INFO] Service starting up. Logging start...
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:loadAMRMProxyState():[INFO] Recovered for AMRMProxy: next master key id ...
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:processResponseQueue():[DEBUG] Application {} has one container finished ({}).
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:enableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.EnableNameserviceRequest):[ERROR] Unable to enable Nameservice {}
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Finish information is missing for application attempt
org.apache.hadoop.mapred.JobConf:checkAndWarnDeprecation():[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_ULIMIT)
org.apache.hadoop.hdfs.server.namenode.FSImage:saveLegacyOIVImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.util.Canceler):[DEBUG] Creating compression for FS image
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL + url + from user + userName
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set mode to {info.getMode()}
org.apache.hadoop.mapred.QueueManager:getQueueAcls(org.apache.hadoop.security.UserGroupInformation):[INFO] Added access for operation
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float):[INFO] Recalculating schedule, headroom=
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger:merge(java.util.List):[INFO] Initiating in-memory merge with noInMemorySegments segments...
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeReadManifestFile():[INFO] Reading auxiliary services manifest + manifest
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppCompletelyDoneTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] NMTokenSecretManager app finished
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingReplicationBlocks():[DEBUG] Failed to get number of blocks pending replica
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean):[WARN] Found no start time for related entity {} of type {} while deleting {} of type {}
org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration):[INFO] Unable to use java.security.SecureRandom. Falling back to Java SecureRandom.
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createAppLogDirs(java.lang.String,java.util.List,java.lang.String):[WARN] Unable to create the app-log directory : {}, appLogDir, e
org.apache.hadoop.hdfs.DFSOutputStream:flushOrSync(boolean,java.util.EnumSet):[ERROR] Error while syncing
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetReplication(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,short):[DEBUG] Replication remains unchanged at {} for {}
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:sendOOBToPeers():[WARN] Interrupted when sending OOB message.
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:configureRetryContext(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ContainerLaunchContext,org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Set restart interval to minimum value + minimumRestartInterval + ms for container + containerId
org.apache.hadoop.yarn.service.client.ServiceClient:actionDependency(java.lang.String,boolean):[INFO] Running command as user {}
org.apache.hadoop.fs.azure.security.WasbTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[DEBUG] Renewing the delegation token
org.apache.hadoop.mapred.JobConf:checkAndWarnDeprecation():[WARN] JobConf.deprecatedString(JobConf.MAPRED_MAP_TASK_ULIMIT)
org.apache.hadoop.hdfs.server.balancer.Dispatcher:shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo):[TRACE] Excluding datanode {dn}: outOfService={outOfService}, excluded={excluded}, notIncluded={notIncluded}
org.apache.hadoop.hdfs.server.datanode.metrics.OutlierDetector:getOutlierMetrics(java.util.Map):[TRACE] getOutliers: List={}, MedianLatency={}, MedianAbsoluteDeviation={}, upperLimitLatency={}
org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp:createSingleDirectory(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus):[DEBUG] mkdirs: created directory {cur}
org.apache.hadoop.hdfs.server.namenode.BackupImage:applyEdits(long,int,byte[]):[TRACE] data: <formatted_data>
org.apache.hadoop.hdfs.server.namenode.NameNode:doImmediateShutdown(java.lang.Throwable):[ERROR] Error encountered requiring NN shutdown. Shutting down immediately.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:handleHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],java.lang.String,long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports):[DEBUG] DataNode Z reported slow disks: {D, E, F}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Removing RMDTMasterKey.
org.apache.hadoop.hdfs.server.balancer.Dispatcher:dispatchBlockMoves():[INFO] Balancer concurrent dispatcher threads = {concurrentThreads}
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean):[INFO] TrashPolicyDefault#deleteCheckpoint for trashRoot: ...
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[INFO] Found active RM [...]
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:put(org.apache.hadoop.yarn.server.timeline.EntityIdentifier,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity):[ERROR] GenericObjectMapper cannot write + entity.getClass().getName() + into a byte array. Write aborted!
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:addAndScheduleAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar):[INFO] Task scheduled
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeApplicationAttemptInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] Removing state for attempt: + appAttemptId
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} but app not found (Took {latency} ms.)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:authorizeStartAndResourceIncreaseRequest(org.apache.hadoop.yarn.security.NMTokenIdentifier,org.apache.hadoop.yarn.security.ContainerTokenIdentifier,boolean):[ERROR] Unauthorized request to start container. This token is expired. current time is ...
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] RMAppManager processing event for applicationId of type UNKNOWN
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using leveldb path [DB_PATH]
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:handleFairSchedulerConfig(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterParams,org.apache.hadoop.conf.Configuration):[INFO] Using explicitly defined FAIR_SCHEDULER_XML
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:getLabelsToNodesMapping(java.util.Set,java.lang.Class):[WARN] getLabelsToNodes : Label [label] cannot be found
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:submitReservation(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ReservationSubmissionRequestInfo,javax.servlet.http.HttpServletRequest):[INFO] Submit reservation request failed, ue
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster:readClusterInfo():[DEBUG] Using connector : {}
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ExceptionHandler:exceptionCaught(java.lang.Throwable):[TRACE] GOT EXCEPTION
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[DEBUG] RegisterUAM returned existing NM token for node {}
org.apache.hadoop.hdfs.server.datanode.DataNode:handleVolumeFailures(java.util.Set):[WARN] Error occurred when removing unhealthy storage dirs, e
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,java.lang.String,boolean):[ERROR] Incorrect endpoint: {error_message}
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:dumpData(java.io.FileOutputStream,java.io.RandomAccessFile):[TRACE] No need to dump with status(replied,dataState):(replied,dataState)
org.apache.hadoop.mapreduce.split.JobSplitWriter:writeOldSplits(org.apache.hadoop.mapred.InputSplit[],org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.conf.Configuration):[WARN] Max block location exceeded for split: {split} splitsize: {locations.length} maxsize: {maxBlockLocations}
org.apache.hadoop.mapreduce.v2.hs.HistoryContext:getAllJobs(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Fetching all jobs for the given application ID
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[DEBUG] Marking container {containerIdStr} as inactive
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:dumpOutDebugInfo():[INFO] Dump debug output
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String):[DEBUG] {message}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing attempt + attemptId + from app: + appId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler$AsyncScheduleThread:run():[INFO] AsyncScheduleThread[ + getName() + ] exited!
org.apache.hadoop.mapred.uploader.FrameworkUploader:checkSymlink(java.io.File):[WARN] Cannot read symbolic link on
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object):[DEBUG] Registered {name}
org.apache.hadoop.registry.server.services.RegistryAdminService:purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback):[DEBUG] Match on record @ {} with children,
org.apache.hadoop.hdfs.NameNodeProxiesClient:createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory):[DEBUG] Couldn't create proxy provider {failoverProxyProviderClass}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:flush():[DEBUG] Flushing [message]
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:open(org.apache.hadoop.fs.Path,java.util.Optional):[DEBUG] Incrementing stat CALL_OPEN
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] event.getContainerId() + Container Transitioned from + oldState + to + getState()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(int):[ERROR] Encountered Storage Exception for write on Blob : {} Exception details: {} Error Code : {}
org.apache.hadoop.mapreduce.lib.db.FloatSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String):[WARN] You are strongly encouraged to choose an integral split column.
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[DEBUG] Processing {event.getJobId()} of type {event.getType()}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesLogger$APP:recordAppActivityWithoutAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityState,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityLevel):[WARN] Doesn't handle app activities at level level.
org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map):[DEBUG] Updated mapName map size: map.size()
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier):[DEBUG] Storing token [tokenId.getSequenceNumber()]
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:setPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.SetSubClusterPolicyConfigurationRequest):[INFO] Insert into the state store the policy for the queue: {queue}
org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[]):[INFO] {bpReg} Starting thread to transfer {block} to {xferTargetsString}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Arguments):[INFO] Deleting job directory
org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean):[INFO] transitionToActive: Node is already active
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[ERROR] Failed to communicate with NM Collector Service for + appId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] Used resource={} exceeded user-limit={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:clearIpAndHost():[INFO] {} clearing ip and host
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:scanActiveLogs():[DEBUG] scan logs for {} in {}
org.apache.hadoop.lib.server.Server:init():[INFO] Log dir: {}
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:nextValidOp():[ERROR] nextValidOp: got exception while reading
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable):[DEBUG] Done. # tags & metrics= + numMetrics
org.apache.hadoop.yarn.server.timelineservice.storage.SchemaCreator:createTimelineSchema(java.lang.String[]):[INFO] Starting HBase timeline schema creation
org.apache.hadoop.yarn.server.webapp.AppsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Cannot add application {}: {}
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateComponent(org.apache.hadoop.yarn.service.api.records.Component,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[DEBUG] Validating component name format
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:restoreBlockFilesFromTrash(java.io.File):[INFO] Not overwriting {newChild} with smaller file from trash directory. This message can be safely ignored.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:addApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User):[INFO] Application attempt {} is not runnable, parallel limit reached
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:deprecatedConf(org.apache.hadoop.conf.Configuration):[WARN] Configuring queue ACLs in mapred-site.xml or hadoop-site.xml is deprecated. Configure queue ACLs in QUEUE_CONF_FILE_NAME
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:clearActiveBlock():[DEBUG] Clearing active block
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl$ContainerLogAggregator:doContainerLogAggregation(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController,boolean,boolean):[ERROR] Couldn't upload logs for + containerId + ". Skipping this container."
org.apache.hadoop.fs.adl.AdlFileSystem:propagateAccountOptions(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Updating ${generic} from ${origin}
org.apache.hadoop.hdfs.server.namenode.NameNode:reconfigureSlowNodesParameters(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:writeFile(org.apache.hadoop.fs.Path,byte[]):[DEBUG] Creating file system output stream
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:chooseDatanode(org.apache.hadoop.hdfs.server.federation.router.Router,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,java.lang.String):[DEBUG] Datanode information accessed, block locations retrieved
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getINodeWithLeases():[INFO] Took {} ms to collect {} open files with leases {}
org.apache.hadoop.tools.CommandShell:run(java.lang.String[]):[INFO] Shell usage printed
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:nodePublishVolume(org.apache.hadoop.yarn.api.protocolrecords.NodePublishVolumeRequest):[DEBUG] Translate to CSI proto message: {}
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:reInitializeContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.ContainerLaunchContext,boolean):[WARN] Exception when scheduling the event of re-initializing of Container containerId
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:checkVersion():[ERROR] Incompatible version for timeline store: expecting version + getCurrentVersion() + , but loading version + loadedVersion
org.apache.hadoop.fs.s3a.impl.DeleteOperation:deleteDirectoryTree(org.apache.hadoop.fs.Path,java.lang.String):[DEBUG] Delete \"{}\" completed; deleted {} objects
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateRenameSource(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.util.List):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename source cannot be the root
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileReInitTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[ERROR] Container [container.getContainerId()] Re-init failed !! Resource [failedEvent.getResource()] could not be localized !!
org.apache.hadoop.yarn.client.api.impl.TimelineWriter:doPosting(java.lang.Object,java.lang.String):[DEBUG] HTTP error code: {some_error_code} Server response : {some_response}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Allocated VirtualCores: countHere
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:getNamenodeStatusReport():[DEBUG] Probing NN at service address: {}
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String):[DEBUG] PrimaryGroup rule: parent rule failed
org.apache.hadoop.yarn.util.resource.ResourceUtils:getResourceInformationMapFromConfig(org.apache.hadoop.conf.Configuration):[INFO] Adding resource type - name = resourceName, units = resourceUnits, type = resourceTypeName
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:buildNamespace(java.io.InputStream,java.util.List):[DEBUG] Scanned {} directories.
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String):[ERROR] Couldn't create parents for {}, src
org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:logInvalidFileNameFormat(java.lang.String):[WARN] Invalid file name format for mount-table version file: {}. The valid file name format is mount-table-name.<versionNumber>.xml
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Target node is already occupied before moving
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close():[INFO] org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache: closing
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Nodemanager resources is set to: [Resource]
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:loadCache(boolean):[ERROR] Cannot fetch mount table entries from State Store
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:validateAndSetSchedulingRequest(org.apache.hadoop.yarn.api.records.SchedulingRequest):[INFO] Successfully added SchedulingRequest to app= + appSchedulingInfo.getApplicationAttemptId() + placementConstraint=[ + schedulingRequest.getPlacementConstraint() + ]. nodePartition= + targetNodePartition
org.apache.hadoop.crypto.key.kms.server.KMSConfiguration:initLogging():[WARN] Log4j configuration file '{}' not found
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[INFO] Multi-node update recording finished
org.apache.hadoop.ipc.Server$Connection:close():[DEBUG] Ignoring socket shutdown exception
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] The thread pool initial size is + this.initialPoolSize
org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean):[TRACE] Connection {} will skip to set fallbackToSimpleAuth as it is null., remoteId
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:setConf(org.apache.hadoop.conf.Configuration):[WARN] The value of {preference_fraction_key} is greater than 1.0 but should be in the range 0.0 - 1.0
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStop():[WARN] Executor did not terminate
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[INFO] Resource decrease requests : + decreaseRequests
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] Processing event for {} of type {}, appAttemptID, event.getType()
org.apache.hadoop.fs.s3a.S3AUtils:ensureOutputParameterInRange(java.lang.String,long):[WARN] s3a: {} capped to ~2.14GB (maximum allowed size with current output mechanism)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeOrUpdateRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[INFO] Storing RMDelegationToken_SEQUENCE_NUMBER
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[INFO] Log synchronized
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:convert(org.apache.hadoop.conf.Configuration):[INFO] New instance created
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:processPerfectOverWrite(org.apache.hadoop.hdfs.DFSClient,long,int,org.apache.hadoop.nfs.nfs3.Nfs3Constant$WriteStableHow,byte[],java.lang.String,org.apache.hadoop.nfs.nfs3.response.WccData,org.apache.hadoop.security.IdMappingServiceProvider):[INFO] Configuring job jar
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Launching thread to trigger block reports...
org.apache.hadoop.tools.DistCpOptions$Builder:setOptionsForSplitLargeFile():[INFO] Set {DistCpOptionSwitch.APPEND.getSwitch()} to false since {DistCpOptionSwitch.BLOCKS_PER_CHUNK.getSwitch()} is passed.
org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap:init(org.apache.hadoop.conf.Configuration,java.lang.String):[WARN] InMemoryAliasMap location {} is missing. Creating it.
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:verifyRequest(java.lang.String,org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.handler.codec.http.HttpRequest,org.jboss.netty.handler.codec.http.HttpResponse,java.net.URL):[INFO] Request for unknown token appid
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] Adding query param: GET_ACCESS_CONTROL
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:assignContainersToChildQueues(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Trying to assign to queue: {queuePath} stats: {childQueueStats}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:removeResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest):[ERROR] Unable to remove resource {rsrc} from state store
org.apache.hadoop.fs.aliyun.oss.OSSListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Summary: {} {}
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getRollOverLogMaxSize(org.apache.hadoop.conf.Configuration):[WARN] Unable to determine if the filesystem supports append operation
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner:shouldRetry(java.io.IOException,int):[WARN] Original exception is
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.InvariantsChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Invariant checker enabled. Monitoring every {monitoringInterval} ms, throwOnViolation={throwOnInvariantViolation}
org.apache.hadoop.mapred.uploader.FrameworkUploader:buildPackage():[INFO] Adding <file>
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List):[DEBUG] Scanned {} directories.
org.apache.hadoop.yarn.security.AMRMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Looking for a token with service {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:addOrUpdateReservationState(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String,org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction,boolean):[DEBUG] Creating plan node: {planName} at: {planCreatePath}
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:moveAppsInQueueSync(java.lang.String,java.lang.String):[WARN] Encountered unexpected error during migration of application: {} from reservation: {}
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:aboutToComplete(java.lang.String,java.util.List,long,org.apache.hadoop.fs.statistics.IOStatistics):[DEBUG] Closed MPU to {}, saved commit information to {}; data=:\n{}
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute():[INFO] Deleting key: {keyName} from KeyProvider: {provider}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[DEBUG] Initialized application resources
org.apache.hadoop.hdfs.server.datanode.DataStorage:enableTrash(java.lang.String):[INFO] Enabled trash for bpid {}
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceStop():[INFO] Waiting for AsyncDispatcher to drain. Thread state is :{eventHandlingThread.getState()}
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:commitPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit):[WARN] {}: No pending uploads to commit
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:run():[WARN] Ending command processor service for: + this
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[WARN] Metric name {name}, value {value} has no type.
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[INFO] Super post response created for CONCAT
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[INFO] NameNode container started at ID + containerId
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Using leveldb path ...
org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList):[DEBUG] Copying stream to target
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:removeDirectoryFromSerialNumberIndex(org.apache.hadoop.fs.Path):[WARN] Could not find serial portion from path: + serialDirPath.toString() + . Continuing with next
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:getHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId):[WARN] Trying to getHistory non-existing resource skylines for {}.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[WARN] IGNORING ClusterNode [{rmNode.getNodeID()}] with queue wait time [{estimatedQueueWaitTime}] and wait queue length [{waitQueueLength}]
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices$ManifestReloadTask:run():[WARN] Error while reloading manifest:
org.apache.hadoop.hdfs.server.namenode.FSDirectory:removeFromInodeMap(java.util.List):[INFO] Encryption zone removed for inode
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler):[INFO] Initialized queue mappings, override: (value of overrideWithQueueMappings)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assign(java.util.List):[INFO] Got allocated containers ...
org.apache.hadoop.mapred.YARNRunner:killJob(org.apache.hadoop.mapreduce.JobID):[DEBUG] Error when checking for application status
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:saveInternal(java.io.FileOutputStream,org.apache.hadoop.hdfs.server.namenode.FSImageCompression,java.lang.String):[INFO] Begin saveCacheManagerSection
org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object):[DEBUG] Ignoring
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:logException(java.lang.String,java.lang.Throwable):[WARN] comment + message
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:downloadEditsToStorage(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog,org.apache.hadoop.hdfs.server.namenode.NNStorage):[INFO] Skipping download of remote edit log + log + since it already is stored locally at + f
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[],java.io.PrintStream):[DEBUG] Executing command {subCommand}
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:parseTokenFromStream(java.io.InputStream,boolean):[DEBUG] AADToken: got exception when parsing json token {ex.toString()}
org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer:close():[ERROR] e.getMessage()
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:getMapOutputInfo(java.lang.String,int,java.lang.String,java.lang.String):[DEBUG] getMapOutputInfo: jobId= + jobId + , mapId= + mapId + ,dataFile= + pathInfo.dataPath + , indexFile= + pathInfo.indexPath
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease):[DEBUG] Moving {} to {}
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadDataStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[WARN] Failed to add storage directory {dataDir}
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:removeFromClusterNodeLabels(java.util.Collection):[ERROR] NODE_LABELS_NOT_ENABLED_ERR
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveMetadata(java.lang.String):[DEBUG] Found {} as an explicit blob. Checking if it's a file or folder.
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[INFO] Launching NAMENODE on a new container. + ...
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assign(java.util.List):[INFO] Releasing unassigned container ...
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$StopContainerTransition:onExceptionRaised(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent,java.lang.Throwable):[INFO] Unchecked exception is thrown from onStopContainerError for Container event.getContainerId()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAddCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean):[DEBUG] logRpcIds started
org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int):[DEBUG] Checking if blockNumber is negative
org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection):[DEBUG] Thread.currentThread().getName(): disconnecting client + connection +. Number of active connections: + size()
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId):[TRACE] {}: pulled slot {} out of {}, this, slot.getSlotIdx(), shm
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon):[DEBUG] Killing Daemon
org.apache.hadoop.hdfs.server.namenode.CacheManager:removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker):[INFO] removeDirective of + id + successful.
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onError(java.lang.Throwable):[ERROR] Error in AMRMClient callback handler
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Task attempt succeeded event handled
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet):[DEBUG] Getting locations for path
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getDestinationForPath(java.lang.String):[ERROR] The {} cannot find a location for {}, super.getClass().getSimpleName(), path
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean):[INFO] File committed without rate limiting
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$TooManyFetchFailureTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Not generating HistoryFinish event since start event not generated for taskAttempt: + taskAttempt.getID()
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[DEBUG] Getting NameNode ID
org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int):[WARN] releasing 'ready' block: releaseTarget
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:run():[DEBUG] Running DeletionTask : {}
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:run():[INFO] Processing the event JOB_SETUP
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close():[ERROR] Interrupted while waiting for CleanerThreadPool to terminate
org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.String,java.util.List):[DEBUG] GroupCacheLoader - reload (async).
org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean):[WARN] Log wait warning as the lock was held for too long
org.apache.hadoop.yarn.sls.SLSRunner:startAMFromRumenTrace(java.lang.String,long):[ERROR] Failed to create an AM
org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean):[INFO] Yielding from election
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute):[DEBUG] Incoming requestAttribute:{} is not present in {}, however opcode is NE. Hence accept this node.
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [FLEX COMPONENT {}]: Flex deferred because dependencies not satisfied.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Container + container + of + removed node + container.getNodeId() + completed with event + event
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition:transition(java.lang.Object,java.lang.Object):[WARN] rmApp.getApplicationId() + final state ( + appState + ) was recorded, but + appAttempt.applicationAttemptId + final state ( + appAttempt.recoveredFinalState + ) was not recorded.
org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int):[DEBUG] Resolving SASL properties for {clientAddress} {ingressPort}
org.apache.hadoop.registry.server.dns.RegistryDNS:generateReply(org.xbill.DNS.Message,java.net.Socket):[DEBUG] calling addAnswer
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:saveDirectives(java.io.DataOutputStream,java.lang.String):[INFO] Cache directive written
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Reading file
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:close():[ERROR] IOException occurred in close
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:writeDistCacheFilesList():[INFO] Number of HDFS based distributed cache files to be generated is ${fileCount}. Total size of HDFS based distributed cache files to be generated is ${byteCount}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:authorizeGetAndStopContainerRequest(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,boolean,org.apache.hadoop.yarn.security.NMTokenIdentifier,java.lang.String):[WARN] {msg}
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[INFO] Recovering app attempt {}
org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker:processErasureCodingTasks(java.util.Collection):[WARN] Failed to reconstruct striped block {block info}, {exception info}
org.apache.hadoop.io.ReadaheadPool:submitReadahead(java.lang.String,java.io.FileDescriptor,long,long):[TRACE] submit readahead: + req
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:updateNonActiveUsersResourceUsage(java.lang.String):[DEBUG] User 'userName' has become non-active.Hence move user to non-active list.Active users size = activeUsersSet.size() Non-active users size = nonActiveUsersSet.size() Total Resource usage for active users=totalResUsageForActiveUsers.getAllUsed().Total Resource usage for non-active users=totalResUsageForNonActiveUsers.getAllUsed().
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] An exception occurred while processing URL {url} from user {userName}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:decrInactiveNMMetrics(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Unexpected node state
org.apache.hadoop.hdfs.server.datanode.DataNode:makeInstance(java.util.Collection,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources):[DEBUG] DataNode instance created
org.apache.hadoop.hdfs.DFSOutputStream:completeFile(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[INFO] Could not complete {src} retrying...
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[INFO] Attempting to kill workload app: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getQueues(java.lang.String):[DEBUG] CSConf - getQueues: queuePrefix={}, queues={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:updateMetrics(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue):[DEBUG] allocate: applicationId={} container={} host={} user={} resource={} type={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:initPlugin(org.apache.hadoop.conf.Configuration):[WARN] Intel FPGA for OpenCL diagnose failed!
org.apache.hadoop.yarn.server.webapp.WebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String):[DEBUG] User information retrieved
org.apache.hadoop.fs.AbstractFileSystem:listStatus(org.apache.hadoop.fs.Path):[INFO] ViewFs listStatus call invoked
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Error when executing command.
org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder():[WARN] Warning, no kerberos ticket found while attempting to renew ticket
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics:updatePreemptionInfo(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] AM container preempted, current appAttemptId=%s, containerId=%s, resource=%s
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkSubClusterId(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[WARN] Invalid SubCluster Id information. Please try again by specifying valid Subcluster Id.
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] Using pre-installed keytab from localhost: <preInstalledKeytab>
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$MoveContainerToSucceededFinishingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Task attempt finalized
org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException):[WARN] {cause.toString()}
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.io.SequenceFile$Writer,org.apache.hadoop.tools.DistCpContext):[DEBUG] Adding source dir for traverse: {dirPath}
org.apache.hadoop.security.SecurityUtil:setTokenServiceUseIp(boolean):[DEBUG] Setting HADOOP_SECURITY_TOKEN_SERVICE_USE_IP to true
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[INFO] Creating directory
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:unRegisterNM():[WARN] Unregistration of the Node + this.nodeId + failed., e
org.apache.hadoop.security.KDiag:printDefaultRealm():[WARN] Host has no default realm
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnfinalizedSegments():[INFO] Starting recovery process for unclosed journal segments...
org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long):[WARN] The option %s value "%s" is not a long integer; using the default value %s
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ContainerLauncher:call():[INFO] reInitializing container {container.getId()} with version {componentLaunchContext.getServiceVersion()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaBinaryHelper):[INFO] Discovered GPU information: ...
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:setPassword(com.zaxxer.hikari.HikariDataSource,java.lang.String):[DEBUG] NULL Credentials specified for Store connection, so ignoring
org.apache.hadoop.fs.s3a.S3AUtils:buildEncryptionSecrets(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Using SSE-C with {diagnostics}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:checkAndInitializeLocalDirs():[INFO] Attempting to initialize + dir
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:closeWriter():[DEBUG] Closing Writer
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:restartContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Callback handler does not implement container restart callback methods
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Available VirtualCores: countHere
org.apache.hadoop.hdfs.server.federation.router.Quota:setQuotaInternal(java.lang.String,java.util.List,long,long,org.apache.hadoop.fs.StorageType):[DEBUG] Set quota for path: nsId: {nsId}, dest: {dest}.
org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap():[DEBUG] Updated last update time
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:verifyPlanHash(java.lang.String,java.lang.String):[ERROR] Disk Balancer - Invalid plan hash.
org.apache.hadoop.hdfs.server.datanode.DataXceiver:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[]):[DEBUG] writeBlock receive buf size {} tcp no delay {}
org.apache.hadoop.tools.CommandShell:run(java.lang.String[]):[INFO] Subcommand executed
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:computeMove(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume):[DEBUG] {} Skipping disk from computation. Minimum data size achieved., highVolume.getPath()
org.apache.hadoop.hdfs.server.namenode.CacheManager:setCachedLocations(org.apache.hadoop.hdfs.protocol.LocatedBlocks):[WARN] Datanode {} is not a valid cache location for block {} because that node does not have a backing replica!
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[INFO] Total duration of deletion operation: {0}
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String):[DEBUG] Unknown protocol {}, delegating to default implementation
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$UpdateContainerResourceTransition:transition(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer,org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEvent):[INFO] Unchecked exception is thrown from onUpdateContainerResourceError for Container
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,boolean,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[INFO] executing REST operation
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[]):[INFO] Operation category checked
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:closeInMemoryMergedFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput):[INFO] closeInMemoryMergedFile -> size: + mapOutput.getSize() + , inMemoryMergedMapOutputs.size() -> + inMemoryMergedMapOutputs.size()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Invalid configuration for YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS default value is 10Min(600000).
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:isContainerOutOfLimit(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not access memory resource for %s
org.apache.hadoop.mapred.LocatedFileStatusFetcher:getFileStatuses():[DEBUG] Scan failed
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable:run():[ERROR] ERROR IN CONTACTING RM.
org.apache.hadoop.yarn.applications.distributedshell.Client:main(java.lang.String[]):[INFO] Initializing Client
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:load(java.lang.String):[DEBUG] Loading section STRING_TABLE length: {s.getLength()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:handle(org.apache.hadoop.yarn.event.Event):[INFO] Application finished
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setOwner(java.lang.String,java.lang.String,java.lang.String):[INFO] Audit Event: setOwner successful for src
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateNodeResource(javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceOptionInfo):[ERROR] Failed to update the node resource {}.
org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileDeleter:handleFile(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata):[DEBUG] Deleting dangling file {}
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg$AppLevelAggregator:aggregate():[DEBUG] App-level real-time aggregating
org.apache.hadoop.fs.azure.CachingAuthorizer:put(java.lang.Object,java.lang.Object):[DEBUG] {}: CACHE PUT: {}, {}
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:refresh():[WARN] Host list file [{}] does not exist or is not a file !!
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:setRootNodeAcls():[DEBUG] Before setting ACLs\n
org.apache.hadoop.hdfs.server.common.Util:receiveFile(java.lang.String,java.util.List,org.apache.hadoop.hdfs.server.common.Storage,boolean,long,org.apache.hadoop.io.MD5Hash,java.lang.String,java.io.InputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler):[WARN] Unable to download file ...
org.apache.hadoop.hdfs.DFSClient:inferChecksumTypeByReading(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo):[INFO] trying to read block from datanode
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:postRemove(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[WARN] Failed to cancel token for app collector with appId + appId
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppTransition:transition(java.lang.Object,java.lang.Object):[INFO] Removing info for app: [appId]
org.apache.hadoop.tools.HadoopArchives:archive(org.apache.hadoop.fs.Path,java.util.List,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Unable to clean tmp directory
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes):[WARN] Can't append file: <fileIdPath>. Possibly the file is being closed. Drop the request: <request>, wait for the client to retry...
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assign(java.util.List):[INFO] Cannot assign container...
org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int):[DEBUG] doGetGroups({}) returned {},user,groups
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:getHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId):[INFO] Successfully query resource skylines for {}.
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeOrUpdateRMDT(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long,boolean):[INFO] Store RMDT with sequence number + rmDTIdentifier.getSequenceNumber()
org.apache.hadoop.yarn.util.resource.Resources:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[WARN] (warning message placeholder)
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkList(java.lang.String):[DEBUG] ls {}
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Unable to store master key ...
org.apache.hadoop.hdfs.DFSInputStream:hedgedFetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks):[DEBUG] Waited {}ms to read from {}; spawning hedged read
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[DEBUG] Skipping deletion of {0}
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run():[ERROR] Lost job
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:invalidateLocationCache(java.lang.String):[DEBUG] Removing {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:checkAndGetApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Priority ' + appPriority.getPriority() + ' is acceptable in queue : + queuePath + for application: + applicationId
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(long,long,int):[DEBUG] pMemCheck [current={} + asked={} > allowed={}]
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache$StreamMonitor:run():[INFO] StreamMonitor got interrupted
org.apache.hadoop.hdfs.server.namenode.ImageServlet:validateRequest(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.hdfs.server.namenode.FSImage,java.lang.String):[WARN] Received an invalid request file transfer request from a secondary with storage info {theirStorageInfoString}
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[ERROR] No component instance exists for + containerId
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] Authentication exception: {ex.getMessage()}
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:run(java.lang.String[]):[DEBUG] Executing command {subCommand}
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:removePolicy(java.lang.String):[INFO] Remove erasure coding policy + name
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleMapContainerRequest(org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent):[INFO] The required MAP capability is more than the supported max container capability in the cluster. Killing the Job. mapResourceRequest: + mapResourceRequest + maxContainerCapability: + supportedMaxContainerCapability
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:poll():[DEBUG] timed poll(): poll() returned null, sleeping for {} ms
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$RMAppStateFileProcessor:processChildNode(java.lang.String,java.lang.String,byte[]):[DEBUG] Loading application from node: {childNodeName}
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[DEBUG] NoOpTimelineWriter is configured. Not storing TimelineEntities.
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForAndGetNameNodeProperties(java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.slf4j.Logger):[WARN] Unable to fetch NameNode host information; retrying
org.apache.hadoop.hdfs.server.federation.resolver.order.HashResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[DEBUG] Namespace for {path} ({finalPath}) is {hashedSubcluster}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.StoreFile):[DEBUG] postFlush store = + store.getColumnFamilyName() + flushableSize= + store.getFlushableSize() + flushedCellsCount= + store.getFlushedCellsCount() + compactedCellsCount= + store.getCompactedCellsCount() + majorCompactedCellsCount= + store.getMajorCompactedCellsCount() + memstoreSize= + store.getMemStoreSize() + size= + store.getSize() + storeFilesCount= + store.getStorefilesCount()
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:cancelDelegationToken(org.apache.hadoop.security.token.Token):[DEBUG] Cancel delegation token
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:call():[WARN] Recovered container exited with a non-zero exit code ${retCode}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:initializeQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] Initialized queues with configuration
org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[]):[DEBUG] retryCount == {retryCount}. It's time to normally process the response
org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask:compute():[DEBUG] Setting quota for...
org.apache.hadoop.hdfs.server.namenode.BackupNode:handshake(org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol):[ERROR] Incompatible build versions: active name-node BV = {0}; backup node BV = {1}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptLaunchedTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] update the launch time for applicationId: {appId}, attemptId: {attemptId}, launchTime: {timestamp}
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:periodicInvoke():[DEBUG] Updating State Store cache
org.apache.hadoop.streaming.PipeMapRed$MRErrorThread:run():[INFO] MRErrorThread done
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Block deletion is delayed during NameNode startup. The deletion will start after {} ms.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:saveNamespace(long,long):[INFO] New namespace image has been created
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Launching AM with application attempt id ...
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:deleteFilesystem(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] deleteFilesystem for filesystem: {}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[DEBUG] Rescanning because of pending operations
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice):[ERROR] Intel aocl program {ipPath} to {aclName} failed!, {e}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore:initDatabase():[INFO] Using conf database at storeRoot
org.apache.hadoop.mapreduce.JobSubmissionFiles:getStagingDir(org.apache.hadoop.mapreduce.Cluster,org.apache.hadoop.conf.Configuration):[INFO] Permissions on staging directory are incorrect: ... Fixing permissions to correct value ...
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:extractChecksumProperties(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto,org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,int):[DEBUG] set bytesPerCRC={}, crcPerBlock={}
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:loadContainerState(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String):[WARN] the container {containerId} will be killed because of the unknown key {key} during recovery.
org.apache.hadoop.ipc.Server$Listener$Reader:run():[ERROR] Error closing read selector in [Thread Name], [IOException]
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] Initializing {},
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$StandByTransitionRunnable:run():[INFO] Transitioning RM to Standby mode
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[DEBUG] Processing [TaskAttemptID] of type [eventType]
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:setContainerCompletedStatus(int):[ERROR] Unable to set exit code for container
org.apache.hadoop.mapreduce.JobResourceUploader:uploadFiles(org.apache.hadoop.mapreduce.Job,java.util.Collection,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,short,java.util.Map,java.util.Map):[INFO] No files to upload
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext):[INFO] Error while creating of RM app master service proxy for attemptId,...
org.apache.hadoop.hdfs.server.datanode.DataStorage:linkAllBlocks(java.io.File,java.io.File,java.io.File,int,org.apache.hadoop.conf.Configuration):[INFO] Linked blocks from {fromDir} to {toDir}. {hardLink.linkStats.report()}
org.apache.hadoop.yarn.service.component.Component$ContainerCompletedTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] Service def state changed from {} -> {}
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:isAvailable():[ERROR] StringUtils.stringifyException(e)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState,boolean):[INFO] Application + applicationAttemptId + is done. finalState= + rmAppAttemptFinalState
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppLogsAggregatedTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[ERROR] Unable to remove application from state store
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:addFileToProcess(org.apache.hadoop.hdfs.server.namenode.sps.ItemInfo,boolean):[DEBUG] Added track info for inode {} to block storageMovementNeeded queue
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String):[WARN] Illegal value: the number of elements in \" + s + \" is + elements.length + but an even number of elements is expected.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Execute renaming operation
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getBlockReaderLocal():[TRACE] {}: trying to construct a BlockReaderLocal for short-circuit reads., this
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[INFO] Using app provided configurations for delegation token renewal, total size = [tokenConf.capacity()]
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Time elapsed calculated
org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext:editLogLoaderPrompt(java.lang.String,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,java.lang.String):[INFO] Continuing
org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String):[INFO] Wildcard expansion for directory: {directoryName}
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:close():[ERROR] Error while closing MultipleOutput file
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[INFO] Finishing application master for {}. Tracking Url: {}
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:setBlockInvalidateLimit(int):[INFO] {} : configured={}, counted={}, effected={}, DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, configuredBlockInvalidateLimit, countedBlockInvalidateLimit, this.blockInvalidateLimit
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult):[ERROR] Unexpected health check result {} for volume {}, result, reference.getVolume()
org.apache.hadoop.registry.cli.RegistryCli:resolve(java.lang.String[]):[INFO] Endpoint details printed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[INFO] Container {container} of finished application {appId} completed with event {event}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Performing satisfy storage policy operation
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Service address obtained
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool():[INFO] Going to finish converging with remaining ...
org.apache.hadoop.yarn.client.cli.TopCLI:showTopScreen():[ERROR] Unable to get application information
org.apache.hadoop.yarn.util.DockerClientConfigHandler:readCredentialsFromConfigFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Token read from Docker client configuration file: ...
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkUpdate(java.lang.String,byte[]):[DEBUG] Updating {} with {} bytes
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:cancelToken(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[INFO] Did not cancel + t
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[]):[INFO] Queue info: queueName={}, queueCurrentCapacity={}, queueMaxCapacity={}, queueApplicationCount={}, queueChildQueueCount={}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:checkDir(java.lang.String,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result,org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result):[INFO] <dir> printed
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:checkSafeMode():[DEBUG] STATE* Safe mode ON.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:doAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[WARN] Couldn't get container for allocation!
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:logAndThrowInvalidInputException(org.slf4j.Logger,java.lang.String):[ERROR] errMsg
org.apache.hadoop.fs.azure.NativeAzureFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Renamed {} to {} successfully.
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppCompletelyDoneTransition:transition(java.lang.Object,java.lang.Object):[INFO] Collector status updated
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration):[INFO] Updated the cluste max priority to maxClusterLevelAppPriority = ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container):[WARN] Failed to get + AppPlacementAllocator.class.getName() + for application= + getApplicationId() + schedulerRequestKey= + schedulerKey
org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int):[WARN] NativeIO.chmod error (%d): %s
org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String):[ERROR] Invalid URI for path: throws URISyntaxException
org.apache.hadoop.ipc.Server$Responder:doRunLoop():[INFO] doAsyncWrite threw exception {e}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:createShortCircuitReplicaInfo():[TRACE] {}: trying to create ShortCircuitReplicaInfo.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[DEBUG] BLOCK* addToInvalidates: storedBlock datanodes
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:verifyAndCreateRemoteLogDir():[WARN] Remote Root Log Dir [ + remoteRootLogDir + ] does not exist. Attempting to create it.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:normalizeNodeLabelExpressionInRequest(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.QueueInfo):[DEBUG] Requested Node Label Expression : {labelExp}
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[DEBUG] Returning JSON response
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path):[DEBUG] Could not reset Keystore to previous state
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String):[DEBUG] BLOCK* findAndMarkBlockAsCorrupt: {} not found on {}
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:execute(org.apache.commons.cli.CommandLine):[ERROR] Query plan failed. ex: {}, ex
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:rememberTargetTransitionsAndStoreState(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent,java.lang.Object,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState):[INFO] Updating application with final state
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceStart():[INFO] AMRMProxyService listening on address: + this.server.getListenerAddress()
org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:validate(java.util.Map,int):[ERROR] Job resource skyline history is invalid, please try again with valid resource skyline history.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] try to use reserved: + getQueuePath() + usedResources: + queueUsage.getUsed() + , clusterResources: + clusterResource + , reservedResources: + resourceCouldBeUnreserved + , capacity-without-reserved: + newTotalWithoutReservedResource + , maxLimitCapacity: + currentLimitResource
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] User {} has been removed!
org.apache.hadoop.hdfs.web.URLConnectionFactory:openConnection(java.net.URL):[DEBUG] open AuthenticatedURL connection {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] Failed to move reservation, two nodes are in different partition
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:processFileInode(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo):[WARN] File {} skipped re-encryption because it is not encrypted! This is very likely a bug.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:addToReplicasMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,java.io.File,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker,boolean,java.util.List,java.util.Queue):[WARN] File is not a block filename, skipping.
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:cancelPlan(java.lang.String):[ERROR] Cancelling plan on {} failed. Result: {}, Message: {}, plan.getNodeName(), ex.getResult().toString(), ex.getMessage()
org.apache.hadoop.security.token.Token:getRenewer():[DEBUG] Failed to load token renewer implementation
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Adding service timelineClient
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:enableBlockPoolId(java.lang.String):[WARN] {}: failed to load block iterator.
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteWithoutCloseCheck(org.apache.hadoop.fs.Path,boolean):[DEBUG] Couldn't delete {} - does not exist: {}
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManagerImpl:loadProfiles():[INFO] Loaded profiles: profiles.keySet()
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:activate():[WARN] [{}] {} Activate {}, currentThreadID(), getSpanId(), getDescription()
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:stop():[DEBUG] Invalid mode:{}, ignoring
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:getTrackingUri(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.yarn.server.webproxy.AppReportFetcher$AppReportSource):[DEBUG] Original tracking url is '{}'. Redirecting to RM app page
org.apache.hadoop.crypto.key.kms.server.KMSACLs:hasAccess(org.apache.hadoop.crypto.key.kms.server.KMSACLs$Type,org.apache.hadoop.security.UserGroupInformation):[DEBUG] No blacklist for {}
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:registerSubCluster(org.apache.hadoop.yarn.server.federation.store.records.SubClusterRegisterRequest):[ERROR] Cannot register subcluster: ${exceptionMessage}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,java.lang.String):[WARN] BLOCK* blockReceived: {} is expected to be removed from an unrecorded node {}
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:processResponseQueue():[DEBUG] Application {} has one reducer finished ({})
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext):[INFO] Localizer CWD set to {} = {}
org.apache.hadoop.security.KDiag:printDefaultRealm():[ERROR] Kerberos.getDefaultRealm() failed
org.apache.hadoop.hdfs.qjournal.server.Journal:journal(org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo,long,long,int,byte[]):[WARN] Sync of transaction range {firstTxnId}-{lastTxnId} took {milliSeconds}ms ; journal id: {journalId}
org.apache.hadoop.fs.s3a.S3AUtils:createAwsConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[DEBUG] Signer override for {}} = {}, awsServiceIdentifier, signerOverride
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:appFinished(org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] No application Attempt for application : {appId} started on this NM.
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[DEBUG] Retrieved locations for path
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS LOOKUP fileId: {fileId} name: {fileName} does not exist
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:run():[ERROR] Thread exiting
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:convert(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterParams):[DEBUG] Configuring job jar
org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject):[DEBUG] Loaded {} tokens from {}
org.apache.hadoop.yarn.service.client.ServiceClient:flexComponents(java.lang.String,java.util.Map,org.apache.hadoop.yarn.service.api.records.Service):[ERROR] <serviceName> is at <liveService.getState()> state, flex can not be invoked when service is upgrading.
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeSection(java.io.InputStream):[DEBUG] Sorting inodes
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:innerClose():[DEBUG] Closing {...}
org.apache.hadoop.util.RunJar:run(java.lang.String[]):[ERROR] JAR does not exist or is not a normal file
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeDirectorySectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String):[ERROR] Interrupted waiting for countdown latch
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:updateApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture,org.apache.hadoop.security.UserGroupInformation):[INFO] Priority 'appPriority' is updated in queue :[queueName] for application: [applicationId] for the user: [user]
org.apache.hadoop.hdfs.qjournal.server.Journal:scanStorageForLatestEdits():[INFO] Scanning storage
org.apache.hadoop.hdfs.DFSClient:getDelegationToken(org.apache.hadoop.io.Text):[INFO] Cannot get delegation token from [renewer]
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation):[WARN] Found null for clusterId. Not proceeding with writing to hbase
org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI):[TRACE] we can write the local file.
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Leave startup safe mode after {} ms
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:updateStateStore():[ERROR] Cannot heartbeat for router: unknown router id
org.apache.hadoop.hdfs.protocol.ClientProtocol:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean):[INFO] Calling RouterClientProtocol mkdirs
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$InitContainerTransition:transition(java.lang.Object,java.lang.Object):[WARN] Killing {} because {} is in state {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RetryFailureTransition:transition(java.lang.Object,java.lang.Object):[INFO] Relaunching container with updated retry context
org.apache.hadoop.tools.mapred.CopyCommitter:concatFileChunks(org.apache.hadoop.conf.Configuration):[DEBUG] concat + targetFile + allChunkSize+ + allChunkPaths.size()
org.apache.hadoop.yarn.server.webapp.LogWebService:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,boolean,java.lang.String,boolean):[DEBUG] getContainerLogsInfo
org.apache.hadoop.hdfs.DFSClient:callAppend(java.lang.String,java.util.EnumSet,org.apache.hadoop.util.Progressable,java.lang.String[]):[DEBUG] NameNode is on an older version, request file info with additional RPC call for file: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:doChooseVolume(java.util.List,long,java.lang.String):[DEBUG] Volumes are imbalanced. Selecting volume from high available space volumes for write of block size replicaSize
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:initializeProcessTrees(java.util.Map$Entry):[INFO] {} is missing. Not setting ip and hostname
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:closePrintStream(java.io.OutputStream):[DEBUG] No cleanup needed for System.out
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptFailedTransition:transition(java.lang.Object,java.lang.Object):[INFO] The number of failed attempts in previous {app.attemptFailuresValidityInterval} milliseconds is {numberOfFailure}. The max attempts is {app.maxAppAttempts}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests:add(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId):[INFO] Assigned container <containerId> to <tId>
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Invalid LOOKUP request
org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String):[WARN] Can not create a symLink with a target = null and link = null
org.apache.hadoop.yarn.server.webapp.AppsBlock:render():[INFO] Cannot add application {}: {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS READ fileHandle: {} offset: {} count: {} client: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:addQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue):[INFO] Creation of AutoCreatedLeafQueue succeeded
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] NodeHealthyStatus
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:getNextBlock(java.util.List,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem):[ERROR] Unable to get json from Item.
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Applications Submitted: countHere
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: FAIR_AS_DRF
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNodeUsage():[ERROR] Cannot get the live nodes: {e.getMessage()}
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[INFO] Help text displayed
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerPoolTracker:run():[INFO] Problem in submitting renew tasks in token renewer thread.
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:flush():[DEBUG] Keystore hasn't changed, returning.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for dir fileId: dirHandle.getFileId()
org.apache.hadoop.hdfs.server.namenode.BackupNode:registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[ERROR] Name-node ... is not active. Shutting down.
org.apache.hadoop.tools.SimpleCopyListing:doBuildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[INFO] Build file listing completed.
org.apache.hadoop.hdfs.server.namenode.NameNode:doRollback(org.apache.hadoop.conf.Configuration,boolean):[INFO] Rollback aborted.
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] User: {}
org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders:getComponent(java.lang.String):[INFO] Resolving path {}
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionFlex(java.lang.String,java.util.Map):[ERROR] Fail to flex application: , e
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecomDeadDataNodes():[DEBUG] Failed to get the number of dead decommissioned datanodes
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMApps(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Skipping extraneous data + key
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:noteTokenCreated(org.apache.hadoop.security.token.Token):[INFO] Created S3A Delegation Token: {}
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:rollMasterKey():[INFO] Rolling master-key for nm-tokens
org.apache.hadoop.fs.s3a.auth.STSClientFactory:builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider):[DEBUG] STS Endpoint={}; region='{}'
org.apache.hadoop.yarn.util.resource.Resources:componentwiseMin(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[WARN] Resource is missing:{ye.getMessage()}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createTimelineSchema(java.lang.String[]):[INFO] Starting the schema creation
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:moveToDoneNow(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] Copied from: fromPath to done location: toPath
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:saveReplicas(org.apache.hadoop.hdfs.protocol.BlockListAsLongs):[WARN] Failed to write replicas to cache
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateNodeResource(javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceOptionInfo):[ERROR] Failed to update the node resource {RMNodeID}.
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureTokens(org.apache.hadoop.security.token.Token,org.apache.hadoop.security.Credentials,java.util.Map):[INFO] Size of containertokens_dob is + taskCredentials.numberOfTokens()
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl$MonitoringThread:run():[ERROR] Get Node GPU Utilization error: e
org.apache.hadoop.yarn.service.client.ServiceClient:upgradePrecheck(org.apache.hadoop.yarn.service.api.records.Service):[ERROR] All components have NEVER restart policy
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleExitCode(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Exception from container-launch with container ID: {} and exit code: {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction):[INFO] rollingUpgrade QUERY
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:clearDirectory():[INFO] Will remove files: {}
org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader:numRecStats(byte[],int,int):[INFO] status
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:containerFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerFinishData):[ERROR] Error when writing finish information of container ...
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] App attempt: + appAttemptID + can't handle this event at current state, e
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:handleNMTimelineEvent(org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelineEvent):[ERROR] Unknown NMTimelineEvent type: + event.getType()
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int):[TRACE] Exiting generateEncryptedKeys method.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:save():[TRACE] save({}, {}): saved {}
org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable):[DEBUG] val + is a zero-length value
org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL client callback: setting userPassword
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumInMaintenanceLiveDataNodes():[DEBUG] Failed to get number of live in maintenance nodes
org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater:run():[ERROR] Exception in block key updater thread
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:completeFile(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long):[INFO] DIR* completeFile: ${src} is closed by ${holder}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:renameLocalDir(org.apache.hadoop.fs.FileContext,java.lang.String,java.lang.String,long):[WARN] Failed to rename the local file under {localDir}/{localSubDir}
org.apache.hadoop.yarn.api.ApplicationMasterProtocol:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[DEBUG] Proxy allocation request sent
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:checkIfAlreadyBootstrapped(java.lang.String):[INFO] Bootstrap check succeeded
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:processTimelineResponseErrors(org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse):[ERROR] Error when publishing entity [entityType,entityId], server side error code: errorCode
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:extractChecksumProperties(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto,org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,int):[DEBUG] Retrieving checksum from an earlier-version DataNode: inferring checksum by reading first byte
org.apache.hadoop.yarn.applications.distributedshell.Client:setAMResourceCapability(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.Map,java.util.List):[WARN] AM Resource capability={capability}
org.apache.hadoop.tools.DistCh:run(java.lang.String[]):[INFO] isIgnoreFailures= + isIgnoreFailures
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:receivePacket():[DEBUG] Receiving one packet for block xyz: header_info
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Yarn-site XML loaded and validated
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:rename(java.lang.String,java.lang.String):[WARN] Rename: CopyBlob: StorageException: ServerBusy: Retry complete, will attempt client side copy for page blob
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer:crcPartialBlock():[DEBUG] Partial length is greater than zero
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:allocate(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Allocation done
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:getPathToRead(java.lang.String,java.util.List):[WARN] Failed to find pathStr at dir
org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder():[WARN] destroy ticket failed
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:handleLifeline(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary):[DEBUG] Received handleLifeline from nodeReg = + nodeReg
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishContainerLocalizationEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent,java.lang.String):[DEBUG] Failed to publish Container metrics for container {} with exception e
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeRMDTMasterKeyState(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Storing RMDelegationKey_ + masterKey.getKeyId()
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:render():[INFO] 'QueueName' Queue Status: Queue State: ..., Minimum Queue Memory Capacity: ..., Maximum Queue Memory Capacity: ..., Number of Nodes: ..., Used Node Capacity: ..., Available Node Capacity: ..., Total Node Capacity: ..., Number of Node Containers: ...
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter:close():[ERROR] Exception in closing {org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter:close()->org.slf4j.Logger:warn(java.lang.String,java.lang.Throwable)}
org.apache.hadoop.hdfs.DeadNodeDetector$Probe:run():[ERROR] Probe failed, datanode: {}, type: {}., datanodeInfo, type, e
org.apache.hadoop.tools.HadoopArchiveLogs:checkFilesAndSeedApps(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Skipping logs under [appLogPath] due to [ioe.getMessage()]
org.apache.hadoop.hdfs.server.datanode.DataXceiver:requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean):[ERROR] Request short-circuit read file descriptor failed with unknown error.
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:recoverUnfinalizedSegments():[INFO] Deleting zero-length edit log file elf
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getIsNamespaceEnabled(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Get root ACL status
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:squashCGroupOperations(java.util.List):[WARN] Invalid number of args: + args.size()
org.apache.hadoop.util.LogAdapter:info(java.lang.String):[INFO] LOG.info(msg)
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[INFO] Attempting to kill infrastructure app: {}
org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus():[DEBUG] Monitoring active leader for (object_instance)
org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path):[INFO] Resolving file system path
org.apache.hadoop.hdfs.server.datanode.DataStorage:clearTrash(java.lang.String):[INFO] Cleared trash for bpid {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initializing SchedulingMonitor= + getName()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:resetAllowedLocalityLevel(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType):[INFO] Raising locality level from [old] to [level] at priority [schedulerKey.getPriority()]
org.apache.hadoop.yarn.util.FSDownload:unpack(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem):[WARN] Treating [{source}] as an archive even though it was specified as PATTERN
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String):[DEBUG] Creating token with ugi:{}, renewer:{}, service:{}.
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] KeyStore initialized anew successfully !!
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$EntityLogFD:writeEntities(java.util.List):[DEBUG] Writing entity list of size {}
org.apache.hadoop.mapred.TaskAttemptListenerImpl:reportDiagnosticInfo(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String):[INFO] Diagnostics report from [taskAttemptID.toString()]: [diagnosticInfo]
org.apache.hadoop.hdfs.server.namenode.NNStorage:writeAll():[WARN] Error during write properties to the VERSION file to {}, sd, e
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] NodeManager started on
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:createPlanFollower():[INFO] Using PlanFollowerPolicy: planFollowerPolicyClassName
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KilledDuringCommitTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[DEBUG] Setting finish time
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setOverCommitTimeoutPerNode(java.lang.String,int):[DEBUG] DRConf - setOverCommitTimeoutPerNode: nodePrefix={}, overCommitTimeout={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:getConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set):[DEBUG] Application {} is not registered in the Placement Constraint Manager.
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg$AppLevelAggregator:aggregate():[DEBUG] App-level real-time aggregation complete
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:run(java.lang.String[]):[INFO] New instance created
org.apache.hadoop.hdfs.tools.DFSck:listCorruptFileBlocks(java.lang.String,java.lang.String):[INFO] The list of corrupt files under path 'dir' are:
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:getUploadId(java.lang.String):[INFO] Initiate a multipart upload. bucket: [{}], COS key: [{}].
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:cacheBlock(java.lang.String,long):[WARN] Failed to cache block with id + blockId + , pool + bpid + : volume not found.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] New instance created
org.apache.hadoop.hdfs.server.datanode.DataStorage:linkBlocks(java.io.File,java.io.File,java.lang.String,int,org.apache.hadoop.fs.HardLink,org.apache.hadoop.conf.Configuration):[ERROR] There are {} duplicate block entries within the same volume.
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:initCosmosDBClient(org.apache.hadoop.conf.Configuration):[INFO] Creating Cosmos DB Writer Async Client...
org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature:initializeBlockRecovery(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,boolean):[WARN] BLOCK* BlockUnderConstructionFeature.initializeBlockRecovery: No blocks found, lease removed.
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeRMDelegationTokenState(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[INFO] Removing RMDelegationToken_{sequenceNumber}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],boolean):[INFO] updatePipeline( + oldBlock.getLocalBlock() + , newGS= + newBlock.getGenerationStamp() + , newLength= + newBlock.getNumBytes() + , newNodes= + Arrays.asList(newNodes) + , client= + clientName + )
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handleContainerStatus(java.util.List):[DEBUG] Container {containerId} is the first container get launched for application {containerAppId}
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration):[WARN] Encountered exception during format
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkStreamers():[DEBUG] healthy streamer count= ...
org.apache.hadoop.mapreduce.task.reduce.MergeThread:startMerge(java.util.Set):[INFO] <getName>: Starting merge with <toMergeInputs.size()> segments, while ignoring <inputs.size()> segments
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Processing {} of type {}, event.getNodeId(), event.getType()
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReportImpl(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.List):[TRACE] Added block {} to CACHED list.
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[DEBUG] Getting pid for container {containerIdStr} to kill from pid file {pidFilePath}
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] File is not splittable so no parallelization is possible: [file.getPath()]
org.apache.hadoop.yarn.event.AsyncDispatcher:dispatch(org.apache.hadoop.yarn.event.Event):[ERROR] Error in dispatcher thread
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:trackContainerForPreemption(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Allocated resource updated
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[WARN] Fsck: can't copy the remains of {fullName} to {lost+found} because {target} already exists.
org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[]):[WARN] options parsing failed: + e.getMessage()
org.apache.hadoop.mapreduce.JobResourceUploader:useSharedCache(java.net.URI,java.lang.String,java.util.Map,org.apache.hadoop.conf.Configuration,boolean):[WARN] Shared cache does not support directories (see YARN-6097). Will not upload {filePath} to the shared cache.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:bootstrap(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration):[INFO] Removing CPU constraints for YARN containers.
org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key):[DEBUG] Filesystem {} created while awaiting semaphore
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:maybeFlush(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent):[DEBUG] Flush invoked
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:readReplicasFromCache(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Successfully read replica from cache file : path
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:addDiagnostics(java.lang.String[]):[WARN] Unable to update diagnostics in state store for [containerId], e
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getClusterMetricsInfo():[ERROR] Subcluster {} failed to return Cluster Metrics.
org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer():[WARN] Failed to build URI for auditor: {exception_message}
org.apache.hadoop.hdfs.server.datanode.DataStorage:removeDuplicateEntries(java.util.ArrayList,java.util.ArrayList):[WARN] Unexpectedly low genstamp on {}.
org.apache.hadoop.fs.http.server.HttpFSServer:post(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] Unset storage policy [path]
org.apache.hadoop.lib.server.Server:init():[INFO] Home dir: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:appAdminClientCleanUp(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl):[WARN] Type-specific cleanup of application {app.applicationId} of type {app.applicationType} did not succeed with exit code {result}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:populateTokenSequenceNo(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest,org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[DEBUG] Sending System credentials for apps as part of NodeHeartbeat response.
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$JobFailWaitTimedOutTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Timeout expired in FAIL_WAIT waiting for tasks to get killed. Going to fail job anyway
org.apache.hadoop.hdfs.server.namenode.FSImage:finalizeUpgrade(boolean):[INFO] Finalizing upgrade for local dirs.
org.apache.hadoop.fs.cosn.CosNOutputStream:close():[INFO] Upload the last part..., blockId: [{}], written bytes: [{}]
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam):[ERROR] Unsupported operation attempted
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:setConfig(java.lang.Boolean):[DEBUG] Default rule instantiated with default queue name: {}, and create flag: {}
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint():[INFO] Deleted trash checkpoint: + dir
org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB:init(org.apache.hadoop.conf.Configuration):[INFO] Connected to InMemoryAliasMap at {}
org.apache.hadoop.tools.HadoopArchiveLogs:checkFilesAndSeedApps(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.Path):[INFO] Skipping [appName] due to total file size being too large ([totalFileSize] > [maxTotalLogsSize])
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:call():[INFO] Recovered container [containerId] succeeded
org.apache.hadoop.mapred.QueueManager:refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher):[WARN] MSG_REFRESH_FAILURE_WITH_CHANGE_OF_HIERARCHY
org.apache.hadoop.mapreduce.Cluster:initialize(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration):[DEBUG] Picked ... as the ClientProtocolProvider
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:redirectURI(javax.ws.rs.core.Response$ResponseBuilder,org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[]):[TRACE] redirectURI=<generated-URI>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.CentralizedOpportunisticContainerAllocator:allocatePerSchedulerKey(long,org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.util.Set,int):[INFO] Opportunistic allocation requested for [priority={}, allocationRequestId={}, num_containers={}, capability={}] allocated = {}
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String,org.apache.hadoop.fs.azure.SelfRenewingLease):[WARN] Got unexpected exception trying to acquire lease on key.{ExceptionMessage}
org.apache.hadoop.fs.s3a.impl.RenameOperation:recursiveDirectoryRename():[DEBUG] rename: renaming directory {} to {}
org.apache.hadoop.hdfs.server.namenode.NameNode:reconfigureBlockInvalidateLimit(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed blockInvalidateLimit to {}
org.apache.hadoop.security.authentication.util.KerberosName:getShortName():[WARN] auth_to_local rule mechanism not set. Using default of DEFAULT_MECHANISM
org.apache.hadoop.mapreduce.CryptoUtils:wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataInputStream):[DEBUG] IV read from Stream [ + Base64.encodeBase64URLSafeString(iv) + ]
org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean):[WARN] Couldn't read "/proc/meminfo"; can't determine memory settings
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:throttle():[DEBUG] Re-encryption handler throttling because total tasks pending re-encryption updater is {}
org.apache.hadoop.tools.rumen.HistoryEventEmitter:parseCounters(java.lang.String):[WARN] HistoryEventEmitters: null counter detected:
org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int):[WARN] Quitting election but indicating that fencing is necessary
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue):[INFO] newState + has added children in refresh
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:containerStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerStartData):[INFO] Start information of container + containerStart.getContainerId() + is written
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Initializing DelegationTokenManager for {}
org.apache.hadoop.hdfs.tools.GetGroups:setConf(org.apache.hadoop.conf.Configuration):[DEBUG] Using NN principal: + nameNodePrincipal
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[INFO] Killed {jobName} ({jobID})
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for fileId: {}
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(java.lang.Object,java.lang.Object):[INFO] [FLEX UP COMPONENT + component.getName() + ]: scaling up from + before + to + event.getDesired()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startActiveServices():[INFO] Reprocessing replication and invalidation queues
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:updateNewContainerInfo(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[INFO] update exist container (containerId), strExposedPorts = (strExposedPorts)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:deleteRMStateStore(org.apache.hadoop.conf.Configuration):[INFO] State store deleted
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerDiagnostics(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.StringBuilder):[DEBUG] storeContainerDiagnostics: containerId={}, diagnostics=
org.apache.hadoop.hdfs.server.namenode.JournalSet:getEditLogManifest(long):[WARN] Cannot list edit logs in {FileJournalManager}
org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation):[INFO] Credentials
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$KillAllocatedAMTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[INFO] Transition initiated
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:setUsername(com.zaxxer.hikari.HikariDataSource,java.lang.String):[DEBUG] Setting non NULL Username for Store connection
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgrade(org.apache.hadoop.yarn.service.api.records.Service,java.util.List):[INFO] instances to upgrade {containerIdsToUpgrade}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Storing RMDTMasterKey., e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:validateResourceRequestsAgainstQueueMaxResource(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.Resource):[TRACE] Resources with zero amount: ...
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:put(java.lang.String,byte[],boolean):[INFO] {} already existed and we are not updating
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:logAndThrowRetriableException(org.slf4j.Logger,java.lang.String,java.lang.Throwable):[ERROR] {errMsg}, {t}
org.apache.hadoop.hdfs.server.namenode.NNStorage:writeTransactionIdFileToStorage(long):[WARN] writeTransactionIdToStorage failed on {}
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:run():[INFO] Setting up application submission context for ASM
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:truncate(java.lang.String,long,java.lang.String):[DEBUG] *DIR* NameNode.truncate: src to newLength
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean):[INFO] Source is a file
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String):[ERROR] {f.toString()}
org.apache.hadoop.yarn.service.component.Component$DecommissionInstanceTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] Instance {} already decommissioned
org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration):[DEBUG] Server accepts auth methods:[...]
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[TRACE] Evaluating rule, subnet: {}, path: {}
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String):[WARN] The value + n + <= 0: it is parsed from the string \ + s + \ which is the index + i + element in \ + originalString + \
org.apache.hadoop.fs.s3a.impl.ChangeTracker:processResponse(com.amazonaws.services.s3.model.S3Object,java.lang.String,long):[DEBUG] Copy result {}: {}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumberOfMissingBlocks():[DEBUG] Failed to get number of missing blocks
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getLogs(java.lang.String,java.lang.String,java.lang.String,java.lang.String):[DEBUG] Can not access the aggregated log for the container:{}
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getInitialCachedResources(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[WARN] Key key is already mapped to file initialMappedEntry; file fileName will not be added
org.apache.hadoop.yarn.event.AsyncDispatcher$MultiListenerHandler:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Handling event
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL {url} (Took {latency} ms.)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:reAttachUAMAndMergeRegisterResponse(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] No existing UAM for application {} found in Yarn Registry
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map):[INFO] {} running containers including AM recovered from home RM {}
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleCommit(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.FileHandle,long,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,int):[INFO] Inactive with pending write
org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder():[WARN] Ticket is already destroyed, remove it.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createLocatedBlock(org.apache.hadoop.hdfs.server.blockmanagement.LocatedBlockBuilder,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long):[WARN] Inconsistent number of corrupt replicas for {...} + blockMap has {...} but corrupt replicas map has {...}
org.apache.hadoop.tools.mapred.CopyMapper:copyFileWithRetry(java.lang.String,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.mapreduce.Mapper$Context,org.apache.hadoop.tools.mapred.CopyMapper$FileAction,java.util.EnumSet):[INFO] FILE_COPIED: source=<sourcePath>, size=<sourceLen> --> target=<targetPath>, size=<targetLen>
org.apache.hadoop.yarn.service.utils.HttpUtil:generateToken(java.lang.String):[DEBUG] The user credential is {}
org.apache.hadoop.yarn.service.ServiceMaster:doSecureLogin():[INFO] No keytab localized at <keytab>
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInputValidator:validateReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystem,org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String):[INFO] validate reservation input
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:allocate(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Pre-check for node candidate set
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:msync(org.apache.hadoop.fs.Path):[TRACE] {}: msync('{}'), getName(), path
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[DEBUG] requested offset={} and current offset={}
org.apache.hadoop.hdfs.server.namenode.CacheManager:removeDirective(long,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker):[WARN] removeDirective of + id + failed: , e
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] MXBean registered and service initialized
org.apache.hadoop.registry.server.dns.RegistryDNS:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Error initializing Registry DNS Server, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Available resource information: + availableRI
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[]):[ERROR] Missing options: must have host, port and api
org.apache.hadoop.hdfs.web.URLConnectionFactory:openConnection(java.net.URL,boolean):[DEBUG] open URL connection
org.apache.hadoop.yarn.service.component.Component:assignContainerToCompInstance(org.apache.hadoop.yarn.api.records.Container):[INFO] [COMPONENT {}]: No pending component instance left, release surplus container {}
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:addPersistedPassword(org.apache.hadoop.security.token.Token):[DEBUG] Adding password for + identifier.getApplicationAttemptId()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:get(java.lang.Class):[ERROR] Cannot fetch records for {}
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntity(java.lang.String,java.lang.String,java.lang.Long,java.util.EnumSet,org.iq80.leveldb.DBIterator,byte[],int):[WARN] Error while decoding...
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:start():[DEBUG] JvmPauseMonitor started
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:deleteAppLogDir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.yarn.server.nodemanager.DeletionService,java.lang.String):[INFO] delete app log dir,application_n_n_DEL_n
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:shouldRun(java.lang.String,java.lang.String):[WARN] File {} for script "{}" does not exist.
org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl:getContainerSASUri(java.lang.String,java.lang.String):[DEBUG] Retrieving Container SAS URI For {}@{}
org.apache.hadoop.util.RunJar:run(java.lang.String[]):[ERROR] Delete failed for workDir
org.apache.hadoop.fs.s3a.S3AInputStream:readCombinedRangeAndUpdateChildren(org.apache.hadoop.fs.impl.CombinedFileRange,java.util.function.IntFunction):[DEBUG] Finished reading range {} from path {}
org.apache.hadoop.mapred.gridmix.JobSubmitter$SubmitTask:run():[INFO] Time taken to build splits for job: [JobID] ms.
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:writeSpillFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.conf.Configuration):[DEBUG] writeSpillFileCB.. path:{}; pos:{}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest):[DEBUG] Selected from StateStore the policy for the queue: {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[ERROR] Error in handling event type + event.getType() + for node + nodeId
org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean):[DEBUG] Error while changing permission : filename Exception: exceptionMessage
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:getECTopologyVerifierResult(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.util.Collection):[TRACE] No erasure coding policy is given.
org.apache.hadoop.hdfs.server.datanode.BlockRecoveryWorker$RecoveryTaskStriped:recover():[WARN] Recovery for replica " + block + " on data-node " + id + " is already in progress. Recovery id = " + rBlock.getNewGenerationStamp() + " is aborted.
org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction):[ERROR] Exception while getting login user
org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String):[INFO] PowerShell command: {cmd}
org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path):[ERROR] Got Exception while build resolve result. ResultKind:%s, resolvedPathStr:%s, targetOfResolvedPathStr:%s, remainingPath:%s, will return null.
org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean):[DEBUG] Initiating re-login for {username}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent):[DEBUG] {} reported decommissioning, eventNode
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:commitLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Exception when scheduling the event Commit re-initialization of Container containerId
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:getAllPartialJobs():[DEBUG] Called getAllPartialJobs()
org.apache.hadoop.fs.s3a.S3AInputStream:seekInStream(long,long):[WARN] Failed to seek on {uri} to {targetPos}. Current position {pos}
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[INFO] Deleting absolute path : {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stopAggregators():[WARN] Aggregation stop interrupted!
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[TRACE] Removed BR lease 0x{Long.toHexString(id)} for DN {dn.getDatanodeUuid()}. numPending = {numPending}
org.apache.hadoop.hdfs.server.namenode.NNStorage:determineClusterId():[WARN] couldn't find any VERSION file containing valid ClusterId
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:refreshCaches(boolean):[ERROR] Cache update failed for cache {}
org.apache.hadoop.hdfs.server.sps.ExternalSPSBlockMoveTaskHandler$BlockMovingTask:moveBlock():[WARN] Failed to move block:{} from src:{} to destin:{} to satisfy storageType:{}
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Active Nodes: countHere
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Using default endpoint -no need to generate a configuration
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent):[DEBUG] {} reported usable, eventNode
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode):[DEBUG] Initializing SSL Context to channel mode Default_JSSE_with_GCM
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map):[WARN] Localized resource is null!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Committer statistics
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setVcoresPerNode(java.lang.String,int):[DEBUG] DRConf - setVcoresPerNode: nodePrefix={}, vcores={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean):[DEBUG] No pending resource for: nodeType=, node=, requestKey=, application=
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationAttemptStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptStartData):[ERROR] Error when writing start information of application attempt
org.apache.hadoop.fs.cosn.CosNOutputStream:waitForFinishPartUploads():[ERROR] Interrupt the part upload.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getNextSubDir(java.lang.String,java.io.File):[TRACE] getNextSubDir({}, {}): no subdirectories found in {}
org.apache.hadoop.ipc.Server$Connection:buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[]):[DEBUG] Will send state token of size replyToken.length from saslServer.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:recoverAssignedGpus(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] ContainerId {} is assigned to GpuDevice {} on recovery.
org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options):[INFO] Key creation with KMSClientProvider successful
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:getECTopologyVerifierResult(int,int,java.util.Collection):[TRACE] No erasure coding policy is given.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:getDiagnosticsLimitKCOrThrow(org.apache.hadoop.conf.Configuration):[ERROR] DIAGNOSTIC_LIMIT_CONFIG_ERROR_MESSAGE
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext):[INFO] Copying from {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceStop():[INFO] ContainersMonitorImpl monitoring thread interrupted
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:getAllVolumesMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Time to add replicas to map for block pool on volume : ms
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:getBlockList():[TRACE] getBlocks( + getDatanodeInfo() + , + StringUtils.TraditionalBinaryPrefix.long2String(size, B, 2) + ) returns + newBlksLocs.getBlocks().length + blocks.
org.apache.hadoop.mapred.TaskAttemptListenerImpl:fatalError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String,boolean):[ERROR] Task: <taskAttemptID> - exited : <msg>
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Proxy users configuration refreshed
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:sendStreamOutputResponse(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,boolean):[DEBUG] Exception
org.apache.hadoop.hdfs.DFSInputStream:isResolveableAndLocal(java.net.InetSocketAddress):[DEBUG] Got an error checking if {} is local
org.apache.hadoop.service.CompositeService:removeService(org.apache.hadoop.service.Service):[DEBUG] Removing service {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getConfigVersion():[INFO] Failed to read config version at {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:getResourceDemandFromAppsPerQueue(java.lang.String,java.lang.String):[DEBUG] Selected to preempt {} resource from partition:{}
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[INFO] {}: Saving pending data information to {}
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:validateRangeRequest(java.util.List,long):[WARN] Requested range [%d, %d) is beyond EOF for path %s
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logSetQuota(java.lang.String,long,long):[INFO] Diskspace quota set
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$FSAction:runWithRetries():[INFO] Maxed out FS retries. Giving up!
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getContainerReport(org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Failed to fetch container report from ATS v2
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Retrieved file status: not null
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:addIfAbsent(org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo):[WARN] Waiting to remove MOVE_FAILED state histories (e.g. firstMoveFailedKey) from JobListCache because it is not in done yet. Total count is moveFailedCount.
org.apache.hadoop.hdfs.server.namenode.BackupNode:registerWith(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[ERROR] Registration rejected by ... Shutting down.
org.apache.hadoop.hdfs.DFSStripedOutputStream:flushAllInternals():[WARN] Caught ExecutionException while waiting all streamer flush,
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:sendLifelineIfDue():[DEBUG] Skipping sending lifeline for + BPServiceActor.this + , because it is not due.
org.apache.hadoop.fs.s3a.S3AUtils:propagateBucketOptions(org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Ignoring bucket option {}
org.apache.hadoop.hdfs.qjournal.server.Journal:getSegmentInfo(long):[INFO] getSegmentInfo( + segmentTxId + ): + elf + -> + TextFormat.shortDebugString(ret) + ; journal id: + journalId
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[DEBUG] Initializing configuration store
org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory:getCluster(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Creating NameNode connector
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:commit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit,java.lang.String):[WARN] Failed to commit upload against unknown destination, described in {}: {}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker):[INFO] Recovered {numRecovered} replicas from {lazypersistDir}
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:initializePipeline(java.lang.String):[ERROR] Init RESTRequestInterceptor error for user: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:markContainerForKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] {}: container {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String):[WARN] Metrics system not started: + e.getMessage()
org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] "getFileStatus({}) failed; returning null"
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createServiceManager():[INFO] Using SystemServiceManager: schedulerClassName
org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]):[ERROR] Automatic failover is not enabled for localTarget. Please ensure that automatic failover is enabled in the configuration before running the ZK failover controller.
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:resetDigestACLs():[DEBUG] Cleared digest ACLs
org.apache.hadoop.mapred.gridmix.Statistics$StatCollector:run():[ERROR] Statistics io exception while polling JT
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:activateNextMasterKey():[INFO] Activating next master key with id: ${keyId}
org.apache.hadoop.streaming.StreamXmlRecordReader:init():[INFO] StreamBaseRecordReader.init: start_=... end_=... length_=... start_ > in_.getPos() =...
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier):[ERROR] Error in removing RMDelegationToken with sequence number: {ident.getSequenceNumber()}
org.apache.hadoop.hdfs.DFSClient:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,boolean,short,long,org.apache.hadoop.util.Progressable,int,org.apache.hadoop.fs.Options$ChecksumOpt,java.net.InetSocketAddress[],java.lang.String):[DEBUG] {}: masked={}
org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable):[DEBUG] Queue: {} responseTime: {} backoffThreshold: {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processQueuedMessages(java.lang.Iterable):[DEBUG] Processing previouly queued message
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:copyFileToStream(java.io.OutputStream,java.io.File,java.io.FileInputStream,org.apache.hadoop.hdfs.util.DataTransferThrottler,org.apache.hadoop.hdfs.util.Canceler):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:completeContainerLaunch():[INFO] yarn docker env var has been set {}
org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer):[WARN] Unknown rpc kind + header.getRpcKind() + from client + getHostAddress()
org.apache.hadoop.yarn.server.AMRMClientRelayer:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[WARN] Out of sync with RM ${rmId} for ${this.appId}, hence resyncing.
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:read(java.nio.ByteBuffer):[TRACE] read(arr.length={}, off={}, len={}, filename={}, block={}, canSkipChecksum={}): returning {}
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:updateLogAggregationStatus(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.LogAggregationStatus,long,java.lang.String,boolean):[WARN] Ignore the log aggregation status update request for the application: + appId + . The cached log aggregation status is + tracker.getLogAggregationStatus() + .
org.apache.hadoop.hdfs.server.federation.store.StateStoreCache:loadCache(boolean):[INFO] CachedRecordStore cache loaded
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader:getParallelExecutorService():[INFO] The fsimage will be loaded in parallel using {} threads
org.apache.hadoop.ipc.Server:stop():[INFO] Stopping server on
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[INFO] IS_RELATED_TO field filter added
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Missing configuration for fs.viewfs.mounttable.path. Proceeding with core-site.xml mount-table information if available.
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$FSAction:runWithRetries():[INFO] Maxed out FS retries. Giving up!
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(java.lang.Object):[INFO] {}: Cleanup of {} disabled, getName(), baseDir
org.apache.hadoop.lib.server.Server:init():[INFO] Server [{}] starting
org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock:render():[ERROR] Error while reading conf file: confPath
org.apache.hadoop.hdfs.DataStreamer:queuePacket(org.apache.hadoop.hdfs.DFSPacket):[DEBUG] Queued {}, {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:checkFaultTolerantRetry(org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,java.lang.String,java.io.IOException,org.apache.hadoop.hdfs.server.federation.resolver.RemoteLocation,java.util.List):[ERROR] Cannot invoke {} for {}: {}
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] ACL not found for access-type {} for application {} owned by {}. Using default [{}]
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String):[DEBUG] New token service set. Token: ({})
org.apache.hadoop.mapreduce.v2.hs.JobHistory$HistoryCleaner:run():[INFO] History Cleaner started
org.apache.hadoop.mapred.gridmix.Statistics:add(java.lang.Object):[DEBUG] Reached maximum limit of jobs in a polling interval + completedJobsInCurrentInterval
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Timeline service is not enabled
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int,int):[DEBUG] Check the condition for main loop.
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Keytab <keytabStatus>
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterConfig,org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData,java.lang.Throwable,boolean,boolean):[DEBUG] Summary directory set in to {}{}<OPT_SUMMARY_REPORT_DIR, reportDir>
org.apache.hadoop.mapred.BackupStore:reset():[DEBUG] Reset - First segment offset is + firstSegmentOffset + Segment List Size is + segmentList.size()
org.apache.hadoop.yarn.api.ApplicationMasterProtocol:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Resource manager allocation request completed
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getMissingLogSegments(java.util.List,org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto,org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer$JournalNodeProxy):[ERROR] EditLogManifest response does not have fromUrl field set. Aborting current sync attempt
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Application recovery: ApplicationId=[id], AttemptCount=[count], FinalState=NONE
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:formatSchedulerConfiguration(javax.servlet.http.HttpServletRequest):[ERROR] Exception thrown when formating configuration
org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry):[TRACE] Execution trace
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop():[DEBUG] AHS client stopped
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[ERROR] Unable to update token + tokenId.getSequenceNumber(), e
org.apache.hadoop.fs.azure.NativeFileSystemStore:delete(java.lang.String):[DEBUG] Deleting blob with key: ${key}
org.apache.hadoop.fs.http.server.FSOperations$FSMkdirs:execute(org.apache.hadoop.fs.FileSystem):[INFO] Operation mkdirs incremented
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:doConsistencyCheck():[WARN] SafeMode is in inconsistent filesystem state. BlockManagerSafeMode data: blockTotal={}, blockSafe={}; BlockManager data: activeBlocks={}
org.apache.hadoop.fs.s3a.impl.NetworkBinding:logDnsLookup(org.apache.hadoop.conf.Configuration):[DEBUG] Bucket endpoint : {}, Hostname : {}, DNSAddress : {}
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[INFO] Loading the INode section in parallel with {} sub-sections
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:loadFromDisk(org.apache.hadoop.conf.Configuration):[INFO] Finished loading FSImage in {timeTakenToLoadFSImage} msecs
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkStreamers():[DEBUG] checkStreamers: ...
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeProxies():[ERROR] Cannot sync as there is no other JN available for sync.
org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl:initialize(org.apache.hadoop.conf.Configuration):[DEBUG] Initializing RemoteSASKeyGeneratorImpl instance
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockChecksumComputer:crcPartialBlock():[INFO] Stream closed
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:copyBytes(org.apache.hadoop.tools.CopyListingFileStatus,long,java.io.OutputStream,int,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Updating context status
org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[INFO] MergeQueue started merging
org.apache.hadoop.hdfs.server.federation.router.RouterFsck:fsck():[WARN] Fsck error_message
org.apache.hadoop.hdfs.server.namenode.FSDirectory:addLastINodeNoQuotaCheck(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode):[WARN] FSDirectory.addChildNoQuotaCheck - unexpected
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollEdits():[INFO] Executing rollEditLog
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection):[DEBUG] Reported block {} on {} size {} replicaState = {}
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:serviceStop():[WARN] failed to stop the flusher task in time. will still proceed to close the writer.
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationHomeSubClusterRequest):[DEBUG] Got the information about the specified application {}. The AM is running in {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:addRMContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] recovered container + id + from previous attempt + rmContainer.getApplicationAttemptId()
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:scheduleOldDBsForEviction():[INFO] Scheduling ${getName()} eviction for ${fdf.format(entry.getKey())}
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionStart(java.lang.String):[ERROR] Fail to start application:
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplitRatio(int,int):[WARN] nMaps == 1. Why use DynamicInputFormat?
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:unRegisterNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.UnRegisterNodeManagerRequest):[INFO] Node with node id : + nodeId + has shutdown, hence unregistering the node.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[INFO] Queue for application {applicationId} not found during recovery. Placed in recovery queue.
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[INFO] Updating info for app: + appId
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:cleanupExpiredQueues(java.lang.String,boolean,java.util.Set,java.lang.String):[INFO] Killing applications in queue: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$NvidiaCommandExecutor:searchBinary():[INFO] Use nvidia gpu binary: pathOfGpuBinary
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:rollMasterKey():[INFO] Rolling master-key for amrm-tokens
org.apache.hadoop.lib.server.Server:init():[INFO] Runtime information:
org.apache.hadoop.registry.server.services.RegistryAdminService$AsyncPurge:call():[DEBUG] Executing {org.apache.hadoop.registry.server.services.RegistryAdminService$AsyncPurge}
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String):[WARN] Illegal value: there is no element in \" + s + \".
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Endpoint URI = {epr}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction):[DEBUG] checkAccess operation started
org.apache.hadoop.fs.s3a.S3AFileSystem:processDeleteOnExit():[DEBUG] The exception for deleteOnExit is {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:reportResourceUsage(org.apache.hadoop.yarn.api.records.ContainerId,long,float):[INFO] {} does not exist to report, containerId
org.apache.hadoop.fs.s3a.S3AFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction):[DEBUG] check access mode {} for {}
org.apache.hadoop.hdfs.DFSInputStream:refreshBlockLocations(java.util.Map):[DEBUG] Refreshing {} for path {}
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:setProperty(com.zaxxer.hikari.HikariDataSource,java.lang.String,java.lang.String):[DEBUG] Setting property {} with value {}, property, value
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:validatePlacement(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.ConstraintPlacementAlgorithmOutput,java.util.List,java.util.Map):[WARN] Got exception from TagManager !
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEvent):[INFO] Container {containerId} not running, nothing to signal.
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration):[INFO] Usage printed
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter$KMSResponse:sendError(int,java.lang.String):[WARN] Logging warning with status code
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.AbstractPreemptableResourceCalculator:resetCapacity(org.apache.hadoop.yarn.api.records.Resource,java.util.Collection,boolean):[DEBUG] Initialized active resources
org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode():[INFO] Successfully created + znodeWorkingDir + in ZK.
org.apache.hadoop.crypto.key.kms.server.KMS:invalidateCache(java.lang.String):[TRACE] Exiting invalidateCache for key name {}.
org.apache.hadoop.fs.s3a.S3AFileSystem:uploadPart(com.amazonaws.services.s3.model.UploadPartRequest,org.apache.hadoop.fs.statistics.DurationTrackerFactory):[ERROR] Incrementing put completed statistics with exception
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] Got response from RM for container ask, completedCnt=
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[INFO] Demotion Update requests : + demotionRequests
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceStop():[INFO] Stopping NamenodeHeartbeat service for, NS {} NN {} , this.nameserviceId, this.namenodeId
org.apache.hadoop.mapreduce.security.TokenCache:mergeBinaryTokens(org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration):[INFO] Binary tokens merged successfully
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] ContainersMonitor enabled: {}
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[DEBUG] Configuring job jar
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.ReservationAgent:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] Updated reservation using PlanningAlgorithm
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:bootstrap(java.lang.String,int,int):[INFO] TC configuration is already in place. Not wiping state.
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:init():[INFO] Using FileSystemAccess JARs version {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeCachePool(java.lang.String,boolean):[ERROR] Exception in writeLock during removeCachePool, {poolName: {poolName}}, false
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerStartFailedEventOnTimelineServiceV2(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[ERROR] Container start failed event could not be published for containerId
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handle(org.apache.hadoop.yarn.event.Event):[WARN] Very low remaining capacity in the event-queue of RMContainerAllocator: + remCapacity
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed:refresh():[ERROR] ReplicaCachingGetSpaceUsed refresh error
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[DEBUG] Skipping set to false
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:run():[INFO] NMClientAsync initialized
org.apache.hadoop.yarn.service.component.Component$FlexComponentTransition:transition(java.lang.Object,java.lang.Object):[INFO] [FLEX DOWN COMPONENT + component.getName() + ]: scaling down from + before + to + event.getDesired()
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[WARN] Exception while trying to get password for alias {alias}: {ioe.getMessage()}
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,long):[INFO] storing RMDelegation token with sequence number
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateOrReserveNewContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,boolean):[WARN] Exception when trying to get exclusivity
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state, e
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:loadFSImage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption):[INFO] Need to save fs image? true (staleImage=false, haEnabled=false, isRollingUpgrade=false)
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent):[INFO] Speculator notified for request
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:createBlobClient(java.net.URI):[ERROR] createBlobClient is an invalid operation in SAS Key Mode
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:executeStage(org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Arguments):[DEBUG] Task attempt dir not found
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:logNodeIsNotChosen(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason):[DEBUG] Datanode is not chosen since
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[WARN] The queried SubCluster: {} does not exist.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:updateStarvedApps():[INFO] Updating starved apps with remaining minshare
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:loggingCostTable(java.util.Map):[DEBUG] The costTable is:...
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[INFO] Container resources localized
org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl:run():[ERROR] Error while trying to run jobs.
org.apache.hadoop.yarn.applications.distributedshell.Client:prepareTimelineDomain():[WARN] Cannot put the domain + domainId + because the timeline service is not enabled
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService$ApplicationEventHandler:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] Event {event} sent to absent application {ApplicationID}
org.apache.hadoop.fs.s3a.S3AInstrumentation$InputStreamStatistics:merge(boolean):[DEBUG] Merging statistics into FS statistics in {}: {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource):[DEBUG] Registered source + name
org.apache.hadoop.fs.s3a.S3ListResult:logAtDebug(org.slf4j.Logger):[DEBUG] Prefix count = {}; object count={}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:listStatusIterator(org.apache.hadoop.fs.Path):[TRACE] {}: listStatusIterator('{}'), getName(), path
org.apache.hadoop.mapred.YARNRunner:setupLocalResources(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Job jar is not present. Not adding any jar to the list of resources.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:getBytesSentPerContainer():[WARN] No bytes sent metric found for container: + containerId + with classId: + classId
org.apache.hadoop.util.AsyncDiskService:awaitTermination(long):[WARN] AsyncDiskService awaitTermination timeout.
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:run():[DEBUG] GetMapEventsThread about to sleep
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[WARN] Can not load log meta from the log file
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[WARN] Got an overlapping write {}, nextOffset={}. Remove and trim it
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:generateContainerStatusList():[DEBUG] NodeManager {} released container ({}).
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:loadBpStorageDirectories(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.datanode.StorageLocation,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[WARN] Failed to analyze storage directories for block pool {}, nsInfo.getBlockPoolID(), e
org.apache.hadoop.mapred.lib.CombineFileInputFormat:listStatus(org.apache.hadoop.mapred.JobConf):[DEBUG] Listing status for job
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Application cache size is {}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction):[INFO] rollingUpgrade FINALIZE
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:getLocalizationStatusesInternal(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.security.NMTokenIdentifier,java.lang.String):[INFO] Getting localization status for {containerID}
org.apache.hadoop.ipc.Server$Responder:doRunLoop():[INFO] connection aborted from {call.connection}
org.apache.hadoop.yarn.webapp.WebApps$Builder:build(org.apache.hadoop.yarn.webapp.WebApp):[ERROR] dev mode does NOT work with ephemeral port!
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List):[DEBUG] Waiting for task completion
org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils:getDefaultFileContext():[INFO] Default file system is set solely by core-default.xml therefore - ignoring
org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint():[INFO] Created trash checkpoint: {checkpoint.toUri().getPath()}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignToFailedMap(org.apache.hadoop.yarn.api.records.Container):[INFO] Assigned from earlierFailedMaps
org.apache.hadoop.tools.dynamometer.ApplicationMaster:cleanup():[ERROR] Failed to unregister application
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:sendRequest(byte[],int,int):[DEBUG] Getting output stream failed without expect header enabled, throwing exception
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.files.SuccessData,java.lang.Throwable,boolean,boolean):[INFO] Job summary saved to {path}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:stopContainerInternal(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String):[INFO] Container killed by the ApplicationMaster.
org.apache.hadoop.util.Progress:set(float):[DEBUG] Illegal progress value found, progress is Float.NEGATIVE_INFINITY. Progress will be changed to 0
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:createUserCacheDirs(java.util.List,java.lang.String):[WARN] Unable to create file cache directory : {}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {username}
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:getEncryptionKeyInfo(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.crypto.CryptoProtocolVersion[]):[DEBUG] Start file with no key
org.apache.hadoop.yarn.api.ApplicationMasterProtocol:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] Allocating resources
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:addBlockToBeErasureCoded(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor[],org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[],byte[],byte[],org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy):[DEBUG] Adding block reconstruction task [task] to [getName], current queue size is [erasurecodeBlocks.size]
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Not enabling OAuth2 in WebHDFS
org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL server DIGEST-MD5 callback: setting password for client: tokenIdentifier.getUser()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: MAX_CHILD_CAPACITY
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean):[INFO] TrashPolicyDefault#deleteCheckpoint for trashRoot: + trashRoot
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set):[INFO] Service stopped, return null for the storage
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[ERROR] Unable to kill workload app ({})
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:handleNodeReport(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder,java.lang.String,java.lang.String):[INFO] Node report generated for each DiskBalancerDataNode
org.apache.hadoop.security.authentication.util.KerberosName:getDefaultRealm():[DEBUG] Kerberos krb5 configuration not found, setting default realm to empty
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool():[DEBUG] Logs rolled while catching up to current segment
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:unpublishVolume(org.apache.hadoop.yarn.server.volume.csi.VolumeMetaData):[INFO] Un-publish volume {}, request {}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest):[ERROR] Unable to create the reservation
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[INFO] Log reader initialized
org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline:createStreams(boolean,org.apache.hadoop.util.DataChecksum):[DEBUG] writeTo metafile is ... of size ...
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteObject:close(java.io.InputStream,int):[DEBUG] initiating asynchronous drain of {} bytes
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:notifyNewSubmission():[DEBUG] Notifying handler for new re-encryption command.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler$UpdateThread:run():[ERROR] Exception in scheduler UpdateThread
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[WARN] Ignoring state store operation failure because the resource manager is not configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.
org.apache.hadoop.lib.server.Server:destroyServices():[INFO] Services destroyed
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processCompletedNodes(java.util.List):[INFO] Node {} isn't healthy. It needs to replicate {} more blocks. {} is still in progress.
org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject):[INFO] Token file {} does not exist
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:replaceExpiredDelegationToken():[DEBUG] Replaced expired token: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptFailedFinalStateSavedTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] {msg}
org.apache.hadoop.yarn.service.webapp.ApiServer:getComponentInstances(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List,java.lang.String,java.util.List):[INFO] GET: component instances for service = {}, compNames in {}, version = {}, containerStates in {}, user = {}
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long):[DEBUG] mode.toString() + " TrustStore: " + truststoreLocation + ", reloading at " + truststoreReloadInterval + " millis."
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:send(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.DatanodeID,javax.crypto.SecretKey):[DEBUG] SASL client doing encrypted handshake for addr = {}, datanodeId = {}
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[ERROR] Native output collector doesn't support this key, this key is not comparable in native
org.apache.hadoop.ipc.Server$Responder:run():[INFO] "Stopping " + Thread.currentThread().getName()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logModifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean):[INFO] logEdit invoked
org.apache.hadoop.tools.dynamometer.Client:run():[INFO] Running Client
org.apache.hadoop.hdfs.server.namenode.JournalSet:mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String):[ERROR] Error: status failed for required journal (journal_and_stream)
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp$EDEKCacheLoader:run():[INFO] Failed to warm up EDEKs.
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:retrievePassword(org.apache.hadoop.yarn.security.NMTokenIdentifier):[DEBUG] NMToken password retrieved successfully!!
org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]):[DEBUG] SASL server DIGEST-MD5 callback: setting canonicalized client ID: username
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.api.records.NodeId,boolean):[DEBUG] Containers allocated on multiple nodes
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long):[DEBUG] Created instance {}, instance
org.apache.hadoop.ha.ZKFailoverController:recheckElectability():[INFO] Ensuring that ... does not participate in active master election
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache:createApplicationDir(org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] New app directory created - {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[WARN] Removing unknown + containerEvent.getContainerID() + from application + app.toString()
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] VolumeImpl VOLUME_ID transitioned from OLD_STATE to NEW_STATE
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:main(java.lang.String[]):[ERROR] Failed to start secondary namenode
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishApplicationAttemptEvent(org.apache.hadoop.yarn.client.api.TimelineClient,java.lang.String,org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$DSEvent,java.lang.String,org.apache.hadoop.security.UserGroupInformation):[ERROR] App Attempt start/end event could not be published for {appAttemptID}, {e}
org.apache.hadoop.mapred.YARNRunner:createApplicationSubmissionContext(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.Credentials):[WARN] Invalid reservationId: {jobConf.get(JobContext.RESERVATION_ID)} specified for the app: {applicationId}
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesStoreEvent):[WARN] Unsupported operation
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:renewDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.RenewDelegationTokenRequest):[DEBUG] Getting the current user information
org.apache.hadoop.hdfs.DFSInputStream:seek(long):[WARN] BlockReader failed to seek to targetPos. Instead, it seeked to pos.
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[INFO] Scheduler metrics set to running
org.apache.hadoop.hdfs.ViewDistributedFileSystem:canonicalizeUri(java.net.URI):[WARN] Failed to resolve the uri as mount path
org.apache.hadoop.tools.mapred.CopyCommitter:deleteMissing(org.apache.hadoop.conf.Configuration):[DEBUG] Comparing {0} and {1}
org.apache.hadoop.mapreduce.JobSubmitter:populateTokenCache(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials):[DEBUG] adding the following namenodes' delegation tokens: [nameNode1, nameNode2...]
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy:stopAllProxies():[ERROR] Error closing connection
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:retrievePassword(org.apache.hadoop.yarn.security.AMRMTokenIdentifier):[DEBUG] Trying to retrieve password for {applicationAttemptId}
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:tryToMakeBetterReservationPlacement(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,java.util.List):[INFO] Successfully moved reserved container={reservedContainerId} from targetNode={fromNode} to targetNode={targetNode.getNodeID()}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Can't get path for fileId: {}
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:rollMasterKey():[INFO] Going to activate master-key with key-id ...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$CompactionTimerTask:run():[ERROR] Error compacting database
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[WARN] Exception when trying to cleanup container {containerIdStr}: {StringUtils.stringifyException(e)}
org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class):[DEBUG] Initializing with protocol and prefix
org.apache.hadoop.lib.servlet.HostnameFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[WARN] Request remote address could not be resolved, {0}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.mapreduce.JobResourceUploader:uploadResourcesInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path):[DEBUG] default FileSystem: {jtFs.getUri()}
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseFromNextRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[DEBUG] Failed to choose from the next rack (location = {}), retry choosing randomly
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[INFO] ResourceRequest: C satisfied preempting D
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:removeShm(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm):[TRACE] removing shm + shm
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:finalizeUpgrade():[DEBUG] Checking operation UNCHECKED
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean):[INFO] Deleted trash checkpoint: ...
org.apache.hadoop.util.DurationInfo:close():[DEBUG] {}
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient:getAliasMap(java.lang.String):[ERROR] Exception in retrieving block pool id {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:delete(org.apache.hadoop.fs.Path,boolean):[DEBUG] AzureBlobFileSystem.delete path: {} recursive: {}, f.toString(), recursive
org.apache.hadoop.fs.s3a.WriteOperationHelper:writeFailed(java.lang.Exception):[DEBUG] Write to {} failed
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:validMessageLength(int):[DEBUG] Portmap mapping registration failed, the response size is less than 28 bytes: [len]
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSCreate operation executed
org.apache.hadoop.registry.server.dns.ReverseZoneUtils:getSubnetCountForReverseZones(org.apache.hadoop.conf.Configuration):[ERROR] The subnet or mask is invalid: Subnet: {} Mask: {}
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] hadoop login commit
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:executeWriteBack():[INFO] Another async task is already started before this one is finalized. fileId: {} asyncStatus: {} original startOffset: {} new startOffset: {}. Won't change asyncStatus here.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:cancelDelegationToken(javax.servlet.http.HttpServletRequest):[INFO] Renew delegation token request failed
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStarted(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Map):[DEBUG] Succeeded to start DataNode Container + containerId
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:topologyAwareSchedule(java.util.Set,int,java.util.Map,java.util.Set,java.util.Map):[INFO] Topology scheduler allocated: + allocation
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:logAndThrowRetriableException(org.slf4j.Logger,java.lang.String,java.lang.Throwable):[ERROR] {errMsg}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode):[DEBUG] pre-assignContainers
org.apache.hadoop.hdfs.server.namenode.Checkpointer:doCheckpoint():[INFO] Unable to roll forward using only logs. Downloading image with txid {sig.mostRecentCheckpointTxId}
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[ERROR] Failed to parse file fileName
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:preemptReducer(int):[INFO] Going to preempt " + toPreempt + " due to lack of space for maps
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:allocate(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[INFO] Assigning NUMA node + numaNode.getNodeId() + for memory, + numaNode.getNodeId() + for cpus for the + containerId
org.apache.hadoop.mapreduce.JobResourceUploader:copyLog4jPropertyFile(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,short):[DEBUG] default FileSystem: + jtFs.getUri()
org.apache.hadoop.yarn.util.RackResolver:coreResolve(java.util.List):[INFO] Got an error when resolve hostNames. Falling back to DEFAULT_RACK for all.
org.apache.hadoop.hdfs.server.datanode.BPOfferService:reportRemoteBadBlock(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.ExtendedBlock):[WARN] Couldn't report bad block block to actor, e
org.apache.hadoop.security.authentication.server.AuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] sample log statement for debug
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:get(java.lang.Class):[DEBUG] There is a temporary file {} in {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,io.netty.handler.codec.http.HttpRequest):[INFO] op=GETFILESTATUS target=path
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllSchemas(org.apache.hadoop.conf.Configuration,boolean):[INFO] Will skip existing tables and continue on htable creation exceptions!
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid RENAME request
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:refreshNamenodes(org.apache.hadoop.conf.Configuration):[INFO] Refresh request received for nameservices: ...
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo:updateJiffy(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo):[WARN] Sum of stime () and utime () is greater than
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Publishing the entity, JSON-style content
org.apache.hadoop.mapred.MapTask:run(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.TaskUmbilicalProtocol):[DEBUG] Running job setup task
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal():[WARN] Failed to create archives for + appId
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getApps(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set,java.util.Set,java.lang.String,java.util.Set):[DEBUG] Initializing service
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap):[WARN] Only able to place ... of total expected ...
org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot):[WARN] Waited above threshold(%d ms) to acquire lock: lock identifier: %s waitTimeMs=%d ms. Suppressed %d lock wait warnings. Longest suppressed WaitTimeMs=%d. The stack trace is: %s
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[DEBUG] Getting Namenode Name Service ID
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$SnapshotDiffSectionProcessor:process():[DEBUG] Processing SnapshotDiffSection
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KillOnNewTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] NMAuditLogger: Success
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] Skipping blockpool <nnc.getBlockpoolID()>
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:requestCreated(com.amazonaws.AmazonWebServiceRequest):[DEBUG] Creating a request outside an audit span, unaudited
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor:run():[WARN] Unexpected throwable:
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:commit(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS COMMIT fileHandle: {} offset={} count={} client: {}
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[INFO] Sleeping for ...ms before retrying again. Got null now.
org.apache.hadoop.fs.cosn.CosNFileSystem:mkDirRecursively(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Making dir: [{}] in COS
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Audit event logged
org.apache.hadoop.yarn.server.resourcemanager.DBManager$CompactionTimerTask:run():[ERROR] Error compacting database
org.apache.hadoop.fs.s3a.S3AInstrumentation:incrementGauge(org.apache.hadoop.fs.s3a.Statistic,long):[DEBUG] No Gauge: + op
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:submitApp(org.apache.hadoop.yarn.api.records.ReservationId):[INFO] Submit a new application {$appId}
org.apache.hadoop.fs.s3a.S3AFileSystem:removeKeysS3(java.util.List,boolean):[DEBUG] Partitioning the keys to delete as it is more than page size. Number of keys: {}, Page size: {}
org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin():[DEBUG] Renewed ticket. kinit output: {}
org.apache.hadoop.tools.dynamometer.Client:monitorInfraApplication():[INFO] Kill the application using: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:prepareOutputFiles(java.lang.String,boolean):[INFO] Console mode is enabled, {YARN_SITE_XML} and {CAPACITY_SCHEDULER_XML} will be only emitted to the console!
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:shutdown():[INFO] Interrupted waiting to join on checkpointer thread
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[INFO] Task failed
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[WARN] short-circuit read access is disabled for DataNode {datanode}. reason: {resp.getMessage()}
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection):[DEBUG] Looking for a token with service {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersAllocated(java.util.List):[INFO] The requested number of containers have been allocated. Releasing the extra container allocation from the RM.
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$MetricsBase:render():[DEBUG] Rendering metrics div
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:checkApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.ApplicationHomeSubCluster):[WARN] Missing ApplicationHomeSubCluster Info. Please try again by specifying an ApplicationHomeSubCluster information.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[DEBUG] Scanning intermediate dir ...
org.apache.hadoop.fs.Globber:doGlob():[DEBUG] Component {}, patterned={}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadFSEdits(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,long):[INFO] Start loading edits file
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean):[ERROR] No shared edits directory configured for namespace + nsId + namenodeId
org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String):[INFO] Setting the includes file to <includesFile>
org.apache.hadoop.hdfs.server.namenode.BackupImage:recoverCreateRead():[INFO] Formatting ...
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render():[INFO] Resource types
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + (Took + latency + ms.)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startRollingUpgrade():[INFO] Rolling upgrade started
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(long,byte[],int,int):[DEBUG] read requested b = null offset = {} len = {}, off, len
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:put(org.apache.hadoop.yarn.server.timeline.EntityIdentifier,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity):[ERROR] e.getMessage()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getFinalConfigPath(org.apache.hadoop.fs.Path):[WARN] tempPath + does not end with ' + TMP + ' return null
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,java.net.InetAddress,org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger$ArgsBuilder):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:handleEvent(org.apache.hadoop.mapreduce.v2.app.rm.ContainerAllocatorEvent):[INFO] Processing the event
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:addApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User):[DEBUG] Adding runnable application: application.getApplicationAttemptId()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[INFO] Can't get path for toHandle fileId: {}
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenSingleCall(java.lang.String,java.lang.String,java.util.Hashtable,java.lang.String,boolean):[DEBUG] {logMessage}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.InternalScanner,org.apache.hadoop.hbase.regionserver.ScanType,org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest):[INFO] Compactionrequest= ... MINOR_COMPACTION/Major_COMPATION ... RegionName= ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:call():[WARN] Unable to locate pid file for container ${containerIdStr}
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:run():[DEBUG] got
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] State Store metrics not enabled
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[INFO] preempting Y running task: Z
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:collect(java.lang.Object,java.lang.Object,int):[INFO] Record too large for in-memory buffer
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection):[DEBUG] nthValidToReturn is {}
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit():[DEBUG] Using local user: {user}
org.apache.hadoop.yarn.util.Times:elapsed(long,long):[WARN] Finished time + finished + is ahead of started time + started
org.apache.hadoop.lib.server.Server:init():[INFO] Built timestamp : {}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:write(org.apache.hadoop.oncrpc.XDR,io.netty.channel.Channel,int,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Error writing to fileId {} at offset {} and length {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:lookupBinaryInDefaultDirsInternal():[WARN] Failed to locate GPU device discovery binary, tried paths: + triedBinaryPaths + ! Please double check the value of config + YarnConfiguration.NM_GPU_PATH_TO_EXEC + . Using default binary: + DEFAULT_BINARY_NAME
org.apache.hadoop.yarn.server.timeline.security.TimelineDelgationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Secret key interval obtained
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:getContainerReqToReplace(org.apache.hadoop.yarn.api.records.Container):[INFO] Replacing MAP container
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[DEBUG] Cleaned up container resources
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanFile(org.apache.hadoop.hdfs.protocol.CacheDirective,org.apache.hadoop.hdfs.server.namenode.INodeFile):[TRACE] Directive {}: can't cache block {} because it is in state {}, not COMPLETE.
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:closeInMemoryFile(org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput):[INFO] closeInMemoryFile -> map-output of size: ... , inMemoryMapOutputs.size() -> ... , commitMemory -> ... , usedMemory -> ...
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:readResultsWithTimestamps(org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix,org.apache.hadoop.yarn.server.timelineservice.storage.common.KeyConverter):[DEBUG] null prefix was specified; returning all columns
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadDataStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[WARN] Failed to upgrade storage directory {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getRollingUpgradeStatus():[WARN] Encountered exception setting Rollback Image
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Finalize upgrade for + sd.getRoot() + is complete.
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:logAllocatedBlock(java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo):[INFO] BLOCK* allocate ... for ...
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[DEBUG] INFO field filter added
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getMountTable():[ERROR] Cannot generate JSON of mount table from store: {exception_message}
org.apache.hadoop.hdfs.client.HdfsUtils:isHealthy(java.net.URI):[DEBUG] Is namenode in safemode? true; uri= + uri
org.apache.hadoop.hdfs.server.namenode.GlobalStateIdContext:receiveRequestState(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,long):[WARN] The client stateId: {} is greater than the server stateId: {}
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker:getVolumesLowOnSpace():[DEBUG] Going to check the following volumes disk space: + volumes
org.apache.hadoop.hdfs.server.namenode.LeaseManager:checkLeases(java.util.Collection):[INFO] {} has expired hard limit
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:setDecommissionedNMs():[INFO] Handling decommission event
org.apache.hadoop.registry.server.dns.ApplicationServiceRecordProcessor:initTypeToInfoMapping(org.apache.hadoop.registry.client.types.ServiceRecord):[INFO] serviceRecord.description: No external endpoints defined.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignMapsWithLocality(java.util.List):[DEBUG] Assigned based on * match
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:lock():[INFO] Locking is disabled for {}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:scheduleReduces(int,int,int,int,int,int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int,float,float):[INFO] Ramping up
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:releaseContainers(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt):[ERROR] Unauthorized access or invalid container
org.apache.hadoop.mapred.LocalJobRunner$Job:statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[INFO] {stateString}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path):[TRACE] {}: {} createNewDirectory('{}'), getName(), operation, path
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getAclStatus(java.lang.String):[INFO] Get locations for path
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:handleException(java.lang.Exception,java.lang.String,long,java.lang.String):[ERROR] Error while processing REST request, e
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:storeEvents(byte[],java.util.Set,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix,org.apache.hadoop.yarn.server.timelineservice.storage.common.TypedBufferedMutator):[WARN] timestamp is not set for event eventId! Using the current timestamp
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createEncryptionZone(java.lang.String,java.lang.String,boolean):[INFO] Encryption zone created
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkAddress(java.lang.String):[WARN] Missing SubCluster Endpoint information. Please try again by specifying SubCluster Endpoint information.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent):[WARN] Destroyed application resources
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:replayLog(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand):[DEBUG] IOException:
org.apache.hadoop.hdfs.server.namenode.NameNode:stop():[WARN] Encountered exception while exiting state
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuNodeResourceUpdateHandler:updateConfiguredResource(org.apache.hadoop.yarn.api.records.Resource):[ERROR] GPU is enabled, but could not find any usable GPUs on the NodeManager!
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:refreshNodeAttributesToScheduler(org.apache.hadoop.yarn.api.records.NodeId):[INFO] Updated NodeAttribute event to RM: {newNodeToAttributesMap}
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:startStorage():[INFO] Creating state database at [dbfile]
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:onChangeMonitoringContainerResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEvent,org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Changing resource-monitoring for {}
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest):[INFO] submitApplication appId {appId} try #0 on SubCluster {subClusterId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:verifyAccess():[WARN] The remote file + remotePath + has changed since it's localized; will not consider it for upload
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanupStagingDir():[ERROR] Failed to cleanup staging dir + jobTempDir, io
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanUpApplicationsOnNMShutDown():[INFO] Waiting for Applications to be Finished
org.apache.hadoop.hdfs.DeadNodeDetector:checkDeadNodes():[DEBUG] Add dead node to check: {datanodeInfo}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:handle(org.apache.hadoop.yarn.event.Event):[INFO] Created localizer for locId
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getECTopologyResultForPolicies(java.lang.String[]):[INFO] getEcTopologyVerifierResultForEnabledPolicies
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock():[DEBUG] block[{}]: skipping re-entrant closeBlock()
org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory():[WARN] Unable to get user name. Fall back to system property user.name
org.apache.hadoop.mapreduce.util.ProcessTree:sendSignal(java.lang.String,int,java.lang.String):[INFO] Signaling process ${pid} with ${signalName}. Exit code ${shexec.getExitCode()}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:stopStandbyServices():[INFO] Stopping services started for {} state
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:moveBlock(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.net.Socket,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataEncryptionKeyFactory,org.apache.hadoop.security.token.Token):[INFO] Successfully moved block
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:getSaslStreams(java.net.InetAddress,java.io.OutputStream,java.io.InputStream,org.apache.hadoop.security.token.Token,javax.crypto.SecretKey):[DEBUG] DataNode overwriting downstream QOP
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:launchAppHistoryServer(java.lang.String[]):[ERROR] Error starting ApplicationHistoryServer
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:doCheckpoint():[WARN] Exception encountered while saving legacy OIV image; continuing with other checkpointing steps
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:middleStep():[DEBUG] Container {} has completed
org.apache.hadoop.mapred.FileInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int):[DEBUG] Total # of splits generated by getSplits: + splits.size() + , TimeTaken: + sw.now(TimeUnit.MILLISECONDS)
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkSubClusterId(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId):[WARN] Missing SubCluster Id information. Please try again by specifying Subcluster Id information.
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:removePartFiles():[WARN] Failed to delete temporary file {}
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object):[INFO] op=GETCONTENTSUMMARY target=path
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode):[DEBUG] Node + node.getNodeName() + offered to queue: + getName() + fairShare: + getFairShare()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getAuthParameters(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op):[INFO] Returning authentication parameters
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection):[DEBUG] Chosen node {} from first random
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:waitForRecoveredContainers():[INFO] Waiting for containers: [...]
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSASTokenToQuery(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsUriQueryBuilder):[TRACE] Using cached SAS token.
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(byte[]):[ERROR] Encountered Storage Exception for write on Blob : {} Exception details: {} Error Code : {}
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:doCopy(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.Mapper$Context,java.util.EnumSet):[INFO] Writing to {useTempTarget ? "temporary" : "direct"} target file path {targetPath}
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInputValidator:getPlanFromQueue(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystem,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[ERROR] validate reservation input
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set):[DEBUG] Thread sleep in monitoring loop interrupted
org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String):[WARN] Error sending metrics to StatsD, e
org.apache.hadoop.tools.HadoopArchives:main(java.lang.String[]):[DEBUG] Exception in archives
org.apache.hadoop.fs.s3a.Invoker:ignoreIOExceptions(org.slf4j.Logger,java.lang.String,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE):[INFO] Executing operation action on path
org.apache.hadoop.crypto.JceAesCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration):[WARN] {exception message placeholder}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:internalAssignDevices(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Try allocating {} {}, requestedDeviceCount, resourceName
org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo):[DEBUG] Debug message with throwable
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:updateResourceUsagePerUser(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean):[DEBUG] User resource is updated. Total Resource usage for active users=totalResUsageForActiveUsers.getAllUsed(). Total Resource usage for non-active users=totalResUsageForNonActiveUsers.getAllUsed()
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:refreshCallQueue():[INFO] Refreshing call queue.
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:checkCapability(java.lang.String):[WARN] Invalid capability information. Please try again by specifying valid Capability Information.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Queue x is STOPPED. Cannot accept submission of application: y
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:editSchedule():[DEBUG] Total time used= + (clock.getTime() - startTs) + ms.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getAclStatus(java.lang.String):[ERROR] logAuditEvent(false, getAclStatus, src)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[ERROR] Error storing resource state for + rsrc
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:flushAndSync(boolean):[INFO] Nothing to flush
org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask():[WARN] Another reconfiguration task is running.
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:sendContainerLaunchFailedMsg(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,java.lang.String):[ERROR] {message}
org.apache.hadoop.tools.mapred.CopyCommitter:preserveFileAttributesForDirectories(org.apache.hadoop.conf.Configuration):[INFO] Preserved status on + preservedEntries + dir entries on target
org.apache.hadoop.security.SaslInputStream:readMoreData():[DEBUG] Actual length is + length
org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Uncaught exception in thread {} -exiting
org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path):[WARN] Unresolved link encountered
org.apache.hadoop.yarn.applications.distributedshell.Client:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Thread sleep in monitoring loop interrupted
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:getLatestImages():[ERROR] This is a rare failure scenario!!!
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getPathStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] getPathStatus for filesystem: {} path: {}
org.apache.hadoop.streaming.PipeReducer:configure(org.apache.hadoop.mapred.JobConf):[INFO] Reducer configuration started
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[INFO] Unauthenticated call marked
org.apache.hadoop.yarn.applications.distributedshell.Client:setAMResourceCapability(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.Map,java.util.List):[WARN] AM Memory not specified, use {DEFAULT_AM_MEMORY} mb as AM memory
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader:loadEdits():[ERROR] Got IOException at position + inputStream.getPosition()
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:run():[WARN] Failed to delete {}
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getInterceptorChain(javax.servlet.http.HttpServletRequest):[ERROR] Cannot get user: {}
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[ERROR] Application: %s is not found
org.apache.hadoop.tools.util.DistCpUtils:toCopyListingFileStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,boolean,boolean,boolean,int):[DEBUG] toCopyListing: + fileStatus + chunkSize: + blocksPerChunk + isDFS: + (fileSystem instanceof DistributedFileSystem)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$IBRTaskHandler:run():[ERROR] Exception in IBRTaskHandler.
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:processResponseQueue():[INFO] Application {}'s AM is going to be killed. Waiting for rescheduling...
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:requestCreated(com.amazonaws.AmazonWebServiceRequest):[TRACE] Created Request {} in span {}
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:verifyAdminAccess(org.apache.hadoop.yarn.security.YarnAuthorizationProvider,java.lang.String,org.slf4j.Logger):[WARN] User ${user.getShortUserName()} doesn’t have permission to call '${method}'
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:allocate(float):[WARN] ApplicationMaster is out of sync with ResourceManager, hence resyncing.
org.apache.hadoop.mapred.gridmix.Statistics:addJobStats(org.apache.hadoop.mapred.gridmix.Statistics$JobStats):[INFO] Not tracking job + stats.getJob().getJobName() + as seq id is less than zero: + seq
org.apache.hadoop.mapred.gridmix.LoadJob:buildSplits(org.apache.hadoop.mapred.gridmix.FilePool):[DEBUG] String.format("SPEC(%d) %d -> %d %d %d %d %d %d %d", id(), i, i + j * maps, info.getOutputRecords(), info.getOutputBytes(), info.getResourceUsageMetrics().getCumulativeCpuUsage(), info.getResourceUsageMetrics().getPhysicalMemoryUsage(), info.getResourceUsageMetrics().getVirtualMemoryUsage(), info.getResourceUsageMetrics().getHeapUsage())
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy:call():[WARN] Exception from remote name node + currentNN + , try next.
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:getProxyAddresses():[WARN] Could not locate {} - skipping
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest):[ERROR] RPCUtil.getRemoteException(new AccessControlException)
org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent):[INFO] Session disconnected. Entering neutral mode...
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:postDelegationTokenExpiration(javax.servlet.http.HttpServletRequest):[DEBUG] Delegation token renewed successfully
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set):[WARN] Waited ... seconds after process completed for AppReport to reach desired final state. Not waiting anymore. CurrentState = ..., ExpectedStates = ...
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:nextOp():[INFO] Next operation retrieved from backup input stream
org.apache.hadoop.yarn.client.cli.TopCLI:setTerminalWidth():[WARN] Couldn't determine terminal width, setting to 80
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:loadRuleMap(java.lang.String):[DEBUG] Loaded rule: user: {}, network/bits: {} path: {}
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:initializeInputStream(java.net.HttpURLConnection):[DEBUG] open file: + conn.getURL()
org.apache.hadoop.fs.FileSystem:getHomeDirectory():[WARN] Unable to get user name. Fall back to system property user.name
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextValidOp():[WARN] encountered an exception
org.apache.hadoop.hdfs.server.datanode.DataNode:refreshVolumes(java.lang.String):[ERROR] Failed to remove volume
org.apache.hadoop.hdfs.server.federation.router.PeriodicService:serviceStop():[INFO] Stopping periodic service {serviceName}
org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile():[WARN] Error closing the stream [IOException]
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:initializeLocalizationStatusRetriever(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] {} retrieve localization statuses, compInstanceId
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[INFO] Current application state of {} is {}, will retry later.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getFileStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] AzureBlobFileSystem.getFileStatus path: {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext,java.lang.String):[WARN] The atomic rename feature is not supported by the ABFS scheme; however rename, create and delete operations are atomic if Namespace is enabled for your Azure Storage account.
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:enqueueContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[INFO] Opportunistic container [containerId] will not be queued at the NM since max queue length [maxOppQueueLength] has been reached
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:evictOldDBs():[INFO] Removing old db directory contents in {path}
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain):[INFO] Writing to HBaseTimelineWriter
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:updateContainerStatus(org.apache.hadoop.yarn.api.records.ContainerStatus):[INFO] new IP = {status.getIPs()}, host = {status.getHost()}, updating registry
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[TRACE] Block {}: DataNode {} is not a valid possibility because the block has size {}, but the DataNode only has {} bytes of cache remaining ({} pending bytes, {} already cached.)
org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation):[DEBUG] closeAllForUGI UGI: <ugi>
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:getEntryToEvict():[DEBUG] Got one inactive stream:
org.apache.hadoop.mapreduce.v2.hs.HistoryServerStateStoreService:removeTokenMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] Token master key removed from file system state store
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:seek(long):[DEBUG] Seek to position {}. Bytes skipped {}, pos, this.pos
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[INFO] modifyCachePool of {info.getPoolName()} successful; set limit to {info.getLimit()}
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:renewDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.RenewDelegationTokenRequest):[DEBUG] Renewing the token
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Restored {} block files from trash., restored
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getNode(java.lang.String):[WARN] Failed to get node report
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:renameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,boolean,org.apache.hadoop.fs.Options$Rename[]):[DEBUG] DIR* FSDirectory.renameTo: path1 to path2
org.apache.hadoop.ha.ActiveStandbyElector:monitorLockNodeAsync():[INFO] Ignore duplicate monitor lock-node request.
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities):[ERROR] The owner of the posted timeline entities is not set
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop():[INFO] Interrupted Exception while stopping
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:doRecovery():[WARN] Unable to delete dir + curFile + before rename
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:cleanup(org.apache.hadoop.hdfs.server.federation.router.ConnectionPool):[DEBUG] Removed connection {} used {} seconds ago. Pool has {}/{} connections
org.apache.hadoop.streaming.PipeMapRed:envPut(java.util.Properties,java.lang.String,java.lang.String):[DEBUG] Add env entry: + name + = + value
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:listContainerLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[ERROR] Exception logged from logContainerLogs: (ex.getMessage())
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfDfsUsageParameters(java.lang.String,java.lang.String):[INFO] RECONFIGURE* changed {} to {}
org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall):[DEBUG] Adding saslServer wrapped token of size + token.length + as call response.
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:updateLogAggregationStatus(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.LogAggregationStatus,long,java.lang.String,boolean):[WARN] Ignore the log aggregation status update request for the application: + appId + . The request log aggregation status update is older than the cached log aggregation status.
org.apache.hadoop.yarn.sls.SLSRunner:start():[INFO] DNS Caching enabled
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread:run():[WARN] Failed to cancel token [Token_Info] [Exception]
org.apache.hadoop.mapred.lib.MultipleOutputs$InternalFileOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable):[INFO] New instance created
org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable:run():[INFO] Starting task: + mapId
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean):[INFO] logAuditEvent: success, operationName, src, null, auditStat
org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client):[DEBUG] removing client from cache: [ClientInstance]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:addAndScheduleAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar):[DEBUG] Task attempt added
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String,java.lang.String[],java.lang.String,boolean,boolean,org.apache.hadoop.hdfs.server.federation.resolver.order.DestinationOrder,org.apache.hadoop.hdfs.tools.federation.RouterAdmin$ACLEntity):[DEBUG] Creating new mount table entry
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam):[ERROR] Unsupported operation exception thrown
org.apache.hadoop.security.alias.LocalKeyStoreProvider:getOutputStreamForKeystore():[DEBUG] using ' + file + ' for output stream.
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllSchemas(org.apache.hadoop.conf.Configuration,boolean):[INFO] Schema creation finished successfully
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:createRetriableProxy():[ERROR] Unable to create proxy to the ResourceManager
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:doSpnegoSequence(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] No subject in context, logging in
org.apache.hadoop.crypto.key.kms.server.KMS:getKey(java.lang.String):[DEBUG] Getting key information for key with name {}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recoverApplication(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto):[DEBUG] Recovering Flow context: {} for an application {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String):[DEBUG] BLOCK* NameSystem.abandonBlock: {b} is removed from pendingCreates
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:unsetErasureCodingPolicy(java.lang.String,boolean):[INFO] Permission check passed
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState):[INFO] Recovering storage directory {} from failed checkpoint
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] parameters = <parameters>
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,boolean,boolean):[INFO] {}: no pending commits to abort
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite():[TRACE] range.getMin()={} nextOffset={}
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectInputStreams(java.util.Collection,long,boolean):[DEBUG] Tailing edits starting from txn ID + fromTxnId + via RPC mechanism
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:checkPlacementPoliciesPresent(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler,org.apache.hadoop.conf.Configuration):[INFO] Parsing allocation file
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor:pendingReconstructionCheck():[WARN] PendingReconstructionMonitor timed out [block]
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[INFO] Namenode is in safemode. It will retry again.
org.apache.hadoop.yarn.server.resourcemanager.AdminService:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest):[DEBUG] Configuration loaded
org.apache.hadoop.hdfs.qjournal.client.QuorumCall:waitFor(int,int,int,int,java.lang.String):[INFO] Waited X ms (timeout=Y ms) for a response for Z...
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:cleanupContainersOnNMResync():[WARN] Interrupted while sleeping on container kill on resync
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:middleStep():[DEBUG] NodeManager {} releases an AM ({}) or
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet):[WARN] Error while decoding:otherInfo
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setPermission(java.lang.String,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] Audit: setPermission successful for src
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:alternateAuthenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[INFO] sending redirect to: + loginURL
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:postDelegationTokenExpiration(javax.servlet.http.HttpServletRequest):[ERROR] Authorization failed during setAuthenticationMethod
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementContextWithParent(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping,java.lang.String):[WARN] Placement rule specified a parent queue {}, but it is not a managed parent queue, and no queue exists with name {} under it.
org.apache.hadoop.hdfs.DFSClient:createWrappedOutputStream(org.apache.hadoop.hdfs.DFSOutputStream,org.apache.hadoop.fs.FileSystem$Statistics):[DEBUG] Decrypted EDEK for file: {dfsos.getSrc()}, output stream: 0x{Integer.toHexString(dfsos.hashCode())}
org.apache.hadoop.hdfs.DeadNodeDetector:probeCallBack(org.apache.hadoop.hdfs.DeadNodeDetector$Probe,boolean):[DEBUG] Remove the node out from suspect node list: {}.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEvent):[ERROR] Unable to update container resource in store
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:handleNodeReport(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder,java.lang.String,java.lang.String):[INFO] The value for '-node' is neither specified or empty.
org.apache.hadoop.fs.audit.CommonAuditContext:remove(java.lang.String):[TRACE] Remove context entry {}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:publishContainerStartEvent(org.apache.hadoop.yarn.client.api.TimelineClient,org.apache.hadoop.yarn.api.records.Container,java.lang.String,org.apache.hadoop.security.UserGroupInformation):[ERROR] Container start event could not be published for [container_id]
org.apache.hadoop.mapreduce.v2.app.webapp.TasksBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] TaskInfo object initialized
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil:getGenerationStampFromFile(java.io.File[],java.io.File,int):[WARN] Block <blockFile> does not have a metafile!
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:run():[ERROR] {} encountered fatal exception and exit., getName(), t
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor$MonitorThread:run():[WARN] Got failure attempting to read from storage, assuming Storage is down
org.apache.hadoop.mapred.pipes.Application$PingSocketCleaner:run():[DEBUG] close socket cause client has closed.
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:uploadCurrentBlock(boolean):[DEBUG] Writing block # {}
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop():[INFO] prefix + metrics system stopped (again)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:deleteRMConfStore(org.apache.hadoop.conf.Configuration):[DEBUG] ResourceManager initialized with conf
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:scan():[INFO] Scan Results: {}
org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object):[DEBUG] Created instance for key {}: {} overwritten by {}, key, strongRef, resolvedStrongRef
org.apache.hadoop.fs.azurebfs.oauth2.RefreshTokenBasedTokenProvider:refreshToken():[DEBUG] AADToken: refreshing refresh-token based token
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:nullProcedure():[DEBUG] NFS NULL
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:close():[DEBUG] Upload complete to {} by {}, key, writeOperationHelper
org.apache.hadoop.streaming.StreamJob:parseArgv():[WARN] -dfs option is deprecated, please use -fs instead.
org.apache.hadoop.tools.dynamometer.Client:attemptCleanup():[INFO] Attempting to clean up remaining running applications.
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:readResultsWithTimestamps(org.apache.hadoop.hbase.client.Result,byte[],byte[],org.apache.hadoop.yarn.server.timelineservice.storage.common.KeyConverter,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,boolean):[DEBUG] null prefix was specified; returning all columns
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String):[WARN] Disk error on Unknown DataNode: ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:updateRuleSet(java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Initialising new rule set
org.apache.hadoop.hdfs.protocol.ClientProtocol:fsync(java.lang.String,long,java.lang.String,long):[INFO] fsync called in RouterRpcServer
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] JvmMetrics singleton initialized
org.apache.hadoop.fs.cosn.CosNCopyFileTask:run():[INFO] Thread.currentThread().getName() + "copying..."
org.apache.hadoop.hdfs.DFSClient:clearDataEncryptionKey():[DEBUG] Clearing encryption key
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:exportKeys():[DEBUG] Exporting access keys
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] No such priority={}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Cannot rename a directory to a subdirectory of self
org.apache.hadoop.io.erasurecode.CodecRegistry:updateCoders(java.lang.Iterable):[ERROR] Coder {coderFactory.getClass().getName()} cannot be registered because its coder name {coderFactory.getCoderName()} has conflict with {coder.getClass().getName()}
org.apache.hadoop.hdfs.util.IOUtilsClient:cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[]):[DEBUG] Exception in closing closeable
org.apache.hadoop.io.MapFile$Reader:readIndex():[WARN] Unexpected EOF reading index at entry #count. Ignoring.
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:getConnConfigurator(org.apache.hadoop.security.ssl.SSLFactory):[DEBUG] Cannot load customized ssl related configuration. Fallback to system-generic settings.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[ERROR] Failed to reload allocations file
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[]):[TRACE] url={}
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshUserToGroupsMappings():[ERROR] Refresh user to groups mapping failed for address
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:processResponseQueue():[INFO] Application {} goes to finish.
org.apache.hadoop.hdfs.DFSStripedInputStream:createBlockReader(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,org.apache.hadoop.hdfs.protocol.LocatedBlock[],org.apache.hadoop.hdfs.StripeReader$BlockReaderInfo[],int,long):[WARN] Failed to connect to dnInfo.addr for block block.getBlock(), e
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$Dir:getPath():[DEBUG] Not root inode with id {} having no parent.
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:addCounter(java.lang.String,java.lang.String,long):[WARN] {name} is not a known counter.
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:copyBlocksToLostFound(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.protocol.LocatedBlocks):[INFO] Fsck: copied the remains of the corrupted file {fullName} to /lost+found
org.apache.hadoop.hdfs.DFSStripedOutputStream:allocateNewBlock():[DEBUG] Allocating new block group. The previous block group: {prevBlockGroup}
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:dumpLocalResources():[DEBUG] {} = {}
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[WARN] The queried SubCluster: {} does not exist.
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForRead(org.apache.hadoop.fs.Path,java.util.Optional,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Fallback to getPathStatus REST call as provided filestatus is not of type VersionedFileStatus
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateSourceTags(java.util.Set):[WARN] Only a single tag can be associated with a placement constraint currently.
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:loadMap(java.util.HashMap,java.lang.String,int,int):[DEBUG] Loading identity map from file {}
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:init(org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor,org.apache.hadoop.yarn.util.ResourceCalculatorPlugin):[INFO] YARN containers restricted to + yarnProcessors + cores
org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] Get kerberos info proto: ...;
org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String):[INFO] Setting the excludes file to <excludesFile>
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:putObjects(java.net.URI,java.lang.String,javax.ws.rs.core.MultivaluedMap,java.lang.Object):[ERROR] Error getting HTTP response from the timeline server.
org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean):[DEBUG] Connecting to + server
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Overwriting file {}
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:parseEntry(java.lang.String,java.lang.String,java.lang.String):[WARN] Failed to parse `%s` in `%s`. Ignoring in the %s list.
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:evict(org.apache.hadoop.fs.azurebfs.services.ReadBuffer):[TRACE] Evicting buffer idx {}; was used for file {} offset {} length {}
org.apache.hadoop.mapred.LocalDistributedCacheManager:makeClassLoader(java.lang.ClassLoader):[INFO] <URL representation of localClasspaths item>
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:getApplicationAttemptEntities(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.util.Map,long,java.lang.String):[INFO] Limit parameter added
org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean):[DEBUG] Failure during shutdown: {} , failure, failure
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:call():[ERROR] Failed to launch container due to configuration error.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[ERROR] Bootstrap + resourceName + failed. Null value got from plugin's getDevices method
org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread:run():[ERROR] Error in SerialJobFactory while waiting for job completion ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String):[WARN] Script envBinaryPath is not executable
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream):[INFO] Bandwidth: MiB/s
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:createShortCircuitReplicaInfo():[WARN] this + I/O error requesting file descriptors. Disabling domain socket {}
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue:write(java.io.DataOutputStream,java.util.Set):[WARN] logFile.getAbsolutePath() + " is a directory. Ignore it."
org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec):[DEBUG] Got recycled decompressor
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager:activateApplication(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] User {} added to activeUsers, currently: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[ERROR] Exception when trying to get usable GPU device
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[DEBUG] Executing remove method with record class and query
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[INFO] Storing reservation allocation. + reservationEvent.getReservationIdName()
org.apache.hadoop.hdfs.server.sps.ExternalStoragePolicySatisfier:getNameNodeConnector(org.apache.hadoop.conf.Configuration):[WARN] Failed to connect with namenode
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] DataNode {} completed successfully, containerId={}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS LOOKUP dir fileHandle: {} name: {} client: {}
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:createFilterListForColsOfInfoFamily():[INFO] fetchColumnsFromFilterList executed for RELATES_TO
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStart():[INFO] Dispatcher started
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:chooseExcessRedundancyStriped(org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] Choose redundant EC replicas to delete from blk_{} which is located in {}
org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable:run():[INFO] Finishing task: + mapId
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:afterOutput():[INFO] Outputting {} more corrupted nodes.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setQuota(java.lang.String,long,long,org.apache.hadoop.fs.StorageType):[INFO] logAuditEvent success
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:validateAndCreateResourceRequest(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,boolean):[WARN] RM app submission failed in validating AM resource request for application
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] User retrieved
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnclosedSegment(long):[INFO] Using already-accepted recovery for segment starting at txid + segmentTxId + : + bestEntry
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:writeChannel(io.netty.channel.Channel,org.apache.hadoop.oncrpc.XDR,int):[DEBUG] WRITE_RPC_END + xid
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:clearQueues():[WARN] Clearing all the queues from StoragePolicySatisfier. So, user requests on satisfying block storages would be discarded.
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteWithoutCloseCheck(org.apache.hadoop.fs.Path,boolean):[WARN] Cannot create directory marker at {}: {}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean):[DEBUG] Attempted to remove a non-existing znode {nodeRemovePath}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Removing RMDelegationToken and SequenceNumber , e
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRename(java.lang.String,java.lang.String,long,boolean,org.apache.hadoop.fs.Options$Rename[]):[DEBUG] logRpcIds called
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration):[INFO] queueName, capacity=..., absoluteCapacity=..., maxCapacity=..., absoluteMaxCapacity=..., state=..., acls=..., labels=..., reservationsContinueLooking=..., orderingPolicy=..., priority=...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] Reserved container X on node Y for application Z
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService$ReplicaLazyPersistTask:run():[WARN] LazyWriter failed to async persist RamDisk block pool id: {bpId} block Id: {blockId}
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog):[INFO] Downloading missing Edit Log from {url} to {jnStorage.getRoot()}
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:updateTimelineCollectorContext(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector):[DEBUG] Setting the flow run id: {}, flowRunId
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:updateJMXParameters(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.NamenodeStatusReport):[ERROR] Cannot get stat from {} using JMX
org.apache.hadoop.mapreduce.CryptoUtils:wrapIfNecessary(org.apache.hadoop.conf.Configuration,java.io.InputStream,long):[DEBUG] IV read from Stream [*Base64EncodedIV*]
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState, getAppAttemptState(), event.getType())
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:finishLogAggregation():[INFO] Application just finished : {applicationId}
org.apache.hadoop.yarn.service.utils.CoreFileSystem:verifyDirectoryNonexistent(org.apache.hadoop.fs.Path):[ERROR] Dir {} exists: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:checkInterfaceCompatibility(java.lang.Class,java.lang.Class):[DEBUG] Method {m.getName()} found in class {actualClass.getSimpleName()}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:addToClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsRequest):[ERROR] Exception in adding labels
org.apache.hadoop.hdfs.DFSOutputStream:flushOrSync(boolean,java.util.EnumSet):[DEBUG] DFSClient flush(): bytesCurBlock={}, lastFlushOffset={}, createNewBlock={}
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:parseTokenFromStream(java.io.InputStream,boolean):[DEBUG] AADToken: fetched token with expiry {token.expiry.toString()}, expiresOn passed: {expiresOnInSecs}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:tryCreatingHistoryDirs(boolean):[INFO] Waiting for FileSystem at {doneDirPrefixPath.toUri().getAuthority()} to be available
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:checkDiskScheduler():[WARN] Unable to determine disk scheduler type for partition <partition>
org.apache.hadoop.security.UserGroupInformation:print():[DEBUG] Groups:
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeReservationState(java.lang.String,java.lang.String):[INFO] Removing state for reservation
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterConfig,org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData,java.lang.Throwable,boolean,boolean):[INFO] Job summary saved to {}
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getListing(java.lang.String,byte[],boolean):[DEBUG] Cannot get listing from {}
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List):[ERROR] Script {scriptName} returned {m.size()} values when {names.size()} were expected.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[WARN] Leaving safe mode due to forceExit. This will cause a data loss of {} byte(s).
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] createSuccessLog(user, operation, target, null, null, null, null)
org.apache.hadoop.fs.s3a.impl.MkdirOperation:execute():[DEBUG] Making directory: {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:execute():[WARN] While completing all active copies
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:requestFileDescriptors(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot):[WARN] error creating ShortCircuitReplica., e
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$MetricsLogRunnable:run():[INFO] e.getMessage()
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:run(org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Configuring multithread runner to use + numberOfThreads + threads
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[WARN] Problem connecting to server: + nnAddr
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:releaseAnyRemainingReservedSpace():[WARN] Block {} has not released the reserved bytes. Releasing {} bytes as part of close.
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkStreamers():[DEBUG] newly failed streamers: ...
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:loadEditRecords(org.apache.hadoop.hdfs.server.namenode.EditLogInputStream,boolean,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext):[ERROR] FSImage.formatEditLogReplayError
org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Failure to retrieve storage account key for {}, accountName, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:pickAndDoSchedule(java.util.Set,java.util.Map,java.util.Set,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,int,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.DevicePluginScheduler):[DEBUG] Customized device plugin implemented, use customized logic
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFolderContentsToDelete(org.apache.hadoop.fs.azure.FileMetadata,java.util.ArrayList):[DEBUG] Cannot delete {} since some of its contents cannot be deleted
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Containers Pending: countHere
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:setAllocatedResourcesForContainers(org.apache.hadoop.yarn.api.records.Resource):[INFO] Setting the resources allocated to containers to {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler$ResourceCommitterService:run():[ERROR] <InterruptedException message>
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:readRegistry(org.apache.hadoop.registry.client.api.RegistryOperations,org.apache.hadoop.security.UserGroupInformation,java.lang.String,boolean):[ERROR] Registry resolve key {key} failed
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[ERROR] Unable to store master key {keyId}, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest):[DEBUG] Used resource= + queueUsage.getUsed(partition) + exceeded maxResourceLimit of the queue = + maxResourceLimit
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:uploadSourceFromFS():[WARN] Empty directories detected and created
org.apache.hadoop.fs.FileSystem:loadFileSystems():[DEBUG] Loading filesystems
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:adjustBlockTotals(int,int):[DEBUG] Adjusting block totals from {}/{} to {}/{}, blockSafe, blockTotal, blockSafe + deltaSafe, blockTotal + deltaTotal
org.apache.hadoop.registry.server.dns.RegistryDNS:initializeZones(org.apache.hadoop.conf.Configuration):[INFO] DNS zones:
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:call():[WARN] Failed to launch container.
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] Block locations retrieved
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration):[INFO] strict mode is set to :{strictMode}{System.lineSeparator()}containers will be allowed to use spare YARN bandwidth.{System.lineSeparator()}containerBandwidthMbit soft limit (in mbit/sec) is set to : {containerBandwidthMbit}
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded$SPSPathIdProcessor:run():[INFO] Interrupted while waiting in SPSPathIdProcessor
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int):[DEBUG] Removing thread capacity: {}. Max wait: {}
org.apache.hadoop.ha.SshFenceByTcpPort:cleanup(com.jcraft.jsch.ChannelExec):[WARN] Couldn't disconnect ssh channel
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:render():[INFO] Getting list of all Jobs.
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:createACLfromUsername(java.lang.String,int):[DEBUG] Appending kerberos realm to make {}
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long):[WARN] No KEY found for persisted identifier {identifier.toString()}
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateAudiences(com.nimbusds.jwt.SignedJWT):[WARN] JWT audience validation failed.
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:rescanCacheDirectives():[DEBUG] Directive {}: the directive expired at {} (now = {})
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyNames():[TRACE] Exiting getKeyNames method.
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:storeEdits(byte[],long,long,int):[ERROR] Attempted to cache data of length %d with newStartTxn %d and newEndTxn %d
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:init(org.apache.hadoop.conf.Configuration):[INFO] Initializing ZooKeeper connection
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:reacquireContainerClasses(java.lang.String):[WARN] Unable to match classid in string: [tcClass]
org.apache.hadoop.hdfs.qjournal.server.JournalNode:getOrCreateJournal(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption):[INFO] Journal created
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:enterSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.EnterSafeModeRequest):[ERROR] Unable to enter safemode.
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadRMDelegationKeyState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] Content of + childNodePath + is broken.
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:validateOverwrite(java.lang.String,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename destination {dst} already exists
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:getAppPriority(javax.servlet.http.HttpServletRequest,java.lang.String):[INFO] Priority retrieved for app: {appId}
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] No such resourceName={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[ERROR] Incorrect format for ip and host
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType):[DEBUG] Completed container: {} in state: {} event:{}
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest):[INFO] Process application start request
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getApplicationsHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetApplicationsHomeSubClusterRequest):[ERROR] FederationStateStoreUtils log and throw retriable exception
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] DataNode container stopped: [ContainerId]
org.apache.hadoop.examples.terasort.TeraScheduler:main(java.lang.String[]):[INFO] starting solve
org.apache.hadoop.fs.shell.Command:run(java.lang.String[]):[ERROR] Interrupted
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:invalidateLocationCache(java.lang.String):[DEBUG] Invalidating {} from {}
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:makeCompositeCrcResult():[WARN] Last block length {} is less than reportedLastBlockSize {}, length - sumBlockLengths, reportedLastBlockSize
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration):[INFO] Restored {} block files from trash before the layout upgrade. These blocks will be moved to the previous directory during the upgrade, restored
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:getCredentials():[ERROR] getCredentials is an invalid operation in SAS Key Mode
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renamePath(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext,java.lang.String,boolean,boolean):[TRACE] Rename source queryparam added {}
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] SASL server skipping handshake in secured configuration for peer = {}, datanodeId = {}
org.apache.hadoop.fs.cosn.CosNFileSystem:delete(org.apache.hadoop.fs.Path,boolean):[DEBUG] Ready to delete path: [{}]. recursive: [{}].
org.apache.hadoop.hdfs.server.datanode.DataNode:registerBlockPoolWithSecretManager(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String):[INFO] Block token params received from NN: for block pool {} keyUpdateInterval={} min(s), tokenLifetime={} min(s)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] AzureBlobFileSystem.setPermission path: {}
org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter:init(org.apache.commons.daemon.DaemonContext):[INFO] Initializing secure datanode resources
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptFailedTransition:transition(java.lang.Object,java.lang.Object):[DEBUG] Not generating HistoryFinish event since start event not generated for task: task.getID()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:reopen(long):[WARN] interrupted when wait a read buffer
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:blockReport(long):[INFO] Unsuccessfully sent block report 0x...
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreRMDTMasterKeyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Storing RMDTMasterKey., e
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:main(org.apache.hadoop.conf.Configuration,java.lang.String[]):[ERROR] Only specify cancel, renew or print.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:onStopMonitoringContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEvent,org.apache.hadoop.yarn.api.records.ContainerId):[INFO] Stopping resource-monitoring for {}
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope):[DEBUG] Emitting metric {name}, type {type}, value {value}, slope {gSlope}, from hostname {getHostName()}
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetReplication(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,short):[DEBUG] Decreasing replication from {} to {} for {}
org.apache.hadoop.hdfs.server.namenode.JournalSet:setOutputBufferCapacity(int):[ERROR] Error in setting outputbuffer capacity
org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Thread {} threw an error: {}. Shutting down
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Updated reserved container + container.getContainer().getId() + on node + this + for application attempt + application.getApplicationAttemptId()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] Event {event.getType()} handled by {previousFailedAttempt}
org.apache.hadoop.yarn.service.client.ApiServiceClient:getRMWebAddress():[DEBUG] Root cause: {}
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList):[DEBUG] Configuring job jar
org.apache.hadoop.examples.terasort.TeraSort:run(java.lang.String[]):[ERROR] {}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Context initialized with ResourceManager and Yarn Configuration
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] Event handled by
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read():[DEBUG] read requested b = null offset = {} len = {}, off, len
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateToken(com.nimbusds.jwt.SignedJWT):[WARN] Audience validation failed.
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:serviceStart():[INFO] Instantiated SCMWebApp at bindAddress
org.apache.hadoop.mapred.YARNRunner:setupContainerLaunchContextForAM(org.apache.hadoop.conf.Configuration,java.util.Map,java.nio.ByteBuffer,java.util.List):[DEBUG] Command to launch container for ApplicationMaster is : {mergedCommand}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:innerClose():[DEBUG] Block[{}]: Deleting buffer file as upload did not start
org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]):[DEBUG] Operation failed
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:updateProcessTree():[DEBUG] {}
org.apache.hadoop.fs.s3a.WriteOperationHelper:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String):[DEBUG] S3 Select request against {}:\n{}
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:dispatchBlocks():[INFO] The maximum iteration time ( + maxIterationTime / 1000 + seconds) has been reached. Stopping + this
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:handleNMContainerStatus(org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus,org.apache.hadoop.yarn.api.records.NodeId):[ERROR] Received finished container : [containerId] for unknown application [applicationId] Skipping.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:check():[INFO] Node {} {} healthy. It needs to replicate {} more blocks. {} is still in progress.
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineClientConnectionRetry:logException(java.lang.Exception,int):[INFO] ConnectionException caught by TimelineClientConnectionRetry, will keep retrying. Message: {e.getMessage()}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set):[DEBUG] Try timeline store {}:{}
org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallable:call():[DEBUG] ProcessInitialInputPathCallable path {}
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List,java.lang.String):[WARN] Target queue + targetQueue + does not exist while trying to move + app.getApplicationId
org.apache.hadoop.yarn.service.ServiceManager$CancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.ServiceManager,org.apache.hadoop.yarn.service.ServiceEvent):[ERROR] [SERVICE]: Cancellation of upgrade failed
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:startMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Decommissioned node {node} is put in maintenance state immediately.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:createSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean):[INFO] logAuditEvent(false, "createSymlink", link, target, null)
org.apache.hadoop.hdfs.server.namenode.FSDirectory:constructRemainingPath(byte[][],byte[][],int):[DEBUG] Resolved path is [result of DFSUtil.byteArray2PathString(components)]
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$StatusUpdateWhenUnHealthyTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[DEBUG] Node remained UNHEALTHY
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[WARN] Ignored {expanded} only jars are supported
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:updateMetricsForGracefulDecommission(org.apache.hadoop.yarn.api.records.NodeState,org.apache.hadoop.yarn.api.records.NodeState):[WARN] Unexpected final state
org.apache.hadoop.crypto.key.kms.server.KMS:handleEncryptedKeyOp(java.lang.String,java.lang.String,java.util.Map):[TRACE] Entering decryptEncryptedKey method.
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:startWebApp():[INFO] Hosting {name} from {onDiskPath} at {webPath}
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:getCredentials():[WARN] Credentials requested when closed
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainer(org.apache.hadoop.yarn.api.records.ContainerId,int,long,org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest):[DEBUG] storeContainer: containerId= {}, startRequest= {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.nodemanager.Context):[INFO] Initialized pluggable runtime
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Upper limit on the thread pool size is + this.limitOnPoolSize
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext):[DEBUG] SQLException closing statement: [details]
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:retrieve(java.lang.String,long):[ERROR] Retrieving COS key: [{}] occurs an exception. byte range start: [{}], exception: [{}].
org.apache.hadoop.fs.FileSystem:processDeleteOnExit():[INFO] Ignoring failure to deleteOnExit for path {}
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:checkRpcAdminAccess():[WARN] Disallowed RPC access from [ugi] at [Server.getRemoteAddress()]. Not listed in [DFSConfigKeys.DFS_ADMIN]
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed:refresh():[DEBUG] Refresh dfs used, bpid: {}, replicas size: {}, dfsUsed: {} on volume: {}, duration: {}ms
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:getContainerPid():[INFO] Could not get pid for {}. Waited for {} ms.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext):[DEBUG] Committer statistics logged
org.apache.hadoop.util.DataChecksum:newCrc32C():[ERROR] CRC32C creation failed, switching to PureJavaCrc32C
org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission):[DEBUG] NativeIO.createDirectoryWithMode error, path = %s, mode = %o
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain):[ERROR] The owner of the posted timeline domain is not set
org.apache.hadoop.fs.s3a.S3AInputStream:closeStream(java.lang.String,boolean,boolean):[DEBUG] Closing stream {}: {}
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[DEBUG] Cannot find active collector while publishing entity
org.apache.hadoop.yarn.server.router.RouterServerUtil:logAndThrowException(java.lang.String,java.lang.Throwable):[ERROR] {errMsg}, {t}
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:serviceStop():[INFO] Force-killing UAM id {} for application {}
org.apache.hadoop.hdfs.server.namenode.JournalSet:mapJournalsAndReportErrors(org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalClosure,java.lang.String):[ERROR] Error: status failed for (journal journal_and_stream)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:printReferenceTraceInfo(java.lang.String):[TRACE] Joiner.on("\n").join(Thread.currentThread().getStackTrace())
org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long):[DEBUG] Cleanup input stream with logger
org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer:populateBlockChecksumBuf(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto):[DEBUG] {CrcUtil.toSingleCrcString()}
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:buildNextStatusBatch(org.apache.hadoop.fs.s3a.S3ListResult):[DEBUG] {}: {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:getRemoteUgi():[WARN] Cannot obtain the user-name. Got exception: ...
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Error in removing RMDelegationToken with sequence number: {sequence_number}
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessInfo,java.lang.String):[WARN] Unexpected: procfs stat file is not in the expected format for process with pid
org.apache.hadoop.yarn.service.component.Component$ContainerRecoveredTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[INFO] [COMPONENT {}]: Trying to recover {} but event did not specify component instance
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:monitorCurrentAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState):[WARN] Interrupted while waiting for current attempt of + appId + to reach + attemptState
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:generateTokenAndSetTimer(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector):[INFO] Generated a new token {timelineToken} for app {appId}
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] You can either use the CLI tool: 'mapred job -history' to view large jobs or adjust the property [JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX].
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Upper bound of the thread pool size is + maxThreadPoolSize
org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Initializing Volume AMS Processor
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain):[ERROR] Error putting domain, e
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[ERROR] Can't handle this event at current state
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:deriveCapacityFromAbsoluteConfigurations(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue):[INFO] LeafQueue: + leafQueue.getQueuePath() + , maxApplications= + maxApplications + , maxApplicationsPerUser= + maxApplicationsPerUser + , Abs Cap: + childQueue.getQueueCapacities().getAbsoluteCapacity(label)
org.apache.hadoop.fs.azure.NativeAzureFileSystem:recoverFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] Recovering files with dangling temp data in {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMLaunchedTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent):[DEBUG] AM launched
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:init():[DEBUG] FileSystemAccess FileSystem configuration:
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:readNextRpcPacket():[DEBUG] reading next wrapped RPC packet
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[INFO] Retrieved job using jobID from jid
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task):[DEBUG] IOException when iterating through {}
org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class):[DEBUG] Found existing [servletName] servlet at path [pathSpec]; will replace mapping with [newServletName] servlet
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream():[DEBUG] drained fewer bytes than expected; {} remaining
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long):[ERROR] Unable to read transaction ids from the configured shared edits storage. Error: {IOException.getLocalizedMessage()}
org.apache.hadoop.hdfs.server.federation.router.FederationUtil:getJmx(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.web.URLConnectionFactory,java.lang.String):[ERROR] Cannot parse JMX output for {} from server {}: {}
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:doFinalize(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory):[INFO] Finalize upgrade for + sd.getRoot() + is not required.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:shouldDefer():[WARN] Forcibly uncaching {} after {} because client(s) {} refused to stop using it.
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[DEBUG] Creating timeline event
org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot):[WARN] Lock held time above threshold(%d ms): lock identifier: %s lockHeldTimeMs=%d ms. Suppressed %d lock warnings. Longest suppressed LockHeldTimeMs=%d. The stack trace is: %s
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:run():[ERROR] Exception in BPOfferService for this
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[INFO] <getQueuePath() + ": re-configured queue: " + childQueue>
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor:run():[DEBUG] PendingReconstructionMonitor thread is interrupted.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:run():[INFO] Checked {} blocks and {} nodes this tick. {} nodes are now in maintenance or transitioning state. {} nodes pending., numBlocksChecked, numNodesChecked, outOfServiceNodeBlocks.size(), getPendingNodes().size()
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:invokeConcurrent(java.util.ArrayList,org.apache.hadoop.yarn.server.router.clientrm.ClientMethod,java.lang.Class):[DEBUG] Cannot execute {} on {}: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:getParentRule(org.w3c.dom.Element,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Creating new parent rule: {parent.getAttribute("name")}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(java.lang.Object,java.lang.Object):[INFO] Updating AMRMToken
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] JSON response created with location
org.apache.hadoop.hdfs.NameNodeProxies:createNonHAProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext):[ERROR] Unsupported protocol found when creating the proxy connection to NameNode
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:unsetErasureCodingPolicy(java.lang.String):[DEBUG] Unset erasure coding policy on {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:initApp(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.Credentials,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,long):[WARN] Application failed to init aggregation
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$CachingTask:run():[WARN] Failed to cache + key + : failed to open file
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:getServiceDefinition(org.apache.hadoop.fs.Path):[DEBUG] Loading service definition from FS: {}
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:validateAndGetSchedulerConfiguration(org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo,javax.servlet.http.HttpServletRequest):[WARN] CapacityScheduler configuration validation failed:...
org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy:preempt(org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy$Context,org.apache.hadoop.yarn.api.records.PreemptionMessage):[INFO] Checkpoint preemption policy activated
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationAttemptStateInternal(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData):[INFO] Error updating info for attempt: {appAttemptId}, {e}
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:resetSpill():[INFO] (RESET) equator {e} kv {kvstart} ({kvstart * 4}) kvi {kvindex} ({kvindex * 4})
org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.util.Map,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType):[DEBUG] No ACL available for key, denying access for {}
org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList):[DEBUG] Getting target path
org.apache.hadoop.hdfs.NameNodeProxies:createNonHAProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean):[ERROR] Unsupported protocol found when creating the proxy connection to NameNode:
org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth):[DEBUG] tokens aren't supported for this protocol or user doesn't have one
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector:selectToken(java.net.URI,java.util.Collection,org.apache.hadoop.conf.Configuration):[DEBUG] Building token service
org.apache.hadoop.hdfs.server.namenode.CacheManager$SerializerCompat:saveDirectives(java.io.DataOutputStream,java.lang.String):[DEBUG] End step SAVING_CHECKPOINT
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:chooseDataNode(java.lang.String,java.util.Collection):[INFO] Chosen node: ...
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSListStatusBatch operation executed
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] App: + appId + successfully moved from + sourceQueueName + to: + destQueueName
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:onInvalidStateTransition(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState):[ERROR] Invalid event {rmContainerEventType} on container {this.getContainerId()}
org.apache.hadoop.ipc.DecayRpcScheduler:updateAverageResponseTime(boolean):[DEBUG] updateAverageResponseTime queue: {} Average: {} Count: {}
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:scanEditLog(java.io.File,long,boolean):[WARN] Log file + file + has no valid header, e
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:periodicInvoke():[DEBUG] Checking state store connection
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doRollback(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] Rollback of {} is complete
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:removeApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Application removed - appId: [applicationId] user: [user] leaf-queue of parent: [getQueuePath()] #applications: [getNumApplications()]
org.apache.hadoop.hdfs.server.namenode.CacheManager:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo):[DEBUG] Exception occurred while modifying cache pool
org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap:completeBootstrapTransfer(java.io.File):[WARN] Failed to fully delete aliasmap archive: + tarname
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:invokeRefresh(java.util.List):[WARN] Not all router admins updated their cache
org.apache.hadoop.yarn.service.component.Component$CheckStableTransition:transition(org.apache.hadoop.yarn.service.component.Component,org.apache.hadoop.yarn.service.component.ComponentEvent):[DEBUG] Checking if component is stable ]]>
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[INFO] Storage version updated
org.apache.hadoop.hdfs.server.datanode.DataNode:parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration):[ERROR] -r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:rollbackLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId):[ERROR] Callback handler does not implement container rollback callback methods
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppPriority(javax.servlet.http.HttpServletRequest,java.lang.String):[ERROR] Trying to get priority of an absent application
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:run():[DEBUG] Received Heartbeat reply from RM. Allocated Containers:{}
org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse):[ERROR] getting attribute + prs + of + oname + threw an exception
org.apache.hadoop.yarn.service.ServiceMaster:removeHdfsDelegationToken(org.apache.hadoop.security.UserGroupInformation):[INFO] Remove HDFS delegation token {}.
org.apache.hadoop.yarn.service.utils.SliderFileSystem:deleteComponentDir(java.lang.String,java.lang.String):[DEBUG] deleted dir {}, path
org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream:write(byte[],int,int):[DEBUG] wrapping token of length: + len
org.apache.hadoop.hdfs.server.mover.Mover$Processor:processRecursively(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsFileStatus,org.apache.hadoop.hdfs.server.mover.Mover$Result):[WARN] Failed to check the status of {parent}. Ignore it and continue.
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor:run():[INFO] Processing the event
org.apache.hadoop.hdfs.server.namenode.NameNode:printMetadataVersion(org.apache.hadoop.conf.Configuration):[DEBUG] Creating FSNamesystem
org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration):[WARN] The libjars file ... is not on the local filesystem. It will not be added to the local classpath.
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceStop():[INFO] Stopping AMRMProxyService
org.apache.hadoop.mapred.gridmix.StressJobFactory:update(org.apache.hadoop.mapred.gridmix.Statistics$ClusterStats):[ERROR] Couldn't get the new Status, e
org.apache.hadoop.hdfs.server.sps.ExternalSPSFilePathCollector:remainingCapacity():[DEBUG] SPS processing Q -> maximum capacity:{}, current size:{}, remaining size:{}
org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException):[DEBUG] Exception raised with exit code {exitCode}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithAuthEnabled(org.apache.hadoop.fs.Path,boolean,boolean):[DEBUG] Deleting file: {f}
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer):[WARN] couldn't fulfill an immediate putMetrics request in time. Abandoning.
org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String):[INFO] Refreshing hosts (include/exclude) list
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer):[DEBUG] Node doesn't have enough available resource
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:checkVersion():[INFO] Loaded timeline state store version info {loadedVersion}
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:containerFailedOnHost(java.lang.String):[INFO] <failures> failures on node <hostName>
org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler:runTask(org.apache.hadoop.mapreduce.v2.app.launcher.ContainerRemoteLaunchEvent,java.util.Map):[ERROR] oopsie... this can never happen: + StringUtils.stringifyException(ioe)
org.apache.hadoop.mapreduce.v2.hs.JobHistory$HistoryCleaner:run():[WARN] Error trying to clean up
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[ERROR] [message]
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:storeEdits(byte[],long,long,int):[WARN] A single batch of edits was too large to fit into the cache: startTxn = %d, endTxn = %d, input length = %d. The capacity of the cache (%s) must be increased for it to work properly (current capacity %d). Cache is now empty.
org.apache.hadoop.mapred.MapOutputCollector:collect(java.lang.Object,java.lang.Object,int):[INFO] Collection by NativeMapOutputCollectorDelegator started
org.apache.hadoop.mapred.ClientServiceDelegate:instantiateAMProxy(java.net.InetSocketAddress):[TRACE] Connected to ApplicationMaster at: + serviceAddr
org.apache.hadoop.io.MapFile:main(java.lang.String[]):[INFO] MapFile reader created
org.apache.hadoop.hdfs.server.namenode.JournalSet:startLogSegment(long,int):[INFO] starting log segment
org.apache.hadoop.hdfs.server.mover.Mover$Processor:processNamespace():[ERROR] Failed to move some block's after {retryMaxAttempts} retries.
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForBlockPoolId(java.lang.String):[ERROR] Cannot locate eligible NNs for {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:findAndMarkBlockAsCorrupt(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String,java.lang.String):[DEBUG] BLOCK* findAndMarkBlockAsCorrupt: {} not found
org.apache.hadoop.fs.cosn.ByteBufferWrapper:close():[WARN] Delete the tmp file: [{}] failed.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[INFO] movedContainer queueMoveIn=... usedCapacity=... absoluteUsedCapacity=... used=... cluster=...
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:publishContainerEvent(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[DEBUG] {} is not a desired ContainerEvent which needs to be published by NMTimelinePublisher, event.getType()
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[ERROR] Invalid READDIRPLUS request
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Service initialized
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:run():[INFO] Listening on %s with %s
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:validate(org.apache.hadoop.yarn.server.federation.store.records.DeleteApplicationHomeSubClusterRequest):[WARN] Missing DeleteApplicationHomeSubCluster Request. Please try again by specifying an ApplicationHomeSubCluster information.
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadDirectoriesInINodeSection(java.io.InputStream):[INFO] Loading directories in INode section.
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFolderContentsToDelete(org.apache.hadoop.fs.azure.FileMetadata,java.util.ArrayList):[DEBUG] Authorization check failed for {}
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:publishCompressedDataStatistics(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,long):[INFO] Total number of compressed input data files : [numCompressedFiles]
org.apache.hadoop.mapred.JobConf:checkAndWarnDeprecation():[WARN] JobConf.deprecatedString(JobConf.MAPRED_TASK_MAXVMEM_PROPERTY) + Instead use + JobConf.MAPREDUCE_JOB_MAP_MEMORY_MB_PROPERTY + and + JobConf.MAPREDUCE_JOB_REDUCE_MEMORY_MB_PROPERTY
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:tryDatanode(org.apache.hadoop.hdfs.protocol.LocatedStripedBlock,org.apache.hadoop.hdfs.protocol.StripedBlockInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long):[DEBUG] got reply from {}: blockChecksum={}, blockChecksumType={}, datanode, blockChecksumForDebug, getBlockChecksumType()
org.apache.hadoop.mapred.CleanupQueue:deletePath(org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext):[DEBUG] Trying to delete + context.fullPath
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:handleException(java.lang.Exception,java.lang.String,long,java.lang.String):[INFO] Processed URL + url + but encountered exception (Took + (endTime - startTime) + ms.)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:getOverallLimits(float):[WARN] The period calculated for the cgroup was too low. The minimum value is MIN_PERIOD_US, calculated value is periodUS. Using all available CPU.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:getUserAMResourceLimitPerPartition(java.lang.String,java.lang.String):[DEBUG] Effective user AM limit for "{}":{}. Effective weighted user AM limit: {}. User weight: {}
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:constructProcessSMAPInfo(org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$ProcessTreeSmapMemInfo,java.lang.String):[ERROR] {e.toString()}
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:validate(java.lang.Object):[ERROR] Invalid Node Label(s) from Provider : + errorMsg
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:allocateAsync(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.util.AsyncCallback):[DEBUG] Interrupted while waiting to put on response queue, ex
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] String.format(STATE_CHANGE_MESSAGE, appID, oldState, getState(), event.getType())
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[ERROR] Calling allocate on removed or non existent application ...
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:serviceStart():[INFO] Starting NodeSortingService= + getName()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:bumpBlockGenerationStamp(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String):[DEBUG] Write operation checked
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:initPerfMonitoring(org.apache.hadoop.hdfs.protocol.DatanodeInfo[]):[DEBUG] Will collect peer metrics for downstream node {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem$RecoverLeaseOp,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,java.lang.String,boolean):[INFO] recoverLease: + lease + , src= + src + from client + clientName
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProvider():[TRACE] {} init complete
org.apache.hadoop.yarn.service.utils.ServiceUtils:tarGzipFolder(java.lang.String[],java.io.File,java.io.FilenameFilter):[INFO] Tar-gzipping folders {} to {}
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:addStreamsToCollectionFromFiles(java.util.Collection,java.util.Collection,long,long,boolean):[DEBUG] selecting edit log stream + elf
org.apache.hadoop.fs.s3a.S3AFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path):[DEBUG] Path is a file
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:failDestinationExists(org.apache.hadoop.fs.Path,java.lang.String):[INFO] Discarding exception raised when listing {...}: {...}
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onRequestsRejected(java.util.List):[ERROR] Error in AMRMClient callback handler. Following scheduling requests were rejected: {rejectedSchedulingRequests}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String):[INFO] Failed to submit application to parent-queue: x
org.apache.hadoop.yarn.service.ServiceScheduler:registerServiceInstance(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.service.api.records.Service):[INFO] Registered service under {}; absolute path {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getCpuUsagePercent():[DEBUG] Jiffy Comparison: + procfs.getCumulativeCpuTime() + + cgroup.getCumulativeCpuTime()
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:splitCopyListingIntoChunksWithShuffle(org.apache.hadoop.mapreduce.JobContext):[INFO] Number of dynamic-chunk-files created: {chunksFinal.size()}
org.apache.hadoop.fs.s3a.S3AInstrumentation:decrementGauge(org.apache.hadoop.fs.s3a.Statistic,long):[DEBUG] No Gauge: {}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:handle(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] Transitioned from {oldState} to {newState} on {eventType} event
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:main(java.lang.String[]):[DEBUG] Exception raised
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceStart():[INFO] Starting Router ClientRMService
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$NvidiaCommandExecutor:searchBinary():[INFO] Skip searching, the nvidia gpu binary is already set: pathOfGpuBinary
org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData):[INFO] Deleted item
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceAllocator:updateFpga(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice,java.lang.String,java.lang.String):[INFO] Update IPID to + newIPID + for this allocated device: + device
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:handleAppSubmitEvent(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$AbstractDelegationTokenRenewerAppEvent):[INFO] applicationId found existing hdfs token
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[WARN] BR lease 0x{} is not valid for DN {}. Expected BR lease 0x{}.
org.apache.hadoop.tools.dynamometer.Client:run():[INFO] Max mem capability of resources in this cluster {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:fromXml(org.w3c.dom.Element,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[DEBUG] Reloading placement policy from allocation config
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run():[DEBUG] Updating the overload status.
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:putEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.ApplicationId):[ERROR] Seems like client has been removed before the entity could be published for {}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:processQueueMessages():[DEBUG] BPServiceActor ( {} ) processing queued messages. Action item: {}
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Force release cache {}.
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignToReduce(org.apache.hadoop.yarn.api.records.Container):[INFO] Assigned to reduce
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadRMAppState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[ERROR] Failed to load state.
org.apache.hadoop.hdfs.server.balancer.Balancer:doBalance(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration):[INFO] included nodes = <p.getIncludedNodes()>
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:createNewMemorySegment(java.lang.String,org.apache.hadoop.net.unix.DomainSocket):[TRACE] createNewMemorySegment: created info.shmId
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:finalizeUpgrade():[INFO] Invoking method 'finalizeUpgrade' concurrently on namespaces
org.apache.hadoop.hdfs.DFSUtil:loadSslConfiguration(org.apache.hadoop.conf.Configuration):[WARN] SSL config sslProp is missing. If dfs.server.https.keystore.resource is specified, make sure it is a relative path
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:fetchHadoopTarball(java.io.File,java.lang.String,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Completed downloading of Hadoop tarball
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[DEBUG] Overwriting file {}
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:parseFile(java.lang.String):[DEBUG] Parse logFile: {}., logFile
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reservationExceedsThreshold(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType):[DEBUG] Reservation Exceeds Allowed number of nodes: app_id= + getApplicationId() + existingReservations= + existingReservations + totalAvailableNodes= + totalAvailNodes + reservableNodesRatio= + df.format(scheduler.getReservableNodesRatio()) + numAllowedReservations= + numAllowedReservations
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:run():[DEBUG] Clear markedDeleteQueue over {} millisecond to release the write lock
org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String):[DEBUG] Deprecation logged for oldName with provider
org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String):[INFO] New token created: ({})
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveReservationAllocationTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error while removing reservation allocation. e
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[INFO] Updating balance throttler bandwidth from ... to: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeReadManifestFile():[WARN] Manifest file + manifest + doesn't exist
org.apache.hadoop.hdfs.DeadNodeDetector$Probe:run():[DEBUG] Check node: {}, type: {}., datanodeInfo, type
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close():[INFO] ops.getSummary(false)
org.apache.hadoop.yarn.service.webapp.ApiServer:stopService(java.lang.String,boolean,org.apache.hadoop.security.UserGroupInformation):[INFO] Successfully stopped service {}
org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask:compute():[WARN] Namespace quota violation in image for...
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] estimated simulation time is {} seconds
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:pauseContainer():[INFO] Container [containerId] not paused as resume already called
org.apache.hadoop.fs.s3a.impl.DeleteOperation:asyncDeleteAction(java.util.List):[DEBUG] Deleting of {number} directory markers
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[INFO] Request URL appended
org.apache.hadoop.mapred.ClientCache:instantiateHistoryProxy():[DEBUG] Connected to HistoryServer at: serviceAddr
org.apache.hadoop.hdfs.PeerCache:clear():[INFO] Cleanup with logger
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String):[DEBUG] nthValidToReturn is {nthValidToReturn}
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction):[DEBUG] Starting vectored read on path {} for ranges {}, pathStr, ranges
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] [JOBID] Job Transitioned from [OLD_STATE] to [NEW_STATE]
org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions):[DEBUG] Failed to create raw erasure decoder , fallback to next codec if possible
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:processResponseQueue():[DEBUG] Application {} has one mapper killed ({})
org.apache.hadoop.hdfs.qjournal.server.Journal:doRollback():[INFO] FileJournalManager.doRollback called
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getTask(org.apache.hadoop.mapred.JvmContext):[INFO] JVM with ID : + jvmId + asked for a task
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:stop():[DEBUG] Storage policy satisfier is not enabled, ignoring
org.apache.hadoop.fs.s3a.Listing:getListFilesAssumingDir(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Listing$FileStatusAcceptor,org.apache.hadoop.fs.store.audit.AuditSpan):[DEBUG] Requesting all entries under {} with delimiter '{}'
org.apache.hadoop.minikdc.MiniKdc:start():[INFO] MiniKdc started.
org.apache.hadoop.fs.azure.NativeAzureFileSystem$DanglingFileRecoverer:handleFile(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata):[DEBUG] Recovering {file.getKey()}
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:handle(org.apache.hadoop.oncrpc.RpcDeniedReply):[WARN] Portmap mapping registration failed, accept state: + acceptState
org.apache.hadoop.fs.cosn.CosNOutputStream:waitForFinishPartUploads():[ERROR] Multipart upload with id: [{}] to COS key: [{}]
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[INFO] task.commitAttempt + already given a go for committing the task output, so killing + attemptID
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:serviceStart():[INFO] YARN API server running on <resolved_bindAddress>
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container):[DEBUG] Assigning container <allocated> to reduce
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:postEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineEntities):[ERROR] The owner of the posted timeline entities is not set
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[DEBUG] Saving {} pending commit(s)) to file {}
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:updateReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationUpdateRequest):[INFO] ClientRMService: [reservationId]
org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils:getDefaultFileContext():[INFO] Default file system [ + fc.getDefaultFileSystem().getUri() + ]
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Did not remove "{existingRecord}"
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render():[INFO] Sorry, {jid} not found.
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:handle(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent):[INFO] [Container release] Container + relEvent.getContainer() + sent RELEASE event on a resource request + req + not present in cache.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.RMContext):[DEBUG] nodeLookupPolicy used for {appSchedulingInfo.getApplicationId()} is {((multiNodeSortPolicyName != null) ? multiNodeSortPolicyName : "")}
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:run():[WARN] Re-encryption updater thread interrupted. Exiting.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:checkAndDeleteCgroup(java.io.File):[WARN] Failed attempt to delete cgroup: {cfg}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf):[INFO] Initialized PmemMappableBlockLoader
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyNames():[TRACE] Entering getKeyNames method.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Initialized Reservation system
org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger:logAuditSimpleFormat(org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$OpStatus,org.apache.hadoop.crypto.key.kms.server.KMSAuditLogger$AuditEvent):[INFO] {} {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveRMDTTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Illegal event type: + event.getClass()
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:putEntities(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[]):[DEBUG] Writing entity log for {} to {}
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable):[DEBUG] Updating info cache...
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:redirectURI(org.apache.hadoop.hdfs.server.federation.router.Router,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,java.lang.String,org.apache.hadoop.hdfs.web.resources.Param[]):[TRACE] redirectURI={}
org.apache.hadoop.hdfs.DFSInputStream:tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress):[DEBUG] Access token was invalid when connecting to {targetAddr}: {ex}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:removeApplication(org.apache.hadoop.conf.Configuration,java.lang.String):[INFO] Deleting application + removeAppId + from state store
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:submitApplication(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo,javax.servlet.http.HttpServletRequest):[INFO] submitApplication appId {} try #{} on SubCluster {}
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String):[DEBUG] Creating endpoint configuration for "{endpoint}"
org.apache.hadoop.hdfs.qjournal.server.JNStorage:format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,boolean):[INFO] Formatting journal {} with nsid: {}
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:heartbeat():[INFO] New instance created
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[DEBUG] Storing master key ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[INFO] Cleaning up container {containerIdStr}
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher:serviceStop():[INFO] launcherHandlingThread.getName() + " interrupted during join ", ie
org.apache.hadoop.mapred.SortedRanges:remove(org.apache.hadoop.mapred.SortedRanges$Range):[DEBUG] removed previousRange
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KillInitedJobTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Job received Kill in INITED state.
org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Access control method '{accessControlRequestMethod}' not allowed. Returning
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:removeResourceFromCacheFileSystem(org.apache.hadoop.fs.Path):[ERROR] We were not able to rename the directory to {renamedPath.toString()}. We will leave it intact.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:recoverRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,long):[INFO] At datanode display name, Recovering rbw
org.apache.hadoop.hdfs.server.datanode.DataXceiver:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[INFO] "opReadBlock {} received exception: {}", block, e
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:processSingleResource(org.apache.hadoop.fs.FileStatus):[ERROR] Exception thrown while removing dead appIds.
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:run():[WARN] Failed to reconstruct striped block: {BLOCK_GROUP}, {EXCEPTION}
org.apache.hadoop.lib.server.Server:init():[INFO] Source Repository : {}
org.apache.hadoop.hdfs.server.balancer.Balancer:logUtilizationCollections():[DEBUG] logUtilizationCollection("below-average", belowAvgUtilized)
org.apache.hadoop.hdfs.server.datanode.DataXceiver:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[]):[DEBUG] isDatanode={}, isClient={}, isTransfer={}
org.apache.hadoop.yarn.csi.client.CsiGrpcClient:close():[ERROR] Failed to gracefully shutdown gRPC communication channel in 5 seconds
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier):[DEBUG] Generating block token for +
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:init(javax.servlet.FilterConfig):[INFO] adminAclList=<adminAclList.getUsers()>
org.apache.hadoop.tools.dynamometer.ApplicationMaster$NMCallbackHandler:onStopContainerError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to stop DataNode Container + containerId
org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask:compute():[WARN] Storagespace quota violation in image for...
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[DEBUG] BLOCK* addBlock: logged info for {} of {} reported.
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier):[DEBUG] Check the condition for main loop.
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Authenticated from delegation token. url={}, token={}
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:unpublishVolumes():[INFO] Volumes to un-publish {}
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStart():[INFO] Dispatcher initialized
org.apache.hadoop.hdfs.server.diskbalancer.connectors.JsonNodeConnector:getNodes():[INFO] Reading cluster info from file : dataFilePath
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptCommitPendingTransition:transition(java.lang.Object,java.lang.Object):[INFO] attemptID given a go for committing the task output.
org.apache.hadoop.tools.rumen.Folder:run():[DEBUG] The first job has a submit time of ...<br>
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:run():[ERROR] Unable to mark container {containerId} killed in store {e}
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread:cancelToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration):[WARN] Unable to add token [token] for cancellation. Will retry..
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:writeAuditLog(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] App failed with state: FAILED
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:fence(org.apache.hadoop.hdfs.server.protocol.JournalInfo,long,java.lang.String):[INFO] Fenced by {fencerInfo} with epoch {epoch}
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent):[ERROR] Invalid event + event.getType() + on Node + this.nodeId + oldState + oldState
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:waitForAllPartUploads():[DEBUG] While waiting for upload completion, ee
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData:call():[ERROR] Error while adding input path
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:remove(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[ERROR] Attempt to remove absent resource: rem.getRequest() from getUser()
org.apache.hadoop.hdfs.server.datanode.BlockScanner:addVolumeScanner(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeReference):[ERROR] Already have a scanner for volume {}.
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain):[DEBUG] doAsUser = {}, RemoteUser = {} , RemoteAddress = {}
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:createAllSchemas(org.apache.hadoop.conf.Configuration,boolean):[ERROR] Error in creating hbase tables:
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:processResponseQueue():[DEBUG] Application {} starts to launch a stream ({})
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:checkOperation(org.apache.hadoop.hdfs.server.namenode.NameNode$OperationCategory,boolean):[DEBUG] Proxying operation: {methodName}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeBlocksAssociatedTo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Removed blocks associated with storage {} from DataNode {}
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:reinitialize(java.util.Map,java.util.Set):[WARN] All active and enabled subclusters have expired last heartbeat time. Ignore the expiry check for this request
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getRMWebAppURLWithScheme(org.apache.hadoop.conf.Configuration):[DEBUG] Get RM web app URL without scheme
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:offerService():[INFO] For namenode using...
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.yarn.service.webapp.ApiServer:stopService(java.lang.String,boolean,org.apache.hadoop.security.UserGroupInformation):[INFO] Service {} is already stopped
org.apache.hadoop.mapred.JobConf:setMaxVirtualMemoryForTask(long):[WARN] setMaxVirtualMemoryForTask() is deprecated. Instead use setMemoryForMapTask() and setMemoryForReduceTask()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:startStorage():[INFO] Using state database at [storeRoot] for recovery
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:recoverClose(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long):[INFO] Recover failed close b
org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory:getCluster(java.net.URI,org.apache.hadoop.conf.Configuration):[DEBUG] Creating a JsonNodeConnector
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:deleteFileWithRetries(org.apache.hadoop.fs.FileContext,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.Path):[DEBUG] Checking existence of the delete path
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil:createNullChecksumByteArray():[ERROR] Exception in creating null checksum stream: {e}
org.apache.hadoop.hdfs.DFSStripedOutputStream:allocateNewBlock():[DEBUG] Excluding DataNodes when allocating new block: {excludedNodes}
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[ERROR] Status: ERROR, message: Exception message
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:calculateRollingMonitorInterval(org.apache.hadoop.conf.Configuration):[WARN] rollingMonitorInterval should be more than or equal to {} seconds. Using {} seconds instead.
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateSignature(com.nimbusds.jwt.SignedJWT):[WARN] Error while validating signature
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:completeExecute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Retrying REST operation {}. RetryCount = {}
org.apache.hadoop.yarn.server.resourcemanager.placement.FairQueuePlacementUtils:cleanName(java.lang.String):[WARN] Name {} is converted to {} when it is used as a queue name.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:waitForRecoveredContainers():[WARN] Timeout waiting for recovered containers
org.apache.hadoop.mapred.gridmix.GenerateData:publishPlainDataStatistics(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path):[INFO] Total number of input data files : + fileCount
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processConf():[ERROR] {} is set to an invalid value, it must be greater than zero. Defaulting to {}
org.apache.hadoop.fs.s3a.S3AInputStream:seekInStream(long,long):[INFO] Switching to Random IO seek policy
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initRecordStorage(java.lang.String,java.lang.Class):[ERROR] Cannot initialize ZK node for {}: {}, className, e.getMessage()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[DEBUG] Policy initialization complete
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:makeDoneSubdir(org.apache.hadoop.fs.Path):[INFO] Explicitly setting permissions to : (permission), (fsp)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:removeVolumes(java.util.Collection,boolean):[TRACE] checking for block {blockId} with storageLocation {blockStorageLocation}
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:leave():[INFO] Leaving safe mode after {} milliseconds
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[DEBUG] rpcServer.isInvokeConcurrent returned TRUE
org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:run():[INFO] Slow ReadProcessor read fields for block XXX took YYYms
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[DEBUG] Application is registered for timeout monitor
org.apache.hadoop.fs.azure.BlockBlobAppendStream:writeBlockRequestInternal(java.lang.String,java.nio.ByteBuffer,boolean):[DEBUG] Encountered exception during uploading block for Blob {} Exception : {}
org.apache.hadoop.util.LogAdapter:warn(java.lang.String,java.lang.Throwable):[WARN] Log warning with msg and throwable
org.apache.hadoop.hdfs.util.ByteArrayManager$Impl:newByteArray(int):[DEBUG] allocate(arrayLength), return byte[array.length]
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.DevicePlugin:getDevices():[INFO] Fetching devices from NEC VE plugin
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Saver:serializeFilesUCSection(java.io.OutputStream):[WARN] Fail to save the lease for inode id + id + as the file is not under construction
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state for + this.attemptId, e
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Registered StateStoreMBean: {}
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:startStorage():[INFO] Loading the existing database at th path: {dbPath}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getFileStatusInternal(org.apache.hadoop.fs.Path):[DEBUG] Found the path: {} as a file.
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:handleShutdownOrResyncCommand(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[WARN] Node is out of sync with ResourceManager, hence resyncing.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[WARN] Unknown block status code reported by {}: {}
org.apache.hadoop.fs.s3a.auth.SignerManager:initCustomSigners():[DEBUG] No custom signers specified
org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration):[DEBUG] Creating new Groups object
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam):[INFO] URI redirection initiated
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:dumpStaleNodes(java.util.List):[INFO] staleLogMSG.toString()
org.apache.hadoop.http.HttpServer2:getWebAppsPath(java.lang.String):[INFO] Web server is in development mode. Resources will be read from the source tree.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:handleTimelineEvent(org.apache.hadoop.mapreduce.jobhistory.JobHistoryEvent):[DEBUG] In HistoryEventHandler, handle timelineEvent: EVENT_TYPE
org.apache.hadoop.hdfs.server.datanode.BlockSender:manageOsCache():[DEBUG] Performing readahead
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:renameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean):[INFO] Audit success: renameSnapshot
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:sortLocatedBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.lang.String,java.util.Comparator):[ERROR] Node Resolution failed. Please make sure that rack awareness scripts are functional.
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext):[WARN] Output Path is null in setupJob()
org.apache.hadoop.hdfs.util.CombinedHostsFileReader:readFile(java.lang.String):[WARN] hostsFilePath + is empty. + REFER_TO_DOC_MSG
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:serviceStop():[ERROR] Error joining with heartbeat thread, ex
org.apache.hadoop.yarn.YarnUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable):[ERROR] Thread [thread] threw an Exception.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:markAllDatanodesStale():[INFO] Marking all datanodes as stale
org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber:run():[WARN] LazyPersistFileScrubber encountered an exception while scanning for lazyPersist files with missing blocks. Scanning will retry in {} seconds.
org.apache.hadoop.hdfs.tools.DFSHAAdmin:runCmd(java.lang.String[]):[ERROR] Aborted
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Invalid eventtype + event.getType() + . Ignoring!
org.apache.hadoop.fs.s3a.S3AUtils:closeAutocloseables(org.slf4j.Logger,java.lang.AutoCloseable[]):[DEBUG] Exception in closing {}
org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean):[DEBUG] parsing File + file
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:close():[DEBUG] AzureBlobFileSystem.close
org.apache.hadoop.examples.Sort:run(java.lang.String[]):[INFO] Job ended: end_time
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:checkAcls(java.lang.String):[WARN] User does not have permission to call method
org.apache.hadoop.service.launcher.AbstractLaunchableService:bindArgs(org.apache.hadoop.conf.Configuration,java.util.List):[DEBUG] Service {} passed in {} arguments:
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:packetSentInTime():[WARN] A packet was last sent {}ms ago. Maximum idle time: {}ms.
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL ... from user ...
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:storeToken(org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier,java.lang.Long):[DEBUG] Storing token + tokenId.getSequenceNumber()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:rollEditLog(int):[INFO] Rolling edit logs
org.apache.hadoop.security.UserGroupInformation:getGroups():[DEBUG] Failed to get groups for user {}
org.apache.hadoop.hdfs.server.common.JspHelper:getTokenUGI(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Token decoded
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkAllVolumes(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi):[TRACE] Skipped checking all volumes, time since last check {} is less than the minimum gap between checks ({} ms), gap, minDiskCheckGapMs
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizedWhileRunningTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Symlink file already exists: linkFile
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:getMajorNumber(java.lang.String):[DEBUG] stat output:{}
org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String):[DEBUG] Error getting groups for {user}
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntity(java.lang.String,java.lang.String,java.lang.Long,java.util.EnumSet,org.apache.hadoop.yarn.server.utils.LeveldbIterator,byte[],int):[WARN] Found unexpected column for entity %s of type %s (0x%02x)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:run():[WARN] Uncaught exception in ContainersMonitorImpl while monitoring resource of containerId
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:dumpOutDebugInfo():[INFO] System CWD content: ...
org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig):[WARN] Allowed Origin pattern ' + discouragedAllowedOrigin + ' is discouraged, use the 'regex:' prefix and use a Java regular expression instead.
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:writeDomain(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain):[DEBUG] Writing domains for {} to {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:getGpuDeviceInformation():[ERROR] msg
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:killContainersToEnforceMaxQueueCapacity(java.lang.String,org.apache.hadoop.yarn.api.records.Resource):[INFO] Killed container= + toKillContainer.getContainerId() + from queue= + lq.getQueuePath() + to make queue= + this.getQueuePath() + 's max-capacity enforced
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanupStagingDir():[WARN] Job Staging directory is null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:moveAllApps(java.lang.String,java.lang.String):[WARN] e.toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:handle(org.apache.hadoop.yarn.event.Event):[INFO] Got event {eventType} for appId {applicationId}
org.apache.hadoop.ipc.Server$Connection:readAndProcess():[WARN] Incorrect RPC Header length
org.apache.hadoop.ipc.Server:shutdownMetricsUpdaterExecutor():[INFO] Hadoop Metrics Updater executor could not be shutdown.
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection):[DEBUG] nthValidToReturn is {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:publishVolumes():[INFO] Found {} volumes to be published on this node
org.apache.hadoop.yarn.service.ServiceScheduler:recoverComponents(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[INFO] Record not found in registry for container {} from previous attempt, releasing
org.apache.hadoop.security.token.DtUtilShell$Get:validate():[ERROR] Must provide -service with http/https URL.
org.apache.hadoop.crypto.key.kms.server.KMS:deleteKey(java.lang.String):[TRACE] Entering deleteKey method.
org.apache.hadoop.mapreduce.v2.util.LocalResourceBuilder:createLocalResources(java.util.Map):[WARN] Resource description
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[ERROR] Attempting to remove non-existent node {nodeId}
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:retrieve(java.lang.String,long):[DEBUG] Retrieve COS key:[{}]. range start:[{}].
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appAttemptRegistered(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,long):[DEBUG] Configuring entity info
org.apache.hadoop.hdfs.server.namenode.FSImage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Reading [EDITLOG_STREAM] expecting start txid #[NEXT_TXID] [LOG_SUPPRESSED]
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgradeExpress(java.lang.String,java.io.File):[ERROR] Service {} express upgrade to version {} failed because {}
org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier):[ERROR] Failed to get token in SQL secret manager
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:parseDiagnosticOutput(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor,java.lang.String):[WARN] The diagnostic has failed
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$NvidiaCommandExecutor:searchBinary():[ERROR] No binary found from env variable: ENV_BINARY_PATH or path DEFAULT_BINARY_SEARCH_DIRS.toString()
org.apache.hadoop.yarn.util.resource.ResourceUtils:getRequestedResourcesFromConfig(org.apache.hadoop.conf.Configuration,java.lang.String):[ERROR] Invalid resource request specified for property ...
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:waitFor(java.util.function.Supplier,int):[DEBUG] Check the condition for main loop.
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromDomain():[TRACE] {}: trying to create a remote block reader from the UNIX domain socket at {}
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:deleteCorruptedFile(java.lang.String):[ERROR] Fsck: error deleting corrupted file
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:assignWithoutLocality(org.apache.hadoop.yarn.api.records.Container):[INFO] Assigning container <allocated> to fast fail map
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean):[INFO] Service {} is being gracefully stopped...
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:loadAMRMTokenSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[WARN] There is no data saved
org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path):[WARN] isDirectory is deprecated
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] UGI: <ugi>
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:runWithPrincipal(java.lang.String,byte[],org.apache.commons.codec.binary.Base64,javax.servlet.http.HttpServletResponse):[TRACE] SPNEGO completed for client principal [{}]
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processMisReplicatedBlocks(java.util.List):[DEBUG] BLOCK* processMisReplicatedBlocks: Re-scanned block {}, result is {}
org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]):[DEBUG] Native Hadoop and required libraries loaded
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse):[WARN] Found non empty placement constraints map in RegisterApplicationMasterRequest for application=...
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:activateSavedReplica(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica):[INFO] Moved <blockFile> to <targetBlockFile>
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration):[DEBUG] Creating filter configuration
org.apache.hadoop.hdfs.qjournal.server.Journal:format(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,boolean):[INFO] Formatting journal id : [journalId] with namespace info: [nsInfo] and force: [force]
org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:requestNextBatch():[DEBUG] Listing found {} upload(s)
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[DEBUG] Considering jobs with submit time greater than [startsAfter] ms. Skipped [skippedCount] jobs.
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:renameToInt(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,java.lang.String,boolean):[DEBUG] DIR* NameSystem.renameTo: with options - src to dst
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreCollectionCreator:createTimelineSchema(java.lang.String[]):[INFO] Creating database and collections for DocumentStore : {vendor}
org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop():[ERROR] Error in Reader
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node):[DEBUG] NetworkTopology became:\n + this.toString()
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:createShortCircuitReplicaInfo():[DEBUG] {}: closing stale domain peer {}
org.apache.hadoop.tools.CopyListing:buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext):[INFO] Number of paths in the copy list: + this.getNumberOfPaths()
org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocol:addToClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsRequest):[DEBUG] Adding to cluster node labels in AdminService
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Inserting ClusterNode [{rmNode.getNodeID()}] with queue wait time [{estimatedQueueWaitTime}] and wait queue length [{waitQueueLength}]
org.apache.hadoop.yarn.service.client.ServiceClient:actionDestroy(java.lang.String):[INFO] Successfully destroyed service {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:computeQueueManagementChanges():[DEBUG] Parent queue = ... : Found ... leaf queues to be activated with ... apps
org.apache.hadoop.hdfs.protocol.ClientProtocol:refreshNodes():[INFO] RouterRpcServer refresh invoked
org.apache.hadoop.mapred.uploader.FrameworkUploader:collectPackages():[INFO] Ignored {jar} because it is a directory
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:logAllLiveStackTraces():[DEBUG] \tat <stack_trace_element>
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[]):[DEBUG] Child starting
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreOrUpdateAMRMTokenTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: {event.getClass()}
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.Class):[ERROR] We got a closed connection from {}
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute():[INFO] Credential <alias> has been successfully deleted.
org.apache.hadoop.tools.RegexCopyFilter:initialize():[ERROR] Can't find filters file
org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline:moveReplicaFrom(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File):[WARN] Cannot move meta file ... back to the finalized directory ...
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:processFinishedContainer(org.apache.hadoop.yarn.api.records.ContainerStatus):[INFO] Received completed container {containerId}
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:process():[INFO] Processing + numResources + resources in the shared cache
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:getTokenFromMsi(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean):[DEBUG] AADToken: starting to fetch token using MSI
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:startCacheCleanerThreadIfNeeded():[DEBUG] {}: starting cache cleaner thread which will run every {} ms, this, rateMs
org.apache.hadoop.applications.mawo.server.common.TaskStatus:setStartTime():[DEBUG] Setting task start time
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:read(java.nio.ByteBuffer):[TRACE] read(arr.length={}, off={}, len={}, filename={}, block={}, canSkipChecksum={}): starting
org.apache.hadoop.yarn.server.timeline.LogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem):[WARN] {} no longer exists. Skip for scanning.
org.apache.hadoop.security.KDiag:printDefaultRealm():[DEBUG] Exception cause details
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForRead(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] openFileForRead filesystem: {} path: {}
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:createShortCircuitReplicaInfo():[TRACE] allocShmSlot used up our previous socket...
org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Add token with service alias
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean):[INFO] Node already renewed by peer {nodeRemovePath} so this token should not be deleted
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Resources subtracted
org.apache.hadoop.mapreduce.v2.util.MRApps:createJobClassLoader(org.apache.hadoop.conf.Configuration):[INFO] Starting job class loader creation
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:run():[ERROR] {} exiting because of exception
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:becomeStandby():[ERROR] RM could not transition to Standby
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[INFO] Node delete event for: {}
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getReader(java.lang.String):[DEBUG] Loading file: {}
org.apache.hadoop.hdfs.server.namenode.NNStorage:processStartupOptionsForUpgrade(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int):[INFO] Using clusterid: {}
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS SETATTR fileHandle: {} client: {}
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceStart():[INFO] Router ClientRMService listening on address: + this.server.getListenerAddress()
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startRollingUpgrade():[DEBUG] Start rolling upgrade
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:run():[ERROR] Exception during StoragePolicySatisfier execution - will continue next cycle
org.apache.hadoop.hdfs.DFSClient:getQuotaUsage(java.lang.String):[DEBUG] The version of namenode doesn't support getQuotaUsage API. Fall back to use getContentSummary API.
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:processor():[DEBUG] Stop processing
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:scanForUserServiceDefinition(org.apache.hadoop.fs.Path,java.util.Map):[INFO] Scanner skips for unknown file extension, filename = {}
org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map):[ERROR] error parsing filename
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:validateJobDef(org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer$JobDefinition):[INFO] Detected old JobDefinition format. Converting.
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:delete(java.lang.String):[DEBUG] Delete object key: [{}] from bucket: {}.
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:logAuditEvent(boolean,java.lang.String,java.lang.String):[DEBUG] Operation: + cmd + Status: + succeeded + TokenId: + tokenId
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:signRequest(org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation,int):[DEBUG] Signing request with shared key
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap):[WARN] Could not find a target for file
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:startThreads():[INFO] Loaded token cache in {} milliseconds
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:refreshLogRetentionSettings():[WARN] Failed to execute refreshLogRetentionSettings : Aggregated Log Deletion Service is not started
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Skip app_attempt= + application.getApplicationAttemptId() + , because it doesn't need more resource, schedulingMode= + schedulingMode.name() + node-label= + candidates.getPartition()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logCreateSnapshot(java.lang.String,java.lang.String,boolean,long):[DEBUG] Log that a snapshot is created
org.apache.hadoop.fs.http.server.HttpFSServer:get(java.lang.String,javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [{}]
org.apache.hadoop.tools.HadoopArchiveLogs:checkMaxEligible():[INFO] Too many applications (...)
org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date):[INFO] Created trash checkpoint: [checkpoint_path]
org.apache.hadoop.yarn.util.FSDownload:call():[DEBUG] Starting to download {} {} {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent):[INFO] Removing + containerEvent.getContainerID() + from application + app.toString()
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:bindToFileSystem(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path):[DEBUG] Bonded to filesystem with resilient commits under path {}
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getContainerReportFromHistory(org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[WARN] Got an error while fetching container report from ATSv2
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:logConf(org.apache.hadoop.conf.Configuration):[INFO] NNTop conf: + DFSConfigKeys.NNTOP_BUCKETS_PER_WINDOW_KEY + = +
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:handleWrite(org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes):[DEBUG] handleWrite org.apache.hadoop.nfs.nfs3.request.WRITE3Request
org.apache.hadoop.hdfs.protocol.ClientProtocol:renewLease(java.lang.String):[DEBUG] Lease renewed with RouterRpcServer
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String):[DEBUG] attribute + : + a
org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration):[ERROR] ...
org.apache.hadoop.hdfs.DFSClient:closeAllFilesBeingWritten(boolean):[ERROR] Failed to abort file: ... with inode: ...
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] Container {} transitioned from {} to {}
org.apache.hadoop.fs.s3a.S3AUtils:longBytesOption(org.apache.hadoop.conf.Configuration,java.lang.String,long,long):[DEBUG] Value of {} is {}, key, v
org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object):[ERROR] Closing proxy or invocation handler caused exception
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:loadDetailLog(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId):[DEBUG] Try refresh logs for {log.getFilename()}
org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation):[DEBUG] +token: {token}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Unexpected exception in getting the filesystem
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:load(java.lang.String):[DEBUG] Loading section INODE length: {s.getLength()}
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int):[WARN] Got overwrite [{start}-{end}) smaller than current offset {currentOffset}, drop the request
org.apache.hadoop.hdfs.server.mover.Mover$Cli:run(java.lang.String[]):[INFO] Mover took X milliseconds
org.apache.hadoop.mapred.QueueConfigurationParser:parseResource(org.w3c.dom.Element):[INFO] At root level only " queue " tags are allowed
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:initializePipeline(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,org.apache.hadoop.security.token.Token,org.apache.hadoop.security.token.Token,java.util.Map,boolean,org.apache.hadoop.security.Credentials):[WARN] Request to start an already existing appId was received. This can happen if an application failed and a new attempt was created on this machine. ApplicationId: applicationAttemptId.toString()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:shutdown():[INFO] Stopping resourceestimator service at: {}., baseURI.toString()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest):[INFO] Disallowed NodeManager from host, Sending SHUTDOWN signal to the NodeManager.
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:doShuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,org.apache.hadoop.mapred.IFileInputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter):[INFO] Read X bytes from map-output for Y
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Failed to match {} to named-volume regex pattern
org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String):[INFO] Http request log for {} is not defined
org.apache.hadoop.hdfs.server.balancer.Balancer$Cli:run(java.lang.String[]):[INFO] ...(InterruptedException e.Message)
org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Header origin is null. Returning
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL ... (Took ... ms.)
org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp:createSymlinkInt(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean):[DEBUG] DIR* NameSystem.createSymlink: target= + target + link= + link
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:setStoragePolicy(java.lang.String,java.lang.String):[INFO] Operation completed. Audit event logged successfully
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:checkVersion():[INFO] Loaded state version info
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:renewToken(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[INFO] Renewed delegation-token= [ + dttr + ]
org.apache.hadoop.hdfs.server.federation.router.RouterStoragePolicy:satisfyStoragePolicy(java.lang.String):[INFO] Retrieved locations for path
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[]):[WARN] Invalid configuration detected
org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean):[WARN] Couldn't delete checkpoint: ... Ignoring.
org.apache.hadoop.service.launcher.ServiceLauncher:warn(java.lang.String):[WARN] text
org.apache.hadoop.fs.azure.NativeAzureFileSystem:updateParentFolderLastModifiedTime(java.lang.String):[ERROR] Unable to free lease on parentKey e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[ERROR] Exception during version check
org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.util.function.Supplier):[DEBUG] Error formatting message
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:startTimelineClient(org.apache.hadoop.conf.Configuration):[WARN] Timeline service is not enabled
org.apache.hadoop.hdfs.server.namenode.NameNode:startCommonServices(org.apache.hadoop.conf.Configuration):[WARN] ServicePlugin + p + " could not be started"
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:init(org.apache.hadoop.conf.Configuration):[DEBUG] Retrieving schedule configuration
org.apache.hadoop.mapred.uploader.FrameworkUploader:run():[INFO] Suggested mapreduce.application.classpath $PWD/ + alias + /*
org.apache.hadoop.yarn.server.AMRMClientRelayer:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[INFO] ResponseId out of sync with RM, expect ... but ... used by .... Will override in the next allocate.
org.apache.hadoop.fs.s3a.S3AFileSystem:stopAllServices():[DEBUG] When shutting down
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationAttemptFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptFinishData):[ERROR] Error when writing finish information of application attempt + appAttemptFinish.getApplicationAttemptId(), e
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Container Log Monitor Enabled: {}
org.apache.hadoop.mapred.MapTask$MapOutputBuffer:init(org.apache.hadoop.mapred.MapOutputCollector$Context):[INFO] soft limit at {softLimit}
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:close():[TRACE] close(filename={}, block={})
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map):[WARN] IP_ID environment is empty, skip downloading
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$JobFailWaitTimedOutTransition:transition(java.lang.Object,java.lang.Object):[INFO] Timeout expired in FAIL_WAIT waiting for tasks to get killed. Going to fail job anyway
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[DEBUG] Updating token {tokenId.getSequenceNumber()}
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:handleNMContainerStatus(org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus,org.apache.hadoop.yarn.api.records.NodeId):[DEBUG] Ignoring container completion status for unmanaged AM [applicationId]
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.AbstractPreemptableResourceCalculator:resetCapacity(org.apache.hadoop.yarn.api.records.Resource,java.util.Collection,boolean):[DEBUG] Resetting capacity with ignoreGuar set to false
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads():[INFO] Fetched initial range of seq num, from 1 to 100
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session):[ERROR] Failed to establish WebSocket connection with Client, e
org.apache.hadoop.yarn.service.client.ServiceClient:getStatus(java.lang.String):[WARN] {} AM hostname is empty
org.apache.hadoop.hdfs.server.sps.ExternalStoragePolicySatisfier:main(java.lang.String[]):[ERROR] Failed to start storage policy satisfier.
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest):[WARN] Unresolved nodemanager registration: hostname cannot be resolved
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$RenewalTimerTask:run():[INFO] The token was removed already. Token = [dttr]
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:outputINodes(java.io.InputStream):[WARN] Ignored {} nodes, including {} in snapshots. Please turn on debug log for details
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getProvidedCapacity():[DEBUG] Failed to get provided capacity
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:get(java.lang.String):[ERROR] Cannot find znode
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String):[ERROR] BUG: Found lastValidNode {lastValidNode} but not nth valid node. parentNode={parentNode}, excludedScopeNode={excludedScopeNode}, excludedNodes={excludedNodes}, totalInScopeNodes={totalInScopeNodes}, availableNodes={availableNodes}, nthValidToReturn={nthValidToReturn}.
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] ConfigurationProvider initialized
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:formatAndDownloadAliasMap(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo):[INFO] Bootstrapping the InMemoryAliasMap from {proxyInfo.getHttpAddress()}
org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp:mkdirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean):[DEBUG] DIR* NameSystem.mkdirs: src
org.apache.hadoop.tools.SimpleCopyListing$TraverseDirectory:prepareListing(org.apache.hadoop.fs.Path):[DEBUG] Recording source-path: {} for copy.
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseLists():[INFO] Environment {key} {value}
org.apache.hadoop.fs.azure.NativeAzureFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable):[WARN] Warn message with param
org.apache.hadoop.yarn.webapp.view.InfoBlock:render():[DEBUG] Rendering info block
org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String):[WARN] Path is null
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl$DecommissioningNodeTransition:transition(java.lang.Object,java.lang.Object):[INFO] Put Node NODE_ID in DECOMMISSIONING.
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:loadINodeSectionInParallel(java.util.concurrent.ExecutorService,java.util.ArrayList,java.lang.String,org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step):[INFO] Completed loading all INode sections. Loaded {} inodes.
org.apache.hadoop.hdfs.server.datanode.DataStorage:prepareVolume(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.util.List):[WARN] Storage directory is in use.
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration):[DEBUG] Reinit compressor with new compression configuration
org.apache.hadoop.hdfs.server.datanode.metrics.OutlierDetector:getOutlierMetrics(java.util.Map):[DEBUG] Skipping statistical outlier detection as we don't have latency data for enough resources. Have {}, need at least {}
org.apache.hadoop.yarn.client.api.impl.TimelineWriter:doPostingObject(java.lang.Object,java.lang.String):[DEBUG] POST to {}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingCached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[DEBUG] Block {}: can't cache this block, because it is not yet + complete.
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:run(java.lang.String,java.lang.String):[DEBUG] Processing XML
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startActiveServices():[INFO] Will take over writing edit logs at txnid ...
org.apache.hadoop.security.authentication.server.AuthenticationFilter:constructSecretProvider(javax.servlet.ServletContext,java.util.Properties,boolean):[WARN] Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: {e.getMessage()}
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[WARN] DN {node.getDatanodeUuid()} ({node.getXferAddr()}) requested a lease even though it wasn't yet registered. Registering now.
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:onChangeMonitoringContainerResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEvent,org.apache.hadoop.yarn.api.records.ContainerId):[WARN] Failed to track container {}. It may have already completed.
org.apache.hadoop.hdfs.client.impl.DfsClientConf:getChecksumType(org.apache.hadoop.conf.Configuration):[WARN] Bad checksum type: {}. Using default {}
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent):[INFO] Job {jobId} Transitioned from {oldState} to {getInternalState()}
org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map):[INFO] Added filter (name) (class=classname) to context ctx.getDisplayName()
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close():[ERROR] Forcing SlotReleaserThreadPool to shutdown!
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedInteractiveOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation):[DEBUG] Arrays.toString(fullCommandArray)
org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean):[DEBUG] Path to resolve: + pathStrToResolve + , srcPattern: + getSrcPathRegex()
org.apache.hadoop.tools.DistCh:setup(java.util.List,org.apache.hadoop.fs.Path):[INFO] log=/log_path
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:tryLock():[INFO] Lock on {} acquired by nodename {}
org.apache.hadoop.hdfs.server.namenode.JournalSet:getEditLogManifest(long):[DEBUG] Generated manifest for logs since
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:getPreviousJobHistoryFileStream(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationAttemptId):[INFO] History file is at {historyFile}
org.apache.hadoop.mapreduce.JobSubmitter:submitJobInternal(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.mapreduce.Cluster):[INFO] Cleaning up the staging area submitJobDir
org.apache.hadoop.fs.s3a.S3AFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter):[DEBUG] Path is a file
org.apache.hadoop.mapred.gridmix.JobMonitor:onFailure(org.apache.hadoop.mapreduce.Job):[INFO] {job.getJobName()} ({job.getJobID()}) failure
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:removeTemporaryFiles():[WARN] Failed to delete temporary file {}
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:containerBasedPreemptOrKill(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] So far, total {totalSelected} containers selected to be preempted, {curSelected} containers selected this round\n
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:run():[INFO] aggregated log deletion finished.
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:setupDataGeneratorConfig(org.apache.hadoop.conf.Configuration):[INFO] GridMix is configured to generate compressed input data with a compression ratio of ratio
org.apache.hadoop.fs.s3a.auth.SignerManager:maybeRegisterSigner(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration):[DEBUG] Registering Custom Signer - [signerName->clazz.getName()]
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[DEBUG] NFS MKDIR dirHandle: {} filename: {} client: {}
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet,boolean):[DEBUG] logAuditEvent failure
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:checksumBlockGroup(org.apache.hadoop.hdfs.protocol.LocatedStripedBlock):[DEBUG] Got access token error in response to OP_BLOCK_CHECKSUM for file {} for block {} from datanode {}. Will retry the block once.
org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Exception encountered while connecting to the server {}
org.apache.hadoop.yarn.util.resource.Resources:addTo(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[WARN] Resource is missing:
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo):[ERROR] Can't get path for fileId: {}
org.apache.hadoop.hdfs.qjournal.server.JNStorage:purgeMatching(java.io.File,java.util.List,long):[WARN] Unable to delete no-longer-needed data {}
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:addHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List):[ERROR] Trying to addHistory duplicate resource skylines for {recurrenceId}. Use updateHistory function instead.
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:readAggregatedLogsMeta(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest):[WARN] Can not get log meta from the log file:
org.apache.hadoop.hdfs.server.namenode.CacheManager:processCacheReport(org.apache.hadoop.hdfs.protocol.DatanodeID,java.util.List):[DEBUG] Processed cache report from {}, blocks: {}, processing time: {} msecs
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:loadINodeDirectorySection(java.io.InputStream,java.util.List):[INFO] Loaded + counter + directories
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier):[INFO] Exits the main loop.
org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext:editLogLoaderPrompt(java.lang.String,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,java.lang.String):[ERROR] prompt
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:accept(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheVisitor):[DEBUG] visiting {} with outstandingMmapCount={}, replicas={}, failedLoads={}, evictable={}, evictableMmapped={}
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[DEBUG] Check access completed successfully
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:shouldContinue(long):[DEBUG] No block has been moved for X iterations, maximum notChangedIterations before exit is: Y
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:waitVolumeRemoved(int,java.util.concurrent.locks.Condition):[INFO] Volume reference is released.
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:checkSubClusterPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.SubClusterPolicyConfiguration):[WARN] Missing SubClusterPolicyConfiguration. Please try again by specifying a SubClusterPolicyConfiguration.
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceInit(org.apache.hadoop.conf.Configuration):[ERROR] Failed to register State Store bean {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:loadRMDTSecretManagerState(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loaded RMDelegationTokenIdentifier: {} renewDate={}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest):[ERROR] SubCluster {} does not exist
org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper():[DEBUG] setZooKeeperRef called
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logModifyCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean):[INFO] logRpcIds
org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl:stopClient(java.lang.Object):[ERROR] Cannot call close method due to Exception. Ignoring.
org.apache.hadoop.fs.s3a.S3AFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration):[TRACE] Filesystem created
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration):[DEBUG] isValidRequestor is allowing: ...
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextDestroyed(javax.servlet.ServletContextEvent):[ERROR] Error closing KeyProviderCryptoExtension, ioe
org.apache.hadoop.fs.s3a.S3AUtils:intOption(org.apache.hadoop.conf.Configuration,java.lang.String,int,int):[DEBUG] Value of {} is {}, key, v
org.apache.hadoop.hdfs.DFSStripedOutputStream:hflush():[DEBUG] DFSStripedOutputStream does not support hflush. Caller should check StreamCapabilities before calling.
org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object):[WARN] Failed to register MBean \ + name + \: Instance already exists.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:executePrivilegedInteractiveOperation(java.util.List,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation):[WARN] IOException executing command: , e
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:processTaskLine(org.apache.hadoop.tools.rumen.ParsedLine):[ERROR] A task type you don't know about is "unknownType".
org.apache.hadoop.fs.s3a.impl.ChangeTracker:processResponse(com.amazonaws.services.s3.transfer.model.CopyResult):[DEBUG] Copy result {policy.getSource()}: {newRevisionId}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer:addResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent):[INFO] Downloading public resource: {key}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:continuousSchedulingAttempt():[ERROR] Error while attempting scheduling for node {node}: {ex.toString()}
org.apache.hadoop.mapred.lib.MultipleOutputs$InternalFileOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable):[DEBUG] Configuring job jar
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:removeDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[WARN] BLOCK* removeDatanode: {node} does not exist
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:getFileInfo(long):[DEBUG] Path:{} doesn't exist!
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadDelegationKey(byte[]):[DEBUG] DelegationKey readFields invoked
org.apache.hadoop.yarn.server.AMRMClientRelayer:reRegisterApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest):[INFO] Concurrent thread successfully re-registered, moving on.
org.apache.hadoop.yarn.service.provider.ProviderUtils:createConfigFileAndAddLocalResource(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,java.util.Map,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams):[INFO] {instance.getCompInstanceId()} version {compLaunchContext.getServiceVersion()} : Creating dir on hdfs: {compInstanceDir}
org.apache.hadoop.hdfs.qjournal.server.JNStorage:purgeMatching(java.io.File,java.util.List,long):[INFO] Purging no-longer needed file {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:handle(org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent):[ERROR] Invalid eventtype + event.getType() + . Ignoring!
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock:writeUnlock():[INFO] Write lock metrics added: ... [additional formatted placeholders]
org.apache.hadoop.hdfs.server.datanode.DataNode:startPlugins(org.apache.hadoop.conf.Configuration):[WARN] ServicePlugin {} could not be started
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:configureTokens(org.apache.hadoop.security.token.Token,org.apache.hadoop.security.Credentials,java.util.Map):[WARN] Cannot locate shuffle secret in credentials. + Using job token as shuffle secret.
org.apache.hadoop.util.SignalLogger$Handler:handle(sun.misc.Signal):[ERROR] RECEIVED SIGNAL X: SIGY
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:checkAccess(org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode):[ERROR] Unable to de-serialize block token identifier for user=userId, block=block, access mode=mode
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:start():[INFO] Cleaning every {} seconds
org.apache.hadoop.hdfs.server.federation.resolver.order.HashResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[ERROR] Cannot find subcluster for {srcPath} ({path} -> {finalPath})
org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser:handleTaskAttemptFailedEvent(org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent):[WARN] AttemptInfo is null for TaskAttemptUnsuccessfulCompletionEvent taskAttemptId: [event.getTaskAttemptId().toString()]
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int):[DEBUG] Requesting buffer of size {}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error While Storing CA Certificate and Private Key, e
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:run():[INFO] Re-encryption caught exception, will retry
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:startWepApp():[INFO] Using webapps at: /path/to/webapps
org.apache.hadoop.hdfs.server.datanode.DataNode:handleVolumeFailures(java.util.Set):[DEBUG] DataNode failed volumes: {volumes list}
org.apache.hadoop.util.SysInfo:getNumVCoresUsed():[INFO] Obtained number of VCores used from Linux
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:isLeader():[INFO] rmId + is elected leader, transitioning to active
org.apache.hadoop.hdfs.server.federation.router.RouterSnapshot:renameSnapshot(java.lang.String,java.lang.String,java.lang.String):[DEBUG] rpcServer.getLocationsForPath invoked
org.apache.hadoop.yarn.service.ServiceMaster:loadApplicationJson(org.apache.hadoop.yarn.service.ServiceContext,org.apache.hadoop.yarn.service.utils.SliderFileSystem):[INFO] context.service.toString()
org.apache.hadoop.security.token.DtUtilShell$Get:validate():[ERROR] Only provide -service with http/https URL.
org.apache.hadoop.util.Shell:isSetsidSupported():[DEBUG] setsid is not allowed to run by the JVM security manager. So not using it.
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Saver:enableSubSectionsIfRequired():[WARN] {} is set to {}. It must be greater than zero. Setting to default of {}, DFSConfigKeys.DFS_IMAGE_PARALLEL_INODE_THRESHOLD_KEY, inodeThreshold, DFSConfigKeys.DFS_IMAGE_PARALLEL_INODE_THRESHOLD_DEFAULT
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:dispatch():[INFO] Start moving + this
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start():[WARN] prefix + metrics system already started!, new MetricsException(Illegal start)
org.apache.hadoop.fs.s3a.impl.DeleteOperation:execute():[DEBUG] deleting simple file {}
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:initHistoricalDBs():[WARN] Failed to initialize rolling leveldb {dbName} for {getName()}
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatus(org.apache.hadoop.fs.Path):[DEBUG] The file list contains the COS key [{}] to be listed.
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceStop():[WARN] Interrupted while waiting for deletion thread to complete, closing db now
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress):[WARN] Got a command from standby NN {} - ignoring command: {}
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor:executeParallel(org.apache.hadoop.fs.azure.FileMetadata[],org.apache.hadoop.fs.azure.AzureFileSystemThreadTask):[DEBUG] Using thread pool for {} operation with threads {}, operation, threadCount
org.apache.hadoop.mapred.JobConf:getMaxVirtualMemoryForTask():[WARN] getMaxVirtualMemoryForTask() is deprecated. Instead use getMemoryForMapTask() and getMemoryForReduceTask()
org.apache.hadoop.hdfs.server.namenode.FSImage:doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem):[INFO] Can perform rollback for shared edit log.
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] cleanUpPartialOutputForTask: removing everything belonging to + context.getTaskAttemptID().getTaskID() + in: + getCommittedTaskPath(context).getParent()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor:handleJobSetup(org.apache.hadoop.mapreduce.v2.app.commit.CommitterJobSetupEvent):[WARN] Job setup failed, e
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:initRetryCache(org.apache.hadoop.conf.Configuration):[INFO] Retry cache on namenode is enabled
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:scan(long):[TRACE] openFileMap size: + size()
org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile():[WARN] Error reading the stream /proc/diskstats
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:writeData(org.apache.hadoop.hdfs.client.HdfsDataOutputStream):[ERROR] Failed to get request data offset: + getPlainOffset() + " " + "count:" + count + " error:" + e1
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:deleteRMConfStore(org.apache.hadoop.conf.Configuration):[INFO] Scheduler Configuration format only supported by MutableConfScheduler.
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:isAvailable():[INFO] CGroupsResourceCalculator currently is supported only on Linux.
org.apache.hadoop.hdfs.DFSInputStream:openInfo(boolean):[WARN] Last block locations not available. Datanodes might not have reported blocks completely. Will retry for ${retriesForLastBlockLength} times
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$TaskCompletedTransition:checkReadyForCompletionWhenAllReducersDone(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl):[INFO] Killing map task
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:loadPolicyConfiguration(java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade):[INFO] No policy configured for queue {} in StateStore, fallback to default queue
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:validateSpillIndexFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration):[DEBUG] validateSpillIndexFileCB.. Path: {}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode):[DEBUG] Skip this queue= + getQueuePath() + , because it doesn't need more resource, schedulingMode= + schedulingMode.name() + node-partition= + candidates.getPartition()
org.apache.hadoop.hdfs.server.namenode.NameNode:doRecovery(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.conf.Configuration):[INFO] RECOVERY FAILED: caught exception
org.apache.hadoop.fs.s3a.S3AFileSystem:innerListStatus(org.apache.hadoop.fs.Path):[DEBUG] List status for path: {path}
org.apache.hadoop.fs.FSInputChecker:read(long,byte[],int,int):[DEBUG] Read operation successful; bytes count reached
org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB:close():[INFO] Stopping rpcProxy in InMemoryAliasMapProtocolClientSideTranslatorPB
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:recover():[INFO] Recovering {} running applications for AMRMProxy
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:deleteNextEntity(java.lang.String,byte[],org.apache.hadoop.yarn.server.utils.LeveldbIterator,org.apache.hadoop.yarn.server.utils.LeveldbIterator,boolean):[DEBUG] Deleting entity type:{} id:{}
org.apache.hadoop.tools.dynamometer.ApplicationMaster$LaunchContainerRunnable:getContainerStartCommand():[INFO] Completed setting up command for datanode: ...
org.apache.hadoop.hdfs.server.namenode.Checkpointer:run():[ERROR] Exception in doCheckpoint:
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadDelegationKey(byte[]):[DEBUG] IOUtils.cleanupWithLogger invoked
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp:unprotectedRenameTo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long):[WARN] DIR* FSDirectory.unprotectedRenameTo: rename destination parent ... not found.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:validateReconstructionWork(org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork):[DEBUG] BLOCK* block {} is moved from neededReconstruction to pendingReconstruction
org.apache.hadoop.mapred.TaskAttemptListenerImpl:statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[INFO] Setting preemption bit for task: + yarnAttemptID + of type + yarnAttemptID.getTaskId().getTaskType()
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:logAndThrowStoreException(org.slf4j.Logger,java.lang.String):[ERROR] {errMsg}
org.apache.hadoop.mapreduce.util.ProcessTree:sigKillInCurrentThread(java.lang.String,boolean,long):[WARN] Thread sleep is interrupted.
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:closeInternal(java.lang.Object):[WARN] Exception while trying to close proxy
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates):[DEBUG] allocate: post-update applicationId=applicationAttemptId application=application
org.apache.hadoop.yarn.service.timelineservice.ServiceMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord):[DEBUG] Publishing Component metrics. {}
org.apache.hadoop.hdfs.server.common.sps.BlockStorageMovementTracker:run():[DEBUG] Completed block movement. {}
org.apache.hadoop.yarn.server.resourcemanager.AdminService:checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo):[WARN] Allowing manual failover from + org.apache.hadoop.ipc.Server.getRemoteAddress() + even though automatic failover is enabled, because the user specified the force flag
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path):[DEBUG] AzureBlobFileSystem.removeDefaultAcl path: {}
org.apache.hadoop.hdfs.server.common.MetricsLoggerTask:makeMetricsLoggerAsync(org.apache.commons.logging.Log):[WARN] Metrics logging will not be async since the logger is not log4j
org.apache.hadoop.fs.s3a.S3AFileSystem:innerListFiles(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.s3a.Listing$FileStatusAcceptor,org.apache.hadoop.fs.s3a.S3AFileStatus):[DEBUG] listFiles({}, {})
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:invokeConcurrent(java.util.Collection,org.apache.hadoop.hdfs.server.federation.router.RemoteMethod,boolean,boolean,java.lang.Class):[ERROR] Invocation to "{location}" for "{method.getMethodName()}" timed out
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:readAggregatedLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.OutputStream):[DEBUG] Attempting to read aggregated logs using IndexedFileController
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:loadINodeDirSection(java.io.FileInputStream,java.util.List,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary,org.apache.hadoop.conf.Configuration,java.util.List):[INFO] Loading INode directory section.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:pickAndDoSchedule(java.util.Set,java.util.Map,java.util.Set,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,int,java.lang.String,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.DevicePluginScheduler):[DEBUG] Customized device plugin scheduler is preferred but not implemented, use default logic
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoCandidatesSelector:selectCandidates(java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] skipping from queue={} because it's a non-preemptable queue
org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean):[DEBUG] RETRY {}) policy={}
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreProxyCACertTransition:transition(java.lang.Object,java.lang.Object):[ERROR] Error While Storing CA Certificate and Private Key, e
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:checkPauseForTesting():[INFO] Sleeping in the re-encrypt handler for unit test.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser:getPriorityAcl(org.apache.hadoop.yarn.api.records.Priority,java.lang.String):[WARN] ACL configuration for '%s' is greater that cluster max priority. Resetting ACLs to %s
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:updateCurrentMasterKey(org.apache.hadoop.yarn.server.security.MasterKeyData):[ERROR] Unable to update current master key in state store, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:checkPlacementPoliciesPresent(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler,org.apache.hadoop.conf.Configuration):[DEBUG] Allocation file loaded
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Invalid blacklist request by application + appAttemptId
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:abort():[INFO] failure to clean up + tmpOutputPath, ie
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.event.Event):[WARN] Transitioning the resource manager to standby.
org.apache.hadoop.yarn.webapp.Router:load(java.lang.Class,java.lang.String):[DEBUG] found {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:launchContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext):[DEBUG] config.json used:
org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager:addPrioirityACLs(java.util.List,java.lang.String):[DEBUG] Priority ACL group added: max-priority - {priorityACLGroup.getMaxPriority()} default-priority - {priorityACLGroup.getDefaultPriority()}
org.apache.hadoop.mapred.gridmix.Statistics$StatCollector:run():[ERROR] Statistics interrupt while waiting for completion of a job.
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:uploadBlockAsync(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock,java.lang.Boolean):[DEBUG] Uploading part {} for id '{}'
org.apache.hadoop.mapred.TaskAttemptListenerImpl:statusUpdate(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskStatus):[ERROR] Status update was called with illegal TaskAttemptId: + yarnAttemptID
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:writeInternal(java.lang.String,java.lang.String,java.lang.String,java.lang.String,long,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.api.records.timelineservice.TimelineWriteResponse):[WARN] Interrupted operation: + ioe.getMessage()
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:getEntryToEvict():[WARN] All opened streams are busy, can't remove any from cache.
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getSubdirEntries():[TRACE] getSubdirEntries({}, {}): listed {} entries in {}
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:verifyPlanHash(java.lang.String,java.lang.String):[ERROR] Disk Balancer - Invalid plan.
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]):[DEBUG] Auth method <loginMeth>
org.apache.hadoop.yarn.client.AHSProxy:getProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.InetSocketAddress):[DEBUG] UserGroupInformation current user obtained
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS SETATTR fileHandle: {} client: {}
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render():[INFO] You can either use the CLI tool: 'mapred job -history' to view large jobs or adjust the property {JHAdminConfig.MR_HS_LOADED_JOBS_TASKS_MAX}.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServiceProtocol:dumpSchedulerLogs(java.lang.String,javax.servlet.http.HttpServletRequest):[INFO] Scheduler logs dumped
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:readClusterInfo(org.apache.commons.cli.CommandLine):[DEBUG] using name node URI : {}
org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[WARN] createFailureLog(user, operation, perm, target, description)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$ContainerEventProcessor:onExceptionRaised(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[INFO] Unchecked exception is thrown from onGetContainerStatusError for Container + containerId, thr
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:checkNNVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[WARN] {ive.getMessage()}
org.apache.hadoop.tools.dynamometer.DynoInfraUtils:waitForNameNodeReadiness(java.util.Properties,int,boolean,java.util.function.Supplier,org.apache.hadoop.conf.Configuration,org.slf4j.Logger):[INFO] Waiting for UnderReplicatedBlocks to fall below {}...
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:read(byte[],int,int):[TRACE] read(arr.length={}, off={}, len={}, filename={}, block={}, canSkipChecksum={}): returning {}
org.apache.hadoop.hdfs.DataStreamer:waitForAckedSeqno(long):[DEBUG] {} waiting for ack for: {}
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[WARN] Unmanaged AM still not successfully launched/registered yet. Stopping the UAM heartbeat thread anyways.
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:evictOldStartTimes(long):[DEBUG] Deleted batch of {batchSize}. Total start times deleted so far this cycle: {startTimesCount}
org.apache.hadoop.yarn.util.ResourceCalculatorPlugin:getResourceCalculatorPlugin(java.lang.Class,org.apache.hadoop.conf.Configuration):[WARN] Failed to instantiate default resource calculator. [Exception Message]
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[INFO] Task {} committed {} files
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run():[DEBUG] Status polling for job
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processIncrementalBlockReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks):[DEBUG] *BLOCK* NameNode.processIncrementalBlockReport: from {} receiving: {}, received: {}, deleted: {}
org.apache.hadoop.yarn.client.AHSProxy:getProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.InetSocketAddress):[INFO] PrivilegedAction execution started
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render():[INFO] Cluster Status
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:cleanUpPreviousJobOutput():[ERROR] Error while trying to clean up previous job's temporary files
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:writeAMRMTokenForUAM(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,org.apache.hadoop.security.token.Token):[INFO] Writing/Updating amrmToken for subClusterId to registry for appId
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:downloadCheckpointFiles(java.net.URL,org.apache.hadoop.hdfs.server.namenode.FSImage,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature,org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest):[INFO] Image has changed. Downloading updated image from NN.
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersUpdated(java.util.List):[INFO] Container {} updated, updateType={}, resource={}, execType={}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:updateRuleSet(java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler):[WARN] Rules after rule (i+1) in queue placement policy can never be reached
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool():[INFO] Successfully synced BackupNode with NameNode at txnid ...
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[INFO] Skipping copy of {sourceCurrStatus.getPath()} to {target}
org.apache.hadoop.portmap.RpcProgramPortmap:unset(int,org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR):[DEBUG] Portmap remove key= + key
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Requested resource value after conversion: + requestedResourceValue
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher:handle(org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent):[ERROR] FATAL, Shutting down the resource manager because a state store operation failed, and the resource manager is configured to fail fast. See the yarn.fail-fast and yarn.resourcemanager.fail-fast properties.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:updateFailedStorage(java.util.Set):[INFO] {storageInfo} failed.
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:loadDetailLog(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId):[INFO] File {log.getPath(getAppDirPath())} no longer exists, removing it from log list
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:waitForAllPartUploads():[DEBUG] Cancelling futures
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:wipeState():[INFO] Wiping tc state.
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getJson(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord):[ERROR] Cannot serialize field {fieldName} into JSON
org.apache.hadoop.mapred.MapOutputCollector:collect(java.lang.Object,java.lang.Object,int):[INFO] Collection by MapOutputBuffer started
org.apache.hadoop.fs.s3a.S3AFileSystem:finishedWrite(java.lang.String,long,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.impl.PutObjectOptions):[DEBUG] Finished write to {}, len {}. etag {}, version {}, key, length, eTag, versionId
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List):[DEBUG] Checking EC block group
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:recoverContainer(org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$RecoveredContainerState):[WARN] {containerId} has no corresponding application!
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithFailureToDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[WARN] Audit log: Container failed with state: ...
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:main(java.lang.String[]):[INFO] Web server started
org.apache.hadoop.mapreduce.v2.app.webapp.JobBlock:render():[INFO] Job Overview
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeApplicationStateInternal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData):[INFO] Removing info for app: + appId + at: + nodeRemovePath
org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Old Queue: + stringRepr(oldQ) + , + Replacement: + stringRepr(newQ)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer):[DEBUG] Try to allocate from reserved container, but node is not reserved
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:doAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer):[DEBUG] Resetting scheduling opportunities
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:computeAppsIdealAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,float):[DEBUG] Queue Name:queueName, partition:partition
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:populateMembers(org.apache.hadoop.mapreduce.v2.app.AppContext):[INFO] Obtained taskID from tid
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils:isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.slf4j.Logger):[DEBUG] Skipping 'host' {} for {} since it has been blacklisted
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext):[WARN] {}: No files to commit
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:chooseDatanode(org.apache.hadoop.hdfs.server.federation.router.Router,java.lang.String,org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,long,java.lang.String):[ERROR] Cannot get the datanodes from the RPC server
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:toResponse(java.lang.Exception):[WARN] INTERNAL_SERVER_ERROR, e
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:initialize(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics):[ERROR] Proxy error: ...
org.apache.hadoop.mapred.gridmix.Gridmix:start(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.Path,long,org.apache.hadoop.mapred.gridmix.UserResolver):[ERROR] Startup failed. {exception}
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:computeMove(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume,org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume):[DEBUG] {} Skipping disk from computation. Maximum data size achieved., lowVolume.getPath()
org.apache.hadoop.mapreduce.task.reduce.Fetcher:openConnectionWithRetry(java.net.URL):[WARN] Failed to connect to host: + url + after + fetchRetryTimeout + milliseconds.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processCompletedNodes(java.util.List):[ERROR] Node {} is in an unexpected state {} and has been removed from tracking for decommission or maintenance
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:handle(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent):[DEBUG] Processing {event.getContainerId()} of type {event.getType()}
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Could not update cgroup for container
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:moveLazyPersistReplicasToFinalized(java.io.File):[WARN] Failed to move block file from + blockFile + to + targetBlockFile, e
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent):[DEBUG] RECV: request.getUri() \nmapId: mapIds \nreduceId: reduceQ \njobId: jobQ \nkeepAlive: keepAliveParam
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:close():[DEBUG] Closing AbfsOutputStream : {this}
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:buildCommandExecutor(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.Resource,java.io.File,java.util.Map):[INFO] launchContainer: {}
org.apache.hadoop.fs.s3a.impl.RenameOperation:renameFileToDest():[DEBUG] rename: renaming file {} to {}
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:applyEditLogOp(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp,org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,int,long):[DEBUG] replaying edit log: ...
org.apache.hadoop.yarn.client.cli.ApplicationCLI:killApplication(java.lang.String):[INFO] Application appId has already finished
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:start():[INFO] Storage policy satisfier is configured as external, please start external sps service explicitly to satisfy policy
org.apache.hadoop.hdfs.DataStreamer:waitForAckedSeqno(long):[ERROR] No ack received, took {}ms (threshold={}ms). File being written: {}, block: {}, Write pipeline datanodes: {}.
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getBlockLocations(java.lang.String,java.lang.String,long,long):[INFO] Safe mode exception encountered
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[DEBUG] Storing token
org.apache.hadoop.io.erasurecode.CodecRegistry:updateCoders(java.lang.Iterable):[DEBUG] Codec registered: codec = {coderFactory.getCodecName()}, coder = {coderFactory.getCoderName()}
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:register(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo):[INFO] this + successfully registered with NN
org.apache.hadoop.tools.rumen.TraceBuilder:run(java.lang.String[]):[WARN] No job found in traces
org.apache.hadoop.lib.server.Server:init():[INFO] Services initialized
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getMountPointStatus(java.lang.String,int,long):[ERROR] Cannot get remote user: {msg}
org.apache.hadoop.hdfs.LocatedBlocksRefresher:run():[DEBUG] Finished refreshing {} of {} streams in {}ms
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer$ReInitializeContainerTransition:transition(java.lang.Object,java.lang.Object):[INFO] Unchecked exception is thrown in handler for event [COMMIT_LAST_REINT] for Container
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:getTaskReports(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsRequest):[DEBUG] Task report added]]>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:autoCreateLeafQueue(org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext):[ERROR] SchedulerDynamicEditException: Invalid parent queue
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfDfsUsageParameters(java.lang.String,java.lang.String):[INFO] Reconfiguring {} to {}
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] JobHistory Init
org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName):[WARN] Error unregistering [Dynamic Value: mbeanName]
org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean):[ERROR] Unable to clear zk parent znode
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:pushToZK(byte[],byte[],byte[]):[ERROR] An unexpected exception occurred pushing data to ZooKeeper, ex
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread:recordUsage(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree,org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$ProcessTreeInfo,long,long,org.apache.hadoop.yarn.api.records.ResourceUtilization):[DEBUG] Resource usage of ProcessTree {} for container-id {}: {} %CPU: {} %CPU-cores: {} vCores-used: {} of {} Cumulative-CPU-ms: {}
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:deprecateEnv(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String):[WARN] Environment variable {} is deprecated and overriding property {}', please set the property in {} instead.
org.apache.hadoop.hdfs.server.datanode.DataStorage:loadDataStorage(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.util.Collection,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.concurrent.ExecutorService):[INFO] Storage directory {dataDir} has already been used.
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:run():[INFO] Re-encrypt handler interrupted. Exiting
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:loadServices(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords,org.apache.hadoop.conf.Configuration,boolean):[DEBUG] Auxiliary service already loaded: {service.getName()}
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy:chooseStorageTypes(short,java.lang.Iterable,java.util.EnumSet,boolean):[WARN] Failed to place enough replicas: expected size is {} but only {} storage types can be selected (replication={}, selected={}, unavailable={}, removed={}, policy={})
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:addToCluserNodeLabels(java.util.Collection):[ERROR] NODE_LABELS_NOT_ENABLED_ERR
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:mountCgroups(java.util.List,java.lang.String):[WARN] Exception in LinuxContainerExecutor mountCgroups
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode):[INFO] Unknown application + containerId.getApplicationAttemptId().getApplicationId() + launched container + containerId + on node: + node
org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern):[WARN] Could not set last modified time for {} file(s)
org.apache.hadoop.hdfs.server.mover.Mover:checkKeytabAndInit(org.apache.hadoop.conf.Configuration):[INFO] Keytab is configured, will login using keytab.
org.apache.hadoop.yarn.service.client.ApiServiceClient:actionUpgradeInstances(java.lang.String,java.util.List):[ERROR] Failed to upgrade component instance: {}
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore$AppCheckTask:run():[INFO] There are now [number] entries in the list.
org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader:getProcessId(org.apache.hadoop.fs.Path):[DEBUG] Accessing pid from pid file {}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:shutDownTimer():[DEBUG] Shutting down timer {toString()}
org.apache.hadoop.fs.FileSystem:closeAll():[DEBUG] debugLogFileSystemClose
org.apache.hadoop.hdfs.server.datanode.DataNode:getBlockLocalPathInfo(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token):[TRACE] getBlockLocalPathInfo successful block={} blockfile {} metafile {}
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextOp():[ERROR] Got error reading edit log input stream...
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:recoverUnclosedSegment(long):[INFO] Recovery prepare phase complete. Responses:\n + QuorumCall.mapToString(prepareResponses)
org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]):[INFO] Displaying usage information
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object):[DEBUG] Registering the metrics source
org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit):[DEBUG] Executor service has not shutdown yet. Forcing. Will wait up to an additional {} {} for shutdown
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy):[INFO] Read block from sender
org.apache.hadoop.hdfs.server.namenode.FSDirectory:setINodeAttributeProvider(org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider):[INFO] Fallback to the old authorization provider API because the expected method is not found.
org.apache.hadoop.hdfs.protocol.ClientProtocol:rollEdits():[INFO] Rolled edit log using RouterClientProtocol
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:checkUserAccessToQueue(java.lang.String,java.lang.String,java.lang.String,javax.servlet.http.HttpServletRequest):[DEBUG] Initializing services
org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation):[DEBUG] Logging user info
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onGetContainerStatusError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable):[ERROR] Failed to query the status of Container + containerId
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[DEBUG] NFS RMDIR dir fileHandle: {} fileName: {} client: {}
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:getFilesystemProperties(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] getFilesystemProperties for filesystem: {}
org.apache.hadoop.hdfs.server.namenode.LeaseManager:stopMonitor():[WARN] Encountered exception
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:cancelDelegationToken(org.apache.hadoop.security.token.Token):[INFO] Cancel request by {canceller}
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[INFO] {}: Validating output.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:chooseExcessRedundancyStriped(org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,java.util.Collection,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[WARN] excess types chosen for block {} among storages {} is empty
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Received URL {url} from user {userName}
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String):[TRACE] Got user: {}, remoteIp: {}, path: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] {}
org.apache.hadoop.fs.s3a.Invoker:quietly(java.lang.String,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE):[DEBUG] Action {} failed
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String):[ERROR] Error getting entity
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:generate(java.lang.Class,java.lang.Class,java.lang.String,java.lang.String):[INFO] Generating {} methods
org.apache.hadoop.hdfs.DFSClient:getRandomLocalInterfaceAddr():[DEBUG] Using local interface {}
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:processRecovery():[WARN] Unable to parse prior job history, aborting recovery
org.apache.hadoop.service.CompositeService:stop():[DEBUG] Stopping service #i: {service}
org.apache.hadoop.mapred.CleanupQueue$PathCleanupThread:run():[DEBUG] {getName() + " started."}
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:getObjectMetadata(java.lang.String):[DEBUG] Exception thrown when get object meta: + key + , exception: + osse
org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider:refreshToken():[DEBUG] AADToken: refreshing client-credential based token
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run():[WARN] Unexpected exception:
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:markSuspectBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock):[DEBUG] {}: Scheduling suspect block {} for rescanning.
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:chooseFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation):[DEBUG] Local namespace for {} is {}, clientAddr, localSubcluster
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:run():[ERROR] The shared cache root + location + was not found. The cleaner task will do nothing.
org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int):[INFO] Initializing RPC stats for {} priority levels
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] No such Execution Type={}
org.apache.hadoop.mapreduce.v2.util.MRApps:createJobClassLoader(org.apache.hadoop.conf.Configuration):[DEBUG] Debugging createJobClassLoader
org.apache.hadoop.hdfs.server.datanode.DataNode:transferReplicaForPipelineRecovery(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],java.lang.String):[DEBUG] Transferring a replica to {xferTargetsString}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: DYNAMIC_MAX_ASSIGN
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:startActiveServices():[WARN] Lazy persist file scrubber is disabled, configured scrub interval is zero.
org.apache.hadoop.yarn.service.client.ServiceClient:actionDestroy(java.lang.String):[ERROR] Error on destroy ' + serviceName + ': not found.
org.apache.hadoop.fs.azurebfs.services.AbfsClient:getAclStatus(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext):[DEBUG] Creating default URI query builder
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:setTimerForTokenRenewal(org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew):[INFO] Will not renew token {token}
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:addApplicationHomeSubCluster(org.apache.hadoop.yarn.server.federation.store.records.AddApplicationHomeSubClusterRequest):[INFO] Application: {} already present with SubCluster: {}
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[]):[INFO] Can not find any valid fileControllers.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:parseDiagnosticOutput(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin$InnerShellExecutor,java.lang.String):[WARN] Unsupported diagnose output
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:fetchOrCreate(org.apache.hadoop.hdfs.ExtendedBlockId,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$ShortCircuitReplicaCreator):[TRACE] {}: can't fethchOrCreate {} because the cache is closed.
org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String):[INFO] Can't map user ${user}. Use its string hashcode: ${uid}
org.apache.hadoop.lib.server.Server:init():[INFO] Config dir: {}
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$CancelUpgradeTransition:transition(java.lang.Object,java.lang.Object):[INFO] {} nothing to cancel
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long):[DEBUG] Storing token {tokenId.getSequenceNumber()}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onContainersCompleted(java.util.List):[INFO] Ignoring completed status of [containerId]; unknown container(probably launched by previous attempt)
org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean):[WARN] Exception in get all trash roots for mount points
org.apache.hadoop.yarn.server.webapp.AppBlock:render():[ERROR] Invalid Application ID: [aid]
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:check():[WARN] {} nodes are decommissioning but only {} nodes will be tracked at a time. {} nodes are currently queued waiting to be decommissioned.
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int):[DEBUG] this + : + caller + : sendCallback processed fd + fd + in toRemove.
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.security.SecurityHandler,java.net.SocketAddress):[INFO] Nonpositive maxcount in invalid READDIRPLUS request: {maxCount}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[DEBUG] Core-site XML loaded and processed
org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream:flushAndSync(boolean):[INFO] Nothing to flush
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:processBlocksInternal(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Iterator,java.util.List,boolean):[TRACE] Removing unknown block
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:removeAcl(java.lang.String):[DEBUG] Audit Event: removeAcl success
org.apache.hadoop.lib.server.Server:init():[INFO] Version : {}
org.apache.hadoop.hdfs.DFSStripedInputStream:reportLostBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection):[WARN] Arrays.toString(nodes) + " are unavailable and all striping blocks on them are lost. IgnoredNodes = " + ignoredNodes
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Unable to locate user for <appId>
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor):[INFO] Placement Algorithm Iterator[iteratorName]
org.apache.hadoop.yarn.util.FSDownload:unpack(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem):[WARN] Cannot unpack {source}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:releaseContainers(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt):[INFO] {containerId} doesn’t exist. Add the container to the release request cache as it may be on recovery.
org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor):[WARN] NativeIO.getFstat error (%d): %s
org.apache.hadoop.fs.http.server.HttpFSServer:post(java.io.InputStream,javax.ws.rs.core.UriInfo,java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest):[INFO] [path]
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:receive(org.apache.hadoop.hdfs.net.Peer,java.io.OutputStream,java.io.InputStream,int,org.apache.hadoop.hdfs.protocol.DatanodeID):[DEBUG] SASL server doing general handshake for peer = {}, datanodeId = {}
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[INFO] Recovering + appStates.size() + applications
org.apache.hadoop.lib.service.FileSystemAccess$FileSystemExecutor:execute(org.apache.hadoop.fs.FileSystem):[INFO] FSListStatus operation executed
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getDelegationToken(org.apache.hadoop.io.Text):[WARN] trying to get DT with no secret manager running
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:splitAllocateRequest(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest):[DEBUG] Processing release list for subClusters
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:verifyRMHeartbeatResponseForNodeAttributes(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse):[ERROR] NM node attributes { + getPreviousValue() + } were not accepted by RM and message from RM :
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:applicationFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationFinishData):[ERROR] Error when writing finish information of application + appFinish.getApplicationId(), e
org.apache.hadoop.conf.Configuration:reloadExistingConfigurations():[DEBUG] Reloading + REGISTRY.keySet().size() + existing configurations
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List):[WARN] EntryFile write queue inactive; discarding {} entries submitted to {}
org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date):[INFO] Created trash checkpoint: {checkpoint.toUri().getPath()}
org.apache.hadoop.ipc.Server$Listener$Reader:run():[INFO] Starting [Thread Name]
org.apache.hadoop.mapred.QueueManager:refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher):[ERROR] msg.toString()
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl):[DEBUG] getEntities type={} primary={}
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:getSimpleUgi():[ERROR] Error parsing simple UGI <{}>; falling back to current user
org.apache.hadoop.hdfs.server.datanode.BlockSender:readChecksum(byte[],int,int):[WARN] Could not read or failed to verify checksum for data at offset [dynamic_offset] for block [dynamic_block]
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processCompletedNodes(java.util.List):[INFO] Node {} is sufficiently replicated and healthy, marked as {}.
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:mkdir(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission):[INFO] Explicitly setting permissions to : ...
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:getRemoteEditLogs(long,boolean):[ERROR] got IOException while trying to validate header of + elf + . Skipping., e
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:checkVersion():[INFO] Loaded RM state version info [loadedVersion]
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:initializePriorityDigraph():[DEBUG] - Added priority ordering edge: {q1} >> {q2}
org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String):[DEBUG] Creating DataFactory of type : {name}
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:prepareOutputFiles(java.lang.String,boolean):[INFO] Output directory for {YARN_SITE_XML} and {CAPACITY_SCHEDULER_XML} is: {outputDirectory}
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] Emitting job history data to the timeline server is not enabled
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersAllocated(java.util.List):[INFO] [COMPONENT {}]: remove {} outstanding container requests for allocateId {}
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run():[ERROR] Unexpected exception
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:handle(org.apache.hadoop.yarn.event.Event):[INFO] taskId + Task Transitioned from + oldState + to + getInternalState()
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager:remove(org.apache.hadoop.hdfs.server.datanode.BPOfferService):[INFO] Removed BPOfferService
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileStatus(org.apache.hadoop.fs.Path):[DEBUG] Path: [{}] is a dir. COS key: [{}]
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext):[DEBUG] logCommitterStatisticsAtDebug
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:finalizeRollingUpgrade():[WARN] NameNode safe mode check failed: Failed to finalize rolling upgrade
org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int):[INFO] Successfully ensured local node is in standby mode
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress):[INFO] Path {path} is not shared.
org.apache.hadoop.yarn.service.ServiceScheduler$ComponentInstanceEventHandler:handle(org.apache.hadoop.yarn.event.Event):[ERROR] instance.getCompInstanceId() + ": Error in handling event type " + event.getType()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$AttemptFailedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[DEBUG] Not generating HistoryFinish event since start event not generated for task: task.getID()
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache:prepareAddressMap():[INFO] Added export: exportPath FileSystem URI: exportPath with namenodeId: namenodeId
org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] No valid mount-table file exist at: {}. At least one mount-table file should present with the name format: mount-table.<versionNumber>.xml
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:load():[INFO] Loading rack into resolver: rackName --> subClusterId
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:executeStage(java.lang.Object):[INFO] {}: Renaming manifests to {}
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob():[ERROR] Error Launching job :
org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean):[WARN] Current time + current + is ahead of started time + started
org.apache.hadoop.hdfs.protocol.ClientProtocol:renewLease(java.lang.String):[DEBUG] Lease renewed with RouterClientProtocol
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:clean():[DEBUG] Obtained history dirs for cleaning
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:handle(org.apache.hadoop.yarn.event.Event):[ERROR] Can't handle this event at current state, e
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:loadServiceFrom(org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path):[INFO] Loading service definition from + appDefPath
org.apache.hadoop.yarn.client.api.AMRMClient:waitFor(java.util.function.Supplier):[INFO] Waiting in main loop.
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer):[TRACE] Thread.currentThread().getId() + ": Response <- " + remoteId + ": " + method.getName() + " {" + TextFormat.shortDebugString(returnMessage) + "}"
org.apache.hadoop.mapred.ShuffleHandler:checkVersion():[INFO] Storing state DB schema version info
org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration):[WARN] IPC_FCQ_DECAYSCHEDULER_FACTOR_KEY is deprecated. Please use IPC_SCHEDULER_DECAYSCHEDULER_FACTOR_KEY.
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:processConf():[INFO] Initialized the Backoff Decommission and Maintenance Monitor
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveFailureTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent):[ERROR] Unexpected event for REDUCE task (event.getType())
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Recovery message with actual state
org.apache.hadoop.hdfs.server.datanode.DataNode:startMetricsLogger():[DEBUG] Starting Metrics Logger Timer
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizedWhileRunningTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[INFO] Created symlink: linkFile -> rsrcEvent.getLocation()
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:tryDatanode(org.apache.hadoop.hdfs.protocol.LocatedStripedBlock,org.apache.hadoop.hdfs.protocol.StripedBlockInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo,long):[DEBUG] write to {}: {}, blockGroup={}, datanode, Op.BLOCK_GROUP_CHECKSUM, blockGroup
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:removeVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl):[DEBUG] Remove volume method invoked
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token):[DEBUG] Performing our own SPNEGO sequence.
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logRemoveCachePool(java.lang.String,boolean):[DEBUG] Removed cache pool
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:lookupToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text):[DEBUG] Logging debug message
org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int):[INFO] Failed to get groups from the first lookup. Initiating the second LDAP query using the user's DN.
org.apache.hadoop.hdfs.DFSOutputStream:completeFile():[WARN] Caught exception
org.apache.hadoop.fs.FileSystem:loadFileSystems():[DEBUG] Stack Trace
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Requested resource information: + requestedRI
org.apache.hadoop.mapred.ShuffleHandler:recoverJobShuffleInfo(java.lang.String,byte[]):[INFO] New instance created
org.apache.hadoop.mapred.TaskLog:getLogFileDetail(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskLog$LogName,boolean):[DEBUG] Closed file for DEBUGOUT or PROFILE
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Illegal event type: $event.getClass()
org.apache.hadoop.hdfs.server.mover.Mover$Processor:getSnapshottableDirs():[WARN] Failed to get snapshottable directories. Ignore and continue.
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$StoppedAfterCancelUpgradeTransition:transition(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEvent):[INFO] {} received stopped but cancellation pending
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:validateComponent(org.apache.hadoop.yarn.service.api.records.Component,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration):[INFO] Validating configuration files
org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts():[DEBUG] After decaying the stored costs, totalDecayedCost: ..., totalRawCallCost: ...;
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onRequestsRejected(java.util.List):[INFO] Exiting, since retries are exhausted !!
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:reconstruct():[INFO] Transferring data to targets...
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:enqueueEdit(org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$Edit):[INFO] Edit pending queue is full
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:readEntityTypes(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection):[DEBUG] FilterList created for scan is - {...}
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:shouldRetry(java.io.IOException,int,java.lang.String):[WARN] e.getMessage()
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities):[ERROR] Application: [appId] is not found
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render():[ERROR] User [USER] is not authorized to view the logs for
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl$FSAction:runWithRetries():[INFO] Will retry operation on FS. Retry no. #{retry + 1} after sleeping for #{fsRetryInterval} seconds
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:main(java.lang.String[]):[DEBUG] Not found:
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:startSyncJournalsDaemon():[INFO] Stopping Journal Node Sync.
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:purge(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica):[TRACE] this: : purged replica from the cache. Removed from the replicaInfoMap. Removed from evictionMapName
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List):[DEBUG] AzureBlobFileSystem.setAcl path: {}
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceStart():[INFO] Starting HTTP server
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RemoveAppAttemptTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore,org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent):[ERROR] Error removing attempt: someAttemptId, someException
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeBlocksAssociatedTo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo):[INFO] Removed blocks associated with storage {storageInfo} from DataNode {node}
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponent(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,org.apache.hadoop.yarn.service.api.records.Component):[INFO] PUT: upgrade component {} for service {} user = {}
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.processor.VolumeAMSProcessor:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse):[WARN] Volume provisioning task failed
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:launchUAM():[DEBUG] Blocking call to RM
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:queryObjectMetadata(java.lang.String):[DEBUG] Retrieve file metadata. COS key: [{}], ETag: [{}], length: [{}].
org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature:initializeBlockRecovery(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,boolean):[DEBUG] BLOCK* {this} recovery started, primary={primary}
org.apache.hadoop.tools.mapred.CopyCommitter:deleteAttemptTempFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,java.lang.String):[INFO] Cleaning up {file.getPath()}
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanIntermediateDirectory(org.apache.hadoop.fs.Path):[DEBUG] Duplicate: deleting
org.apache.hadoop.yarn.util.resource.ResourceUtils:checkSpecialResources(java.util.Map):[WARN] Attempt to define resource ' + key + ', but it is not allowed.
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:handle(org.apache.hadoop.yarn.event.Event):[WARN] couldn't find container + containerId + while processing FINISH_CONTAINERS event
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:transition(java.lang.Object,java.lang.Object):[INFO] Updating info for app: [appId]
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNodeLabelsProvider(org.apache.hadoop.conf.Configuration):[ERROR] Failed to create NodeLabelsProvider based on Configuration, e
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent):[INFO] KMS Started
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:ensurePluginIsSupported(java.lang.String):[ERROR] Trying to initialize resource plugin with name
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] Killed application
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:expireOldScannedBytesRecords(long):[TRACE] {}: updateScannedBytes is zeroing out slotIdx {}. curMinute = {}; newMinute = {}
org.apache.hadoop.registry.server.dns.ReverseZoneUtils:splitIp(java.lang.String):[ERROR] Base IP address is invalid
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:initPlugin(org.apache.hadoop.conf.Configuration):[INFO] Succeed in finding FPGA discoverer executable: executable
org.apache.hadoop.tools.rumen.Folder:run(java.lang.String[]):[INFO] Starts-after time is specified. Initial job submit time : [job.getSubmitTime()]
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[WARN] Runtime spec in non-Docker container is not supported yet!
org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration):[INFO] Excluded Cipher List: + excludeCiphers
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:logCommitterStatisticsAtDebug():[DEBUG] Committer Statistics
org.apache.hadoop.yarn.server.webapp.ContainerBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[INFO] Invalid container ID: ${containerid}
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$NMCallbackHandler:onContainerStopped(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] Succeeded to stop Container {}
org.apache.hadoop.net.TableMapping$RawTableMapping:load():[WARN] {filename} cannot be read.
org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int):[INFO] Not joining election since service has not yet been reported as healthy.
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:join():[DEBUG] Exception , ie
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration):[INFO] Time taken to scan block pool {bpid} on {v}: {timeTaken}ms
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stopAggregators():[INFO] Waiting for aggregation to complete for {appId}
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads():[ERROR] Could not stop Delegation Token Cache
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:storeContainerPaused(org.apache.hadoop.yarn.api.records.ContainerId):[DEBUG] storeContainerPaused: containerId={}
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getApplicationReport(org.apache.hadoop.yarn.api.records.ApplicationId):[WARN] Failed to fetch application report from ATS v2, ex
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:addExports():[INFO] FS:{fsScheme} adding export Path:{exportPath} with URI: {exportURI}
org.apache.hadoop.fs.azurebfs.services.AbfsClient:close():[DEBUG] Cleanup with logger
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:verifyChunks(java.nio.ByteBuffer,java.nio.ByteBuffer):[INFO] report corrupt + block + from datanode + srcDataNode + to namenode
org.apache.hadoop.tools.OptionsParser:parse(java.lang.String[]):[WARN] -s is a deprecated option. Ignoring.
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:updateDockerRunCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container):[DEBUG] Try to update docker run command for
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:genericForward(java.lang.String,javax.servlet.http.HttpServletRequest,java.lang.Class,org.apache.hadoop.yarn.server.router.webapp.HTTPMethods,java.lang.String,java.lang.Object,java.util.Map,org.apache.hadoop.conf.Configuration):[ERROR] Unable to obtain user name, user not authenticated
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer):[DEBUG] Try to allocate from a non-existed reserved container
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest):[WARN] Failed to finish unmanaged application master: RM address: {} ApplicationId: {}, e
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:commonReserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource):[DEBUG] Application attempt + getApplicationAttemptId() + reserved container + rmContainer + on node + node + . This attempt currently has + reservedContainers.size() + reserved containers at priority + schedulerKey.getPriority() + ; currentReservation + reservedResource
org.apache.hadoop.fs.s3a.S3AFileSystem:doBucketProbing():[WARN] Unknown bucket probe option: falling back to check #2
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logEnableErasureCodingPolicy(java.lang.String,boolean):[DEBUG] Setting erasure coding policy
org.apache.hadoop.mapred.ClientServiceDelegate:instantiateAMProxy(java.net.InetSocketAddress):[TRACE] Connecting to ApplicationMaster at: + serviceAddr
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:recoverActiveContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container,org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService$RecoveredContainerState):[ERROR] UnKnown execution type received
org.apache.hadoop.hdfs.server.namenode.JournalSet:disableAndReportErrorOnJournals(java.util.List):[ERROR] Disabling journal [JournalAndStream object]
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode():[DEBUG] from system property: [value of System.getProperty(MS_INIT_MODE_KEY)]
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[DEBUG] Sorry, can't do anything without a JobID.
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[]):[ERROR] Missing options: must have value for uri and api
org.apache.hadoop.yarn.sls.SLSRunner:printSimulationInfo():[INFO] number of queues = {} average number of apps = {}
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean):[ERROR] Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of {} byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNodeHealthyForDecommissionOrMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor):[INFO] Node {} is dead and there are no low redundancy blocks or blocks pending reconstruction. Safe to decommission or put in maintenance.
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block):[ERROR] Error getting logs for [logEntity] [Exception]
org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String):[INFO] found resource + name + at + url
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerDoneTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[DEBUG] Container metrics finished
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:initRollingLevelDB(java.lang.Long,org.apache.hadoop.fs.Path):[WARN] Failed to open rolling leveldb instance :{new File(rollingInstanceDBPath.toUri().getPath())}
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:shutdown():[INFO] Shutting down all async disk service threads
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockReplicasAsCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,long,long,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[]):[DEBUG] BLOCK* markBlockReplicasAsCorrupt: mark block replica {b} on {storage.getDatanodeDescriptor()} as corrupt because the dn is not in the new committed storage list.
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:checkAppNumCompletedLimit():[INFO] Application should be expired, max number of completed apps kept in memory met: maxCompletedAppsInMemory = {this.maxCompletedAppsInMemory}, removing app {removeId} from memory.
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:setupEventWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId,org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent):[ERROR] Log Directory is null, returning
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId):[INFO] postComplete for container: + containerId.toString()
org.apache.hadoop.hdfs.DFSUtilClient:isLocalAddress(java.net.InetSocketAddress):[TRACE] Address {targetAddr} is {cached ? "" : "not"} local
org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse):[DEBUG] Completed cross origin filter checks. Populating HttpServletResponse
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.AbstractPreemptableResourceCalculator:resetCapacity(org.apache.hadoop.yarn.api.records.Resource,java.util.Collection,boolean):[DEBUG] Resetting capacity with ignoreGuar set to true
org.apache.hadoop.hdfs.qjournal.server.JournaledEditsCache:updateLayoutVersion(int,long):[INFO] Updating edits cache to use layout version {newLayoutVersion} starting from txn ID {newStartTxn}
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:serviceInit(org.apache.hadoop.conf.Configuration):[INFO] UserGroupInformation set with configuration
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:initPropertyActions():[INFO] Action set for property: MAX_CAPACITY_PERCENTAGE
org.apache.hadoop.mapred.IFileInputStream:getFileDescriptorIfAvail(java.io.InputStream):[INFO] Unable to determine FileDescriptor
org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy:chooseVolume(java.util.List,long,java.lang.String):[WARN] The volume[{}] with the available space (={} B) is less than the block size (={} B).
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode):[DEBUG] Node update event from: {}
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:emitCells(java.util.List,java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,long):[TRACE] In emitCells " + this.action + " currentColumnCells size= " + currentColumnCells.size() + " currentAggOp" + currentAggOp
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceStop():[DEBUG] Cleaning up with logger
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:unprotectedRemoveBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block):[DEBUG] DIR* FSDirectory.removeBlock: with block is removed from the file system
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(byte[],int,int):[DEBUG] read requested b.length = {}, offset = {}, len = {}
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:handle(org.apache.hadoop.yarn.event.Event):[DEBUG] {} reported decommissioning
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:run():[WARN] Starting late by {-1 * delay} ms
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMDTSecretManagerTokens(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState):[DEBUG] Loaded RM delegation token from {}: tokenId={}, renewDate={}
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:addAttempt(org.apache.hadoop.mapreduce.v2.api.records.Avataar):[DEBUG] Created attempt attempt.getID
org.apache.hadoop.yarn.applications.distributedshell.Client:main(java.lang.String[]):[INFO] Application completed successfully
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:execute(org.apache.hadoop.fs.azurebfs.utils.TracingContext):[ERROR] UncheckedIOException thrown
org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec:parse(java.lang.String):[INFO] Parsing Placement Specs: [{}]
org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject):[DEBUG] Reading credentials from location {}
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext):[INFO] Deleting path : {}
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:addNewPendingUncached(int,org.apache.hadoop.hdfs.server.namenode.CachedBlock,java.util.List,java.util.List):[WARN] Logic error: we're trying to uncache more replicas than actually exist for cachedBlock
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition):[INFO] placing the following ReservationRequest: contract
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:generateApplicationReport(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity,org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore$ApplicationReportField):[WARN] "Failed to authorize when generating application report for " + app.appReport.getApplicationId() + ". Use a placeholder for its latest attempt id. "
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:convertProtoToDeletionTaskRecoveryInfo(org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[DEBUG] Successfully retrieved successor IDs
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:reAttachUAMAndMergeRegisterResponse(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse,org.apache.hadoop.yarn.api.records.ApplicationId):[INFO] UAM {} reattached for {}
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult):[INFO] Override State Store record {}: {}
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context):[DEBUG] Error in resource usage emulation! Message: [Exception message here]
org.apache.hadoop.tools.dynamometer.Client:launchAndMonitorWorkloadDriver(java.util.Properties):[WARN] Workload job failed.
org.apache.hadoop.fs.s3a.S3AUtils:getS3EncryptionKey(java.lang.String,org.apache.hadoop.conf.Configuration,boolean):[WARN] Cannot retrieve S3_ENCRYPTION_KEY for bucket %s
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey):[INFO] storing master key with keyID
org.apache.hadoop.fs.s3a.S3AFileSystem:initiateRename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path):[DEBUG] rename: destination path {} not found
org.apache.hadoop.hdfs.DFSStripedOutputStream:allocateNewBlock():[WARN] Cannot allocate parity block(index={i}, policy={ecPolicy.getName()}). Exclude nodes={excludedNodes}. There may not be enough datanodes or racks...
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:renderOtherPluginScheduler(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager):[WARN] Render default scheduler page as scheduler page configured doesn't exist
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query):[ERROR] Cannot remove records {clazz} query {query}, {e}
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean):[DEBUG] *DIR* NameNode.mkdirs: [src]
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded:clearQueuesWithNotification():[WARN] Failed to remove SPS xattr for track id [trackId]
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:generateApplicationReport(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity,org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore$ApplicationReportField):[INFO] "No application attempt found for " + app.appReport.getApplicationId() + ". Use a placeholder for its latest attempt id. "
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processMisReplicatedBlocks():[INFO] Caught InterruptedException while scheduling replication work for mis-replicated blocks
org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object):[INFO] IOStatistics: {}
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppNewlySavingTransition:transition(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent):[INFO] Application {app.applicationId} is registered for timeout monitor, type={ApplicationTimeoutType.LIFETIME} value={applicationLifetime} seconds
org.apache.hadoop.applications.mawo.server.common.MawoConfiguration:readConfigFile():[ERROR] No configuration file + CONFIG_FILE + found in classpath.
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:onContainersUpdated(java.util.List):[INFO] onContainersUpdated: ...
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:parseEntry(java.lang.String,java.lang.String,java.lang.String):[WARN] Failed to resolve address `%s` in `%s`. Ignoring in the %s list.
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long):[TRACE] BR lease 0x{} is valid for DN {}.
org.apache.hadoop.util.functional.TaskPool:waitFor(java.util.Collection,int):[DEBUG] Finished count -> {}/{}
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[]):[DEBUG] Call: methodName callTime
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:killApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String):[INFO] Killed application {applicationId}
org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService:bind(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord,int):[DEBUG] Bound at {} : ServiceRecord = {}, path, record
org.apache.hadoop.mapred.BackupStore$FileCache:createInDiskSegment():[DEBUG] Disk Segment added to List. Size is [segmentList.size()]
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.Resource):[WARN] createFailureLog(user, operation, perm, target, description, appId, null, null, null, callerContext, queueName, partition)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:recover():[DEBUG] Exception while removing old mirror
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$UpdateTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent):[WARN] Could not store container [ + container.containerId + ] update.., e
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String):[INFO] Processed URL + url + but entity not found + (Took + additionalTime + ms.)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:topologyAwareSchedule(java.util.Set,int,java.util.Map,java.util.Set,java.util.Map):[ERROR] Unknown error happened in topology scheduler
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:init(org.apache.hadoop.yarn.server.nodemanager.Context):[WARN] Exit code from container executor initialization is : {}
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:cleanupLogDirs(org.apache.hadoop.fs.FileContext,org.apache.hadoop.yarn.server.nodemanager.DeletionService):[WARN] failed to cleanup app log dir ...
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:logDecommissioningNodesStatus():[DEBUG] Decommissioning node: ...
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:setMasterKey(org.apache.hadoop.yarn.server.api.records.MasterKey):[INFO] Rolling master-key for container-tokens, got key with id + masterKeyRecord.getKeyId()
org.apache.hadoop.hdfs.server.namenode.FSEditLog:logAllocateBlockId(long):[DEBUG] Recording a newly allocated block ID in the edit log
