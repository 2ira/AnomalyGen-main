{
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "['ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT', 'ENTRY -> IF_FALSE: response != null -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: !errorState.hasDatanodeError() && !shouldHandleExternalError()</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_TRUE: response != null -> LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit. -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_TRUE: response != null -> LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit.</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_TRUE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet. -> CALL: lastException.set -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_TRUE: span != null -> CALL: endBlock -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> CALL: closeStream -> CALL: setPipeline -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_FALSE: span != null -> CALL: endBlock -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> CALL: closeStream -> CALL: setPipeline -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block</log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_CLOSE -> CALL: initDataStreaming -> ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)</log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_FALSE: !streamerClosed && dfsClient.clientRunning -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C8\" reason=\"Conflicting conditions between parent and child nodes\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "['ENTRY -> CALL: isNodeMarked -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: span != null -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_FALSE: span != null -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C6\" reason=\"Conflicting conditions: IF_TRUE: response != null vs IF_FALSE: response != null\"/>\n    <path id=\"P1-C7\" reason=\"Conflicting conditions: 'span != null' vs 'span == null'\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "['ENTRY -> CALL: hasExternalError -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: nodes == null || nodes.length == 0 -> LOG: LOG.WARN: msg -> CALL: lastException.set -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_TRUE, LOG.WARN: msg, CALL, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null || nodes.length == 0 -> CALL: setupPipelineInternal -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE, CALL, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n    <path id=\"P2-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_TRUE: !handleRestartingDatanode() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, IF_TRUE, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_FALSE: !handleRestartingDatanode() -> IF_TRUE: !handleBadDatanode() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, IF_FALSE, IF_TRUE, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_EXIT -> IF_TRUE: success -> CALL: updatePipeline -> EXIT -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, WHILE_EXIT, IF_TRUE, CALL, EXIT, LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n    <path id=\"C1\" reason=\"No valid logs in child node org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError() to merge with parent paths.\"/>\n    <path id=\"P4-C1\" reason=\"Conflicting conditions: success vs !success\"/>\n    <path id=\"P2-C2\" reason=\"Conflicting conditions: nodes.length == 0 vs !nodes.length == 0\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "['ENTRY -> IF_TRUE: hasInternalError() -> EXIT', 'ENTRY -> IF_FALSE: hasInternalError() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "['ENTRY -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT', 'ENTRY -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: nodes.length == 0 -> LOG: LOG.INFO: nodes are empty for write pipeline of + block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.INFO: nodes are empty for write pipeline of + block</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "['ENTRY -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}, LOG.DEBUG: Thread interrupted, e]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}, LOG.DEBUG: Thread interrupted, e]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C1\" reason=\"Conflict in conditional evaluation: 'response != null'\"/>\n    <path id=\"P1-C2\" reason=\"Conflict in conditional evaluation: 'response != null'\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "['ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_TRUE: error == ErrorType.NONE -> THROW: new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\") -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_FALSE: restartingNodeIndex >= 0 -> EXIT']",
  "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "['ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT', 'ENTRY -> CALL: setName -> IF_FALSE: LOG.isDebugEnabled() -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT']",
  "org.apache.hadoop.hdfs.DFSClient:getTracer()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "['ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_TRUE: j < traceParents.length -> CALL: copyOf -> RETURN -> EXIT', 'ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_FALSE: j < traceParents.length -> RETURN -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "['ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_TRUE: nodes.length == 0 -> LOG: LOG.INFO: nodes are empty for write pipeline of + block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_TRUE: hasInternalError() -> LOG.INFO: nodes are empty for write pipeline of + block -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message -> EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
  "org.apache.hadoop.tracing.TraceScope:span()": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "['ENTRY -> SYNC: dataQueue -> WHILE: !shouldStop() && !ackQueue.isEmpty() -> WHILE_COND: !shouldStop() && !ackQueue.isEmpty() -> EXIT']",
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "['ENTRY -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "['ENTRY -> IF_TRUE: !isNodeMarked() -> EXIT', 'ENTRY -> IF_FALSE: !isNodeMarked() -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "['ENTRY -> CALL: isRestartingNode -> CALL: doWaitForRestart -> RETURN -> EXIT']",
  "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C5\" reason=\"Conflicting conditions: IF_TRUE: response != null vs IF_FALSE: response != null\"/>\n  </pruned_paths>\n</merge_result>\n```"
}