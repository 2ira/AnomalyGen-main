{
    "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": {
        "source_code": "/**\n * This function sleeps for a certain amount of time when the writing\n * pipeline is congested. The function calculates the time based on a\n * decorrelated filter.\n *\n * @see\n * <a href=\"http://www.awsarchitectureblog.com/2015/03/backoff.html\">\n *   http://www.awsarchitectureblog.com/2015/03/backoff.html</a>\n */\nprivate void backOffIfNecessary() throws InterruptedException {\n    int t = 0;\n    synchronized (congestedNodes) {\n        if (!congestedNodes.isEmpty()) {\n            StringBuilder sb = new StringBuilder(\"DataNode\");\n            for (DatanodeInfo i : congestedNodes) {\n                sb.append(' ').append(i);\n            }\n            int range = Math.abs(lastCongestionBackoffTime * 3 - CONGESTION_BACKOFF_MEAN_TIME_IN_MS);\n            int base = Math.min(lastCongestionBackoffTime * 3, CONGESTION_BACKOFF_MEAN_TIME_IN_MS);\n            t = Math.min(CONGESTION_BACK_OFF_MAX_TIME_IN_MS, (int) (base + Math.random() * range));\n            lastCongestionBackoffTime = t;\n            sb.append(\" are congested. Backing off for \").append(t).append(\" ms\");\n            LOG.info(sb.toString());\n            congestedNodes.clear();\n        }\n    }\n    if (t != 0) {\n        Thread.sleep(t);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": {
        "source_code": "synchronized void setBadNodeIndex(int index) {\n    this.badNodeIndex = index;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": {
        "source_code": "/**\n * If this stream has encountered any errors, shutdown threads\n * and mark the stream as closed.\n *\n * @return true if it should sleep for a while after returning.\n */\nprivate boolean processDatanodeOrExternalError() throws IOException {\n    if (!errorState.hasDatanodeError() && !shouldHandleExternalError()) {\n        return false;\n    }\n    LOG.debug(\"start process datanode/external error, {}\", this);\n    if (response != null) {\n        LOG.info(\"Error Recovery for \" + block + \" waiting for responder to exit. \");\n        return true;\n    }\n    closeStream();\n    // move packets from ack queue to front of the data queue\n    synchronized (dataQueue) {\n        dataQueue.addAll(0, ackQueue);\n        ackQueue.clear();\n        packetSendTime.clear();\n    }\n    // If we had to recover the pipeline five times in a row for the\n    // same packet, this client likely has corrupt data or corrupting\n    // during transmission.\n    if (!errorState.isRestartingNode() && ++pipelineRecoveryCount > 5) {\n        LOG.warn(\"Error recovering pipeline for writing \" + block + \". Already retried 5 times for the same packet.\");\n        lastException.set(new IOException(\"Failing write. Tried pipeline \" + \"recovery 5 times without success.\"));\n        streamerClosed = true;\n        return false;\n    }\n    setupPipelineForAppendOrRecovery();\n    if (!streamerClosed && dfsClient.clientRunning) {\n        if (stage == BlockConstructionStage.PIPELINE_CLOSE) {\n            // If we had an error while closing the pipeline, we go through a fast-path\n            // where the BlockReceiver does not run. Instead, the DataNode just finalizes\n            // the block immediately during the 'connect ack' process. So, we want to pull\n            // the end-of-block packet from the dataQueue, since we don't actually have\n            // a true pipeline to send it over.\n            //\n            // We also need to set lastAckedSeqno to the end-of-block Packet's seqno, so that\n            // a client waiting on close() will be aware that the flush finished.\n            synchronized (dataQueue) {\n                // remove the end of block packet\n                DFSPacket endOfBlockPacket = dataQueue.remove();\n                // Close any trace span associated with this Packet\n                Span span = endOfBlockPacket.getSpan();\n                if (span != null) {\n                    span.finish();\n                    endOfBlockPacket.setSpan(null);\n                }\n                assert endOfBlockPacket.isLastPacketInBlock();\n                assert lastAckedSeqno == endOfBlockPacket.getSeqno() - 1;\n                lastAckedSeqno = endOfBlockPacket.getSeqno();\n                pipelineRecoveryCount = 0;\n                dataQueue.notifyAll();\n            }\n            endBlock();\n        } else {\n            initDataStreaming();\n        }\n    }\n    return false;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DFSClient:getTracer()": {
        "source_code": "Tracer getTracer() {\n    return tracer;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": {
        "source_code": "/**\n * Open a DataStreamer to a DataNode so that it can be written to.\n * This happens when a file is created and each time a new block is allocated.\n * Must get block ID and the IDs of the destinations from the namenode.\n * Returns the list of target datanodes.\n */\nprotected LocatedBlock nextBlockOutputStream() throws IOException {\n    LocatedBlock lb;\n    DatanodeInfo[] nodes;\n    StorageType[] nextStorageTypes;\n    String[] nextStorageIDs;\n    int count = dfsClient.getConf().getNumBlockWriteRetry();\n    boolean success;\n    final ExtendedBlock oldBlock = block.getCurrentBlock();\n    do {\n        errorState.resetInternalError();\n        lastException.clear();\n        DatanodeInfo[] excluded = getExcludedNodes();\n        lb = locateFollowingBlock(excluded.length > 0 ? excluded : null, oldBlock);\n        block.setCurrentBlock(lb.getBlock());\n        block.setNumBytes(0);\n        bytesSent = 0;\n        accessToken = lb.getBlockToken();\n        nodes = lb.getLocations();\n        nextStorageTypes = lb.getStorageTypes();\n        nextStorageIDs = lb.getStorageIDs();\n        // Connect to first DataNode in the list.\n        success = createBlockOutputStream(nodes, nextStorageTypes, nextStorageIDs, 0L, false);\n        if (!success) {\n            LOG.warn(\"Abandoning \" + block);\n            dfsClient.namenode.abandonBlock(block.getCurrentBlock(), stat.getFileId(), src, dfsClient.clientName);\n            block.setCurrentBlock(null);\n            final DatanodeInfo badNode = nodes[errorState.getBadNodeIndex()];\n            LOG.warn(\"Excluding datanode \" + badNode);\n            excludedNodes.put(badNode, badNode);\n        }\n    } while (!success && --count >= 0);\n    if (!success) {\n        throw new IOException(\"Unable to create new block.\");\n    }\n    return lb;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": {
        "source_code": "protected void setupPipelineInternal(DatanodeInfo[] datanodes, StorageType[] nodeStorageTypes, String[] nodeStorageIDs) throws IOException {\n    boolean success = false;\n    long newGS = 0L;\n    while (!success && !streamerClosed && dfsClient.clientRunning) {\n        if (!handleRestartingDatanode()) {\n            return;\n        }\n        final boolean isRecovery = errorState.hasInternalError();\n        if (!handleBadDatanode()) {\n            return;\n        }\n        handleDatanodeReplacement();\n        // get a new generation stamp and an access token\n        final LocatedBlock lb = updateBlockForPipeline();\n        newGS = lb.getBlock().getGenerationStamp();\n        accessToken = lb.getBlockToken();\n        // set up the pipeline again with the remaining nodes\n        success = createBlockOutputStream(nodes, storageTypes, storageIDs, newGS, isRecovery);\n        failPacket4Testing();\n        errorState.checkRestartingNodeDeadline(nodes);\n    }\n    // while\n    if (success) {\n        updatePipeline(newGS);\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": {
        "source_code": "synchronized void checkRestartingNodeDeadline(DatanodeInfo[] nodes) {\n    if (restartingNodeIndex >= 0) {\n        if (error == ErrorType.NONE) {\n            throw new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\");\n        }\n        // check badNodeIndex\n        if (badNodeIndex == restartingNodeIndex) {\n            // ignore, if came from the restarting node\n            badNodeIndex = -1;\n        }\n        // not within the deadline\n        if (Time.monotonicNow() >= restartingNodeDeadline) {\n            // expired. declare the restarting node dead\n            restartingNodeDeadline = 0;\n            final int i = restartingNodeIndex;\n            restartingNodeIndex = -1;\n            LOG.warn(\"Datanode \" + i + \" did not restart within \" + datanodeRestartTimeout + \"ms: \" + nodes[i]);\n            // Mark the restarting node as failed. If there is any other failed\n            // node during the last pipeline construction attempt, it will not be\n            // overwritten/dropped. In this case, the restarting node will get\n            // excluded in the following attempt, if it still does not come up.\n            if (badNodeIndex == -1) {\n                badNodeIndex = i;\n            }\n        }\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.tracing.TraceScope:close()": {
        "source_code": "public void close() {\n    if (span != null) {\n        span.close();\n    }\n}",
        "file_path": "hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TraceScope.java"
    },
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": {
        "source_code": "public TraceScope newScope(String description, SpanContext spanCtx, boolean finishSpanOnClose) {\n    return nullTraceScope;\n}",
        "file_path": "hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/Tracer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:endBlock()": {
        "source_code": "protected void endBlock() {\n    LOG.debug(\"Closing old block {}\", block);\n    this.setName(\"DataStreamer for file \" + src);\n    closeResponder();\n    closeStream();\n    setPipeline(null, null, null);\n    stage = BlockConstructionStage.PIPELINE_SETUP_CREATE;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": {
        "source_code": "synchronized boolean hasInternalError() {\n    return error == ErrorType.INTERNAL;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": {
        "source_code": "private void closeInternal() {\n    // close and join\n    closeResponder();\n    closeStream();\n    streamerClosed = true;\n    release();\n    synchronized (dataQueue) {\n        dataQueue.notifyAll();\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": {
        "source_code": "synchronized boolean isNodeMarked() {\n    return badNodeIndex >= 0 || (isRestartingNode() && doWaitForRestart());\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": {
        "source_code": "/**\n * Open a DataStreamer to a DataNode pipeline so that\n * it can be written to.\n * This happens when a file is appended or data streaming fails\n * It keeps on trying until a pipeline is setup\n */\nprivate void setupPipelineForAppendOrRecovery() throws IOException {\n    // Check number of datanodes. Note that if there is no healthy datanode,\n    // this must be internal error because we mark external error in striped\n    // outputstream only when all the streamers are in the DATA_STREAMING stage\n    if (nodes == null || nodes.length == 0) {\n        String msg = \"Could not get block locations. \" + \"Source file \\\"\" + src + \"\\\" - Aborting...\" + this;\n        LOG.warn(msg);\n        lastException.set(new IOException(msg));\n        streamerClosed = true;\n        return;\n    }\n    setupPipelineInternal(nodes, storageTypes, storageIDs);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": {
        "source_code": "synchronized void setInternalError() {\n    this.error = ErrorType.INTERNAL;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": {
        "source_code": "/**\n * Initialize for data streaming\n */\nprivate void initDataStreaming() {\n    this.setName(\"DataStreamer for file \" + src + \" block \" + block);\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"nodes {} storageTypes {} storageIDs {}\", Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs));\n    }\n    response = new ResponseProcessor(nodes);\n    response.start();\n    stage = BlockConstructionStage.DATA_STREAMING;\n    lastPacket = Time.monotonicNow();\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.tracing.TraceScope:span()": {
        "source_code": "public Span span() {\n    return span;\n}",
        "file_path": "hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/TraceScope.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": {
        "source_code": "synchronized void resetInternalError() {\n    if (hasInternalError()) {\n        error = ErrorType.NONE;\n    }\n    badNodeIndex = -1;\n    restartingNodeIndex = -1;\n    restartingNodeDeadline = 0;\n    waitForRestart = true;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": {
        "source_code": "private boolean shouldHandleExternalError() {\n    return errorState.hasExternalError() && blockStream != null;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": {
        "source_code": "private void waitForAllAcks() throws IOException {\n    // wait until all data packets have been successfully acked\n    synchronized (dataQueue) {\n        while (!shouldStop() && !ackQueue.isEmpty()) {\n            try {\n                // wait for acks to arrive from datanodes\n                dataQueue.wait(sendHeartbeat());\n            } catch (InterruptedException e) {\n                LOG.debug(\"Thread interrupted \", e);\n            }\n        }\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": {
        "source_code": "// connects to the first datanode in the pipeline\n// Returns true if success, otherwise return failure.\n//\nboolean createBlockOutputStream(DatanodeInfo[] nodes, StorageType[] nodeStorageTypes, String[] nodeStorageIDs, long newGS, boolean recoveryFlag) {\n    if (nodes.length == 0) {\n        LOG.info(\"nodes are empty for write pipeline of \" + block);\n        return false;\n    }\n    String firstBadLink = \"\";\n    boolean checkRestart = false;\n    if (LOG.isDebugEnabled()) {\n        LOG.debug(\"pipeline = \" + Arrays.toString(nodes) + \", \" + this);\n    }\n    // persist blocks on namenode on next flush\n    persistBlocks.set(true);\n    int refetchEncryptionKey = 1;\n    while (true) {\n        boolean result = false;\n        DataOutputStream out = null;\n        try {\n            assert null == s : \"Previous socket unclosed\";\n            assert null == blockReplyStream : \"Previous blockReplyStream unclosed\";\n            s = createSocketForPipeline(nodes[0], nodes.length, dfsClient);\n            long writeTimeout = dfsClient.getDatanodeWriteTimeout(nodes.length);\n            long readTimeout = dfsClient.getDatanodeReadTimeout(nodes.length);\n            OutputStream unbufOut = NetUtils.getOutputStream(s, writeTimeout);\n            InputStream unbufIn = NetUtils.getInputStream(s, readTimeout);\n            IOStreamPair saslStreams = dfsClient.saslClient.socketSend(s, unbufOut, unbufIn, dfsClient, accessToken, nodes[0]);\n            unbufOut = saslStreams.out;\n            unbufIn = saslStreams.in;\n            out = new DataOutputStream(new BufferedOutputStream(unbufOut, DFSUtilClient.getSmallBufferSize(dfsClient.getConfiguration())));\n            blockReplyStream = new DataInputStream(unbufIn);\n            //\n            // Xmit header info to datanode\n            //\n            BlockConstructionStage bcs = recoveryFlag ? stage.getRecoveryStage() : stage;\n            // We cannot change the block length in 'block' as it counts the number\n            // of bytes ack'ed.\n            ExtendedBlock blockCopy = block.getCurrentBlock();\n            blockCopy.setNumBytes(stat.getBlockSize());\n            boolean[] targetPinnings = getPinnings(nodes);\n            // send the request\n            new Sender(out).writeBlock(blockCopy, nodeStorageTypes[0], accessToken, dfsClient.clientName, nodes, nodeStorageTypes, null, bcs, nodes.length, block.getNumBytes(), bytesSent, newGS, checksum4WriteBlock, cachingStrategy.get(), isLazyPersistFile, (targetPinnings != null && targetPinnings[0]), targetPinnings, nodeStorageIDs[0], nodeStorageIDs);\n            // receive ack for connect\n            BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(PBHelperClient.vintPrefixed(blockReplyStream));\n            Status pipelineStatus = resp.getStatus();\n            firstBadLink = resp.getFirstBadLink();\n            // Got an restart OOB ack.\n            // If a node is already restarting, this status is not likely from\n            // the same node. If it is from a different node, it is not\n            // from the local datanode. Thus it is safe to treat this as a\n            // regular node error.\n            if (PipelineAck.isRestartOOBStatus(pipelineStatus) && !errorState.isRestartingNode()) {\n                checkRestart = true;\n                throw new IOException(\"A datanode is restarting.\");\n            }\n            String logInfo = \"ack with firstBadLink as \" + firstBadLink;\n            DataTransferProtoUtil.checkBlockOpStatus(resp, logInfo);\n            assert null == blockStream : \"Previous blockStream unclosed\";\n            blockStream = out;\n            // success\n            result = true;\n            errorState.resetInternalError();\n            lastException.clear();\n            // remove all restarting nodes from failed nodes list\n            failed.removeAll(restartingNodes);\n            restartingNodes.clear();\n        } catch (IOException ie) {\n            if (!errorState.isRestartingNode()) {\n                LOG.warn(\"Exception in createBlockOutputStream \" + this, ie);\n            }\n            if (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\n                LOG.info(\"Will fetch a new encryption key and retry, \" + \"encryption key was invalid when connecting to \" + nodes[0] + \" : \" + ie);\n                // The encryption key used is invalid.\n                refetchEncryptionKey--;\n                dfsClient.clearDataEncryptionKey();\n                // Don't close the socket/exclude this node just yet. Try again with\n                // a new encryption key.\n                continue;\n            }\n            // find the datanode that matches\n            if (firstBadLink.length() != 0) {\n                for (int i = 0; i < nodes.length; i++) {\n                    // NB: Unconditionally using the xfer addr w/o hostname\n                    if (firstBadLink.equals(nodes[i].getXferAddr())) {\n                        errorState.setBadNodeIndex(i);\n                        break;\n                    }\n                }\n            } else {\n                assert !checkRestart;\n                errorState.setBadNodeIndex(0);\n            }\n            final int i = errorState.getBadNodeIndex();\n            // Check whether there is a restart worth waiting for.\n            if (checkRestart) {\n                errorState.initRestartingNode(i, \"Datanode \" + i + \" is restarting: \" + nodes[i], shouldWaitForRestart(i));\n            }\n            errorState.setInternalError();\n            lastException.set(ie);\n            // error\n            result = false;\n        } finally {\n            if (!result) {\n                IOUtils.closeSocket(s);\n                s = null;\n                IOUtils.closeStream(out);\n                IOUtils.closeStream(blockReplyStream);\n                blockReplyStream = null;\n            }\n        }\n        return result;\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": {
        "source_code": "private void closeResponder() {\n    if (response != null) {\n        try {\n            response.close();\n            response.join();\n        } catch (InterruptedException e) {\n            LOG.debug(\"Thread interrupted\", e);\n            Thread.currentThread().interrupt();\n        } finally {\n            response = null;\n        }\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": {
        "source_code": "synchronized boolean hasError() {\n    return error != ErrorType.NONE;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": {
        "source_code": "synchronized boolean isRestartingNode() {\n    return restartingNodeIndex >= 0;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": {
        "source_code": "synchronized boolean hasDatanodeError() {\n    return error == ErrorType.INTERNAL && isNodeMarked();\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": {
        "source_code": "/**\n * Get the trace parent spans for this packet.\n * <p>\n * Will always be non-null.\n * <p>\n * Protected by the DFSOutputStream dataQueue lock.\n */\npublic SpanContext[] getTraceParents() {\n    // Remove duplicates from the array.\n    int len = traceParentsUsed;\n    Arrays.sort(traceParents, 0, len);\n    int i = 0, j = 0;\n    SpanContext prevVal = null;\n    while (true) {\n        if (i == len) {\n            break;\n        }\n        SpanContext val = traceParents[i];\n        if (!val.equals(prevVal)) {\n            traceParents[j] = val;\n            j++;\n            prevVal = val;\n        }\n        i++;\n    }\n    if (j < traceParents.length) {\n        traceParents = Arrays.copyOf(traceParents, j);\n        traceParentsUsed = traceParents.length;\n    }\n    return traceParents;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSPacket.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": {
        "source_code": "synchronized int getBadNodeIndex() {\n    return badNodeIndex;\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": {
        "source_code": "public TraceScope newScope(String description, SpanContext spanCtx, boolean finishSpanOnClose) {\n    return nullTraceScope;\n}",
        "file_path": "hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/tracing/Tracer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer:run()": {
        "source_code": "/*\n   * streamer thread is the only thread that opens streams to datanode,\n   * and closes them. Any error recovery is also done by this thread.\n   */\n@Override\npublic void run() {\n    TraceScope scope = null;\n    while (!streamerClosed && dfsClient.clientRunning) {\n        // if the Responder encountered an error, shutdown Responder\n        if (errorState.hasError()) {\n            closeResponder();\n        }\n        DFSPacket one;\n        try {\n            // process datanode IO errors if any\n            boolean doSleep = processDatanodeOrExternalError();\n            synchronized (dataQueue) {\n                // wait for a packet to be sent.\n                while ((!shouldStop() && dataQueue.isEmpty()) || doSleep) {\n                    long timeout = 1000;\n                    if (stage == BlockConstructionStage.DATA_STREAMING) {\n                        timeout = sendHeartbeat();\n                    }\n                    try {\n                        dataQueue.wait(timeout);\n                    } catch (InterruptedException e) {\n                        LOG.debug(\"Thread interrupted\", e);\n                    }\n                    doSleep = false;\n                }\n                if (shouldStop()) {\n                    continue;\n                }\n                // get packet to be sent.\n                // regular data packet\n                one = dataQueue.getFirst();\n                SpanContext[] parents = one.getTraceParents();\n                if (parents != null && parents.length > 0) {\n                    // The original code stored multiple parents in the DFSPacket, and\n                    // use them ALL here when creating a new Span. We only use the\n                    // last one FOR NOW. Moreover, we don't activate the Span for now.\n                    scope = dfsClient.getTracer().newScope(\"dataStreamer\", parents[0], false);\n                    //scope.getSpan().setParents(parents);\n                }\n            }\n            // The DataStreamer has to release the dataQueue before sleeping,\n            // otherwise it will cause the ResponseProcessor to accept the ACK delay.\n            try {\n                backOffIfNecessary();\n            } catch (InterruptedException e) {\n                LOG.debug(\"Thread interrupted\", e);\n            }\n            // get new block from namenode.\n            LOG.debug(\"stage={}, {}\", stage, this);\n            if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n                LOG.debug(\"Allocating new block: {}\", this);\n                setPipeline(nextBlockOutputStream());\n                initDataStreaming();\n            } else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {\n                LOG.debug(\"Append to block {}\", block);\n                setupPipelineForAppendOrRecovery();\n                if (streamerClosed) {\n                    continue;\n                }\n                initDataStreaming();\n            }\n            long lastByteOffsetInBlock = one.getLastByteOffsetBlock();\n            if (lastByteOffsetInBlock > stat.getBlockSize()) {\n                throw new IOException(\"BlockSize \" + stat.getBlockSize() + \" < lastByteOffsetInBlock, \" + this + \", \" + one);\n            }\n            if (one.isLastPacketInBlock()) {\n                // wait for all data packets have been successfully acked\n                waitForAllAcks();\n                if (shouldStop()) {\n                    continue;\n                }\n                stage = BlockConstructionStage.PIPELINE_CLOSE;\n            }\n            // send the packet\n            SpanContext spanContext = null;\n            synchronized (dataQueue) {\n                // move packet from dataQueue to ackQueue\n                if (!one.isHeartbeatPacket()) {\n                    if (scope != null) {\n                        one.setSpan(scope.span());\n                        spanContext = scope.span().getContext();\n                        scope.close();\n                    }\n                    scope = null;\n                    dataQueue.removeFirst();\n                    ackQueue.addLast(one);\n                    packetSendTime.put(one.getSeqno(), Time.monotonicNow());\n                    dataQueue.notifyAll();\n                }\n            }\n            LOG.debug(\"{} sending {}\", this, one);\n            // write out data to remote datanode\n            try (TraceScope ignored = dfsClient.getTracer().newScope(\"DataStreamer#writeTo\", spanContext)) {\n                sendPacket(one);\n            } catch (IOException e) {\n                // HDFS-3398 treat primary DN is down since client is unable to\n                // write to primary DN. If a failed or restarting node has already\n                // been recorded by the responder, the following call will have no\n                // effect. Pipeline recovery can handle only one node error at a\n                // time. If the primary node fails again during the recovery, it\n                // will be taken out then.\n                errorState.markFirstNodeIfNotMarked();\n                throw e;\n            }\n            // update bytesSent\n            long tmpBytesSent = one.getLastByteOffsetBlock();\n            if (bytesSent < tmpBytesSent) {\n                bytesSent = tmpBytesSent;\n            }\n            if (shouldStop()) {\n                continue;\n            }\n            // Is this block full?\n            if (one.isLastPacketInBlock()) {\n                // wait for the close packet has been acked\n                try {\n                    waitForAllAcks();\n                } catch (IOException ioe) {\n                    // No need to do a close recovery if the last packet was acked.\n                    // i.e. ackQueue is empty.  waitForAllAcks() can get an exception\n                    // (e.g. connection reset) while sending a heartbeat packet,\n                    // if the DN sends the final ack and closes the connection.\n                    synchronized (dataQueue) {\n                        if (!ackQueue.isEmpty()) {\n                            throw ioe;\n                        }\n                    }\n                }\n                if (shouldStop()) {\n                    continue;\n                }\n                endBlock();\n            }\n            if (progress != null) {\n                progress.progress();\n            }\n            // This is used by unit test to trigger race conditions.\n            if (artificialSlowdown != 0 && dfsClient.clientRunning) {\n                Thread.sleep(artificialSlowdown);\n            }\n        } catch (Throwable e) {\n            // Log warning if there was a real error.\n            if (!errorState.isRestartingNode()) {\n                // Since their messages are descriptive enough, do not always\n                // log a verbose stack-trace WARN for quota exceptions.\n                if (e instanceof QuotaExceededException) {\n                    LOG.debug(\"DataStreamer Quota Exception\", e);\n                } else {\n                    LOG.warn(\"DataStreamer Exception\", e);\n                }\n            }\n            lastException.set(e);\n            assert !(e instanceof NullPointerException);\n            errorState.setInternalError();\n            if (!errorState.isNodeMarked()) {\n                // Not a datanode issue\n                streamerClosed = true;\n            }\n        } finally {\n            if (scope != null) {\n                scope.close();\n                scope = null;\n            }\n        }\n    }\n    closeInternal();\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": {
        "source_code": "synchronized void initRestartingNode(int i, String message, boolean shouldWait) {\n    restartingNodeIndex = i;\n    if (shouldWait) {\n        restartingNodeDeadline = Time.monotonicNow() + datanodeRestartTimeout;\n        // If the data streamer has already set the primary node\n        // bad, clear it. It is likely that the write failed due to\n        // the DN shutdown. Even if it was a real failure, the pipeline\n        // recovery will take care of it.\n        badNodeIndex = -1;\n    } else {\n        this.waitForRestart = false;\n    }\n    LOG.info(message);\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    },
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": {
        "source_code": "/**\n * This method is used when no explicit error report was received, but\n * something failed. The first node is a suspect or unsure about the cause\n * so that it is marked as failed.\n */\nsynchronized void markFirstNodeIfNotMarked() {\n    // There should be no existing error and no ongoing restart.\n    if (!isNodeMarked()) {\n        badNodeIndex = 0;\n    }\n}",
        "file_path": "hadoop/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DataStreamer.java"
    }
}