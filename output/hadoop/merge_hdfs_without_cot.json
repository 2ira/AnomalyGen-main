{
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "['ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos -> CALL: position -> EXIT', 'ENTRY -> IF_FALSE: LOG.isDebugEnabled() -> CALL: position -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:adjustCrcChannelPosition(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos -> CALL: position -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: LOG.isDebugEnabled() -> CALL: position -> EXIT</exec_flow>\n      <log_sequence></log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded due to lack of valid logs.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BlockReceiver:adjustCrcFilePosition()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: flushDataOut -> IF_TRUE: checksumOut != null -> CALL: checksumOut.flush -> LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos -> CALL: adjustCrcChannelPosition -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: flushDataOut, IF_TRUE: checksumOut != null, CALL: checksumOut.flush, LOG.DEBUG: Changing meta file offset of block + b + from + oldPos + to + newPos</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> CALL: flushDataOut -> IF_FALSE: checksumOut != null -> CALL: adjustCrcChannelPosition -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: flushDataOut, IF_FALSE: checksumOut != null</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded due to lack of valid logs.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:close()": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReadBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: readBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: readBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: readBlock -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: readBlock -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C1-C2\" reason=\"Conflict in conditional evaluation: 'traceScope != null' true vs false\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opWriteBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: writeBlock -> IF_TRUE: traceScope != null -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: writeBlock -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: writeBlock -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: writeBlock -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No valid parent path to merge with\"/>\n    <path id=\"C2\" reason=\"No valid parent path to merge with\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReplaceBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: replaceBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: replaceBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: replaceBlock -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: replaceBlock -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opCopyBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: copyBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: copyBlock, IF_TRUE: traceScope != null, ENTRY, IF_TRUE: span != null, CALL: close, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: copyBlock -> IF_FALSE: traceScope != null -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: copyBlock, IF_FALSE: traceScope != null, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: copyBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: copyBlock, IF_TRUE: traceScope != null, ENTRY, IF_FALSE: span != null, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockChecksum -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockChecksum, IF_TRUE: traceScope != null, ENTRY, IF_TRUE: span != null, CALL: close, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockChecksum -> IF_TRUE: traceScope != null -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockChecksum, IF_TRUE: traceScope != null, ENTRY, IF_FALSE: span != null, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockChecksum -> IF_FALSE: traceScope != null -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockChecksum, IF_FALSE: traceScope != null, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opStripedBlockChecksum(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockGroupChecksum -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockGroupChecksum, IF_TRUE: traceScope != null, ENTRY, IF_TRUE: span != null, CALL: close, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockGroupChecksum -> IF_FALSE: traceScope != null -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockGroupChecksum, IF_FALSE: traceScope != null, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN</log_sequence>\n    </path>\n    <path>\n      <id>P1-C1-Alt</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: blockGroupChecksum -> IF_TRUE: traceScope != null -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, TRY, CALL: blockGroupChecksum, IF_TRUE: traceScope != null, ENTRY, IF_FALSE: span != null, EXIT, EXIT, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, RETURN</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opTransferBlock(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: transferBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: transferBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: transferBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: transferBlock -> IF_TRUE: traceScope != null -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: transferBlock -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: transferBlock -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No valid log sequence in child node\"/>\n    <path id=\"C2\" reason=\"No valid log sequence in child node\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: requestShortCircuitFds -> IF_TRUE: traceScope != null -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> CALL: traceScope.close -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: requestShortCircuitFds, IF_TRUE: traceScope != null, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, CALL: getSpanContext, CALL: getTraceInfo, RETURN, CALL: traceScope.close, ENTRY, IF_TRUE: span != null, CALL: close, EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: requestShortCircuitFds -> IF_TRUE: traceScope != null -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> CALL: traceScope.close -> ENTRY -> IF_FALSE: span != null -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: requestShortCircuitFds, IF_TRUE: traceScope != null, CALL: continueTraceSpan, CALL: getSpanContext, CALL: getTraceInfo, CALL: getSpanContext, CALL: getTraceInfo, RETURN, CALL: traceScope.close, ENTRY, IF_FALSE: span != null, EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: requestShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: requestShortCircuitFds, IF_FALSE: traceScope != null, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No valid parent path to merge with\"/>\n    <path id=\"C2\" reason=\"No valid parent path to merge with\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:getSpanContext()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:continueTraceSpan(org.apache.hadoop.thirdparty.protobuf.ByteString,java.lang.String)": "['ENTRY -> CALL: continueTraceSpan -> CALL: getSpanContext -> CALL: getTraceInfo -> CALL: getSpanContext -> CALL: getTraceInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opReleaseShortCircuitFds(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: releaseShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No valid logs in child node to merge with parent paths\"/>\n    <path id=\"C2\" reason=\"Child node logs ('ENTRY -> RETURN -> EXIT') do not align with parent execution flow or conditions\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:getTraceInfo()": "['ENTRY -> CALL: getDefaultInstance -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:opRequestShortCircuitShm(java.io.DataInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_TRUE: traceScope != null -> CALL: traceScope.close -> IF_TRUE: span != null -> CALL: close -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> TRY -> CALL: requestShortCircuitShm -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C1-C1\" reason=\"No direct log or conditional conflict resolution between parent and child paths.\"/>\n    <path id=\"P1-C2-C1\" reason=\"No direct log or conditional conflict resolution between parent and child paths.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:processOp(org.apache.hadoop.hdfs.protocol.datatransfer.Op)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> SWITCH: op -> CASE: [RELEASE_SHORT_CIRCUIT_FDS] -> CALL: opReleaseShortCircuitFds -> TRY -> CALL: releaseShortCircuitFds -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> SWITCH: op -> CASE: [RELEASE_SHORT_CIRCUIT_FDS] -> CALL: opReleaseShortCircuitFds -> TRY -> CALL: releaseShortCircuitFds -> IF_TRUE: traceScope != null -> ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> SWITCH: op -> CASE: [RELEASE_SHORT_CIRCUIT_FDS] -> CALL: opReleaseShortCircuitFds -> TRY -> CALL: releaseShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> SWITCH: op -> CASE: [RELEASE_SHORT_CIRCUIT_FDS] -> CALL: opReleaseShortCircuitFds -> TRY -> CALL: releaseShortCircuitFds -> IF_FALSE: traceScope != null -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No valid logs in child node to merge with parent paths\"/>\n    <path id=\"C2\" reason=\"Child node logs ('ENTRY -> RETURN -> EXIT') do not align with parent execution flow or conditions\"/>\n    <path id=\"P1-C1-C1\" reason=\"No direct log or conditional conflict resolution between parent and child paths.\"/>\n    <path id=\"P1-C2-C1\" reason=\"No direct log or conditional conflict resolution between parent and child paths.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.LocatedBlocksRefresher:addInputStream(org.apache.hadoop.hdfs.DFSInputStream)": "['ENTRY -> LOG: LOG.TRACE: Registering {} for {}, dfsInputStream, dfsInputStream.getSrc() -> CALL: registeredInputStreams.add -> EXIT']",
    "org.apache.hadoop.hdfs.DFSClient:addLocatedBlocksRefresh(org.apache.hadoop.hdfs.DFSInputStream)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: isLocatedBlocksRefresherEnabled() -> CALL: clientContext.getLocatedBlocksRefresher().addInputStream -> ENTRY -> LOG: LOG.TRACE: Registering {} for {}, dfsInputStream, dfsInputStream.getSrc() -> CALL: registeredInputStreams.add -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: isLocatedBlocksRefresherEnabled(), CALL: clientContext.getLocatedBlocksRefresher().addInputStream, LOG: LOG.TRACE: Registering {} for {}, dfsInputStream, dfsInputStream.getSrc(), CALL: registeredInputStreams.add, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: isLocatedBlocksRefresherEnabled() -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_FALSE: isLocatedBlocksRefresherEnabled(), EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"Child path log conflicts with parent path or lacks valid parent context.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:closeResponder()": "['ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT', 'ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT', 'ENTRY -> IF_FALSE: response != null -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()": "['ENTRY -> CALL: isNodeMarked -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()": "['ENTRY -> CALL: hasExternalError -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: !errorState.hasDatanodeError() && !shouldHandleExternalError()</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_TRUE: response != null -> LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit. -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_TRUE: response != null -> LOG: LOG.INFO: Error Recovery for + block + waiting for responder to exit.</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_TRUE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet. -> CALL: lastException.set -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> LOG: LOG.WARN: Error recovering pipeline for writing + block + . Already retried 5 times for the same packet.</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_TRUE: span != null -> CALL: endBlock -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> CALL: closeStream -> CALL: setPipeline -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_TRUE: stage == BlockConstructionStage.PIPELINE_CLOSE -> SYNC: dataQueue -> IF_FALSE: span != null -> CALL: endBlock -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> CALL: closeStream -> CALL: setPipeline -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block</log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_TRUE: !streamerClosed && dfsClient.clientRunning -> IF_FALSE: stage == BlockConstructionStage.PIPELINE_CLOSE -> CALL: initDataStreaming -> ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs)</log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null -> CALL: closeStream -> SYNC: dataQueue -> CALL: dataQueue.addAll -> IF_FALSE: !errorState.isRestartingNode() && ++pipelineRecoveryCount > 5 -> CALL: setupPipelineForAppendOrRecovery -> IF_FALSE: !streamerClosed && dfsClient.clientRunning -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !errorState.hasDatanodeError() && !shouldHandleExternalError() -> LOG: LOG.DEBUG: start process datanode/external error, {}, this -> IF_FALSE: response != null</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C8\" reason=\"Conflicting conditions between parent and child nodes\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DFSPacket:getTraceParents()": "['ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_TRUE: j < traceParents.length -> CALL: copyOf -> RETURN -> EXIT', 'ENTRY -> CALL: Arrays.sort -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> IF_FALSE: j < traceParents.length -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext,boolean)": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:backOffIfNecessary()": "['ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_TRUE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_TRUE: t != 0 -> CALL: Thread.sleep -> EXIT', 'ENTRY -> SYNC: congestedNodes -> IF_FALSE: !congestedNodes.isEmpty() -> IF_FALSE: t != 0 -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:resetInternalError()": "['ENTRY -> IF_TRUE: hasInternalError() -> EXIT', 'ENTRY -> IF_FALSE: hasInternalError() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setBadNodeIndex(int)": "['ENTRY -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode(int,java.lang.String,boolean)": "['ENTRY -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT', 'ENTRY -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: nodes.length == 0 -> LOG: LOG.INFO: nodes are empty for write pipeline of + block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.INFO: nodes are empty for write pipeline of + block</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n    <path>\n      <id>P2-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:getBadNodeIndex()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_TRUE: nodes.length == 0 -> LOG: LOG.INFO: nodes are empty for write pipeline of + block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_TRUE: hasInternalError() -> LOG.INFO: nodes are empty for write pipeline of + block -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message -> EXIT\n      </log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>org.apache.hadoop.hdfs.DataStreamer:nextBlockOutputStream() -> org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream() -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_FALSE: shouldWait -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>\n        ENTRY -> IF_FALSE: hasInternalError() -> LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message -> EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:checkRestartingNodeDeadline(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])": "['ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_TRUE: error == ErrorType.NONE -> THROW: new IllegalStateException(\"error=false while checking\" + \" restarting node deadline\") -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_TRUE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_TRUE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_TRUE: Time.monotonicNow() >= restartingNodeDeadline -> LOG: LOG.WARN: Datanode + i + did not restart within + datanodeRestartTimeout + ms: + nodes[i] -> IF_FALSE: badNodeIndex == -1 -> EXIT', 'ENTRY -> IF_TRUE: restartingNodeIndex >= 0 -> IF_FALSE: error == ErrorType.NONE -> IF_FALSE: badNodeIndex == restartingNodeIndex -> IF_FALSE: Time.monotonicNow() >= restartingNodeDeadline -> EXIT', 'ENTRY -> IF_FALSE: restartingNodeIndex >= 0 -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_TRUE: !handleRestartingDatanode() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, IF_TRUE, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> IF_FALSE: !handleRestartingDatanode() -> IF_TRUE: !handleBadDatanode() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, IF_FALSE, IF_TRUE, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> WHILE: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_COND: !success && !streamerClosed && dfsClient.clientRunning -> WHILE_EXIT -> IF_TRUE: success -> CALL: updatePipeline -> EXIT -> IF_FALSE: nodes.length == 0 -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this -> CALL: persistBlocks.set -> WHILE: true -> WHILE_COND: true -> CALL: org.apache.hadoop.hdfs.DataStreamer$ErrorState:initRestartingNode -> IF_TRUE: shouldWait -> CALL: monotonicNow -> LOG: LOG.INFO: message -> EXIT -> WHILE_EXIT -> EXIT</exec_flow>\n      <log_sequence>ENTRY, WHILE, WHILE_COND, WHILE_EXIT, IF_TRUE, CALL, EXIT, LOG.DEBUG: pipeline = + Arrays.toString(nodes) + , + this, LOG.INFO: message</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Log-less path discarded\"/>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n    <path id=\"C1\" reason=\"No valid logs in child node org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError() to merge with parent paths.\"/>\n    <path id=\"P4-C1\" reason=\"Conflicting conditions: success vs !success\"/>\n    <path id=\"P2-C2\" reason=\"Conflicting conditions: nodes.length == 0 vs !nodes.length == 0\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:setupPipelineForAppendOrRecovery()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: nodes == null || nodes.length == 0 -> LOG: LOG.WARN: msg -> CALL: lastException.set -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_TRUE, LOG.WARN: msg, CALL, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null || nodes.length == 0 -> CALL: setupPipelineInternal -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE, CALL, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Log-less path discarded\"/>\n    <path id=\"P2-C2\" reason=\"Log-less path discarded\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:initDataStreaming()": "['ENTRY -> CALL: setName -> IF_TRUE: LOG.isDebugEnabled() -> LOG: LOG.DEBUG: nodes {} storageTypes {} storageIDs {}, Arrays.toString(nodes), Arrays.toString(storageTypes), Arrays.toString(storageIDs) -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT', 'ENTRY -> CALL: setName -> IF_FALSE: LOG.isDebugEnabled() -> NEW: ResponseProcessor -> CALL: start -> CALL: monotonicNow -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:span()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DFSClient:getTracer()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String,org.apache.hadoop.tracing.SpanContext)": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:markFirstNodeIfNotMarked()": "['ENTRY -> IF_TRUE: !isNodeMarked() -> EXIT', 'ENTRY -> IF_FALSE: !isNodeMarked() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:waitForAllAcks()": "['ENTRY -> SYNC: dataQueue -> WHILE: !shouldStop() && !ackQueue.isEmpty() -> WHILE_COND: !shouldStop() && !ackQueue.isEmpty() -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:endBlock()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}, LOG.DEBUG: Thread interrupted, e]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}, LOG.DEBUG: Thread interrupted, e]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Closing old block {}, block -> CALL: setName -> CALL: closeResponder -> ENTRY -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: setPipeline -> EXIT</exec_flow>\n      <log_sequence>[LOG.DEBUG: Closing old block {}]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C1\" reason=\"Conflict in conditional evaluation: 'response != null'\"/>\n    <path id=\"P1-C2\" reason=\"Conflict in conditional evaluation: 'response != null'\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:setInternalError()": "['ENTRY -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()": "['ENTRY -> CALL: isRestartingNode -> CALL: doWaitForRestart -> RETURN -> EXIT']",
    "org.apache.hadoop.tracing.TraceScope:close()_1": "['ENTRY -> IF_TRUE: span != null -> CALL: close -> EXIT', 'ENTRY -> IF_FALSE: span != null -> EXIT']",
    "org.apache.hadoop.hdfs.DataStreamer:closeInternal()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C5\" reason=\"Conflicting conditions: IF_TRUE: response != null vs IF_FALSE: response != null\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.DataStreamer:run()": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: span != null -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_FALSE: span != null -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> EXCEPTION: close -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_TRUE: response != null -> TRY -> CALL: close -> CALL: join -> EXCEPTION: join -> CATCH: InterruptedException e -> LOG: LOG.DEBUG: Thread interrupted, e -> CALL: Thread.currentThread().interrupt -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT</exec_flow>\n      <log_sequence>[ENTRY -> IF_TRUE: span != null -> CALL: close -> ENTRY -> CALL: closeResponder -> IF_FALSE: response != null -> EXIT -> CALL: closeStream -> CALL: release -> SYNC: dataQueue -> CALL: dataQueue.notifyAll -> EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C6\" reason=\"Conflicting conditions: IF_TRUE: response != null vs IF_FALSE: response != null\"/>\n    <path id=\"P1-C7\" reason=\"Conflicting conditions: 'span != null' vs 'span == null'\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)": "['ENTRY -> CALL: addBlock -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getBlockUCState()": "['ENTRY -> CALL: getBlockUCState -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_TRUE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> CALL: queueReportedBlock -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_TRUE: storedBlock == null -> CALL: toInvalidate.add -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_TRUE: invalidateBlocks.contains(dn, block) -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_FALSE: invalidateBlocks.contains(dn, block) -> IF_TRUE: c != null -> IF_TRUE: shouldPostponeBlocksFromFuture -> CALL: queueReportedBlock -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_FALSE: invalidateBlocks.contains(dn, block) -> IF_TRUE: c != null -> IF_FALSE: shouldPostponeBlocksFromFuture -> CALL: toCorrupt.add -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C6</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_FALSE: invalidateBlocks.contains(dn, block) -> IF_FALSE: c != null -> IF_TRUE: isBlockUnderConstruction(storedBlock, ucState, reportedState) -> CALL: toUC.add -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C7</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_FALSE: invalidateBlocks.contains(dn, block) -> IF_FALSE: c != null -> IF_FALSE: isBlockUnderConstruction(storedBlock, ucState, reportedState) -> IF_TRUE: reportedState == ReplicaState.FINALIZED && (storedBlock.findStorageInfo(storageInfo) == -1 || corruptReplicas.isReplicaCorrupt(storedBlock, dn)) -> CALL: toAdd.add -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n    <path>\n      <id>P1-C8</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_FALSE: invalidateBlocks.contains(dn, block) -> IF_FALSE: c != null -> IF_FALSE: isBlockUnderConstruction(storedBlock, ucState, reportedState) -> IF_FALSE: reportedState == ReplicaState.FINALIZED && (storedBlock.findStorageInfo(storageInfo) == -1 || corruptReplicas.isReplicaCorrupt(storedBlock, dn)) -> RETURN -> EXIT</exec_flow>\n      <log_sequence>LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> LOG.DEBUG: In memory blockUCState = {}, ucState</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C9\" reason=\"No valid logs or conflicting conditions detected\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "['ENTRY -> FOR_INIT -> FOR_COND: idx < len -> IF_TRUE: cur == storageInfo -> RETURN -> EXIT', 'ENTRY -> FOR_INIT -> FOR_COND: idx < len -> FOR_EXIT -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:moveBlockToHead(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "['ENTRY -> CALL: moveBlockToHead -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:next()": "['ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT', 'ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:reportDiff(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: newReport == null -> FOREACH: newReport -> FOREACH_EXIT -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_TRUE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> CALL: queueReportedBlock -> RETURN -> WHILE: it.hasNext() -> WHILE_COND: it.hasNext() -> WHILE_EXIT -> CALL: storageInfo.removeBlock -> ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT -> ENTRY -> CALL: addBlock -> RETURN -> EXIT -> RETURN -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: newReport == null, FOREACH: newReport, FOREACH_EXIT, LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState, IF_TRUE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block), CALL: queueReportedBlock, RETURN, WHILE: it.hasNext(), WHILE_COND: it.hasNext(), WHILE_EXIT, CALL: storageInfo.removeBlock, ENTRY, CALL: listRemove, IF_TRUE: b.removeStorage(this), RETURN, EXIT, ENTRY, CALL: addBlock, RETURN, EXIT, RETURN, EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: newReport == null -> FOREACH: newReport -> FOREACH_EXIT -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_TRUE: storedBlock == null -> CALL: toInvalidate.add -> RETURN -> WHILE: it.hasNext() -> WHILE_COND: it.hasNext() -> WHILE_EXIT -> CALL: storageInfo.removeBlock -> ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT -> ENTRY -> CALL: addBlock -> RETURN -> EXIT -> RETURN -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: newReport == null, FOREACH: newReport, FOREACH_EXIT, LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState, IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block), IF_TRUE: storedBlock == null, CALL: toInvalidate.add, RETURN, WHILE: it.hasNext(), WHILE_COND: it.hasNext(), WHILE_EXIT, CALL: storageInfo.removeBlock, ENTRY, CALL: listRemove, IF_TRUE: b.removeStorage(this), RETURN, EXIT, ENTRY, CALL: addBlock, RETURN, EXIT, RETURN, EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: newReport == null -> FOREACH: newReport -> FOREACH_EXIT -> LOG: LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState -> IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block) -> IF_FALSE: storedBlock == null -> LOG: LOG.DEBUG: In memory blockUCState = {}, ucState -> IF_TRUE: invalidateBlocks.contains(dn, block) -> RETURN -> WHILE: it.hasNext() -> WHILE_COND: it.hasNext() -> WHILE_EXIT -> CALL: storageInfo.removeBlock -> ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT -> ENTRY -> CALL: addBlock -> RETURN -> EXIT -> RETURN -> EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: newReport == null, FOREACH: newReport, FOREACH_EXIT, LOG.DEBUG: Reported block {} on {} size {} replicaState = {}, block, dn, block.getNumBytes(), reportedState, IF_FALSE: shouldPostponeBlocksFromFuture && isGenStampInFuture(block), IF_FALSE: storedBlock == null, LOG.DEBUG: In memory blockUCState = {}, ucState, IF_TRUE: invalidateBlocks.contains(dn, block), RETURN, WHILE: it.hasNext(), WHILE_COND: it.hasNext(), WHILE_EXIT, CALL: storageInfo.removeBlock, ENTRY, CALL: listRemove, IF_TRUE: b.removeStorage(this), RETURN, EXIT, ENTRY, CALL: addBlock, RETURN, EXIT, RETURN, EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C1-B1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: idx < len -> IF_TRUE: cur == storageInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: idx < len, IF_TRUE: cur == storageInfo, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C1-B2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: idx < len -> FOR_EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: idx < len, FOR_EXIT, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C1-B3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: getNext -> CALL: findStorageInfo -> CALL: findStorageInfo -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: getNext, CALL: findStorageInfo, CALL: findStorageInfo, RETURN, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C4\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C5\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C6\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C7\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C8\" reason=\"Conflicting conditions in child paths\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeDescriptor()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getUnderConstructionFeature()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StatefulBlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> CALL: addStoredBlock -> ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> CALL: addStoredBlock -> ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> ENTRY -> IF_FALSE: !block.isComplete() -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> ENTRY -> IF_FALSE: !block.isComplete() -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C3\" reason=\"Conflicting conditions between parent and child paths\"/>\n  </pruned_paths>\n</merge_result>\n``````xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> CALL: addStoredBlock -> ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> CALL: addStoredBlock -> ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> ENTRY -> IF_FALSE: !block.isComplete() -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> ENTRY -> IF_FALSE: !block.isComplete() -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C3\" reason=\"Conflicting conditions between parent and child paths\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isStriped()": "public abstract boolean isStriped();",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getRealDataBlockNum()": "['ENTRY -> IF_TRUE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: isComplete() || getBlockUCState() == BlockUCState.COMMITTED -> CALL: getDataBlockNum -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isComplete()": "['ENTRY -> CALL: equals -> CALL: getBlockUCState -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:decrementSafeBlockCount(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_TRUE: status == BMSafeModeStatus.OFF -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: status == BMSafeModeStatus.OFF -> RETURN -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: checkSafeMode -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: checkSafeMode -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: getRealDataBlockNum -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: getRealDataBlockNum -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C1\" reason=\"Conflicting conditions: status == BMSafeModeStatus.OFF is false and storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 is false\"/>\n    <path id=\"P3-C2\" reason=\"Conflicting conditions: isComplete() || getBlockUCState() == BlockUCState.COMMITTED is false\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)": "['ENTRY -> IF_TRUE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: priLevel >= 0 && priLevel < LEVEL && priorityQueues.get(priLevel).remove(block) -> FOR_INIT -> FOR_COND: i < LEVEL -> FOR_EXIT -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:updateNeededReconstructions(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_TRUE: !isPopulatingReplQueues() || !block.isComplete() -> CALL: equals -> CALL: getBlockUCState -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_TRUE: !isPopulatingReplQueues() || !block.isComplete(), CALL: equals, CALL: getBlockUCState, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.update -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.update, CALL: writeUnlock, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: writeUnlock, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: NameNode.blockStateChangeLog.debug, CALL: decrementBlockStat, CALL: writeUnlock, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Conflict: Conditional mismatch in !isPopulatingReplQueues() || !block.isComplete() and isComplete() evaluation.\"/>\n    <path id=\"P2-C2\" reason=\"Conflict: Data flow inconsistency in !hasEnoughEffectiveReplicas(block, repl, pendingNum) evaluation.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "['ENTRY -> IF_TRUE: set == null -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_TRUE: set.isEmpty() -> CALL: remove -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: removed -> CALL: decrementAndGet -> CALL: blockLog.debug -> IF_FALSE: set.isEmpty() -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: removed -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:removeStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: blockLog.debug, IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), CALL: blockLog.debug, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> IF_FALSE: cblock != null -> IF_TRUE: !storedBlock.isDeleted() -> CALL: bmSafeMode.decrementSafeBlockCount -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: checkSafeMode -> CALL: updateNeededReconstructions -> CALL: excessRedundancyMap.remove -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: blockLog.debug, IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), IF_FALSE: cblock != null, IF_TRUE: !storedBlock.isDeleted(), CALL: bmSafeMode.decrementSafeBlockCount, IF_FALSE: status == BMSafeModeStatus.OFF, IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1, CALL: checkSafeMode, CALL: updateNeededReconstructions, CALL: excessRedundancyMap.remove, CALL: corruptReplicas.removeFromCorruptReplicasMap, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> IF_FALSE: cblock != null -> IF_TRUE: !storedBlock.isDeleted() -> CALL: bmSafeMode.decrementSafeBlockCount -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1 -> CALL: getRealDataBlockNum -> CALL: min -> CALL: getDataBlockNum -> CALL: getNumBytes -> CALL: getCellSize -> RETURN -> CALL: updateNeededReconstructions -> CALL: excessRedundancyMap.remove -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: blockLog.debug, IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), IF_FALSE: cblock != null, IF_TRUE: !storedBlock.isDeleted(), CALL: bmSafeMode.decrementSafeBlockCount, IF_FALSE: status == BMSafeModeStatus.OFF, IF_TRUE: storedBlock.isComplete() && blockManager.countNodes(b).liveReplicas() == safeNumberOfNodes - 1, CALL: getRealDataBlockNum, CALL: min, CALL: getDataBlockNum, CALL: getNumBytes, CALL: getCellSize, RETURN, CALL: updateNeededReconstructions, CALL: excessRedundancyMap.remove, CALL: corruptReplicas.removeFromCorruptReplicasMap, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> IF_FALSE: cblock != null -> IF_FALSE: !storedBlock.isDeleted() -> CALL: excessRedundancyMap.remove -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: blockLog.debug, IF_FALSE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), IF_FALSE: cblock != null, IF_FALSE: !storedBlock.isDeleted(), CALL: excessRedundancyMap.remove, CALL: corruptReplicas.removeFromCorruptReplicasMap, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P3-C2\" reason=\"Conflicting conditions: isComplete() || getBlockUCState() == BlockUCState.COMMITTED is false\"/>\n    <path id=\"P1-C2\" reason=\"Conflict: Conditional mismatch in !isPopulatingReplQueues() || !block.isComplete() and isComplete() evaluation.\"/>\n    <path id=\"P2-C2\" reason=\"Conflict: Data flow inconsistency in !hasEnoughEffectiveReplicas(block, repl, pendingNum) evaluation.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getStorageType()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:numNodes()": "/**\n * Count the number of data-nodes the block currently belongs to (i.e., NN\n * has received block reports from the DN).\n */\npublic abstract int numNodes();",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:completeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: curBlock.isComplete() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_TRUE: curBlock.isComplete(), RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: curBlock.isComplete() -> IF_TRUE: !force && !hasMinStorage(curBlock, numNodes) -> THROW: new IOException(\"Cannot complete block: block does not satisfy minimal replication requirement.\") -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: curBlock.isComplete(), IF_TRUE: !force && !hasMinStorage(curBlock, numNodes), THROW: new IOException(\"Cannot complete block: block does not satisfy minimal replication requirement.\"), EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> IF_FALSE: curBlock.isComplete() -> IF_FALSE: !force && !hasMinStorage(curBlock, numNodes) -> IF_TRUE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED -> CALL: getBlockUCState -> RETURN -> THROW: new IOException(\"Cannot complete block: block has not been COMMITTED by the client\") -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: curBlock.isComplete(), IF_FALSE: !force && !hasMinStorage(curBlock, numNodes), IF_TRUE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED, CALL: getBlockUCState, RETURN, THROW: new IOException(\"Cannot complete block: block has not been COMMITTED by the client\"), EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P4-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: curBlock.isComplete() -> IF_FALSE: !force && !hasMinStorage(curBlock, numNodes) -> IF_FALSE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED -> CALL: convertToCompleteBlock -> CALL: bmSafeMode.adjustBlockTotals -> CALL: bmSafeMode.incrementSafeBlockCount -> ENTRY -> IF_TRUE: status == BMSafeModeStatus.OFF -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: curBlock.isComplete(), IF_FALSE: !force && !hasMinStorage(curBlock, numNodes), IF_FALSE: !force && curBlock.getBlockUCState() != BlockUCState.COMMITTED, CALL: convertToCompleteBlock, CALL: bmSafeMode.adjustBlockTotals, CALL: bmSafeMode.incrementSafeBlockCount, ENTRY, IF_TRUE: status == BMSafeModeStatus.OFF, RETURN, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P4-C1\" reason=\"Conflict in conditional evaluation between parent and child paths\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:incrementSafeBlockCount(int,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: status == BMSafeModeStatus.OFF -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_TRUE: status == BMSafeModeStatus.OFF, RETURN, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storageNum == safeNumberOfNodes -> IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE -> IF_TRUE: this.awaitingReportedBlocksCounter == null -> CALL: getCounter -> CALL: this.awaitingReportedBlocksCounter.increment -> CALL: checkSafeMode -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: status == BMSafeModeStatus.OFF, IF_TRUE: storageNum == safeNumberOfNodes, IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE, IF_TRUE: this.awaitingReportedBlocksCounter == null, CALL: getCounter, CALL: this.awaitingReportedBlocksCounter.increment, CALL: checkSafeMode, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storageNum == safeNumberOfNodes -> IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE -> IF_FALSE: this.awaitingReportedBlocksCounter == null -> CALL: this.awaitingReportedBlocksCounter.increment -> CALL: checkSafeMode -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: status == BMSafeModeStatus.OFF, IF_TRUE: storageNum == safeNumberOfNodes, IF_TRUE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE, IF_FALSE: this.awaitingReportedBlocksCounter == null, CALL: this.awaitingReportedBlocksCounter.increment, CALL: checkSafeMode, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_TRUE: storageNum == safeNumberOfNodes -> IF_FALSE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE -> CALL: checkSafeMode -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: status == BMSafeModeStatus.OFF, IF_TRUE: storageNum == safeNumberOfNodes, IF_FALSE: prog.getStatus(Phase.SAFEMODE) != Status.COMPLETE, CALL: checkSafeMode, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: status == BMSafeModeStatus.OFF -> IF_FALSE: storageNum == safeNumberOfNodes -> EXIT</exec_flow>\n      <log_sequence>ENTRY, IF_FALSE: status == BMSafeModeStatus.OFF, IF_FALSE: storageNum == safeNumberOfNodes, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"C1\" reason=\"No logs in child node source code\"/>\n    <path id=\"C2\" reason=\"No logs in child node source code\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()": "['ENTRY -> CALL: equals -> CALL: equals -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getState()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlockContentsStale()": "['ENTRY -> RETURN -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processExtraRedundancyBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,short,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>\n        processExtraRedundancyBlock -> areBlockContentsStale -> getDatanodeDescriptor\n        [conditions: storage.getState() == State.NORMAL, !storage.areBlockContentsStale(), !isExcess(cur, block), cur.isInService(), corruptNodes == null || !corruptNodes.contains(cur]\n      </exec_flow>\n      <log_sequence>\n        LOG.trace(\"BLOCK* processExtraRedundancyBlock: Postponing {} since storage {} does not yet have up-to-date information.\", block, storage)\n        ENTRY -> RETURN -> EXIT\n      </log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C2\" reason=\"Conflicting conditions or data flow issues\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_TRUE: node == null -> THROW: new IOException(\"Cannot invalidate \" + b + \" because datanode \" + dn + \" does not exist.\") -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: blockLog.debug -> IF_TRUE: node == null -> THROW: new IOException(\"Cannot invalidate \" + b + \" because datanode \" + dn + \" does not exist.\") -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_FALSE: node == null -> IF_TRUE: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately -> CALL: blockLog.debug -> CALL: postponeBlock -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: blockLog.debug -> IF_FALSE: node == null -> IF_TRUE: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately -> CALL: blockLog.debug -> CALL: postponeBlock -> RETURN -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: blockLog.debug -> IF_FALSE: node == null -> IF_FALSE: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately -> CALL: addToInvalidates -> FOREACH: blocksMap.getStorages(storedBlock) -> FOREACH_EXIT -> IF_TRUE: datanodes != null && datanodes.length() != 0 -> CALL: blockLog.debug -> EXIT -> CALL: removeStoredBlock -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> CALL: blockLog.debug -> IF_FALSE: node == null -> IF_FALSE: nr.replicasOnStaleNodes() > 0 && !deleteCorruptReplicaImmediately -> CALL: addToInvalidates -> FOREACH: blocksMap.getStorages(storedBlock) -> FOREACH_EXIT -> IF_TRUE: datanodes != null && datanodes.length() != 0 -> CALL: blockLog.debug -> EXIT -> CALL: removeStoredBlock -> CALL: blockLog.debug -> RETURN -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C4\" reason=\"Conflict: Conditional mismatch in child node execution flow\"/>\n    <path id=\"P1-C5\" reason=\"Conflict: Data flow inconsistency detected\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateCorruptReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: nodes == null -> RETURN -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: nodes == null -> RETURN -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: blk.isStriped() -> CALL: getStorages -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_TRUE: removedFromBlocksMap -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: blk.isStriped() -> CALL: getStorages -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_TRUE: removedFromBlocksMap -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P4-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: blk.isStriped() -> CALL: getStorages -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_FALSE: removedFromBlocksMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: blk.isStriped() -> CALL: getStorages -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_FALSE: removedFromBlocksMap -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P6-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: blk.isStriped() -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_TRUE: removedFromBlocksMap -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: blk.isStriped() -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_TRUE: removedFromBlocksMap -> CALL: corruptReplicas.removeFromCorruptReplicasMap -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P7-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: blk.isStriped() -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_FALSE: removedFromBlocksMap -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: blk.isStriped() -> FOREACH: nodesCopy -> FOREACH_EXIT -> IF_FALSE: removedFromBlocksMap -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P2-C1\" reason=\"Conflict in conditional evaluation: blk.isStriped() vs blk.isStriped()\"/>\n    <path id=\"P5-C1\" reason=\"Data flow conflict: storages != null && blk.isStriped()\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addStoredBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: !block.isComplete(), CALL: getStoredBlock, IF_TRUE: storedBlock == null || storedBlock.isDeleted(), CALL: blockLog.debug, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_FALSE: storedBlock == null || storedBlock.isDeleted() -> IF_TRUE: result == AddBlockResult.ADDED -> CALL: isDecommissioned -> CALL: isDecommissionInProgress -> IF_TRUE: logEveryBlock -> CALL: blockLog.debug -> IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas) -> IF_TRUE: storedBlock.isComplete() && result == AddBlockResult.ADDED -> CALL: bmSafeMode.incrementSafeBlockCount -> IF_TRUE: !storedBlock.isCompleteOrCommitted() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: !block.isComplete(), CALL: getStoredBlock, IF_FALSE: storedBlock == null || storedBlock.isDeleted(), IF_TRUE: result == AddBlockResult.ADDED, CALL: isDecommissioned, CALL: isDecommissionInProgress, IF_TRUE: logEveryBlock, CALL: blockLog.debug, IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas), IF_TRUE: storedBlock.isComplete() && result == AddBlockResult.ADDED, CALL: bmSafeMode.incrementSafeBlockCount, IF_TRUE: !storedBlock.isCompleteOrCommitted(), RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_FALSE: storedBlock == null || storedBlock.isDeleted() -> IF_TRUE: result == AddBlockResult.ADDED -> CALL: isDecommissioned -> CALL: isDecommissionInProgress -> IF_TRUE: logEveryBlock -> CALL: blockLog.debug -> IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas) -> IF_FALSE: storedBlock.isComplete() && result == AddBlockResult.ADDED -> IF_TRUE: !storedBlock.isCompleteOrCommitted() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: !block.isComplete(), CALL: getStoredBlock, IF_FALSE: storedBlock == null || storedBlock.isDeleted(), IF_TRUE: result == AddBlockResult.ADDED, CALL: isDecommissioned, CALL: isDecommissionInProgress, IF_TRUE: logEveryBlock, CALL: blockLog.debug, IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas), IF_FALSE: storedBlock.isComplete() && result == AddBlockResult.ADDED, IF_TRUE: !storedBlock.isCompleteOrCommitted(), RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C4</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_FALSE: storedBlock == null || storedBlock.isDeleted() -> IF_TRUE: result == AddBlockResult.ADDED -> CALL: isDecommissioned -> CALL: isDecommissionInProgress -> IF_TRUE: logEveryBlock -> CALL: blockLog.debug -> IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas) -> IF_FALSE: storedBlock.isComplete() && result == AddBlockResult.ADDED -> IF_FALSE: !storedBlock.isCompleteOrCommitted() -> IF_TRUE: !isPopulatingReplQueues() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: !block.isComplete(), CALL: getStoredBlock, IF_FALSE: storedBlock == null || storedBlock.isDeleted(), IF_TRUE: result == AddBlockResult.ADDED, CALL: isDecommissioned, CALL: isDecommissionInProgress, IF_TRUE: logEveryBlock, CALL: blockLog.debug, IF_FALSE: storedBlock.getBlockUCState() == BlockUCState.COMMITTED && hasMinStorage(storedBlock, numUsableReplicas), IF_FALSE: storedBlock.isComplete() && result == AddBlockResult.ADDED, IF_FALSE: !storedBlock.isCompleteOrCommitted(), IF_TRUE: !isPopulatingReplQueues(), RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C5</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: curBlock.isComplete() -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, IF_TRUE: curBlock.isComplete(), RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.update -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_TRUE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.update, CALL: writeUnlock, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: writeUnlock, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: NameNode.blockStateChangeLog.debug, CALL: decrementBlockStat, CALL: writeUnlock, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P2-C2\" reason=\"Conflict: Data flow inconsistency in !hasEnoughEffectiveReplicas(block, repl, pendingNum) evaluation.\"/>\n    <path id=\"P1-C6\" reason=\"Conflict: Data flow inconsistency detected\"/>\n    <path id=\"P4-C1\" reason=\"Conflict in conditional evaluation between parent and child paths\"/>\n    <path id=\"P1-C2\" reason=\"Conflict: Conditional mismatch in !isPopulatingReplQueues() || !block.isComplete() and isComplete() evaluation.\"/>\n    <path id=\"P1-C2\" reason=\"Conflicting conditions or data flow issues\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:add(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo,boolean)": "['ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_FALSE: set.add(block) -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_FALSE: log -> EXIT', 'ENTRY -> IF_FALSE: set == null -> IF_FALSE: set.add(block) -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addToInvalidates(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.DatanodeInfo)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> IF_TRUE: storage.getState() != State.NORMAL -> CONTINUE -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> IF_TRUE: storage.getState() != State.NORMAL -> CONTINUE -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> FOREACH_EXIT -> IF_TRUE: datanodes != null && datanodes.length() != 0 -> CALL: blockLog.debug -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> FOREACH_EXIT -> IF_TRUE: datanodes != null && datanodes.length() != 0 -> CALL: blockLog.debug -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_TRUE: blockIdManager.isStripedBlock(block) -> CALL: numECBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT</exec_flow>\n      <log_sequence>ENTRY -> IF_TRUE: set == null -> NEW: LightWeightHashSet<> -> CALL: putBlocksSet -> IF_TRUE: set.add(block) -> IF_FALSE: blockIdManager.isStripedBlock(block) -> CALL: numBlocks.increment -> IF_TRUE: log -> CALL: NameNode.blockStateChangeLog.debug -> EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C3\" reason=\"Conditional conflict: datanodes != null && datanodes.length() != 0 vs datanodes == null || datanodes.length() == 0\"/>\n    <path id=\"P2-C3\" reason=\"Data flow conflict: set.add(block) failed\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:addToCorruptReplicasMap(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason,boolean)": "['ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT', 'ENTRY -> IF_FALSE: nodes == null -> IF_FALSE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT']",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:markBlockAsCorrupt(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_TRUE: b.getStored().isDeleted() -> CALL: blockLog.debug -> CALL: addToInvalidates -> EXIT</exec_flow>\n      <log_sequence>[BLOCK markBlockAsCorrupt: {} cannot be marked as corrupt as it does not belong to any file]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: b.getStored().isDeleted() -> CALL: corruptReplicas.addToCorruptReplicasMap -> ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT</exec_flow>\n      <log_sequence>[BLOCK NameSystem.addToCorruptReplicasMap: {} added as corrupt on {} by {} {}]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C3</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> IF_FALSE: b.getStored().isDeleted() -> CALL: corruptReplicas.addToCorruptReplicasMap -> ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_FALSE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT</exec_flow>\n      <log_sequence>[BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for {} to add as corrupt on {} by {} {}]</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: listRemove -> IF_TRUE: b.removeStorage(this) -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[BLOCK NameSystem.removeBlock: {} successfully removed from storage]</log_sequence>\n    </path>\n    <path>\n      <id>P2-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: listRemove -> IF_FALSE: b.removeStorage(this) -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[BLOCK NameSystem.removeBlock: {} removal from storage failed]</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: writeUnlock, EXIT</log_sequence>\n    </path>\n    <path>\n      <id>P3-C2</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: writeLock -> TRY -> IF_FALSE: !isPopulatingReplQueues() || !block.isComplete() -> IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum) -> CALL: neededReconstruction.remove -> CALL: NameNode.blockStateChangeLog.debug -> CALL: decrementBlockStat -> CALL: writeUnlock -> EXIT</exec_flow>\n      <log_sequence>ENTRY, CALL: writeLock, TRY, IF_FALSE: !isPopulatingReplQueues() || !block.isComplete(), IF_FALSE: !hasEnoughEffectiveReplicas(block, repl, pendingNum), CALL: neededReconstruction.remove, CALL: NameNode.blockStateChangeLog.debug, CALL: decrementBlockStat, CALL: writeUnlock, EXIT</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C4\" reason=\"Conflict: Conditional mismatch between parent and child paths\"/>\n    <path id=\"P1-C5\" reason=\"Conflict: Data flow inconsistency detected\"/>\n    <path id=\"P1-C2\" reason=\"Conflict: Conditional mismatch in !isPopulatingReplQueues() || !block.isComplete() and isComplete() evaluation.\"/>\n    <path id=\"P2-C2\" reason=\"Conflict: Data flow inconsistency in !hasEnoughEffectiveReplicas(block, repl, pendingNum) evaluation.\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.BlockListAsLongs)": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> CALL: reportDiff -> FOREACH: toUC -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> CALL: addStoredBlock -> ENTRY -> IF_TRUE: !block.isComplete() -> CALL: getStoredBlock -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> FOREACH_EXIT -> FOREACH: toRemove -> CALL: removeStoredBlock -> ENTRY -> CALL: blockLog.debug -> IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> CALL: blockLog.debug -> RETURN -> EXIT -> FOREACH_EXIT -> FOREACH: toAdd -> FOREACH_EXIT -> IF_TRUE: numBlocksLogged > maxNumBlocksToLog -> CALL: blockLog.info -> FOREACH: toInvalidate -> ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> IF_TRUE: storage.getState() != State.NORMAL -> CONTINUE -> EXIT -> FOREACH_EXIT -> FOREACH: toCorrupt -> CALL: markBlockAsCorrupt -> ENTRY -> IF_TRUE: b.getStored().isDeleted() -> CALL: blockLog.debug -> CALL: addToInvalidates -> EXIT -> FOREACH_EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: reportDiff, FOREACH: toUC, CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent, IF_TRUE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()), CALL: addStoredBlock, ENTRY, IF_TRUE: !block.isComplete(), CALL: getStoredBlock, IF_TRUE: storedBlock == null || storedBlock.isDeleted(), CALL: blockLog.debug, FOREACH_EXIT, FOREACH: toRemove, CALL: removeStoredBlock, ENTRY, CALL: blockLog.debug, IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), CALL: blockLog.debug, RETURN, EXIT, FOREACH_EXIT, FOREACH: toAdd, FOREACH_EXIT, IF_TRUE: numBlocksLogged > maxNumBlocksToLog, CALL: blockLog.info, FOREACH: toInvalidate, ENTRY, IF_FALSE: !isPopulatingReplQueues(), FOREACH: blocksMap.getStorages(storedBlock), IF_TRUE: storage.getState() != State.NORMAL, CONTINUE, EXIT, FOREACH_EXIT, FOREACH: toCorrupt, CALL: markBlockAsCorrupt, ENTRY, IF_TRUE: b.getStored().isDeleted(), CALL: blockLog.debug, CALL: addToInvalidates, EXIT, FOREACH_EXIT, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P1-C2</id>\n      <eval>false</eval>\n      <exec_flow>ENTRY -> CALL: reportDiff -> FOREACH: toUC -> CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent -> IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()) -> ENTRY -> IF_FALSE: !block.isComplete() -> IF_TRUE: storedBlock == null || storedBlock.isDeleted() -> CALL: blockLog.debug -> FOREACH_EXIT -> FOREACH: toRemove -> CALL: removeStoredBlock -> ENTRY -> CALL: blockLog.debug -> IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node) -> CALL: blockLog.debug -> RETURN -> EXIT -> FOREACH_EXIT -> FOREACH: toAdd -> FOREACH_EXIT -> IF_FALSE: numBlocksLogged > maxNumBlocksToLog -> FOREACH: toInvalidate -> ENTRY -> IF_FALSE: !isPopulatingReplQueues() -> FOREACH: blocksMap.getStorages(storedBlock) -> FOREACH_EXIT -> IF_TRUE: datanodes != null && datanodes.length() != 0 -> CALL: blockLog.debug -> EXIT -> FOREACH_EXIT -> FOREACH: toCorrupt -> CALL: markBlockAsCorrupt -> ENTRY -> IF_FALSE: b.getStored().isDeleted() -> CALL: corruptReplicas.addToCorruptReplicasMap -> ENTRY -> IF_TRUE: nodes == null -> NEW: HashMap<DatanodeDescriptor, Reason> -> CALL: corruptReplicasMap.put -> CALL: incrementBlockStat -> IF_TRUE: reason != null -> IF_TRUE: !nodes.keySet().contains(dn) -> CALL: NameNode.blockStateChangeLog.debug -> CALL: put -> EXIT -> FOREACH_EXIT -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, CALL: reportDiff, FOREACH: toUC, CALL: block.getUnderConstructionFeature().addReplicaIfNotPresent, IF_FALSE: ucBlock.reportedState == ReplicaState.FINALIZED && (block.findStorageInfo(storageInfo) < 0) || corruptReplicas.isReplicaCorrupt(block, storageInfo.getDatanodeDescriptor()), ENTRY, IF_FALSE: !block.isComplete(), IF_TRUE: storedBlock == null || storedBlock.isDeleted(), CALL: blockLog.debug, FOREACH_EXIT, FOREACH: toRemove, CALL: removeStoredBlock, ENTRY, CALL: blockLog.debug, IF_TRUE: storedBlock == null || !blocksMap.removeNode(storedBlock, node), CALL: blockLog.debug, RETURN, EXIT, FOREACH_EXIT, FOREACH: toAdd, FOREACH_EXIT, IF_FALSE: numBlocksLogged > maxNumBlocksToLog, FOREACH: toInvalidate, ENTRY, IF_FALSE: !isPopulatingReplQueues(), FOREACH: blocksMap.getStorages(storedBlock), FOREACH_EXIT, IF_TRUE: datanodes != null && datanodes.length() != 0, CALL: blockLog.debug, EXIT, FOREACH_EXIT, FOREACH: toCorrupt, CALL: markBlockAsCorrupt, ENTRY, IF_FALSE: b.getStored().isDeleted(), CALL: corruptReplicas.addToCorruptReplicasMap, ENTRY, IF_TRUE: nodes == null, NEW: HashMap<DatanodeDescriptor, Reason>, CALL: corruptReplicasMap.put, CALL: incrementBlockStat, IF_TRUE: reason != null, IF_TRUE: !nodes.keySet().contains(dn), CALL: NameNode.blockStateChangeLog.debug, CALL: put, EXIT, FOREACH_EXIT, RETURN, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P1-C3\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C4\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C5\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C6\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C7\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C8\" reason=\"Conflicting conditions in child paths\"/>\n    <path id=\"P1-C3\" reason=\"Conflicting conditions between parent and child paths\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.BPOfferService:trySendErrorReport(int,java.lang.String)": "['ENTRY -> FOREACH: bpServices -> FOREACH_EXIT -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:reportBadBlock(org.apache.hadoop.hdfs.server.datanode.BPOfferService,org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)": "['ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> EXIT', 'ENTRY -> IF_FALSE: volume == null -> CALL: reportBadBlocks -> LOG: LOG.WARN: msg -> EXIT']",
    "org.apache.hadoop.hdfs.server.datanode.DataNode$DataTransfer:<init>(org.apache.hadoop.hdfs.server.datanode.DataNode,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)": "",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: ReplicaNotFoundException e -> IF_TRUE: replicaNotExist || replicaStateNotFinalized -> LOG: LOG.INFO: errStr -> CALL: trySendErrorReport -> FOREACH: bpServices -> FOREACH_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: ReplicaNotFoundException e, IF_TRUE: replicaNotExist || replicaStateNotFinalized, LOG: LOG.INFO: errStr, CALL: trySendErrorReport, FOREACH: bpServices, FOREACH_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: UnexpectedReplicaStateException e -> IF_TRUE: replicaNotExist || replicaStateNotFinalized -> LOG: LOG.INFO: errStr -> CALL: trySendErrorReport -> FOREACH: bpServices -> FOREACH_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: UnexpectedReplicaStateException e, IF_TRUE: replicaNotExist || replicaStateNotFinalized, LOG: LOG.INFO: errStr, CALL: trySendErrorReport, FOREACH: bpServices, FOREACH_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: FileNotFoundException e -> IF_TRUE: blockFileNotExist -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: FileNotFoundException e, IF_TRUE: blockFileNotExist, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_TRUE: volume == null, LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block, RETURN, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P4-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: EOFException e -> IF_TRUE: lengthTooShort -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_FALSE: volume == null -> CALL: reportBadBlocks -> LOG: LOG.WARN: msg -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: EOFException e, IF_TRUE: lengthTooShort, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_FALSE: volume == null, CALL: reportBadBlocks, LOG: LOG.WARN: msg, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P5-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: IOException e -> IF_TRUE: blockFileNotExist -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: IOException e, IF_TRUE: blockFileNotExist, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_TRUE: volume == null, LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block, RETURN, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P6\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P7\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P8\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P9\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P10\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n  </pruned_paths>\n</merge_result>\n```",
    "org.apache.hadoop.hdfs.server.datanode.DataNode:transferBlocks(java.lang.String,org.apache.hadoop.hdfs.protocol.Block[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[][],org.apache.hadoop.fs.StorageType[][],java.lang.String[][])": "```xml\n<merge_result>\n  <valid_paths>\n    <path>\n      <id>P1-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: i < blocks.length -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: ReplicaNotFoundException e -> IF_TRUE: replicaNotExist || replicaStateNotFinalized -> LOG: LOG.INFO: errStr -> CALL: trySendErrorReport -> FOR_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: i < blocks.length, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: ReplicaNotFoundException e, IF_TRUE: replicaNotExist || replicaStateNotFinalized, LOG: LOG.INFO: errStr, CALL: trySendErrorReport, FOR_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P2-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: i < blocks.length -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: UnexpectedReplicaStateException e -> IF_TRUE: replicaNotExist || replicaStateNotFinalized -> LOG: LOG.INFO: errStr -> CALL: trySendErrorReport -> FOR_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: i < blocks.length, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: UnexpectedReplicaStateException e, IF_TRUE: replicaNotExist || replicaStateNotFinalized, LOG: LOG.INFO: errStr, CALL: trySendErrorReport, FOR_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P3-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: i < blocks.length -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: FileNotFoundException e -> IF_TRUE: blockFileNotExist -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> FOR_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: i < blocks.length, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: FileNotFoundException e, IF_TRUE: blockFileNotExist, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_TRUE: volume == null, LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block, RETURN, FOR_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P4-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: i < blocks.length -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: EOFException e -> IF_TRUE: lengthTooShort -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_FALSE: volume == null -> CALL: reportBadBlocks -> LOG: LOG.WARN: msg -> FOR_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: i < blocks.length, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: EOFException e, IF_TRUE: lengthTooShort, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_FALSE: volume == null, CALL: reportBadBlocks, LOG: LOG.WARN: msg, FOR_EXIT, EXIT]</log_sequence>\n    </path>\n    <path>\n      <id>P5-C1</id>\n      <eval>true</eval>\n      <exec_flow>ENTRY -> FOR_INIT -> FOR_COND: i < blocks.length -> TRY -> CALL: checkBlock -> EXCEPTION: checkBlock -> CATCH: IOException e -> IF_TRUE: blockFileNotExist -> LOG: LOG.INFO: errStr -> CALL: reportBadBlock -> ENTRY -> IF_TRUE: volume == null -> LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block -> RETURN -> FOR_EXIT -> EXIT</exec_flow>\n      <log_sequence>[ENTRY, FOR_INIT, FOR_COND: i < blocks.length, TRY, CALL: checkBlock, EXCEPTION: checkBlock, CATCH: IOException e, IF_TRUE: blockFileNotExist, LOG: LOG.INFO: errStr, CALL: reportBadBlock, ENTRY, IF_TRUE: volume == null, LOG: LOG.WARN: Cannot find FsVolumeSpi to report bad block: {}, block, RETURN, FOR_EXIT, EXIT]</log_sequence>\n    </path>\n  </valid_paths>\n  <pruned_paths>\n    <path id=\"P6\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P7\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P8\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P9\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n    <path id=\"P10\" reason=\"Conflicting conditions or invalid logs in child paths\"/>\n  </pruned_paths>\n</merge_result>\n```"
}