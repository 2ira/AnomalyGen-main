org.apache.hadoop.fs.cosn.CosNInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.cosn.CosNInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.cosn.CosNInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.cosn.CosNInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.cosn.CosNFileSystem:getScheme()	0	java.lang.String	0	cosn
org.apache.hadoop.fs.cosn.CosNFileSystem:pathToKey(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.fs.cosn.CosNFileSystem:rejectRootDirectoryDelete(boolean,boolean)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:rejectRootDirectoryDelete(boolean,boolean)	1	int	0	0
org.apache.hadoop.fs.cosn.CosNFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	1	int	0	0
org.apache.hadoop.fs.cosn.CosNFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:mkDirRecursively(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:mkdir(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.cosn.CosNFileSystem:copyFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.cosn.CosNFileSystem:getCanonicalServiceName()	0	null	0	null
org.apache.hadoop.fs.cosn.ByteBufferInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.cosn.ByteBufferInputStream:markSupported()	0	int	0	1
org.apache.hadoop.fs.cosn.BufferPool:getByteBuffer()	0	null	0	null
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:getUploadId(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:getFileLength(java.lang.String)	0	long	0	0
org.apache.hadoop.fs.cosn.ByteBufferWrapper:isDiskBuffer()	0	int	0	1
org.apache.hadoop.fs.cosn.ByteBufferWrapper:isDiskBuffer()	1	int	0	0
org.apache.hadoop.fs.cosn.CosN:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.classification.tools.RootDocProcessor$ExcludeHandler:exclude(com.sun.javadoc.Doc)	0	int	0	1
org.apache.hadoop.classification.tools.RootDocProcessor$ExcludeHandler:exclude(com.sun.javadoc.Doc)	1	int	0	0
org.apache.hadoop.classification.tools.RootDocProcessor$ExcludeHandler:isFiltered(java.lang.Object[])	0	int	0	1
org.apache.hadoop.classification.tools.RootDocProcessor$ExcludeHandler:isFiltered(java.lang.Object[])	1	int	0	0
org.apache.hadoop.classification.tools.RootDocProcessor:process(java.lang.Object,java.lang.Class)	0	null	0	null
org.apache.hadoop.classification.tools.ExcludePrivateAnnotationsStandardDoclet:start(com.sun.javadoc.RootDoc)	0	int	0	1
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	0	java.lang.String	0	com.ibm.security.auth.module.Win64LoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	1	java.lang.String	0	com.ibm.security.auth.module.NTLoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	2	java.lang.String	0	com.ibm.security.auth.module.AIX64LoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	3	java.lang.String	0	com.ibm.security.auth.module.AIXLoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	4	java.lang.String	0	com.ibm.security.auth.module.LinuxLoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	5	java.lang.String	0	com.sun.security.auth.module.NTLoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$KerberosConfiguration:getOSLoginModuleName()	6	java.lang.String	0	com.sun.security.auth.module.UnixLoginModule
org.apache.hadoop.security.authentication.client.KerberosAuthenticator$1:run()	0	null	0	null
org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token:isSet()	0	int	0	1
org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token:isSet()	1	int	0	0
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:isTokenKerberos(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)	0	int	0	1
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:isTokenKerberos(org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)	1	int	0	0
org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	1
org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler:getUserName(javax.servlet.http.HttpServletRequest)	0	null	0	null
org.apache.hadoop.security.authentication.server.AuthenticationFilter:isRandomSecret()	0	int	0	1
org.apache.hadoop.security.authentication.server.AuthenticationFilter:isRandomSecret()	1	int	0	0
org.apache.hadoop.security.authentication.server.AuthenticationFilter:isCustomSignerSecretProvider()	0	int	0	1
org.apache.hadoop.security.authentication.server.AuthenticationFilter:isCustomSignerSecretProvider()	1	int	0	0
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:getType()	0	java.lang.String	0	ldap
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	1
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:hasDomain(java.lang.String)	0	int	0	1
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:hasDomain(java.lang.String)	1	int	0	0
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:indexOfDomainMatch(java.lang.String)	0	int	0	-1
org.apache.hadoop.security.authentication.server.AltKerberosAuthenticationHandler:getType()	0	java.lang.String	0	alt-kerberos
org.apache.hadoop.security.authentication.server.AltKerberosAuthenticationHandler:isBrowser(java.lang.String)	0	int	0	0
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler$1:verify(java.lang.String,javax.net.ssl.SSLSession)	0	int	0	1
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:getOriginalQueryString(javax.servlet.http.HttpServletRequest)	0	java.lang.String	0	
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateToken(com.nimbusds.jwt.SignedJWT)	0	int	0	1
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:validateToken(com.nimbusds.jwt.SignedJWT)	1	int	0	0
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	1
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	1
org.apache.hadoop.security.authentication.server.AuthenticationHandlerUtil:checkAuthScheme(java.lang.String)	0	java.lang.String	0	Basic
org.apache.hadoop.security.authentication.server.AuthenticationHandlerUtil:checkAuthScheme(java.lang.String)	1	java.lang.String	0	Negotiate
org.apache.hadoop.security.authentication.server.AuthenticationHandlerUtil:checkAuthScheme(java.lang.String)	2	java.lang.String	0	Digest
org.apache.hadoop.security.authentication.util.KerberosUtil:getKrb5LoginModuleName()	0	java.lang.String	0	com.ibm.security.auth.module.Krb5LoginModule
org.apache.hadoop.security.authentication.util.KerberosUtil:getKrb5LoginModuleName()	1	java.lang.String	0	com.sun.security.auth.module.Krb5LoginModule
org.apache.hadoop.security.authentication.util.KerberosUtil:hasKerberosKeyTab(javax.security.auth.Subject)	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosUtil:hasKerberosKeyTab(javax.security.auth.Subject)	1	int	0	0
org.apache.hadoop.security.authentication.util.KerberosUtil:hasKerberosTicket(javax.security.auth.Subject)	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosUtil:hasKerberosTicket(javax.security.auth.Subject)	1	int	0	0
org.apache.hadoop.security.authentication.util.KerberosUtil$DER:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosUtil$DER:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.authentication.util.KerberosUtil$DER:hasNext()	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosUtil$DER:hasNext()	1	int	0	0
org.apache.hadoop.security.authentication.util.AuthToken:isExpired()	0	int	0	1
org.apache.hadoop.security.authentication.util.AuthToken:isExpired()	1	int	0	0
org.apache.hadoop.security.authentication.util.KerberosName:hasRulesBeenSet()	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosName:hasRulesBeenSet()	1	int	0	0
org.apache.hadoop.security.authentication.util.KerberosName:hasRuleMechanismBeenSet()	0	int	0	1
org.apache.hadoop.security.authentication.util.KerberosName:hasRuleMechanismBeenSet()	1	int	0	0
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:suffix()	0	java.lang.String	0	ms
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:suffix()	0	java.lang.String	0	us
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:suffix()	0	java.lang.String	0	d
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:suffix()	0	java.lang.String	0	ns
org.apache.hadoop.conf.Configuration$IntegerRanges:isIncluded(int)	0	int	0	1
org.apache.hadoop.conf.Configuration$IntegerRanges:isIncluded(int)	1	int	0	0
org.apache.hadoop.conf.Configuration$IntegerRanges:isEmpty()	0	int	0	1
org.apache.hadoop.conf.Configuration$IntegerRanges:isEmpty()	1	int	0	0
org.apache.hadoop.conf.Configuration$IntegerRanges:getRangeStart()	0	int	0	-1
org.apache.hadoop.conf.StorageUnit$2:getLongName()	0	java.lang.String	0	petabytes
org.apache.hadoop.conf.StorageUnit$2:getShortName()	0	java.lang.String	0	pb
org.apache.hadoop.conf.StorageUnit$2:getSuffixChar()	0	java.lang.String	0	p
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:suffix()	0	java.lang.String	0	m
org.apache.hadoop.conf.StorageUnit$1:getLongName()	0	java.lang.String	0	exabytes
org.apache.hadoop.conf.StorageUnit$1:getShortName()	0	java.lang.String	0	eb
org.apache.hadoop.conf.StorageUnit$1:getSuffixChar()	0	java.lang.String	0	e
org.apache.hadoop.conf.StorageUnit$7:getLongName()	0	java.lang.String	0	bytes
org.apache.hadoop.conf.StorageUnit$7:getShortName()	0	java.lang.String	0	b
org.apache.hadoop.conf.StorageUnit$7:getSuffixChar()	0	java.lang.String	0	b
org.apache.hadoop.conf.StorageUnit$6:getLongName()	0	java.lang.String	0	kilobytes
org.apache.hadoop.conf.StorageUnit$6:getShortName()	0	java.lang.String	0	kb
org.apache.hadoop.conf.StorageUnit$6:getSuffixChar()	0	java.lang.String	0	k
org.apache.hadoop.conf.StorageUnit$4:getLongName()	0	java.lang.String	0	gigabytes
org.apache.hadoop.conf.StorageUnit$4:getShortName()	0	java.lang.String	0	gb
org.apache.hadoop.conf.StorageUnit$4:getSuffixChar()	0	java.lang.String	0	g
org.apache.hadoop.conf.StorageUnit$5:getLongName()	0	java.lang.String	0	megabytes
org.apache.hadoop.conf.StorageUnit$5:getShortName()	0	java.lang.String	0	mb
org.apache.hadoop.conf.StorageUnit$5:getSuffixChar()	0	java.lang.String	0	m
org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object)	0	int	0	0
org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object)	1	int	0	1
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:suffix()	0	java.lang.String	0	s
org.apache.hadoop.conf.ConfServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)	0	java.lang.String	0	json
org.apache.hadoop.conf.ConfServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)	1	java.lang.String	0	xml
org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:suffix()	0	java.lang.String	0	h
org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)	0	null	0	null
org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String)	0	int	0	1
org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String)	1	int	0	0
org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String)	0	null	0	null
org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String)	0	null	0	null
org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)	0	null	0	null
org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String)	0	null	0	null
org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)	0	null	0	null
org.apache.hadoop.conf.Configuration:parse(java.io.InputStream,java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)	0	null	0	null
org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String)	0	int	0	1
org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String)	1	int	0	0
org.apache.hadoop.conf.StorageUnit$3:getLongName()	0	java.lang.String	0	terabytes
org.apache.hadoop.conf.StorageUnit$3:getShortName()	0	java.lang.String	0	tb
org.apache.hadoop.conf.StorageUnit$3:getSuffixChar()	0	java.lang.String	0	t
org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:hasNext()	0	int	0	1
org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:hasNext()	1	int	0	0
org.apache.hadoop.conf.ReconfigurationTaskStatus:hasTask()	0	int	0	1
org.apache.hadoop.conf.ReconfigurationTaskStatus:hasTask()	1	int	0	0
org.apache.hadoop.conf.ReconfigurationTaskStatus:stopped()	0	int	0	1
org.apache.hadoop.conf.ReconfigurationTaskStatus:stopped()	1	int	0	0
org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)	0	java.lang.String	0	<redacted>
org.apache.hadoop.conf.ConfigRedactor:configIsSensitive(java.lang.String)	0	int	0	1
org.apache.hadoop.conf.ConfigRedactor:configIsSensitive(java.lang.String)	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:hasUser()	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:hasUser()	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tools.CommandShell$SubCommand:validate()	0	int	0	1
org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.CommandShell:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.ipc.CallerContext:getSignature()	0	null	0	null
org.apache.hadoop.ipc.CallerContext:isContextValid()	0	int	0	1
org.apache.hadoop.ipc.CallerContext:isContextValid()	1	int	0	0
org.apache.hadoop.ipc.CallerContext:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.ipc.CallerContext:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.ipc.CallerContext:toString()	0	java.lang.String	0	
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasExitStatus()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasExitStatus()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasUserMessage()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasUserMessage()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasSenderName()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:hasSenderName()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:hasIdentifier()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:hasIdentifier()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto$Builder:hasIdentifier()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto$Builder:hasIdentifier()	1	int	0	0
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasExitStatus()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasExitStatus()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasUserMessage()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasUserMessage()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasSenderName()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto$Builder:hasSenderName()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.DecayRpcScheduler$1:getPriorityLevel()	0	int	0	0
org.apache.hadoop.ipc.Server$Connection:setShouldClose()	0	int	0	1
org.apache.hadoop.ipc.Server$Connection:isIdle()	0	int	0	1
org.apache.hadoop.ipc.Server$Connection:isIdle()	1	int	0	0
org.apache.hadoop.ipc.Server$Connection:readAndProcess()	0	int	0	-1
org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[])	0	null	0	null
org.apache.hadoop.ipc.DefaultCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)	0	long	0	1
org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getCallQueue()	0	null	0	null
org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)	0	int	0	0
org.apache.hadoop.ipc.DecayRpcScheduler:getSchedulingDecisionSummary()	0	java.lang.String	0	{}
org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)	0	null	0	null
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:hasVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:hasVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:hasUserInfo()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:hasUserInfo()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasMethodName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasMethodName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasDeclaringClassProtocolName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasDeclaringClassProtocolName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasClientProtocolVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:hasClientProtocolVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasMethod()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasMethod()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasMechanism()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasMechanism()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasServerId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasServerId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasChallenge()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:hasChallenge()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$1:assignDescriptors(com.google.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:hasEffectiveUser()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:hasEffectiveUser()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:hasRealUser()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:hasRealUser()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasMethodName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasMethodName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasDeclaringClassProtocolName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasDeclaringClassProtocolName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasClientProtocolVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:hasClientProtocolVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasToken()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:hasToken()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasTraceId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasTraceId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasParentId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasParentId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasSpanContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:hasSpanContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasCallId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasCallId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasServerIpcVersionNum()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasServerIpcVersionNum()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasExceptionClassName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasExceptionClassName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasErrorMsg()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasErrorMsg()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasErrorDetail()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasErrorDetail()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasClientId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasClientId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasRetryCount()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasRetryCount()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasStateId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasStateId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasRouterFederatedState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:hasRouterFederatedState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasMethodName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasMethodName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasDeclaringClassProtocolName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasDeclaringClassProtocolName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasClientProtocolVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:hasClientProtocolVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasMethodName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasMethodName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasDeclaringClassProtocolName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasDeclaringClassProtocolName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasClientProtocolVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto$Builder:hasClientProtocolVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasTraceId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasTraceId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasParentId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasParentId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasSpanContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:hasSpanContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRpcOp()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRpcOp()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasCallId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasCallId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasClientId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasClientId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRetryCount()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRetryCount()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasTraceInfo()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasTraceInfo()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasCallerContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasCallerContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasStateId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasStateId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRouterFederatedState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:hasRouterFederatedState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasMethod()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasMethod()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasMechanism()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasMechanism()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasServerId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasServerId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasChallenge()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:hasChallenge()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasCallId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasCallId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasStatus()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasStatus()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasServerIpcVersionNum()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasServerIpcVersionNum()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasExceptionClassName()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasExceptionClassName()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasErrorMsg()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasErrorMsg()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasErrorDetail()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasErrorDetail()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasClientId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasClientId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasRetryCount()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasRetryCount()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasStateId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasStateId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasRouterFederatedState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:hasRouterFederatedState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:hasEffectiveUser()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:hasEffectiveUser()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:hasRealUser()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:hasRealUser()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:hasContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:hasContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:hasSignature()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto$Builder:hasSignature()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRpcKind()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRpcKind()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRpcOp()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRpcOp()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasCallId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasCallId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasClientId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasClientId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRetryCount()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRetryCount()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasTraceInfo()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasTraceInfo()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasCallerContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasCallerContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasStateId()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasStateId()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRouterFederatedState()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:hasRouterFederatedState()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:hasContext()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:hasContext()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:hasSignature()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:hasSignature()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCCallerContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:hasUserInfo()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:hasUserInfo()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:hasProtocol()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:hasProtocol()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary()	0	java.lang.String	0	No Active Scheduler
org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary()	0	java.lang.String	0	No Active Scheduler
org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount()	0	int	0	-1
org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume()	0	long	0	-1
org.apache.hadoop.ipc.Client$Connection$1:run()	0	null	0	null
org.apache.hadoop.ipc.Client$Connection:addCall(org.apache.hadoop.ipc.Client$Call)	0	int	0	0
org.apache.hadoop.ipc.Client$Connection:addCall(org.apache.hadoop.ipc.Client$Call)	1	int	0	1
org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb()	0	int	0	1
org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb()	1	int	0	0
org.apache.hadoop.ipc.Client$Connection:updateAddress()	0	int	0	1
org.apache.hadoop.ipc.Client$Connection:updateAddress()	1	int	0	0
org.apache.hadoop.ipc.Client$Connection:waitForWork()	0	int	0	1
org.apache.hadoop.ipc.Client$Connection:waitForWork()	1	int	0	0
org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)	0	int	0	0
org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)	0	int	0	1
org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)	1	int	0	0
org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable)	0	int	0	1
org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)	0	int	0	1
org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)	1	int	0	0
org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable)	0	null	0	null
org.apache.hadoop.ipc.RetryCache$CacheEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.RetryCache$CacheEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.RetryCache$CacheEntry:isSuccess()	0	int	0	1
org.apache.hadoop.ipc.RetryCache$CacheEntry:isSuccess()	1	int	0	0
org.apache.hadoop.ipc.CallerContext$Builder:isValid(java.lang.String)	0	int	0	1
org.apache.hadoop.ipc.CallerContext$Builder:isValid(java.lang.String)	1	int	0	0
org.apache.hadoop.ipc.ExternalCall:getDetailedMetricsName()	0	java.lang.String	0	(external)
org.apache.hadoop.ipc.ExternalCall:run()	0	null	0	null
org.apache.hadoop.ipc.RetryCache:skipRetryCache()	0	int	0	1
org.apache.hadoop.ipc.RetryCache:skipRetryCache()	1	int	0	0
org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache)	0	null	0	null
org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object)	0	null	0	null
org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])	0	null	0	null
org.apache.hadoop.ipc.RPC:getProtocolName(java.lang.Class)	0	null	0	null
org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing)	0	long	0	0
org.apache.hadoop.ipc.DefaultRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)	0	int	0	0
org.apache.hadoop.ipc.DefaultRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)	0	int	0	0
org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.Client$ConnectionId:isEqual(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.Client$ConnectionId:isEqual(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])	0	null	0	null
org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)	0	null	0	null
org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)	0	null	0	null
org.apache.hadoop.ipc.Server:getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)	0	null	0	null
org.apache.hadoop.ipc.Server:getCallId()	0	int	0	-2
org.apache.hadoop.ipc.Server:getCallRetryCount()	0	int	0	-1
org.apache.hadoop.ipc.Server:getRemotePort()	0	int	0	0
org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP()	0	null	0	null
org.apache.hadoop.ipc.Server:getRemoteAddress()	0	null	0	null
org.apache.hadoop.ipc.Server:isRpcInvocation()	0	int	0	1
org.apache.hadoop.ipc.Server:isRpcInvocation()	1	int	0	0
org.apache.hadoop.ipc.Server:getPriorityLevel()	0	int	0	0
org.apache.hadoop.ipc.Server$ConnectionManager:isFull()	0	int	0	1
org.apache.hadoop.ipc.Server$ConnectionManager:isFull()	1	int	0	0
org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)	0	null	0	null
org.apache.hadoop.ipc.ClientId:toString(byte[])	0	java.lang.String	0	
org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)	0	int	0	0
org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)	0	int	0	1
org.apache.hadoop.ipc.CallQueueManager:queueIsReallyEmpty(java.util.concurrent.BlockingQueue)	0	int	0	0
org.apache.hadoop.ipc.CallQueueManager:queueIsReallyEmpty(java.util.concurrent.BlockingQueue)	1	int	0	1
org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration)	0	int	0	-1
org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)	0	null	0	null
org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)	0	null	0	null
org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])	0	int	0	1
org.apache.hadoop.ipc.Server$Call:isOpen()	0	int	0	1
org.apache.hadoop.ipc.Server$Call:run()	0	null	0	null
org.apache.hadoop.ipc.Server$Call:getRemoteUser()	0	null	0	null
org.apache.hadoop.ipc.Server$Call:getHostInetAddress()	0	null	0	null
org.apache.hadoop.ipc.Server$Call:getRemotePort()	0	int	0	0
org.apache.hadoop.ipc.Server$Call:getProtocol()	0	null	0	null
org.apache.hadoop.ipc.Server$RpcCall:getProtocol()	0	java.lang.String	0	rpc
org.apache.hadoop.ipc.Server$RpcCall:run()	0	null	0	null
org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String)	0	null	0	null
org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String)	0	null	0	null
org.apache.hadoop.http.HttpConfig$Policy:isHttpEnabled()	0	int	0	1
org.apache.hadoop.http.HttpConfig$Policy:isHttpEnabled()	1	int	0	0
org.apache.hadoop.http.HttpConfig$Policy:isHttpsEnabled()	0	int	0	1
org.apache.hadoop.http.HttpConfig$Policy:isHttpsEnabled()	1	int	0	0
org.apache.hadoop.http.HttpRequestLogAppender:requiresLayout()	0	int	0	0
org.apache.hadoop.http.HttpServer2$QuotingInputFilter:inferMimeType(javax.servlet.ServletRequest)	0	null	0	null
org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.http.HttpServer2:getConnectorAddress(int)	0	null	0	null
org.apache.hadoop.http.HttpServer2:isAlive()	0	int	0	1
org.apache.hadoop.http.HttpServer2:isAlive()	1	int	0	0
org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	1
org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	1	int	0	0
org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)	0	int	0	1
org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)	1	int	0	0
org.apache.hadoop.http.lib.StaticUserWebFilter$User:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.http.lib.StaticUserWebFilter$User:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.http.HtmlQuoting:needsQuoting(byte[],int,int)	0	int	0	1
org.apache.hadoop.http.HtmlQuoting:needsQuoting(byte[],int,int)	1	int	0	0
org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String)	0	int	0	0
org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String)	0	null	0	null
org.apache.hadoop.http.HtmlQuoting:unquoteHtmlChars(java.lang.String)	0	null	0	null
org.apache.hadoop.log.metrics.EventCounter:requiresLayout()	0	int	0	0
org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:shouldLog()	0	int	0	0
org.apache.hadoop.log.LogThrottlingHelper:getLogSupressionMessage(org.apache.hadoop.log.LogThrottlingHelper$LogAction)	0	java.lang.String	0	
org.apache.hadoop.log.Log4Json:getContentType()	0	java.lang.String	0	application/json
org.apache.hadoop.log.Log4Json:ignoresThrowable()	0	int	0	0
org.apache.hadoop.log.LogLevel:isValidProtocol(java.lang.String)	0	int	0	1
org.apache.hadoop.log.LogLevel:isValidProtocol(java.lang.String)	1	int	0	0
org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.metrics2.MetricsFilter:accepts(org.apache.hadoop.metrics2.MetricsRecord)	0	int	0	1
org.apache.hadoop.metrics2.MetricsFilter:accepts(org.apache.hadoop.metrics2.MetricsRecord)	1	int	0	0
org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.source.JvmMetrics:calculateMaxMemoryUsage(java.lang.management.MemoryUsage)	0	float	0	-1.0
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag)	0	int	0	1
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag)	1	int	0	0
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable)	0	int	0	1
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable)	1	int	0	0
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.String)	0	int	0	1
org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.String)	1	int	0	0
org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.lib.Interns$Info$1:expireKey1At(int)	0	int	0	1
org.apache.hadoop.metrics2.lib.Interns$Info$1:expireKey1At(int)	1	int	0	0
org.apache.hadoop.metrics2.lib.Interns$Info$1:expireKey2At(int)	0	int	0	1
org.apache.hadoop.metrics2.lib.Interns$Info$1:expireKey2At(int)	1	int	0	0
org.apache.hadoop.metrics2.lib.Interns$Tags$1:expireKey1At(int)	0	int	0	1
org.apache.hadoop.metrics2.lib.Interns$Tags$1:expireKey1At(int)	1	int	0	0
org.apache.hadoop.metrics2.lib.Interns$Tags$1:expireKey2At(int)	0	int	0	1
org.apache.hadoop.metrics2.lib.Interns$Tags$1:expireKey2At(int)	1	int	0	0
org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric)	0	null	0	null
org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric)	0	null	0	null
org.apache.hadoop.metrics2.lib.MetricsInfoImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.lib.MetricsInfoImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.lib.MethodMetric:isLong(java.lang.Class)	0	int	0	1
org.apache.hadoop.metrics2.lib.MethodMetric:isLong(java.lang.Class)	1	int	0	0
org.apache.hadoop.metrics2.lib.MethodMetric:isFloat(java.lang.Class)	0	int	0	1
org.apache.hadoop.metrics2.lib.MethodMetric:isFloat(java.lang.Class)	1	int	0	0
org.apache.hadoop.metrics2.lib.MethodMetric:isDouble(java.lang.Class)	0	int	0	1
org.apache.hadoop.metrics2.lib.MethodMetric:isDouble(java.lang.Class)	1	int	0	0
org.apache.hadoop.metrics2.util.MetricsCache:get(java.lang.String,java.util.Collection)	0	null	0	null
org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.util.Quantile:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.util.Quantile:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot()	0	null	0	null
org.apache.hadoop.metrics2.util.SampleQuantiles:toString()	0	java.lang.String	0	[no samples]
org.apache.hadoop.metrics2.util.SampleStat:mean()	0	double	0	0.0
org.apache.hadoop.metrics2.util.SampleStat:variance()	0	double	0	0.0
org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)	0	int	0	0
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)	0	int	0	1
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)	1	int	0	0
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)	0	int	0	0
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)	1	int	0	1
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getHostname()	0	java.lang.String	0	localhost
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown()	0	int	0	1
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown()	1	int	0	0
org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String)	0	null	0	null
org.apache.hadoop.metrics2.impl.MetricsConfig:getClassName(java.lang.String)	0	null	0	null
org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String)	0	null	0	null
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:waitTillNotified(long)	0	int	0	0
org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context()	0	java.lang.String	0	default
org.apache.hadoop.metrics2.impl.SinkQueue:enqueue(java.lang.Object)	0	int	0	0
org.apache.hadoop.metrics2.impl.SinkQueue:enqueue(java.lang.Object)	1	int	0	1
org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:isConnected()	0	int	0	1
org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:isConnected()	1	int	0	0
org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:tooManyConnectionFailures()	0	int	0	1
org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:tooManyConnectionFailures()	1	int	0	0
org.apache.hadoop.net.NetUtils:getAllStaticResolutions()	0	null	0	null
org.apache.hadoop.net.NetUtils:getHostNameOfIP(java.lang.String)	0	null	0	null
org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String)	0	null	0	null
org.apache.hadoop.net.NetUtils:quoteHost(java.lang.String)	0	java.lang.String	0	(unknown)
org.apache.hadoop.net.NetUtils:isValidSubnet(java.lang.String)	0	int	0	1
org.apache.hadoop.net.NetUtils:isValidSubnet(java.lang.String)	1	int	0	0
org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isRack()	0	int	0	0
org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isRack()	1	int	0	1
org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isNodeGroup()	0	int	0	1
org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isNodeGroup()	1	int	0	0
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)	0	null	0	null
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:isSingleSwitch()	0	int	0	1
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:isSingleSwitch()	1	int	0	0
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:toString()	0	java.lang.String	0	no script
org.apache.hadoop.net.NetworkTopology:contains(org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	1	int	0	2147483647
org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	1	int	0	2147483647
org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:isNodeGroupAware()	0	int	0	0
org.apache.hadoop.net.NetworkTopology:isOnSameNodeGroup(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:isSameParents(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	1
org.apache.hadoop.net.NetworkTopology:isSameParents(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	1	int	0	0
org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)	0	null	0	null
org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)	0	null	0	null
org.apache.hadoop.net.NetworkTopology:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopology:normalizeNetworkLocationPath(java.lang.String)	0	java.lang.String	0	/
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitch()	0	int	0	0
org.apache.hadoop.net.AbstractDNSToSwitchMapping:getSwitchMap()	0	null	0	null
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy()	0	int	0	1
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy()	1	int	0	0
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)	0	int	0	1
org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)	1	int	0	0
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)	0	int	0	1
org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)	1	int	0	0
org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket)	0	int	0	0
org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket)	1	int	0	1
org.apache.hadoop.net.SocksSocketFactory:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.net.SocksSocketFactory:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.net.TableMapping$RawTableMapping:load()	0	null	0	null
org.apache.hadoop.net.InnerNodeImpl:isRack()	0	int	0	1
org.apache.hadoop.net.InnerNodeImpl:isRack()	1	int	0	0
org.apache.hadoop.net.InnerNodeImpl:isAncestor(org.apache.hadoop.net.Node)	0	int	0	1
org.apache.hadoop.net.InnerNodeImpl:isAncestor(org.apache.hadoop.net.Node)	1	int	0	0
org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node)	1	int	0	1
org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node)	0	int	0	1
org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node)	1	int	0	0
org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)	0	null	0	null
org.apache.hadoop.net.NodeBase:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.net.NodeBase:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.net.NodeBase:normalize(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.net.StandardSocketFactory:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.net.StandardSocketFactory:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.net.CachedDNSToSwitchMapping:getCachedHosts(java.util.List)	0	null	0	null
org.apache.hadoop.net.SocketIOWithTimeout:isOpen()	0	int	0	1
org.apache.hadoop.net.SocketIOWithTimeout:isOpen()	1	int	0	0
org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)	0	int	0	-1
org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)	1	int	0	0
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameNodeGroup(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isNodeGroupAware()	0	int	0	1
org.apache.hadoop.fs.FSProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.fs.LocalFileSystem:getScheme()	0	java.lang.String	0	file
org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)	0	int	0	0
org.apache.hadoop.fs.LocalFileSystem:supportsSymlinks()	0	int	0	1
org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.FileContext$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$7:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto$Builder:hasMtime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto$Builder:hasMtime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.fs.QuotaUsage:getTypeQuota(org.apache.hadoop.fs.StorageType)	0	long	0	-1
org.apache.hadoop.fs.QuotaUsage:getTypeConsumed(org.apache.hadoop.fs.StorageType)	0	long	0	0
org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet()	0	int	0	1
org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet()	1	int	0	0
org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable()	0	int	0	1
org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable()	1	int	0	0
org.apache.hadoop.fs.QuotaUsage:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.QuotaUsage:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.shell.PathData:checkIfSchemeInferredFromPath(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.shell.PathData:checkIfSchemeInferredFromPath(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.shell.PathData:representsDirectory()	0	int	0	1
org.apache.hadoop.fs.shell.PathData:representsDirectory()	1	int	0	0
org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)	0	java.lang.String	0	.
org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.shell.Ls:isSorted()	0	int	0	1
org.apache.hadoop.fs.shell.Ls:isSorted()	1	int	0	0
org.apache.hadoop.fs.shell.Ls:getListingGroupSize()	0	int	0	0
org.apache.hadoop.fs.shell.Ls:getListingGroupSize()	1	int	0	100
org.apache.hadoop.fs.shell.Ls$Lsr:getReplacementCommand()	0	java.lang.String	0	ls -R
org.apache.hadoop.fs.shell.CommandFormat:getOpt(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.shell.FsUsage$Dus:getReplacementCommand()	0	java.lang.String	0	du -s
org.apache.hadoop.fs.shell.Display$AvroFileInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList)	0	int	0	1
org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList)	1	int	0	0
org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList)	0	int	0	1
org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList)	1	int	0	0
org.apache.hadoop.fs.shell.Command:run(java.lang.String[])	0	int	0	130
org.apache.hadoop.fs.shell.Command:exitCodeForError()	0	int	0	1
org.apache.hadoop.fs.shell.Command:isSorted()	0	int	0	0
org.apache.hadoop.fs.shell.Command:getListingGroupSize()	0	int	0	0
org.apache.hadoop.fs.shell.Command:isDeprecated()	0	int	0	1
org.apache.hadoop.fs.shell.Command:isDeprecated()	1	int	0	0
org.apache.hadoop.fs.shell.Command:getReplacementCommand()	0	null	0	null
org.apache.hadoop.fs.shell.CopyCommands$Merge:isSorted()	0	int	0	1
org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty()	0	int	0	1
org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty()	1	int	0	0
org.apache.hadoop.fs.shell.find.Result:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.shell.find.Result:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.shell.find.FilterExpression:isAction()	0	int	0	0
org.apache.hadoop.fs.shell.find.FilterExpression:isOperator()	0	int	0	0
org.apache.hadoop.fs.shell.find.FilterExpression:getPrecedence()	0	int	0	-1
org.apache.hadoop.fs.shell.find.Print:isAction()	0	int	0	1
org.apache.hadoop.fs.shell.find.BaseExpression:isAction()	0	int	0	1
org.apache.hadoop.fs.shell.find.BaseExpression:isAction()	1	int	0	0
org.apache.hadoop.fs.shell.find.BaseExpression:isOperator()	0	int	0	0
org.apache.hadoop.fs.shell.find.BaseExpression:getPrecedence()	0	int	0	0
org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)	0	int	0	1
org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)	1	int	0	0
org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)	0	int	0	1
org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)	1	int	0	0
org.apache.hadoop.fs.shell.find.And:isOperator()	0	int	0	1
org.apache.hadoop.fs.shell.find.And:getPrecedence()	0	int	0	200
org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)	0	int	0	1
org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)	1	int	0	0
org.apache.hadoop.fs.shell.Delete$Rmr:getReplacementCommand()	0	java.lang.String	0	-rm -r
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.Options$Rename:valueOf(byte)	0	null	0	null
org.apache.hadoop.fs.FileContext$21:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream)	0	int	0	0
org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream)	1	int	0	1
org.apache.hadoop.fs.FileContext$27:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.StorageStatistics:getScheme()	0	null	0	null
org.apache.hadoop.fs.StorageType:supportTypeQuota()	0	int	0	1
org.apache.hadoop.fs.StorageType:supportTypeQuota()	1	int	0	0
org.apache.hadoop.fs.StorageType:isMovable()	0	int	0	1
org.apache.hadoop.fs.StorageType:isMovable()	1	int	0	0
org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.sftp.SFTPInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.sftp.SFTPInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.sftp.SFTPInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasFileType()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasFileType()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasPath()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasPath()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasLength()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasLength()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasPermission()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasPermission()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasOwner()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasOwner()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasGroup()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasGroup()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasModificationTime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasModificationTime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasAccessTime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasAccessTime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasSymlink()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasSymlink()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasBlockReplication()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasBlockReplication()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasBlockSize()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasBlockSize()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasEncryptionData()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasEncryptionData()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasEcData()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasEcData()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasFlags()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:hasFlags()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.FileContext$13:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FileSystem$3:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.TrashPolicyDefault:isEnabled()	0	int	0	1
org.apache.hadoop.fs.TrashPolicyDefault:isEnabled()	1	int	0	0
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.GlobFilter$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long)	0	int	0	1
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long)	1	int	0	0
org.apache.hadoop.fs.Path:hasWindowsDrive(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.Path:hasWindowsDrive(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)	0	int	0	3
org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)	1	int	0	2
org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)	2	int	0	0
org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull()	0	int	0	1
org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull()	1	int	0	0
org.apache.hadoop.fs.Path:isRoot()	0	int	0	1
org.apache.hadoop.fs.Path:isRoot()	1	int	0	0
org.apache.hadoop.fs.Path:getParent()	0	null	0	null
org.apache.hadoop.fs.Path:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.FileContext$31:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.ChecksumFs$1:hasNext()	0	int	0	1
org.apache.hadoop.fs.ChecksumFs$1:hasNext()	1	int	0	0
org.apache.hadoop.fs.FileContext$45:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.FSProtos$FsPermissionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.fs.FSProtos$FsPermissionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.fs.FSProtos$FsPermissionProto$Builder:hasPerm()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FsPermissionProto$Builder:hasPerm()	1	int	0	0
org.apache.hadoop.fs.FileContext$Util$2:hasNext()	0	int	0	0
org.apache.hadoop.fs.FileContext$Util$2:hasNext()	1	int	0	1
org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)	0	int	0	-1
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)	1	int	0	0
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)	2	int	0	1
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString()	0	java.lang.String	0	evaporated token renew
org.apache.hadoop.fs.BatchedRemoteIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.BatchedRemoteIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.AbstractFileSystem:isValidName(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.AbstractFileSystem:isValidName(java.lang.String)	1	int	0	1
org.apache.hadoop.fs.AbstractFileSystem:getInitialWorkingDirectory()	0	null	0	null
org.apache.hadoop.fs.AbstractFileSystem:supportsSymlinks()	0	int	0	0
org.apache.hadoop.fs.AbstractFileSystem:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:hasMtime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:hasMtime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:hasPath()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:hasPath()	1	int	0	0
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:isInitialized()	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:isInitialized()	1	int	0	0
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FSProtos$LocalFileSystemPathHandleProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.FileContext$12:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.http.AbstractHttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	0
org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.http.HttpFileSystem:getScheme()	0	java.lang.String	0	http
org.apache.hadoop.fs.http.HttpsFileSystem:getScheme()	0	java.lang.String	0	https
org.apache.hadoop.fs.FileContext$30:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.EmptyStorageStatistics:getLong(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.EmptyStorageStatistics:isTracked(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.FileSystem:getDefaultPort()	0	int	0	0
org.apache.hadoop.fs.FileSystem:getDelegationToken(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.FileSystem:getChildFileSystems()	0	null	0	null
org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)	0	null	0	null
org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.FileSystem:setReplication(org.apache.hadoop.fs.Path,short)	0	int	0	1
org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.FileSystem:getInitialWorkingDirectory()	0	null	0	null
org.apache.hadoop.fs.FileSystem:getDefaultReplication()	0	int	0	1
org.apache.hadoop.fs.FileSystem:supportsSymlinks()	0	int	0	0
org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)	0	null	0	null
org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FileContext$42:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.permission.AclStatus:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.permission.AclStatus:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List)	0	int	0	-1
org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.permission.FsAction:implies(org.apache.hadoop.fs.permission.FsAction)	0	int	0	1
org.apache.hadoop.fs.permission.FsAction:implies(org.apache.hadoop.fs.permission.FsAction)	1	int	0	0
org.apache.hadoop.fs.permission.AclUtil:isMinimalAcl(java.util.List)	0	int	0	1
org.apache.hadoop.fs.permission.AclUtil:isMinimalAcl(java.util.List)	1	int	0	0
org.apache.hadoop.fs.permission.FsPermission:getMasked()	0	null	0	null
org.apache.hadoop.fs.permission.FsPermission:getUnmasked()	0	null	0	null
org.apache.hadoop.fs.permission.FsPermission:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.permission.FsPermission:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.permission.FsPermission:getAclBit()	0	int	0	0
org.apache.hadoop.fs.permission.FsPermission:getEncryptedBit()	0	int	0	0
org.apache.hadoop.fs.permission.FsPermission:getErasureCodedBit()	0	int	0	0
org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.permission.AclEntry:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.permission.AclEntry:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem$3:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset)	0	null	0	null
org.apache.hadoop.fs.GlobExpander:leftmostOuterCurlyContainingSlash(java.lang.String,int)	0	int	0	-1
org.apache.hadoop.fs.FileSystemStorageStatistics:isTracked(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.FileSystemStorageStatistics:isTracked(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.FileChecksum:getChecksumOpt()	0	null	0	null
org.apache.hadoop.fs.FileChecksum:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FileChecksum:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.FileContext$41:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FileContext$34:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FileContext$10:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.ChecksumFileSystem$2:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasFileType()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasFileType()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasGroup()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasGroup()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasModificationTime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasModificationTime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasAccessTime()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasAccessTime()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasSymlink()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasSymlink()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasBlockReplication()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasBlockReplication()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasBlockSize()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasBlockSize()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasEncryptionData()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasEncryptionData()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasEcData()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasEcData()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasFlags()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Builder:hasFlags()	1	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$9:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem$9:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.HarFileSystem$LruCache:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.fs.HarFileSystem$LruCache:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])	0	null	0	null
org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)	0	int	0	1
org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)	1	int	0	0
org.apache.hadoop.fs.FileUtil:readLink(java.io.File)	0	java.lang.String	0	
org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean)	0	int	0	0
org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean)	1	int	0	1
org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.fs.FileUtil:getDU(java.io.File)	0	long	0	0
org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)	0	int	0	1
org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)	1	int	0	0
org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)	0	int	0	1
org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)	1	int	0	0
org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)	0	int	0	1
org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)	1	int	0	0
org.apache.hadoop.fs.FileUtil:canRead(java.io.File)	0	int	0	0
org.apache.hadoop.fs.FileUtil:canWrite(java.io.File)	0	int	0	0
org.apache.hadoop.fs.FileUtil:canExecute(java.io.File)	0	int	0	0
org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)	0	int	0	0
org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)	1	int	0	1
org.apache.hadoop.fs.FileContext$11:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.local.RawLocalFs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.local.RawLocalFs:isValidName(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)	0	int	0	1
org.apache.hadoop.fs.FileSystem$Statistics$1:aggregate()	0	null	0	null
org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.ChecksumFs:exists(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFs:exists(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)	0	int	0	0
org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)	1	int	0	1
org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.ChecksumFs:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)	0	int	0	0
org.apache.hadoop.fs.FileContext$16:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.FSProtos$FsPermissionProto:hasPerm()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FsPermissionProto:hasPerm()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FsPermissionProto:isInitialized()	0	int	0	1
org.apache.hadoop.fs.FSProtos$FsPermissionProto:isInitialized()	1	int	0	0
org.apache.hadoop.fs.FSProtos$FsPermissionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FSProtos$FsPermissionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long)	0	long	0	0
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)	0	int	0	-1
org.apache.hadoop.fs.Globber:doGlob()	0	null	0	null
org.apache.hadoop.fs.FileStatus:isFile()	0	int	0	1
org.apache.hadoop.fs.FileStatus:isFile()	1	int	0	0
org.apache.hadoop.fs.FileStatus:isSymlink()	0	int	0	1
org.apache.hadoop.fs.FileStatus:isSymlink()	1	int	0	0
org.apache.hadoop.fs.FileStatus:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.FileStatus:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.FSInputChecker:needChecksum()	0	int	0	1
org.apache.hadoop.fs.FSInputChecker:needChecksum()	1	int	0	0
org.apache.hadoop.fs.FSInputChecker:read()	0	int	0	-1
org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)	0	int	0	-1
org.apache.hadoop.fs.FSInputChecker:skip(long)	0	long	0	0
org.apache.hadoop.fs.FSInputChecker:markSupported()	0	int	0	0
org.apache.hadoop.fs.FileContext$29:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.PositionedReadable:minSeekForVectorReads()	0	int	0	4096
org.apache.hadoop.fs.PositionedReadable:maxReadSizeForVectorReads()	0	int	0	1048576
org.apache.hadoop.fs.FileSystem$4:hasNext()	0	int	0	1
org.apache.hadoop.fs.FileSystem$4:hasNext()	1	int	0	0
org.apache.hadoop.fs.UnionStorageStatistics:isTracked(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.UnionStorageStatistics:isTracked(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.HarFs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.FileContext$32:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.DUHelper:getFileSize(java.io.File)	0	long	0	0
org.apache.hadoop.fs.FileSystem$5:hasNext()	0	int	0	0
org.apache.hadoop.fs.FileSystem$5:hasNext()	1	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$6:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ftp.FTPInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.ftp.FtpFs:getUriDefaultPort()	0	int	0	21
org.apache.hadoop.fs.ftp.FTPFileSystem:getScheme()	0	java.lang.String	0	ftp
org.apache.hadoop.fs.ftp.FTPFileSystem:getDefaultPort()	0	int	0	21
org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.AbstractFileSystem$1:hasNext()	0	int	0	1
org.apache.hadoop.fs.AbstractFileSystem$1:hasNext()	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:hasFile()	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:hasFile()	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$DataBlock:hasData()	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$DataBlock:hasData()	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload()	0	null	0	null
org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState()	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState()	1	int	0	0
org.apache.hadoop.fs.store.EtagChecksum:getAlgorithmName()	0	java.lang.String	0	etag
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:markSupported()	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:remainingCapacity()	0	int	0	0
org.apache.hadoop.fs.store.audit.AuditSpan:isValidSpan()	0	int	0	1
org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:lambda$buildHttpReferrer$1(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:lambda$buildHttpReferrer$1(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.fs.viewfs.InodeTree$INode:isLink()	0	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree$INode:isLink()	1	int	0	0
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem()	0	null	0	null
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:supportsSymlinks()	0	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree:breakIntoPathComponents(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.viewfs.InodeTree:hasFallbackLink()	0	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree:hasFallbackLink()	1	int	0	0
org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getInitialWorkingDirectory()	0	null	0	null
org.apache.hadoop.fs.viewfs.ViewFileSystem:getScheme()	0	java.lang.String	0	viewfs
org.apache.hadoop.fs.viewfs.ViewFileSystem:supportAutoAddingFallbackOnNoMounts()	0	int	0	0
org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)	0	int	0	0
org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)	1	int	0	1
org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)	2	int	0	-1
org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isInternalDir()	0	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isInternalDir()	1	int	0	0
org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:isLinkType(org.apache.hadoop.fs.viewfs.InodeTree$LinkType)	0	int	0	1
org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:isLinkType(org.apache.hadoop.fs.viewfs.InodeTree$LinkType)	1	int	0	0
org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.fs.viewfs.ChRootedFs:getInitialWorkingDirectory()	0	null	0	null
org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:isInternalDir()	0	int	0	0
org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.fs.viewfs.NflyFSystem:getRack(java.lang.String)	0	java.lang.String	0	/default-rack
org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)	0	null	0	null
org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isInternalDir()	0	int	0	1
org.apache.hadoop.fs.viewfs.ViewFs:getType()	0	java.lang.String	0	viewfs
org.apache.hadoop.fs.viewfs.ViewFs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.viewfs.ViewFs:supportsSymlinks()	0	int	0	1
org.apache.hadoop.fs.viewfs.ViewFs:isValidName(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)	0	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)	1	int	0	1
org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString()	0	java.lang.String	0	()
org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object)	0	java.lang.String	0	
org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics)	0	java.lang.String	0	
org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics)	0	java.lang.String	0	
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$3(java.lang.Long)	0	int	0	1
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$3(java.lang.Long)	1	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$2(java.lang.Long)	0	int	0	1
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$2(java.lang.Long)	1	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$1(java.lang.Long)	0	int	0	1
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$1(java.lang.Long)	1	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$0(java.lang.Long)	0	int	0	1
org.apache.hadoop.fs.statistics.IOStatisticsLogging:lambda$ioStatisticsToPrettyString$0(java.lang.Long)	1	int	0	0
org.apache.hadoop.fs.statistics.MeanStatistic:isEmpty()	0	int	0	1
org.apache.hadoop.fs.statistics.MeanStatistic:isEmpty()	1	int	0	0
org.apache.hadoop.fs.statistics.MeanStatistic:mean()	0	double	0	0.0
org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsSource:getIOStatistics()	0	null	0	null
org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incAtomicLong(java.util.concurrent.atomic.AtomicLong,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)	0	int	0	0
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)	1	int	0	1
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getThreadSpecificIOStatisticsContext(long)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)	0	int	0	0
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementCounter(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementGauge(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMaximum(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMinimum(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getCounterReference(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMaximumReference(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMinimumReference(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getGaugeReference(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMeanStatistic(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getID()	0	long	0	0
org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString()	0	java.lang.String	0	()
org.apache.hadoop.fs.FileContext$4:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.HarFileSystem:getScheme()	0	java.lang.String	0	har
org.apache.hadoop.fs.HarFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)	0	null	0	null
org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getLength()	0	int	0	28
org.apache.hadoop.fs.FSOutputSummer:createWriteTraceScope()	0	null	0	null
org.apache.hadoop.fs.FSOutputSummer:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.BufferedFSInputStream:skip(long)	0	long	0	0
org.apache.hadoop.fs.BufferedFSInputStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)	1	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)	1	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	1	int	0	0
org.apache.hadoop.fs.RawLocalFileSystem:toString()	0	java.lang.String	0	LocalFS
org.apache.hadoop.fs.RawLocalFileSystem:supportsSymlinks()	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem$5:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.FileContext$44:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.Stat:isAvailable()	0	int	0	1
org.apache.hadoop.fs.Stat:isAvailable()	1	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$1:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long)	0	int	0	1
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long)	1	int	0	0
org.apache.hadoop.fs.FileContext$43:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.ChecksumFileSystem$4:apply(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.impl.FileSystemMultipartUploader:lambda$abort$4(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:get()	0	null	0	null
org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int)	0	int	0	0
org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int)	1	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int)	0	int	0	0
org.apache.hadoop.fs.impl.prefetch.BlockData:isValidOffset(long)	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockData:isValidOffset(long)	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit)	0	int	0	0
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long)	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long)	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long)	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long)	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.FilePosition:isValid()	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.FilePosition:isValid()	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead()	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead()	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData)	0	int	0	0
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData)	1	int	0	1
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get()	0	null	0	null
org.apache.hadoop.fs.impl.prefetch.BufferData:stateEqualsOneOf(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.BufferData:stateEqualsOneOf(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])	1	int	0	0
org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)	0	java.lang.String	0	--
org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)	1	java.lang.String	0	done
org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)	2	java.lang.String	0	not done
org.apache.hadoop.fs.impl.prefetch.BufferData:getBufferStr(java.nio.ByteBuffer)	0	java.lang.String	0	--
org.apache.hadoop.fs.impl.prefetch.Retryer:continueRetry()	0	int	0	0
org.apache.hadoop.fs.impl.prefetch.Retryer:continueRetry()	1	int	0	1
org.apache.hadoop.fs.impl.prefetch.Retryer:updateStatus()	0	int	0	1
org.apache.hadoop.fs.impl.prefetch.Retryer:updateStatus()	1	int	0	0
org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)	0	int	0	0
org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)	1	int	0	1
org.apache.hadoop.fs.impl.StoreImplementationUtils:isProbeForSyncable(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.impl.StoreImplementationUtils:isProbeForSyncable(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.impl.StoreImplementationUtils:objectHasCapability(java.lang.Object,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.FileContext$38:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.LocalFileSystemPathHandle:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.LocalFileSystemPathHandle:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.FileContext$28:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.CompositeCrcFileChecksum:getLength()	0	int	0	4
org.apache.hadoop.fs.FileSystem$Cache$Key:isEqual(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FileSystem$Cache$Key:isEqual(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.BlockLocation:isStriped()	0	int	0	0
org.apache.hadoop.fs.VectoredReadUtils:isOrderedDisjoint(java.util.List,int,int)	0	int	0	0
org.apache.hadoop.fs.VectoredReadUtils:isOrderedDisjoint(java.util.List,int,int)	1	int	0	1
org.apache.hadoop.fs.VectoredReadUtils:lambda$readNonByteBufferPositionedReadable$0(org.apache.hadoop.fs.PositionedReadable,java.lang.Integer,byte[],java.lang.Integer,java.lang.Integer)	0	null	0	null
org.apache.hadoop.fs.GlobalStorageStatistics:get(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem)	0	int	0	-1
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded()	0	int	0	1
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded()	1	int	0	0
org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.FileSystem$Statistics$10:aggregate()	0	null	0	null
org.apache.hadoop.fs.FsShell:getUsagePrefix()	0	java.lang.String	0	Usage: hadoop fs [generic options]
org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:getIter()	0	null	0	null
org.apache.hadoop.fs.GetSpaceUsed$Builder:getInitialUsed()	0	long	0	-1
org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter()	0	long	0	60000
org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion)	0	int	0	0
org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion)	1	int	0	1
org.apache.hadoop.crypto.CryptoStreamUtils:getInputStreamOffset(java.io.InputStream)	0	long	0	0
org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[])	0	int	0	1
org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[])	1	int	0	0
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)	0	null	0	null
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getCanonicalServiceName()	0	null	0	null
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getDelegationToken(java.lang.String)	0	null	0	null
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:renewDelegationToken(org.apache.hadoop.security.token.Token)	0	long	0	0
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:cancelDelegationToken(org.apache.hadoop.security.token.Token)	0	null	0	null
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:selectDelegationToken(org.apache.hadoop.security.Credentials)	0	null	0	null
org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getFormat()	0	java.lang.String	0	KeyMetadata
org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:getUsage()	0	java.lang.String	1	aW52YWxpZGF0ZUNhY2hlIDxrZXluYW1lPiBbLXByb3ZpZGVyIDxwcm92aWRlcj5dIFstaGVscF06CgpUaGUgaW52YWxpZGF0ZUNhY2hlIHN1YmNvbW1hbmQgaW52YWxpZGF0ZXMgdGhlIGNhY2hlZCBrZXkgdmVyc2lvbnMKb2YgdGhlIHNwZWNpZmllZCBrZXksIG9uIHRoZSBwcm92aWRlciBpbmRpY2F0ZWQgdXNpbmcgdGhlIC1wcm92aWRlciBhcmd1bWVudC4K
org.apache.hadoop.crypto.key.KeyShell$CreateCommand:getUsage()	0	java.lang.String	1	Y3JlYXRlIDxrZXluYW1lPiBbLWNpcGhlciA8Y2lwaGVyPl0gWy1zaXplIDxzaXplPl0KICAgICAgICAgICAgICAgICAgICAgWy1kZXNjcmlwdGlvbiA8ZGVzY3JpcHRpb24+XQogICAgICAgICAgICAgICAgICAgICBbLWF0dHIgPGF0dHJpYnV0ZT12YWx1ZT5dCiAgICAgICAgICAgICAgICAgICAgIFstcHJvdmlkZXIgPHByb3ZpZGVyPl0gWy1zdHJpY3RdCiAgICAgICAgICAgICAgICAgICAgIFstaGVscF06CgpUaGUgY3JlYXRlIHN1YmNvbW1hbmQgY3JlYXRlcyBhIG5ldyBrZXkgZm9yIHRoZSBuYW1lIHNwZWNpZmllZApieSB0aGUgPGtleW5hbWU+IGFyZ3VtZW50IHdpdGhpbiB0aGUgcHJvdmlkZXIgc3BlY2lmaWVkIGJ5IHRoZQotcHJvdmlkZXIgYXJndW1lbnQuIFlvdSBtYXkgc3BlY2lmeSBhIGNpcGhlciB3aXRoIHRoZSAtY2lwaGVyCmFyZ3VtZW50LiBUaGUgZGVmYXVsdCBjaXBoZXIgaXMgY3VycmVudGx5ICJBRVMvQ1RSL05vUGFkZGluZyIuClRoZSBkZWZhdWx0IGtleXNpemUgaXMgMTI4LiBZb3UgbWF5IHNwZWNpZnkgdGhlIHJlcXVlc3RlZCBrZXkKbGVuZ3RoIHVzaW5nIHRoZSAtc2l6ZSBhcmd1bWVudC4gQXJiaXRyYXJ5IGF0dHJpYnV0ZT12YWx1ZQpzdHlsZSBhdHRyaWJ1dGVzIG1heSBiZSBzcGVjaWZpZWQgdXNpbmcgdGhlIC1hdHRyIGFyZ3VtZW50LgotYXR0ciBtYXkgYmUgc3BlY2lmaWVkIG11bHRpcGxlIHRpbWVzLCBvbmNlIHBlciBhdHRyaWJ1dGUuCg==
org.apache.hadoop.crypto.key.KeyShell$RollCommand:getUsage()	0	java.lang.String	1	cm9sbCA8a2V5bmFtZT4gWy1wcm92aWRlciA8cHJvdmlkZXI+XSBbLXN0cmljdF0gWy1oZWxwXToKClRoZSByb2xsIHN1YmNvbW1hbmQgY3JlYXRlcyBhIG5ldyB2ZXJzaW9uIGZvciB0aGUgc3BlY2lmaWVkIGtleQp3aXRoaW4gdGhlIHByb3ZpZGVyIGluZGljYXRlZCB1c2luZyB0aGUgLXByb3ZpZGVyIGFyZ3VtZW50LgpJZiAtc3RyaWN0IGlzIHN1cHBsaWVkLCBmYWlsIGltbWVkaWF0ZWx5IGlmIHRoZSBwcm92aWRlciByZXF1aXJlcwphIHBhc3N3b3JkIGFuZCBub25lIGlzIGdpdmVuLg==
org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$16:call(org.apache.hadoop.crypto.key.kms.KMSClientProvider)	0	null	0	null
org.apache.hadoop.crypto.key.kms.KMSClientProvider$5:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$3:call(org.apache.hadoop.crypto.key.kms.KMSClientProvider)	0	null	0	null
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$7:call(org.apache.hadoop.crypto.key.kms.KMSClientProvider)	0	null	0	null
org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation)	0	int	0	1
org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation)	1	int	0	0
org.apache.hadoop.crypto.key.KeyProvider:isTransient()	0	int	0	0
org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String)	0	null	0	null
org.apache.hadoop.crypto.key.KeyProvider:needsPassword()	0	int	0	0
org.apache.hadoop.crypto.key.KeyProvider:noPasswordWarning()	0	null	0	null
org.apache.hadoop.crypto.key.KeyProvider:noPasswordError()	0	null	0	null
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:isBadorWrongPassword(java.io.IOException)	0	int	0	1
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:isBadorWrongPassword(java.io.IOException)	1	int	0	0
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()	0	int	0	1
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()	1	int	0	0
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()	0	int	0	0
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()	1	int	0	1
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:getUsage()	0	java.lang.String	1	ZGVsZXRlIDxrZXluYW1lPiBbLXByb3ZpZGVyIDxwcm92aWRlcj5dIFstc3RyaWN0XSBbLWZdIFstaGVscF06CgpUaGUgZGVsZXRlIHN1YmNvbW1hbmQgZGVsZXRlcyBhbGwgdmVyc2lvbnMgb2YgdGhlIGtleQpzcGVjaWZpZWQgYnkgdGhlIDxrZXluYW1lPiBhcmd1bWVudCBmcm9tIHdpdGhpbiB0aGUKcHJvdmlkZXIgc3BlY2lmaWVkIGJ5IC1wcm92aWRlci4gVGhlIGNvbW1hbmQgYXNrcyBmb3IKdXNlciBjb25maXJtYXRpb24gdW5sZXNzIC1mIGlzIHNwZWNpZmllZC4gSWYgLXN0cmljdCBpcwpzdXBwbGllZCwgZmFpbCBpbW1lZGlhdGVseSBpZiB0aGUgcHJvdmlkZXIgcmVxdWlyZXMgYQpwYXNzd29yZCBhbmQgbm9uZSBpcyBnaXZlbi4=
org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.crypto.key.UserProvider:isTransient()	0	int	0	1
org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String)	0	null	0	null
org.apache.hadoop.crypto.key.UserProvider:toString()	0	java.lang.String	0	user:///
org.apache.hadoop.crypto.key.KeyShell$ListCommand:getUsage()	0	java.lang.String	1	bGlzdCBbLXByb3ZpZGVyIDxwcm92aWRlcj5dIFstc3RyaWN0XSBbLW1ldGFkYXRhXSBbLWhlbHBdOgoKVGhlIGxpc3Qgc3ViY29tbWFuZCBkaXNwbGF5cyB0aGUga2V5bmFtZXMgY29udGFpbmVkIHdpdGhpbgphIHBhcnRpY3VsYXIgcHJvdmlkZXIgYXMgY29uZmlndXJlZCBpbiBjb3JlLXNpdGUueG1sIG9yCnNwZWNpZmllZCB3aXRoIHRoZSAtcHJvdmlkZXIgYXJndW1lbnQuIC1tZXRhZGF0YSBkaXNwbGF5cwp0aGUgbWV0YWRhdGEuIElmIC1zdHJpY3QgaXMgc3VwcGxpZWQsIGZhaWwgaW1tZWRpYXRlbHkgaWYKdGhlIHByb3ZpZGVyIHJlcXVpcmVzIGEgcGFzc3dvcmQgYW5kIG5vbmUgaXMgZ2l2ZW4u
org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)	0	null	0	null
org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)	0	null	0	null
org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.crypto.CryptoInputStream:skip(long)	0	long	0	0
org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.crypto.CryptoInputStream:markSupported()	0	int	0	0
org.apache.hadoop.crypto.CryptoInputStream:read()	0	int	0	-1
org.apache.hadoop.crypto.CryptoInputStream:isOpen()	0	int	0	1
org.apache.hadoop.crypto.CryptoInputStream:isOpen()	1	int	0	0
org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.io.DataInputBuffer$Buffer:read()	0	int	0	-1
org.apache.hadoop.io.DataInputBuffer$Buffer:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.DataInputBuffer$Buffer:read(byte[],int,int)	1	int	0	0
org.apache.hadoop.io.DataInputBuffer$Buffer:skip(long)	0	long	0	0
org.apache.hadoop.io.VLongWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.VLongWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)	0	int	0	-1
org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)	1	int	0	0
org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)	2	int	0	1
org.apache.hadoop.io.LongWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.LongWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)	0	int	0	-1
org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)	1	int	0	0
org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)	2	int	0	1
org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration)	0	int	0	1048576
org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable)	0	int	0	-1
org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable)	0	int	0	-1
org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey()	0	null	0	null
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:isSupported()	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long)	0	null	0	null
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)	0	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance()	0	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance()	1	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd()	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd()	1	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)	0	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)	1	int	0	1
org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)	0	int	0	-1
org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)	1	int	0	1
org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)	2	int	0	0
org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read()	0	int	0	-1
org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:markSupported()	0	int	0	1
org.apache.hadoop.io.file.tfile.Utils$Version:size()	0	int	0	4
org.apache.hadoop.io.file.tfile.Utils$Version:compatibleWith(org.apache.hadoop.io.file.tfile.Utils$Version)	0	int	0	1
org.apache.hadoop.io.file.tfile.Utils$Version:compatibleWith(org.apache.hadoop.io.file.tfile.Utils$Version)	1	int	0	0
org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF()	0	int	0	1
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF()	1	int	0	0
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read()	0	int	0	-1
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long)	0	long	0	0
org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:markSupported()	0	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)	1	int	0	-1
org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)	2	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Location:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Location:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:isValueLengthKnown()	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:isValueLengthKnown()	1	int	0	0
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getCodec()	0	null	0	null
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:isSupported()	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String)	0	null	0	null
org.apache.hadoop.io.file.tfile.TFile$TFileMeta:isSorted()	0	int	0	1
org.apache.hadoop.io.file.tfile.TFile$TFileMeta:isSorted()	1	int	0	0
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getLzoCodecClass()	0	java.lang.String	0	org.apache.hadoop.io.compress.LzoCodec
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported()	0	int	0	1
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported()	1	int	0	0
org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:offset()	0	int	0	0
org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long)	0	int	0	-1
org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List)	0	null	0	null
org.apache.hadoop.io.VIntWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.VIntWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)	0	int	0	-1
org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)	1	int	0	0
org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)	2	int	0	1
org.apache.hadoop.io.MapFile$Reader:midKey()	0	null	0	null
org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)	0	int	0	1
org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)	1	int	0	0
org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)	0	int	0	1
org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)	0	null	0	null
org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:lambda$getBuffer$0(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:lambda$getBuffer$0(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.io.FloatWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.FloatWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)	1	int	0	0
org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)	2	int	0	1
org.apache.hadoop.io.DoubleWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.DoubleWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean)	0	int	0	0
org.apache.hadoop.io.IntWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.IntWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)	0	int	0	-1
org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)	1	int	0	0
org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)	2	int	0	1
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:lessThanUnsigned(long,long)	0	int	0	1
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:lessThanUnsigned(long,long)	1	int	0	0
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)	0	int	0	0
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)	1	int	0	-1
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)	2	int	0	1
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer:compareTo(byte[],int,int,byte[],int,int)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Writer:isCompressed()	0	int	0	1
org.apache.hadoop.io.SequenceFile$Writer:isCompressed()	1	int	0	0
org.apache.hadoop.io.SequenceFile$Writer:isBlockCompressed()	0	int	0	1
org.apache.hadoop.io.SequenceFile$Writer:isBlockCompressed()	1	int	0	0
org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata)	1	int	0	1
org.apache.hadoop.io.SequenceFile$Metadata:hashCode()	0	int	0	42
org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable)	0	int	0	1
org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)	0	null	0	null
org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)	0	null	0	null
org.apache.hadoop.io.WritableUtils:readCompressedByteArray(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])	0	int	0	0
org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])	1	int	0	-1
org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.WritableUtils:readString(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)	0	null	0	null
org.apache.hadoop.io.WritableUtils:isNegativeVInt(byte)	0	int	0	1
org.apache.hadoop.io.WritableUtils:isNegativeVInt(byte)	1	int	0	0
org.apache.hadoop.io.WritableUtils:decodeVIntSize(byte)	0	int	0	1
org.apache.hadoop.io.WritableUtils:getVIntSize(long)	0	int	0	1
org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)	1	int	0	0
org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)	2	int	0	1
org.apache.hadoop.io.ShortWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.ShortWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)	0	int	0	-1
org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)	1	int	0	0
org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)	2	int	0	1
org.apache.hadoop.io.NullWritable:toString()	0	java.lang.String	0	(null)
org.apache.hadoop.io.NullWritable:hashCode()	0	int	0	0
org.apache.hadoop.io.NullWritable:compareTo(org.apache.hadoop.io.NullWritable)	0	int	0	0
org.apache.hadoop.io.MapWritable:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.MapWritable:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.UTF8:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.UTF8:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable)	0	int	0	0
org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.ArrayPrimitiveWritable:isDeclaredComponentType(java.lang.Class)	0	int	0	1
org.apache.hadoop.io.ArrayPrimitiveWritable:isDeclaredComponentType(java.lang.Class)	1	int	0	0
org.apache.hadoop.io.ByteWritable$Comparator:compare(byte[],int,int,byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.ByteWritable$Comparator:compare(byte[],int,int,byte[],int,int)	1	int	0	0
org.apache.hadoop.io.ByteWritable$Comparator:compare(byte[],int,int,byte[],int,int)	2	int	0	1
org.apache.hadoop.io.AbstractMapWritable:getId(java.lang.Class)	0	int	0	-1
org.apache.hadoop.io.SortedMapWritable:comparator()	0	null	0	null
org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception)	0	int	0	0
org.apache.hadoop.io.retry.RetryPolicies:isSaslFailure(java.lang.Exception)	0	int	0	1
org.apache.hadoop.io.retry.RetryPolicies:isSaslFailure(java.lang.Exception)	1	int	0	0
org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception)	0	null	0	null
org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:proxyName()	0	java.lang.String	0	UnknownProxy
org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])	0	null	0	null
org.apache.hadoop.io.retry.RetryInvocationHandler:isRpcInvocation(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.retry.RetryInvocationHandler$Counters:isZeros()	0	int	0	1
org.apache.hadoop.io.retry.RetryInvocationHandler$Counters:isZeros()	1	int	0	0
org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone()	0	int	0	1
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone()	1	int	0	0
org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:searchPair(int)	0	null	0	null
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String)	0	null	0	null
org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String)	0	int	0	-1
org.apache.hadoop.io.retry.AsyncCallHandler:getGracePeriod()	0	long	0	3000
org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getWaitTime(long)	0	null	0	null
org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:getFailoverOrRetrySleepTime(int)	0	long	0	0
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:isRunning(org.apache.hadoop.util.Daemon)	0	int	0	1
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:isRunning(org.apache.hadoop.util.Daemon)	1	int	0	0
org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long)	0	int	0	1
org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long)	1	int	0	0
org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFailover()	0	int	0	1
org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFailover()	1	int	0	0
org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFail()	0	int	0	1
org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFail()	1	int	0	0
org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:idempotentOrAtMostOnce(java.lang.reflect.Method)	0	int	0	1
org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:idempotentOrAtMostOnce(java.lang.reflect.Method)	1	int	0	0
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:isDone()	0	int	0	1
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:isDone()	1	int	0	0
org.apache.hadoop.io.BytesWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class)	0	int	0	1
org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class)	1	int	0	0
org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason()	0	java.lang.String	0	NativeIO is not available.
org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason()	1	java.lang.String	0	The OS is not UNIX.
org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:getMemlockLimit()	0	long	0	1125899906842624
org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:getOperatingSystemPageSize()	0	long	0	4096
org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:verifyCanMlock()	0	int	0	1
org.apache.hadoop.io.nativeio.NativeIO:isAvailable()	0	int	0	1
org.apache.hadoop.io.nativeio.NativeIO:isAvailable()	1	int	0	0
org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit()	0	long	0	0
org.apache.hadoop.io.nativeio.NativeIO:getOperatingSystemPageSize()	0	long	0	4096
org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable()	0	int	0	1
org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable()	1	int	0	0
org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable()	0	int	0	1
org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable()	1	int	0	0
org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)	0	long	0	-1
org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)	1	int	0	1
org.apache.hadoop.io.SequenceFile$Reader:readRecordLength()	0	int	0	-1
org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer)	0	int	0	-1
org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)	0	int	0	-1
org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer)	0	int	0	-1
org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object)	0	null	0	null
org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)	0	int	0	0
org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)	0	int	0	0
org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)	0	int	0	0
org.apache.hadoop.io.compress.GzipCodec:getDefaultExtension()	0	java.lang.String	0	.gz
org.apache.hadoop.io.compress.BZip2Codec:getDefaultExtension()	0	java.lang.String	0	.bz2
org.apache.hadoop.io.compress.SnappyCodec:getDefaultExtension()	0	java.lang.String	0	.snappy
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:chooseBlockSize(long)	0	int	0	9
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSimpleSort(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)	0	int	0	1
org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSimpleSort(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)	1	int	0	0
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)	0	int	0	0
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)	1	int	0	1
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0()	0	int	0	-1
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0()	1	int	0	-2
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit()	0	int	0	1
org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit()	1	int	0	0
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsDictionary()	0	int	0	0
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.io.compress.DecompressorStream:read()	0	int	0	-1
org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.compress.DecompressorStream:available()	0	int	0	0
org.apache.hadoop.io.compress.DecompressorStream:available()	1	int	0	1
org.apache.hadoop.io.compress.DecompressorStream:markSupported()	0	int	0	0
org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded()	0	int	0	1
org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded()	1	int	0	0
org.apache.hadoop.io.compress.ZStandardCodec:getDefaultExtension()	0	java.lang.String	0	.zst
org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsDictionary()	0	int	0	0
org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:finished()	0	int	0	0
org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:decompress(byte[],int,int)	0	int	0	0
org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:getRemaining()	0	int	0	0
org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByClassName(java.lang.String)	0	null	0	null
org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String)	0	null	0	null
org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String)	0	null	0	null
org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsInput()	0	int	0	1
org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsInput()	1	int	0	0
org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibCompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibCompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.Lz4Codec:getDefaultExtension()	0	java.lang.String	0	.lz4
org.apache.hadoop.io.compress.snappy.SnappyCompressor:needsInput()	0	int	0	1
org.apache.hadoop.io.compress.snappy.SnappyCompressor:needsInput()	1	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyCompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.snappy.SnappyCompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)	0	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyCompressor:compressDirectBuf()	0	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsDictionary()	0	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:getRemaining()	0	int	0	0
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirectBuf()	0	int	0	0
org.apache.hadoop.io.compress.DefaultCodec:getDefaultExtension()	0	java.lang.String	0	.deflate
org.apache.hadoop.io.compress.lz4.Lz4Compressor:needsInput()	0	int	0	1
org.apache.hadoop.io.compress.lz4.Lz4Compressor:needsInput()	1	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Compressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.lz4.Lz4Compressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)	0	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Compressor:compressDirectBuf()	0	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsDictionary()	0	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:getRemaining()	0	int	0	0
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompressDirectBuf()	0	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsDictionary()	0	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput()	0	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput()	1	int	0	1
org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished()	0	int	0	1
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished()	1	int	0	0
org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)	1	int	0	0
org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)	2	int	0	1
org.apache.hadoop.io.Text:charAt(int)	0	int	0	-1
org.apache.hadoop.io.Text:find(java.lang.String,int)	0	int	0	-1
org.apache.hadoop.io.Text:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.Text:bytesToCodePoint(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.io.MD5Hash:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	xor_native
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	xor
org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	rs_java
org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	rs
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	rs_native
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	rs
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:preferDirectBuffer()	0	int	0	1
org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	xor_java
org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	xor
org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:cloneAsDirectByteBuffer(byte[],int,int)	0	null	0	null
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	rs-legacy_java
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	rs-legacy
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:preferDirectBuffer()	0	int	0	1
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:preferDirectBuffer()	0	int	0	1
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCoderName()	0	java.lang.String	0	dummy_dummy
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCodecName()	0	java.lang.String	0	dummy
org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMul(byte,byte)	0	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInv(byte)	0	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)	0	int	0	1
org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)	1	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:preferDirectBuffer()	0	int	0	1
org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String)	0	int	0	1
org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String)	1	int	0	0
org.apache.hadoop.io.erasurecode.ErasureCodeNative:isNativeCodeLoaded()	0	int	0	1
org.apache.hadoop.io.erasurecode.ErasureCodeNative:isNativeCodeLoaded()	1	int	0	0
org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getSubPacketSize()	0	int	0	2
org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:preferDirectBuffer()	0	int	0	0
org.apache.hadoop.io.erasurecode.ECSchema:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.erasurecode.ECSchema:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.erasurecode.ECBlockGroup:anyErasedDataBlock()	0	int	0	1
org.apache.hadoop.io.erasurecode.ECBlockGroup:anyErasedDataBlock()	1	int	0	0
org.apache.hadoop.io.erasurecode.ECBlockGroup:anyErasedParityBlock()	0	int	0	1
org.apache.hadoop.io.erasurecode.ECBlockGroup:anyErasedParityBlock()	1	int	0	0
org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup)	0	int	0	1
org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup)	1	int	0	0
org.apache.hadoop.io.EnumSetWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.EnumSetWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.EnumSetWritable:hashCode()	0	int	0	0
org.apache.hadoop.io.EnumSetWritable:toString()	0	java.lang.String	0	(null)
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next()	0	int	0	0
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next()	1	int	0	1
org.apache.hadoop.io.DataInputByteBuffer$Buffer:read()	0	int	0	-1
org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.io.BooleanWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.BooleanWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.BooleanWritable:hashCode()	0	int	0	0
org.apache.hadoop.io.BooleanWritable:hashCode()	1	int	0	1
org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)	0	int	0	0
org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)	1	int	0	-1
org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)	2	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey()	0	int	0	1
org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey()	1	int	0	0
org.apache.hadoop.io.ByteWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.io.ByteWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)	0	int	0	-1
org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)	1	int	0	0
org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)	2	int	0	1
org.apache.hadoop.io.NullWritable$Comparator:compare(byte[],int,int,byte[],int,int)	0	int	0	0
org.apache.hadoop.io.MultipleIOException$Builder:isEmpty()	0	int	0	1
org.apache.hadoop.service.launcher.ServiceLauncher:isClassnameDefined()	0	int	0	1
org.apache.hadoop.service.launcher.ServiceLauncher:isClassnameDefined()	1	int	0	0
org.apache.hadoop.service.launcher.AbstractLaunchableService:execute()	0	int	0	0
org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object)	0	int	0	1
org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object)	1	int	0	0
org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)	0	null	0	null
org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)	0	null	0	null
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasIdentifier()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasIdentifier()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasPassword()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasPassword()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasKind()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasKind()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasService()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto$Builder:hasService()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasAlias()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasAlias()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasSecret()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:hasSecret()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:hasNewExpiryTime()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:hasNewExpiryTime()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasAlias()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasAlias()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasSecret()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CredentialsKVProto$Builder:hasSecret()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:hasRenewer()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:hasRenewer()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenRequestProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder:hasRenewer()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder:hasRenewer()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$CancelDelegationTokenResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasIdentifier()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasIdentifier()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasPassword()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasPassword()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasKind()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasKind()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasService()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:hasService()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:isInitialized()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:isInitialized()	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$TokenProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto$Builder:hasNewExpiryTime()	0	int	0	1
org.apache.hadoop.security.proto.SecurityProtos$RenewDelegationTokenResponseProto$Builder:hasNewExpiryTime()	1	int	0	0
org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.security.Groups:isNegativeCacheEnabled()	0	int	0	1
org.apache.hadoop.security.Groups:isNegativeCacheEnabled()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isInitialized()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isInitialized()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()	0	java.lang.String	0	com.ibm.security.auth.module.JAASLoginModule
org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()	1	java.lang.String	0	com.sun.security.auth.module.NTLoginModule
org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()	2	java.lang.String	0	com.sun.security.auth.module.UnixLoginModule
org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isHadoopLogin()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isHadoopLogin()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isFromKeytab()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isFromKeytab()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:isFromTicket()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:isFromTicket()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:shouldRelogin()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:shouldRelogin()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long)	0	int	0	0
org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long)	1	int	0	1
org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation)	0	null	0	null
org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token)	0	int	0	0
org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.UserGroupInformation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)	0	int	0	1
org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)	1	int	0	0
org.apache.hadoop.security.ShellBasedIdMapping:isExpired()	0	int	0	1
org.apache.hadoop.security.ShellBasedIdMapping:isExpired()	1	int	0	0
org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform()	0	int	0	0
org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform()	1	int	0	1
org.apache.hadoop.security.ShellBasedIdMapping:isInteger(java.lang.String)	0	int	0	1
org.apache.hadoop.security.ShellBasedIdMapping:isInteger(java.lang.String)	1	int	0	0
org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:isNonEmpty()	0	int	0	1
org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:isNonEmpty()	1	int	0	0
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:abort()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout()	0	int	0	1
org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read()	0	int	0	-1
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:containsHeader(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:containsHeader(java.lang.String)	1	int	0	0
org.apache.hadoop.security.http.CrossOriginFilter:encodeHeader(java.lang.String)	0	null	0	null
org.apache.hadoop.security.http.CrossOriginFilter:isCrossOrigin(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.CrossOriginFilter:isCrossOrigin(java.lang.String)	1	int	0	0
org.apache.hadoop.security.http.CrossOriginFilter:areOriginsAllowed(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.CrossOriginFilter:areOriginsAllowed(java.lang.String)	1	int	0	0
org.apache.hadoop.security.http.CrossOriginFilter:areHeadersAllowed(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.CrossOriginFilter:isMethodAllowed(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.CrossOriginFilter:lambda$initializeAllowedOrigins$0(java.lang.String)	0	int	0	1
org.apache.hadoop.security.http.CrossOriginFilter:lambda$initializeAllowedOrigins$0(java.lang.String)	1	int	0	0
org.apache.hadoop.security.http.RestCsrfPreventionFilter:isBrowser(java.lang.String)	0	int	0	0
org.apache.hadoop.security.http.RestCsrfPreventionFilter:isBrowser(java.lang.String)	1	int	0	1
org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers()	0	null	0	null
org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers()	0	null	0	null
org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getPasswordCharArray(java.lang.String)	0	null	0	null
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter$2:getParameter(java.lang.String)	0	null	0	null
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:containsUpperCase(java.lang.Iterable)	0	int	0	1
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:containsUpperCase(java.lang.Iterable)	1	int	0	0
org.apache.hadoop.security.token.Token$PrivateToken:isPrivate()	0	int	0	1
org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text)	0	null	0	null
org.apache.hadoop.security.token.Token:decodeIdentifier()	0	null	0	null
org.apache.hadoop.security.token.Token:isPrivate()	0	int	0	0
org.apache.hadoop.security.token.Token:isPrivateCloneOf(org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.security.token.Token:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.token.Token:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Edit:validate()	0	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Edit:validate()	1	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Edit:getUsage()	0	java.lang.String	0	dtutil edit -service <service> -alias <alias> [-format (java|protobuf)]filename...
org.apache.hadoop.security.token.DtUtilShell$Print:getUsage()	0	java.lang.String	0	dtutil print [-alias <alias>] filename...
org.apache.hadoop.security.token.DtUtilShell$Remove:validate()	0	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Remove:validate()	1	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Remove:getUsage()	0	java.lang.String	0	dtutil cancel -alias <alias> [-format (java|protobuf)] filename...
org.apache.hadoop.security.token.DtUtilShell$Remove:getUsage()	1	java.lang.String	0	dtutil remove -alias <alias> [-format (java|protobuf)] filename...
org.apache.hadoop.security.token.Token$TrivialRenewer:getKind()	0	null	0	null
org.apache.hadoop.security.token.Token$TrivialRenewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	0
org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)	0	int	0	1
org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)	1	int	0	0
org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)	0	int	0	1
org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)	1	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Append:getUsage()	0	java.lang.String	0	dtutil append [-format (java|protobuf)]filename...
org.apache.hadoop.security.token.DelegationTokenIssuer:getAdditionalTokenIssuers()	0	null	0	null
org.apache.hadoop.security.token.DtUtilShell$Import:validate()	0	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Import:getUsage()	0	java.lang.String	0	dtutil import <base64> [-alias <alias>] [-format (java|protobuf)] filename
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)	0	null	0	null
org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	null	0	null
org.apache.hadoop.security.token.delegation.web.ServletUtils:getParameter(javax.servlet.http.HttpServletRequest,java.lang.String)	0	null	0	null
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest)	0	int	0	1
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest)	1	int	0	0
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	0
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getDoAs(javax.servlet.http.HttpServletRequest)	0	null	0	null
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int)	0	null	0	null
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.security.token.delegation.DelegationKey:getKey()	0	null	0	null
org.apache.hadoop.security.token.delegation.DelegationKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.token.delegation.DelegationKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser()	0	null	0	null
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[])	0	int	0	1
org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[])	1	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Get:isGenericUrl()	0	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Get:isGenericUrl()	1	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Get:validate()	0	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Get:validate()	1	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Get:getUsage()	0	java.lang.String	0	dtutil get URL [-service <scheme>] [-format (java|protobuf)][-alias <alias>] [-renewer <renewer>] filename
org.apache.hadoop.security.token.DtUtilShell$Renew:validate()	0	int	0	0
org.apache.hadoop.security.token.DtUtilShell$Renew:validate()	1	int	0	1
org.apache.hadoop.security.token.DtUtilShell$Renew:getUsage()	0	java.lang.String	0	dtutil renew -alias <alias> filename...
org.apache.hadoop.security.SaslInputStream:readMoreData()	0	int	0	-1
org.apache.hadoop.security.SaslInputStream:read()	0	int	0	-1
org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.security.SaslInputStream:skip(long)	0	long	0	0
org.apache.hadoop.security.SaslInputStream:markSupported()	0	int	0	0
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:toString()	0	java.lang.String	0	STRICT
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:toString()	0	java.lang.String	0	ALLOW_ALL
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)	0	int	0	1
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)	1	int	0	0
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:acceptableCountryWildcard(java.lang.String)	0	int	0	1
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:acceptableCountryWildcard(java.lang.String)	1	int	0	0
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String)	0	int	0	1
org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String)	1	int	0	0
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:toString()	0	java.lang.String	0	DEFAULT
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:toString()	0	java.lang.String	0	STRICT_IE6
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:toString()	0	java.lang.String	0	DEFAULT_AND_LOCALHOST
org.apache.hadoop.security.SecurityUtil:isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal)	0	int	0	0
org.apache.hadoop.security.SecurityUtil:isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal)	1	int	0	1
org.apache.hadoop.security.SecurityUtil:getComponents(java.lang.String)	0	null	0	null
org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)	0	null	0	null
org.apache.hadoop.security.SecurityUtil:isPrivilegedPort(int)	0	int	0	1
org.apache.hadoop.security.SecurityUtil:isPrivilegedPort(int)	1	int	0	0
org.apache.hadoop.security.User:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.security.User:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()	0	int	0	0
org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()	1	int	0	1
org.apache.hadoop.security.alias.CredentialShell$CheckCommand:getUsage()	0	java.lang.String	1	Y2hlY2sgPGFsaWFzPiBbLXZhbHVlIGFsaWFzLXZhbHVlXSBbLXByb3ZpZGVyIHByb3ZpZGVyLXBhdGhdIFstc3RyaWN0XToKClRoZSBjaGVjayBzdWJjb21tYW5kIGNoZWNrIGEgcGFzc3dvcmQgZm9yIHRoZSBuYW1lCnNwZWNpZmllZCBhcyB0aGUgPGFsaWFzPiBhcmd1bWVudCB3aXRoaW4gdGhlIHByb3ZpZGVyIGluZGljYXRlZAp0aHJvdWdoIHRoZSAtcHJvdmlkZXIgYXJndW1lbnQuIElmIC1zdHJpY3QgaXMgc3VwcGxpZWQsIGZhaWwKaW1tZWRpYXRlbHkgaWYgdGhlIHByb3ZpZGVyIHJlcXVpcmVzIGEgcGFzc3dvcmQgYW5kIG5vbmUgaXMgZ2l2ZW4uCklmIC12YWx1ZSBpcyBwcm92aWRlZCwgdXNlIHRoYXQgZm9yIHRoZSB2YWx1ZSBvZiB0aGUgY3JlZGVudGlhbAppbnN0ZWFkIG9mIHByb21wdGluZyB0aGUgdXNlci4=
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()	0	int	0	0
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()	1	int	0	1
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:getUsage()	0	java.lang.String	1	ZGVsZXRlIDxhbGlhcz4gWy1mXSBbLXByb3ZpZGVyIHByb3ZpZGVyLXBhdGhdIFstc3RyaWN0XToKClRoZSBkZWxldGUgc3ViY29tbWFuZCBkZWxldGVzIHRoZSBjcmVkZW50aWFsCnNwZWNpZmllZCBhcyB0aGUgPGFsaWFzPiBhcmd1bWVudCBmcm9tIHdpdGhpbiB0aGUgcHJvdmlkZXIKaW5kaWNhdGVkIHRocm91Z2ggdGhlIC1wcm92aWRlciBhcmd1bWVudC4gVGhlIGNvbW1hbmQgYXNrcyBmb3IKY29uZmlybWF0aW9uIHVubGVzcyB0aGUgLWYgb3B0aW9uIGlzIHNwZWNpZmllZC4gSWYgLXN0cmljdCBpcwpzdXBwbGllZCwgZmFpbCBpbW1lZGlhdGVseSBpZiB0aGUgcHJvdmlkZXIgcmVxdWlyZXMgYSBwYXNzd29yZAphbmQgbm9uZSBpcyBnaXZlbi4=
org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()	0	int	0	1
org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()	1	int	0	0
org.apache.hadoop.security.alias.CredentialShell$ListCommand:getUsage()	0	java.lang.String	1	bGlzdCBbLXByb3ZpZGVyIHByb3ZpZGVyLXBhdGhdIFstc3RyaWN0XToKClRoZSBsaXN0IHN1YmNvbW1hbmQgZGlzcGxheXMgdGhlIGFsaWFzZXMgY29udGFpbmVkIHdpdGhpbiAKYSBwYXJ0aWN1bGFyIHByb3ZpZGVyIC0gYXMgY29uZmlndXJlZCBpbiBjb3JlLXNpdGUueG1sIG9yCmluZGljYXRlZCB0aHJvdWdoIHRoZSAtcHJvdmlkZXIgYXJndW1lbnQuIElmIC1zdHJpY3QgaXMgc3VwcGxpZWQsCmZhaWwgaW1tZWRpYXRlbHkgaWYgdGhlIHByb3ZpZGVyIHJlcXVpcmVzIGEgcGFzc3dvcmQgYW5kIG5vbmUgaXMKcHJvdmlkZWQu
org.apache.hadoop.security.alias.CredentialProvider:isTransient()	0	int	0	0
org.apache.hadoop.security.alias.CredentialProvider:needsPassword()	0	int	0	0
org.apache.hadoop.security.alias.CredentialProvider:noPasswordWarning()	0	null	0	null
org.apache.hadoop.security.alias.CredentialProvider:noPasswordError()	0	null	0	null
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getSchemeName()	0	java.lang.String	0	bcfks
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getKeyStoreType()	0	java.lang.String	0	bcfks
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getAlgorithm()	0	java.lang.String	0	HMACSHA512
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword()	0	int	0	1
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword()	1	int	0	0
org.apache.hadoop.security.alias.JavaKeyStoreProvider:getSchemeName()	0	java.lang.String	0	jceks
org.apache.hadoop.security.alias.JavaKeyStoreProvider:getKeyStoreType()	0	java.lang.String	0	jceks
org.apache.hadoop.security.alias.JavaKeyStoreProvider:getAlgorithm()	0	java.lang.String	0	AES
org.apache.hadoop.security.alias.LocalKeyStoreProvider:keystoreExists()	0	int	0	1
org.apache.hadoop.security.alias.LocalKeyStoreProvider:keystoreExists()	1	int	0	0
org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()	0	int	0	0
org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()	1	int	0	1
org.apache.hadoop.security.alias.CredentialShell$CreateCommand:getUsage()	0	java.lang.String	1	Y3JlYXRlIDxhbGlhcz4gWy12YWx1ZSBhbGlhcy12YWx1ZV0gWy1wcm92aWRlciBwcm92aWRlci1wYXRoXSBbLXN0cmljdF06CgpUaGUgY3JlYXRlIHN1YmNvbW1hbmQgY3JlYXRlcyBhIG5ldyBjcmVkZW50aWFsIGZvciB0aGUgbmFtZQpzcGVjaWZpZWQgYXMgdGhlIDxhbGlhcz4gYXJndW1lbnQgd2l0aGluIHRoZSBwcm92aWRlciBpbmRpY2F0ZWQKdGhyb3VnaCB0aGUgLXByb3ZpZGVyIGFyZ3VtZW50LiBJZiAtc3RyaWN0IGlzIHN1cHBsaWVkLCBmYWlsCmltbWVkaWF0ZWx5IGlmIHRoZSBwcm92aWRlciByZXF1aXJlcyBhIHBhc3N3b3JkIGFuZCBub25lIGlzIGdpdmVuLgpJZiAtdmFsdWUgaXMgcHJvdmlkZWQsIHVzZSB0aGF0IGZvciB0aGUgdmFsdWUgb2YgdGhlIGNyZWRlbnRpYWwKaW5zdGVhZCBvZiBwcm9tcHRpbmcgdGhlIHVzZXIu
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getSchemeName()	0	java.lang.String	0	localbcfks
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getKeyStoreType()	0	java.lang.String	0	bcfks
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getAlgorithm()	0	java.lang.String	0	HMACSHA512
org.apache.hadoop.security.alias.UserProvider:isTransient()	0	int	0	1
org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String)	0	null	0	null
org.apache.hadoop.security.alias.UserProvider:toString()	0	java.lang.String	0	user:///
org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[])	0	int	0	1
org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[])	1	int	0	0
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getSchemeName()	0	java.lang.String	0	localjceks
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getKeyStoreType()	0	java.lang.String	0	jceks
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getAlgorithm()	0	java.lang.String	0	AES
org.apache.hadoop.security.KDiag:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.security.KDiag:run(java.lang.String[])	1	int	0	41
org.apache.hadoop.security.KDiag:run(java.lang.String[])	2	int	0	0
org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])	0	int	0	0
org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])	1	int	0	1
org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.security.SaslRpcServer$AuthMethod:valueOf(byte)	0	null	0	null
org.apache.hadoop.security.LdapGroupsMapping:failover(int,int)	0	int	0	1
org.apache.hadoop.security.LdapGroupsMapping:failover(int,int)	1	int	0	0
org.apache.hadoop.security.LdapGroupsMapping:extractPassword(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getPrefix()	0	java.lang.String	0	hadoop.http.cross-origin.
org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)	0	int	0	1
org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)	1	int	0	0
org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)	0	null	0	null
org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)	0	null	0	null
org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)	0	null	0	null
org.apache.hadoop.security.SaslRpcClient:useWrap()	0	int	0	1
org.apache.hadoop.security.SaslRpcClient:useWrap()	1	int	0	0
org.apache.hadoop.security.SaslPlainServer:getMechanismName()	0	java.lang.String	0	PLAIN
org.apache.hadoop.security.SaslPlainServer:evaluateResponse(byte[])	0	null	0	null
org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String)	0	java.lang.String	0	auth
org.apache.hadoop.security.authorize.PolicyProvider$1:getServices()	0	null	0	null
org.apache.hadoop.security.authorize.AccessControlList:isWildCardACLValue(java.lang.String)	0	int	0	1
org.apache.hadoop.security.authorize.AccessControlList:isWildCardACLValue(java.lang.String)	1	int	0	0
org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation)	0	int	0	1
org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation)	1	int	0	0
org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:hasKey()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:hasKey()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:hasValue()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:hasValue()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:hasClassName()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:hasClassName()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceUtils:wrapHadoopConf(java.lang.String,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.tracing.TraceUtils:createAndRegisterTracer(java.lang.String)	0	null	0	null
org.apache.hadoop.tracing.TraceUtils:byteStringToSpanContext(org.apache.hadoop.thirdparty.protobuf.ByteString)	0	null	0	null
org.apache.hadoop.tracing.TraceUtils:spanContextToByteString(org.apache.hadoop.tracing.SpanContext)	0	null	0	null
org.apache.hadoop.tracing.Tracer:getCurrentSpan()	0	null	0	null
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:hasClassName()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:hasClassName()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ListSpanReceiversResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:hasKey()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:hasKey()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:hasValue()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$ConfigPair$Builder:hasValue()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$RemoveSpanReceiverRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto$Builder:hasClassName()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverRequestProto$Builder:hasClassName()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:hasClassName()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$SpanReceiverListInfo$Builder:hasClassName()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.tracing.TraceAdminPB$AddSpanReceiverResponseProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.tracing.TraceAdminPB$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.tracing.Span:getContext()	0	null	0	null
org.apache.hadoop.util.DataChecksum$ChecksumNull:getValue()	0	long	0	0
org.apache.hadoop.util.ProgramDriver:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.util.ProgramDriver:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.util.ComparableVersion$StringItem:getType()	0	int	0	1
org.apache.hadoop.util.ComparableVersion$StringItem:isNull()	0	int	0	1
org.apache.hadoop.util.ComparableVersion$StringItem:isNull()	1	int	0	0
org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	0	int	0	-1
org.apache.hadoop.util.CombinedIPList:isIn(java.lang.String)	0	int	0	1
org.apache.hadoop.util.CombinedIPList:isIn(java.lang.String)	1	int	0	0
org.apache.hadoop.util.bloom.Key:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.util.bloom.Key:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	0	int	0	0
org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	1	int	0	1
org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	0	int	0	0
org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	1	int	0	1
org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key)	0	int	0	0
org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	0	int	0	1
org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)	1	int	0	0
org.apache.hadoop.util.bloom.DynamicBloomFilter:getActiveStandardBF()	0	null	0	null
org.apache.hadoop.util.DiskChecker:mkdirsWithExistsCheck(java.io.File)	0	int	0	1
org.apache.hadoop.util.DiskChecker:mkdirsWithExistsCheck(java.io.File)	1	int	0	0
org.apache.hadoop.util.Shell$ShellCommandExecutor:getOutput()	0	java.lang.String	0	
org.apache.hadoop.util.JvmPauseMonitor:isStarted()	0	int	0	1
org.apache.hadoop.util.JvmPauseMonitor:isStarted()	1	int	0	0
org.apache.hadoop.util.DataChecksum:getCrcPolynomialForType(org.apache.hadoop.util.DataChecksum$Type)	0	int	0	-306674912
org.apache.hadoop.util.DataChecksum:getCrcPolynomialForType(org.apache.hadoop.util.DataChecksum$Type)	1	int	0	-2097792136
org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)	0	null	0	null
org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)	0	int	0	0
org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)	0	int	0	0
org.apache.hadoop.util.DataChecksum:compare(byte[],int)	0	int	0	1
org.apache.hadoop.util.DataChecksum:compare(byte[],int)	1	int	0	0
org.apache.hadoop.util.DataChecksum:getChecksumHeaderSize()	0	int	0	5
org.apache.hadoop.util.DataChecksum:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.util.DataChecksum:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.util.ServletUtil:getParameter(javax.servlet.ServletRequest,java.lang.String)	0	null	0	null
org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String)	0	int	0	0
org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread,long)	0	int	0	1
org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread,long)	1	int	0	0
org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService,long)	0	int	0	1
org.apache.hadoop.util.DirectBufferPool:countBuffersOfSize(int)	0	int	0	0
org.apache.hadoop.util.CrcUtil:getMonomial(long,int)	0	int	0	-2147483648
org.apache.hadoop.util.NativeCrc32:isAvailable()	0	int	0	0
org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class)	0	int	0	0
org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object)	0	int	0	1
org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object)	1	int	0	0
org.apache.hadoop.util.WeakReferenceMap:resolve(java.lang.ref.WeakReference)	0	null	0	null
org.apache.hadoop.util.CleanerUtil:lambda$null$0(java.lang.invoke.MethodHandle,java.nio.ByteBuffer)	0	null	0	null
org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)	0	int	0	0
org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)	1	int	0	1
org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:isJaasConfigurationSet(org.apache.zookeeper.client.ZKClientConfig)	0	int	0	1
org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:isJaasConfigurationSet(org.apache.zookeeper.client.ZKClientConfig)	1	int	0	0
org.apache.hadoop.util.curator.ZKCuratorManager:exists(java.lang.String)	0	int	0	1
org.apache.hadoop.util.curator.ZKCuratorManager:exists(java.lang.String)	1	int	0	0
org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String)	0	int	0	1
org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String)	1	int	0	0
org.apache.hadoop.util.LightWeightGSet:actualArrayLength(int)	0	int	0	1073741824
org.apache.hadoop.util.LightWeightGSet:actualArrayLength(int)	1	int	0	1
org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object)	0	int	0	1
org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object)	1	int	0	0
org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)	0	null	0	null
org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)	0	int	0	0
org.apache.hadoop.util.IdentityHashStore:getElementIndex(java.lang.Object)	0	int	0	-1
org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object)	0	null	0	null
org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object)	0	null	0	null
org.apache.hadoop.util.IdentityHashStore:isEmpty()	0	int	0	1
org.apache.hadoop.util.IdentityHashStore:isEmpty()	1	int	0	0
org.apache.hadoop.util.MachineList:includes(java.lang.String)	0	int	0	1
org.apache.hadoop.util.MachineList:includes(java.lang.String)	1	int	0	0
org.apache.hadoop.util.MachineList:includes(java.net.InetAddress)	0	int	0	1
org.apache.hadoop.util.MachineList:includes(java.net.InetAddress)	1	int	0	0
org.apache.hadoop.util.Preconditions:getDefaultNullMSG()	0	java.lang.String	0	The argument object is NULL
org.apache.hadoop.util.Preconditions:getDefaultCheckArgumentMSG()	0	java.lang.String	0	The argument expression is false
org.apache.hadoop.util.Preconditions:getDefaultCheckStateMSG()	0	java.lang.String	0	The state expression is false
org.apache.hadoop.util.LightWeightCache$1:compare(org.apache.hadoop.util.LightWeightCache$Entry,org.apache.hadoop.util.LightWeightCache$Entry)	0	int	0	1
org.apache.hadoop.util.LightWeightCache$1:compare(org.apache.hadoop.util.LightWeightCache$Entry,org.apache.hadoop.util.LightWeightCache$Entry)	1	int	0	-1
org.apache.hadoop.util.LightWeightCache$1:compare(org.apache.hadoop.util.LightWeightCache$Entry,org.apache.hadoop.util.LightWeightCache$Entry)	2	int	0	0
org.apache.hadoop.util.ApplicationClassLoader:isSystemClass(java.lang.String,java.util.List)	0	int	0	0
org.apache.hadoop.util.Lists:saturatedCast(long)	0	int	0	2147483647
org.apache.hadoop.util.Lists:saturatedCast(long)	1	int	0	-2147483648
org.apache.hadoop.util.ComparableVersion$ListItem:getType()	0	int	0	2
org.apache.hadoop.util.ComparableVersion$ListItem:isNull()	0	int	0	1
org.apache.hadoop.util.ComparableVersion$ListItem:isNull()	1	int	0	0
org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	0	int	0	0
org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	1	int	0	-1
org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	2	int	0	1
org.apache.hadoop.util.ConfTest$1:accept(java.io.File)	0	int	0	1
org.apache.hadoop.util.ConfTest$1:accept(java.io.File)	1	int	0	0
org.apache.hadoop.util.Waitable:hasVal()	0	int	0	1
org.apache.hadoop.util.Waitable:hasVal()	1	int	0	0
org.apache.hadoop.util.UTF8ByteArrayUtils:findByte(byte[],int,int,byte)	0	int	0	-1
org.apache.hadoop.util.UTF8ByteArrayUtils:findBytes(byte[],int,int,byte[])	0	int	0	-1
org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext()	0	int	0	1
org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext()	1	int	0	0
org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object)	0	int	0	1
org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object)	1	int	0	0
org.apache.hadoop.util.ComparableVersion:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.util.ComparableVersion:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.util.CombinedIPWhiteList:isIn(java.lang.String)	0	int	0	1
org.apache.hadoop.util.CombinedIPWhiteList:isIn(java.lang.String)	1	int	0	0
org.apache.hadoop.util.StringInterner:strongIntern(java.lang.String)	0	null	0	null
org.apache.hadoop.util.StringInterner:weakIntern(java.lang.String)	0	null	0	null
org.apache.hadoop.util.IntrusiveCollection:isEmpty()	0	int	0	1
org.apache.hadoop.util.IntrusiveCollection:isEmpty()	1	int	0	0
org.apache.hadoop.util.IntrusiveCollection:contains(java.lang.Object)	0	int	0	0
org.apache.hadoop.util.IntrusiveCollection:add(org.apache.hadoop.util.IntrusiveCollection$Element)	0	int	0	0
org.apache.hadoop.util.IntrusiveCollection:add(org.apache.hadoop.util.IntrusiveCollection$Element)	1	int	0	1
org.apache.hadoop.util.IntrusiveCollection:addFirst(org.apache.hadoop.util.IntrusiveCollection$Element)	0	int	0	0
org.apache.hadoop.util.IntrusiveCollection:addFirst(org.apache.hadoop.util.IntrusiveCollection$Element)	1	int	0	1
org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object)	0	int	0	0
org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object)	1	int	0	1
org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection)	0	int	0	0
org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection)	1	int	0	1
org.apache.hadoop.util.StringUtils:arrayToString(java.lang.String[])	0	java.lang.String	0	
org.apache.hadoop.util.StringUtils:uriToString(java.net.URI[])	0	null	0	null
org.apache.hadoop.util.StringUtils:stringToURI(java.lang.String[])	0	null	0	null
org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[])	0	null	0	null
org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)	0	null	0	null
org.apache.hadoop.util.StringUtils:findNext(java.lang.String,char,char,int,java.lang.StringBuilder)	0	int	0	-1
org.apache.hadoop.util.StringUtils:hasChar(char[],char)	0	int	0	1
org.apache.hadoop.util.StringUtils:hasChar(char[],char)	1	int	0	0
org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])	0	null	0	null
org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])	0	null	0	null
org.apache.hadoop.util.StringUtils:escapeHTML(java.lang.String)	0	null	0	null
org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.Iterable)	0	java.lang.String	0	
org.apache.hadoop.util.StringUtils:popOption(java.lang.String,java.util.List)	0	int	0	0
org.apache.hadoop.util.StringUtils:popOption(java.lang.String,java.util.List)	1	int	0	1
org.apache.hadoop.util.StringUtils:popFirstNonOption(java.util.List)	0	null	0	null
org.apache.hadoop.util.StringUtils:isAlpha(java.lang.String)	0	int	0	0
org.apache.hadoop.util.StringUtils:isAlpha(java.lang.String)	1	int	0	1
org.apache.hadoop.util.StringUtils:wrap(java.lang.String,int,java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.util.FindClass:loadResource(java.lang.String)	0	int	0	3
org.apache.hadoop.util.FindClass:loadResource(java.lang.String)	1	int	0	0
org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)	0	int	0	3
org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)	1	int	0	0
org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)	2	int	0	4
org.apache.hadoop.util.FindClass:loadClass(java.lang.String)	0	int	0	0
org.apache.hadoop.util.FindClass:loadClass(java.lang.String)	1	int	0	3
org.apache.hadoop.util.FindClass:loadClass(java.lang.String)	2	int	0	4
org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)	0	int	0	0
org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)	1	int	0	3
org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)	2	int	0	5
org.apache.hadoop.util.FindClass:usage(java.lang.String[])	0	int	0	2
org.apache.hadoop.util.Shell:isJava7OrAbove()	0	int	0	1
org.apache.hadoop.util.Shell:isJavaVersionAtLeast(int)	0	int	0	1
org.apache.hadoop.util.Shell:isJavaVersionAtLeast(int)	1	int	0	0
org.apache.hadoop.util.Shell:getEnvironmentVariableRegex()	0	java.lang.String	0	%([A-Za-z_][A-Za-z0-9_]*?)%
org.apache.hadoop.util.Shell:getEnvironmentVariableRegex()	1	java.lang.String	0	\$([A-Za-z_][A-Za-z0-9_]*)
org.apache.hadoop.util.Shell:hasWinutilsPath()	0	int	0	1
org.apache.hadoop.util.Shell:hasWinutilsPath()	1	int	0	0
org.apache.hadoop.util.Shell:checkIsBashSupported()	0	int	0	0
org.apache.hadoop.util.Shell:isSetsidSupported()	0	int	0	0
org.apache.hadoop.util.HostsFileReader:readFirstTagValue(org.w3c.dom.Element,java.lang.String)	0	null	0	null
org.apache.hadoop.util.CloseableReferenceCount:unreference()	0	int	0	1
org.apache.hadoop.util.CloseableReferenceCount:unreference()	1	int	0	0
org.apache.hadoop.util.CloseableReferenceCount:isOpen()	0	int	0	1
org.apache.hadoop.util.CloseableReferenceCount:isOpen()	1	int	0	0
org.apache.hadoop.util.ZKUtil:resolveConfIndirection(java.lang.String)	0	null	0	null
org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext()	0	int	0	1
org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext()	1	int	0	0
org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:hasNext()	0	int	0	1
org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:hasNext()	1	int	0	0
org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task)	0	int	0	1
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task)	0	int	0	1
org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task)	1	int	0	0
org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:hasNext()	0	int	0	1
org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:hasNext()	1	int	0	0
org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:fetch()	0	int	0	1
org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:fetch()	1	int	0	0
org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext()	0	int	0	1
org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)	0	int	0	0
org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)	1	int	0	1
org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)	2	int	0	-1
org.apache.hadoop.util.IntrusiveCollection$1:isInList(org.apache.hadoop.util.IntrusiveCollection)	0	int	0	1
org.apache.hadoop.util.IntrusiveCollection$1:isInList(org.apache.hadoop.util.IntrusiveCollection)	1	int	0	0
org.apache.hadoop.util.IntrusiveCollection$1:toString()	0	java.lang.String	0	root
org.apache.hadoop.util.ComparableVersion$IntegerItem:getType()	0	int	0	0
org.apache.hadoop.util.ComparableVersion$IntegerItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	0	int	0	0
org.apache.hadoop.util.ComparableVersion$IntegerItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)	1	int	0	1
org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String)	0	long	0	-1
org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.util.GenericOptionsParser:matchesCurrentDirectory(java.lang.String)	0	int	0	1
org.apache.hadoop.util.GenericOptionsParser:matchesCurrentDirectory(java.lang.String)	1	int	0	0
org.apache.hadoop.util.ChunkedArrayList:isEmpty()	0	int	0	1
org.apache.hadoop.util.ChunkedArrayList:isEmpty()	1	int	0	0
org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:hasNext()	0	int	0	1
org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:hasNext()	1	int	0	0
org.apache.hadoop.util.LimitInputStream:read()	0	int	0	-1
org.apache.hadoop.util.LimitInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.util.LimitInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.util.SequentialNumber:setIfGreater(long)	0	int	0	0
org.apache.hadoop.util.SequentialNumber:setIfGreater(long)	1	int	0	1
org.apache.hadoop.util.SequentialNumber:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.util.ExitUtil:terminateCalled()	0	int	0	1
org.apache.hadoop.util.ExitUtil:terminateCalled()	1	int	0	0
org.apache.hadoop.util.ExitUtil:haltCalled()	0	int	0	1
org.apache.hadoop.util.ExitUtil:haltCalled()	1	int	0	0
org.apache.hadoop.util.ToolRunner:confirmPrompt(java.lang.String)	0	int	0	1
org.apache.hadoop.util.ToolRunner:confirmPrompt(java.lang.String)	1	int	0	0
org.apache.hadoop.util.InstrumentedLock:tryLock()	0	int	0	1
org.apache.hadoop.util.InstrumentedLock:tryLock()	1	int	0	0
org.apache.hadoop.util.LightWeightCache:isExpired(org.apache.hadoop.util.LightWeightCache$Entry,long)	0	int	0	1
org.apache.hadoop.util.LightWeightCache:isExpired(org.apache.hadoop.util.LightWeightCache$Entry,long)	1	int	0	0
org.apache.hadoop.util.ConfTest:parseConf(java.io.InputStream)	0	null	0	null
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:hasReqSource()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:hasReqSource()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasState()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasState()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasReadyToBecomeActive()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasReadyToBecomeActive()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasNotReadyReason()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:hasNotReadyReason()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto$Builder:hasMillisToCede()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto$Builder:hasMillisToCede()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto$Builder:hasReqSource()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto$Builder:hasReqSource()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasReadyToBecomeActive()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasReadyToBecomeActive()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasNotReadyReason()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusResponseProto$Builder:hasNotReadyReason()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:hasMillisToCede()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:hasMillisToCede()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)	0	int	0	0
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable)	0	int	0	1
org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector$6:run()	0	null	0	null
org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget)	0	int	0	1
org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget)	1	int	0	0
org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[])	0	int	0	5
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])	0	int	0	3
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])	1	int	0	0
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])	2	int	0	6
org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])	3	int	0	4
org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)	0	int	0	2
org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)	1	int	0	0
org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)	2	int	0	1
org.apache.hadoop.ha.ZKFailoverController:confirmFormat()	0	int	0	0
org.apache.hadoop.ha.ZKFailoverController:waitForActiveAttempt(int,long)	0	null	0	null
org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists()	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists()	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:getZKSessionIdForTests()	0	long	0	-1
org.apache.hadoop.ha.ActiveStandbyElector:becomeActive()	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:becomeActive()	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:isSuccess(org.apache.zookeeper.KeeperException$Code)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:isSuccess(org.apache.zookeeper.KeeperException$Code)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:isNodeExists(org.apache.zookeeper.KeeperException$Code)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:isNodeExists(org.apache.zookeeper.KeeperException$Code)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:isNodeDoesNotExist(org.apache.zookeeper.KeeperException$Code)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:isNodeDoesNotExist(org.apache.zookeeper.KeeperException$Code)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:isSessionExpired(org.apache.zookeeper.KeeperException$Code)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:isSessionExpired(org.apache.zookeeper.KeeperException$Code)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code)	0	int	0	1
org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code)	1	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code,org.apache.zookeeper.KeeperException$Code)	0	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code,org.apache.zookeeper.KeeperException$Code)	1	int	0	1
org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)	0	int	0	0
org.apache.hadoop.ha.ActiveStandbyElector$7:run()	0	null	0	null
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:isEnabled(int)	0	int	0	0
org.apache.hadoop.ha.HAAdmin:getUsageString()	0	java.lang.String	0	Usage: HAAdmin
org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)	0	int	0	0
org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)	1	int	0	1
org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)	0	int	0	0
org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)	1	int	0	-1
org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.ha.HAAdmin:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)	0	int	0	0
org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)	1	int	0	1
org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[])	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:getAllServiceState()	0	int	0	-1
org.apache.hadoop.ha.HAAdmin:getAllServiceState()	1	int	0	0
org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)	0	int	0	0
org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)	1	int	0	-1
org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)	0	int	0	0
org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)	1	int	0	1
org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)	0	int	0	1
org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)	1	int	0	0
org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorAddress()	0	null	0	null
org.apache.hadoop.ha.HAServiceTarget:isAutoFailoverEnabled()	0	int	0	0
org.apache.hadoop.ha.HAServiceTarget:supportObserver()	0	int	0	0
org.apache.hadoop.ha.ZKFailoverController$2:run()	0	null	0	null
org.apache.hadoop.ha.ZKFailoverController$3:run()	0	null	0	null
org.apache.hadoop.ha.ActiveStandbyElector$1:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMSJSONReader:isReadable(java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	0	int	0	1
org.apache.hadoop.crypto.key.kms.server.KMSJSONReader:isReadable(java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	1	int	0	0
org.apache.hadoop.crypto.key.kms.server.KMS$10:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMS$2:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMS$4:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMS$11:run()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:getKMSUrl()	0	null	0	null
org.apache.hadoop.crypto.key.kms.server.KMSJSONWriter:isWriteable(java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	0	int	0	1
org.apache.hadoop.crypto.key.kms.server.KMSJSONWriter:isWriteable(java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	1	int	0	0
org.apache.hadoop.crypto.key.kms.server.KMSJSONWriter:getSize(java.lang.Object,java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	0	long	0	-1
org.apache.hadoop.crypto.key.kms.server.KMSACLs:checkKeyAccess(java.util.Map,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType)	0	int	0	0
org.apache.hadoop.crypto.key.kms.server.KMSACLs:isACLPresent(java.lang.String,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType)	0	int	0	1
org.apache.hadoop.crypto.key.kms.server.KMSACLs:isACLPresent(java.lang.String,org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider$KeyOpType)	1	int	0	0
org.apache.hadoop.mount.MountInterface$MNTPROC:fromValue(int)	0	null	0	null
org.apache.hadoop.mount.MountEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mount.MountEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.nfs.NfsExports$CIDRMatch:isIncluded(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.nfs.NfsExports$CIDRMatch:isIncluded(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:fromValue(int)	0	null	0	null
org.apache.hadoop.nfs.nfs3.FileHandle:serialize(org.apache.hadoop.oncrpc.XDR)	0	int	0	1
org.apache.hadoop.nfs.nfs3.FileHandle:deserialize(org.apache.hadoop.oncrpc.XDR)	0	int	0	0
org.apache.hadoop.nfs.nfs3.FileHandle:deserialize(org.apache.hadoop.oncrpc.XDR)	1	int	0	1
org.apache.hadoop.nfs.nfs3.FileHandle:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.nfs.nfs3.FileHandle:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.nfs.NfsExports$AnonymousMatch:isIncluded(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.nfs.NfsExports$AnonymousMatch:getHostGroup()	0	java.lang.String	0	*
org.apache.hadoop.nfs.NfsExports$ExactMatch:isIncluded(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.nfs.NfsExports$ExactMatch:isIncluded(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.nfs.NfsExports$AccessCacheEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.nfs.NfsExports$AccessCacheEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.nfs.NfsTime:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.nfs.NfsTime:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.nfs.NfsExports$RegexMatch:isIncluded(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.nfs.NfsExports$RegexMatch:isIncluded(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.oncrpc.RpcMessage$Type:fromValue(int)	0	null	0	null
org.apache.hadoop.oncrpc.RpcProgram:doPortMonitoring(java.net.SocketAddress)	0	int	0	0
org.apache.hadoop.oncrpc.RpcProgram:doPortMonitoring(java.net.SocketAddress)	1	int	0	1
org.apache.hadoop.oncrpc.XDR:readBoolean()	0	int	0	1
org.apache.hadoop.oncrpc.XDR:readBoolean()	1	int	0	0
org.apache.hadoop.oncrpc.XDR:pad(int)	0	int	0	3
org.apache.hadoop.oncrpc.XDR:pad(int)	1	int	0	2
org.apache.hadoop.oncrpc.XDR:pad(int)	2	int	0	1
org.apache.hadoop.oncrpc.XDR:pad(int)	3	int	0	0
org.apache.hadoop.oncrpc.XDR:verifyLength(org.apache.hadoop.oncrpc.XDR,int)	0	int	0	1
org.apache.hadoop.oncrpc.XDR:verifyLength(org.apache.hadoop.oncrpc.XDR,int)	1	int	0	0
org.apache.hadoop.oncrpc.XDR:isLastFragment(byte[])	0	int	0	1
org.apache.hadoop.oncrpc.XDR:isLastFragment(byte[])	1	int	0	0
org.apache.hadoop.oncrpc.RpcCallCache$ClientRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.oncrpc.RpcCallCache$ClientRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.oncrpc.RpcCallCache$1:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.oncrpc.RpcCallCache$1:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.oncrpc.security.SysSecurityHandler:shouldSilentlyDrop(org.apache.hadoop.oncrpc.RpcCall)	0	int	0	0
org.apache.hadoop.oncrpc.security.SecurityHandler:isUnwrapRequired()	0	int	0	0
org.apache.hadoop.oncrpc.security.SecurityHandler:isWrapRequired()	0	int	0	0
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:validMessageLength(int)	0	int	0	0
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:validMessageLength(int)	1	int	0	1
org.apache.hadoop.oncrpc.RpcCallCache$CacheEntry:isInProgress()	0	int	0	1
org.apache.hadoop.oncrpc.RpcCallCache$CacheEntry:isInProgress()	1	int	0	0
org.apache.hadoop.oncrpc.RpcCallCache$CacheEntry:isCompleted()	0	int	0	1
org.apache.hadoop.oncrpc.RpcCallCache$CacheEntry:isCompleted()	1	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:usageError(java.lang.String,java.lang.String)	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:validatePath(java.lang.String)	0	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:validatePath(java.lang.String)	1	int	0	1
org.apache.hadoop.registry.cli.RegistryCli:ls(java.lang.String[])	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:ls(java.lang.String[])	1	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:resolve(java.lang.String[])	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:resolve(java.lang.String[])	1	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[])	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:bind(java.lang.String[])	1	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:mknode(java.lang.String[])	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:mknode(java.lang.String[])	1	int	0	0
org.apache.hadoop.registry.cli.RegistryCli:rm(java.lang.String[])	0	int	0	-1
org.apache.hadoop.registry.cli.RegistryCli:rm(java.lang.String[])	1	int	0	0
org.apache.hadoop.registry.client.binding.RegistryUtils:homePathForUser(java.lang.String)	0	java.lang.String	0	/services/
org.apache.hadoop.registry.client.binding.RegistryPathUtils:lastPathEntry(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.registry.client.binding.RegistryPathUtils:parentOf(java.lang.String)	0	java.lang.String	0	/
org.apache.hadoop.registry.client.binding.RegistryTypeUtils:retrieveAddressesUriType(org.apache.hadoop.registry.client.types.Endpoint)	0	null	0	null
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity$UgiInfo:toString()	0	java.lang.String	0	(null ugi)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:addDigestACL(org.apache.zookeeper.data.ACL)	0	int	0	1
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:addDigestACL(org.apache.zookeeper.data.ACL)	1	int	0	0
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:isValid(java.lang.String)	0	int	0	1
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:isValid(java.lang.String)	1	int	0	0
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:getKerberosAuthModuleForJVM()	0	java.lang.String	0	com.ibm.security.auth.module.Krb5LoginModule
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:getKerberosAuthModuleForJVM()	1	java.lang.String	0	com.sun.security.auth.module.Krb5LoginModule
org.apache.hadoop.registry.client.impl.zk.CuratorService:buildSecurityDiagnostics()	0	java.lang.String	0	security disabled
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkPathExists(java.lang.String)	0	int	0	1
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkPathExists(java.lang.String)	1	int	0	0
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)	0	int	0	1
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)	1	int	0	0
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkSet(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List,boolean)	0	int	0	1
org.apache.hadoop.registry.client.impl.zk.CuratorService:zkSet(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List,boolean)	1	int	0	0
org.apache.hadoop.registry.client.impl.zk.CuratorService:dumpRegistryRobustly(boolean)	0	java.lang.String	0	
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:mknode(java.lang.String,boolean)	1	int	0	1
org.apache.hadoop.registry.client.types.RegistryPathStatus:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.registry.client.types.RegistryPathStatus:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.registry.client.types.RegistryPathStatus:hashCode()	0	int	0	0
org.apache.hadoop.registry.client.types.ServiceRecord:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.registry.client.types.ServiceRecord:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.registry.server.dns.RegistryDNS$6:getLogDescription()	0	java.lang.String	0	Registering 
org.apache.hadoop.registry.server.dns.RegistryDNS:generateReply(org.xbill.DNS.Message,java.net.Socket)	0	null	0	null
org.apache.hadoop.registry.server.dns.RegistryDNS:remoteLookup(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int)	0	int	0	0
org.apache.hadoop.registry.server.dns.RegistryDNS:remoteLookup(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int)	1	int	0	3
org.apache.hadoop.registry.server.dns.RegistryDNS:remoteLookup(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int)	2	int	0	2
org.apache.hadoop.registry.server.dns.RegistryDNS:addAnswer(org.xbill.DNS.Message,org.xbill.DNS.Name,int,int,int,int)	0	int	0	0
org.apache.hadoop.registry.server.dns.RegistryDNS$7:getLogDescription()	0	java.lang.String	0	Deleting 
org.apache.hadoop.registry.server.dns.SecureableZone:getNXTRecord(org.xbill.DNS.Record,org.xbill.DNS.Zone)	0	null	0	null
org.apache.hadoop.registry.server.integration.SelectByYarnPersistence:shouldSelect(java.lang.String,org.apache.hadoop.registry.client.types.RegistryPathStatus,org.apache.hadoop.registry.client.types.ServiceRecord)	0	int	0	1
org.apache.hadoop.registry.server.integration.SelectByYarnPersistence:shouldSelect(java.lang.String,org.apache.hadoop.registry.client.types.RegistryPathStatus,org.apache.hadoop.registry.client.types.ServiceRecord)	1	int	0	0
org.apache.hadoop.registry.server.services.RegistryAdminService:purge(java.lang.String,org.apache.hadoop.registry.server.services.RegistryAdminService$NodeSelector,org.apache.hadoop.registry.server.services.RegistryAdminService$PurgePolicy,org.apache.curator.framework.api.BackgroundCallback)	0	int	0	0
org.apache.hadoop.registry.server.services.MicroZookeeperService:setupSecurity()	0	int	0	1
org.apache.hadoop.registry.server.services.MicroZookeeperService:setupSecurity()	1	int	0	0
org.apache.hadoop.fs.XAttr:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.XAttr:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.XAttr:equalsIgnoreValue(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.XAttr:equalsIgnoreValue(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.Hdfs:getUriDefaultPort()	0	int	0	8020
org.apache.hadoop.fs.Hdfs:supportsSymlinks()	0	int	0	1
org.apache.hadoop.fs.Hdfs$DirListingIterator:hasNext()	0	int	0	0
org.apache.hadoop.fs.Hdfs$DirListingIterator:hasNext()	1	int	0	1
org.apache.hadoop.hdfs.DFSClient:getRandomLocalInterfaceAddr()	0	null	0	null
org.apache.hadoop.hdfs.DFSClient:getDatanodeWriteTimeout(int)	0	int	0	0
org.apache.hadoop.hdfs.DFSClient:getDatanodeReadTimeout(int)	0	int	0	0
org.apache.hadoop.hdfs.DFSClient:renewLease()	0	int	0	1
org.apache.hadoop.hdfs.DFSClient:renewLease()	1	int	0	0
org.apache.hadoop.hdfs.DFSClient:primitiveAppend(java.lang.String,java.util.EnumSet,org.apache.hadoop.util.Progressable)	0	null	0	null
org.apache.hadoop.hdfs.DFSClient:exists(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.DFSClient:exists(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.DFSClient:shouldEncryptData()	0	int	0	1
org.apache.hadoop.hdfs.DFSClient:shouldEncryptData()	1	int	0	0
org.apache.hadoop.hdfs.DFSClient:getStateAtIndex(long[],int)	0	long	0	-1
org.apache.hadoop.hdfs.DFSClient:isHedgedReadsEnabled()	0	int	0	1
org.apache.hadoop.hdfs.DFSClient:isHedgedReadsEnabled()	1	int	0	0
org.apache.hadoop.hdfs.DFSClient:isHDFSEncryptionEnabled()	0	int	0	1
org.apache.hadoop.hdfs.DFSClient:isHDFSEncryptionEnabled()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$53:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$53:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.StripeReader:readChunk(org.apache.hadoop.hdfs.protocol.LocatedBlock,int)	0	int	0	0
org.apache.hadoop.hdfs.StripeReader:readChunk(org.apache.hadoop.hdfs.protocol.LocatedBlock,int)	1	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem$37:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$37:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$49:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$49:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$14:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$14:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:getTxidsBehindEstimate()	0	long	0	-1
org.apache.hadoop.hdfs.DFSOpsCountStatistics:getScheme()	0	java.lang.String	0	hdfs
org.apache.hadoop.hdfs.DFSOpsCountStatistics:getLong(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.DFSOpsCountStatistics:isTracked(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.DFSOpsCountStatistics:isTracked(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.StatefulStripeReader:prepareParityChunk(int)	0	int	0	1
org.apache.hadoop.hdfs.XAttrHelper:buildXAttrMap(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.XAttrHelper:getPrefixedName(org.apache.hadoop.fs.XAttr)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$55:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$55:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasInternalError()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasExternalError()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasExternalError()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasError()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:hasDatanodeError()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:isRestartingNode()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer$ErrorState:isNodeMarked()	1	int	0	0
org.apache.hadoop.hdfs.ExtendedBlockId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.PositionStripeReader:prepareParityChunk(int)	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem$47:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$47:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getHomeDirectory()	0	null	0	null
org.apache.hadoop.hdfs.ViewDistributedFileSystem:supportsSymlinks()	0	int	0	0
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getMountPoints()	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$50:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$50:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.HAUtilClient:isClientFailoverConfigured(org.apache.hadoop.conf.Configuration,java.net.URI)	0	int	0	1
org.apache.hadoop.hdfs.HAUtilClient:isClientFailoverConfigured(org.apache.hadoop.conf.Configuration,java.net.URI)	1	int	0	0
org.apache.hadoop.hdfs.ReplicaAccessor:getNetworkDistance()	0	int	0	0
org.apache.hadoop.hdfs.ReplicaAccessor:getNetworkDistance()	1	int	0	2147483647
org.apache.hadoop.hdfs.DeadNodeDetector:isThreadsShutdown()	0	int	0	1
org.apache.hadoop.hdfs.DeadNodeDetector:isThreadsShutdown()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$57:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$57:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.web.oauth2.AccessTokenTimer:shouldRefresh()	0	int	0	1
org.apache.hadoop.hdfs.web.oauth2.AccessTokenTimer:shouldRefresh()	1	int	0	0
org.apache.hadoop.hdfs.web.resources.AccessTimeParam:getName()	0	java.lang.String	0	accesstime
org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam:getName()	0	java.lang.String	0	oldsnapshotname
org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam:getName()	0	java.lang.String	0	encoding
org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam:getName()	0	java.lang.String	0	flag
org.apache.hadoop.hdfs.web.resources.DeleteOpParam:getName()	0	java.lang.String	0	op
org.apache.hadoop.hdfs.web.resources.StartAfterParam:getName()	0	java.lang.String	0	startafter
org.apache.hadoop.hdfs.web.resources.LongParam$Domain:parse(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.resources.LongParam$Domain:toString(java.lang.Long)	0	java.lang.String	0	null
org.apache.hadoop.hdfs.web.resources.LengthParam:getName()	0	java.lang.String	0	length
org.apache.hadoop.hdfs.web.resources.LengthParam:getLength()	0	long	0	-1
org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam:getName()	0	java.lang.String	0	namespacequota
org.apache.hadoop.hdfs.web.resources.ModificationTimeParam:getName()	0	java.lang.String	0	modificationtime
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:getDoOutput()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam:getName()	0	java.lang.String	0	op
org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam:getName()	0	java.lang.String	0	storagespacequota
org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam:getName()	0	java.lang.String	0	excludedatanodes
org.apache.hadoop.hdfs.web.resources.PermissionParam:getName()	0	java.lang.String	0	permission
org.apache.hadoop.hdfs.web.resources.UserParam:getName()	0	java.lang.String	0	user.name
org.apache.hadoop.hdfs.web.resources.BufferSizeParam:getName()	0	java.lang.String	0	buffersize
org.apache.hadoop.hdfs.web.resources.NoRedirectParam:getName()	0	java.lang.String	0	noredirect
org.apache.hadoop.hdfs.web.resources.EnumSetParam:toString(java.util.EnumSet)	0	java.lang.String	0	
org.apache.hadoop.hdfs.web.resources.NewLengthParam:getName()	0	java.lang.String	0	newlength
org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam:getName()	0	java.lang.String	0	renameoptions
org.apache.hadoop.hdfs.web.resources.StoragePolicyParam:getName()	0	java.lang.String	0	storagepolicy
org.apache.hadoop.hdfs.web.resources.CreateFlagParam:getName()	0	java.lang.String	0	createflag
org.apache.hadoop.hdfs.web.resources.HttpOpParam$TemporaryRedirectOp:getDoOutput()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.HttpOpParam$TemporaryRedirectOp:getRedirect()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:getRequireAuth()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:getDoOutput()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:getRedirect()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam:paths2String(org.apache.hadoop.fs.Path[])	0	java.lang.String	0	
org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam:getName()	0	java.lang.String	0	sources
org.apache.hadoop.hdfs.web.resources.SnapshotNameParam:getName()	0	java.lang.String	0	snapshotname
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:getRequireAuth()	0	int	0	0
org.apache.hadoop.hdfs.web.resources.XAttrValueParam:getName()	0	java.lang.String	0	xattr.value
org.apache.hadoop.hdfs.web.resources.FsActionParam:getName()	0	java.lang.String	0	fsaction
org.apache.hadoop.hdfs.web.resources.PostOpParam:getName()	0	java.lang.String	0	op
org.apache.hadoop.hdfs.web.resources.RenewerParam:getName()	0	java.lang.String	0	renewer
org.apache.hadoop.hdfs.web.resources.AclPermissionParam:getName()	0	java.lang.String	0	aclspec
org.apache.hadoop.hdfs.web.resources.AclPermissionParam:parseAclSpec(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.resources.AclPermissionParam:parseAclSpec(java.util.List)	1	java.lang.String	0	
org.apache.hadoop.hdfs.web.resources.XAttrNameParam:getName()	0	java.lang.String	0	xattr.name
org.apache.hadoop.hdfs.web.resources.DoAsParam:getName()	0	java.lang.String	0	doas
org.apache.hadoop.hdfs.web.resources.RecursiveParam:getName()	0	java.lang.String	0	recursive
org.apache.hadoop.hdfs.web.resources.TokenArgumentParam:getName()	0	java.lang.String	0	token
org.apache.hadoop.hdfs.web.resources.DelegationParam:getName()	0	java.lang.String	0	delegation
org.apache.hadoop.hdfs.web.resources.StorageTypeParam:getName()	0	java.lang.String	0	storagetype
org.apache.hadoop.hdfs.web.resources.StringParam$Domain:getDomain()	0	java.lang.String	0	<String>
org.apache.hadoop.hdfs.web.resources.OffsetParam:getName()	0	java.lang.String	0	offset
org.apache.hadoop.hdfs.web.resources.GroupParam:getName()	0	java.lang.String	0	group
org.apache.hadoop.hdfs.web.resources.BooleanParam$Domain:getDomain()	0	java.lang.String	0	<null | boolean>
org.apache.hadoop.hdfs.web.resources.OverwriteParam:getName()	0	java.lang.String	0	overwrite
org.apache.hadoop.hdfs.web.resources.OwnerParam:getName()	0	java.lang.String	0	owner
org.apache.hadoop.hdfs.web.resources.ShortParam$Domain:parse(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.resources.ShortParam$Domain:toString(java.lang.Short)	0	java.lang.String	0	null
org.apache.hadoop.hdfs.web.resources.GetOpParam:getName()	0	java.lang.String	0	op
org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam:getName()	0	java.lang.String	0	unmaskedpermission
org.apache.hadoop.hdfs.web.resources.CreateParentParam:getName()	0	java.lang.String	0	createparent
org.apache.hadoop.hdfs.web.resources.BlockSizeParam:getName()	0	java.lang.String	0	blocksize
org.apache.hadoop.hdfs.web.resources.DestinationParam:validate(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.resources.DestinationParam:getName()	0	java.lang.String	0	destination
org.apache.hadoop.hdfs.web.resources.IntegerParam$Domain:parse(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.resources.IntegerParam$Domain:toString(java.lang.Integer)	0	java.lang.String	0	null
org.apache.hadoop.hdfs.web.resources.ReplicationParam:getName()	0	java.lang.String	0	replication
org.apache.hadoop.hdfs.web.resources.ECPolicyParam:getName()	0	java.lang.String	0	ecpolicy
org.apache.hadoop.hdfs.web.ByteRangeInputStream:isChunkedTransferEncoding(java.util.Map)	0	int	0	1
org.apache.hadoop.hdfs.web.ByteRangeInputStream:isChunkedTransferEncoding(java.util.Map)	1	int	0	0
org.apache.hadoop.hdfs.web.ByteRangeInputStream:contains(java.util.Map,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.web.ByteRangeInputStream:contains(java.util.Map,java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.web.ByteRangeInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.hdfs.web.ByteRangeInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.hdfs.web.ByteRangeInputStream:available()	0	int	0	2147483647
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathRunner:getResponse(java.net.HttpURLConnection)	0	null	0	null
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:read()	0	int	0	-1
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.hdfs.web.JsonUtilClient:toToken(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toFsPermission(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toFileStatus(java.util.Map,boolean)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toDirectoryListing(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toExtendedBlock(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toDatanodeInfo(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toDatanodeInfoArray(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toStorageTypeArray(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toLocatedBlock(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toLocatedBlockList(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toContentSummary(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toQuotaUsage(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toMD5MD5CRC32FileChecksum(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toAclStatus(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:getPath(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:getXAttr(java.util.Map,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:getXAttr(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toXAttrs(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toXAttrNames(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toXAttrMap(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toLocatedBlocks(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toECPolicy(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toStorageTypes(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toFsServerDefaults(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toSnapshotDiffReport(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toDiffList(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toDiffReportEntry(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toByteArray(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toSnapshottableDirectoryList(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtilClient:toSnapshottableDirectoryStatus(java.util.Map)	0	null	0	null
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getScheme()	0	java.lang.String	0	swebhdfs
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getTransportScheme()	0	java.lang.String	0	https
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDefaultPort()	0	int	0	9871
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.hdfs.web.TokenAspect$TokenManager:handleKind(org.apache.hadoop.io.Text)	0	int	0	1
org.apache.hadoop.hdfs.web.TokenAspect$TokenManager:handleKind(org.apache.hadoop.io.Text)	1	int	0	0
org.apache.hadoop.hdfs.web.TokenAspect$TokenManager:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.hdfs.web.TokenAspect$TokenManager:getSchemeByKind(org.apache.hadoop.io.Text)	0	java.lang.String	0	webhdfs
org.apache.hadoop.hdfs.web.TokenAspect$TokenManager:getSchemeByKind(org.apache.hadoop.io.Text)	1	java.lang.String	0	swebhdfs
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getScheme()	0	java.lang.String	0	webhdfs
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getTransportScheme()	0	java.lang.String	0	http
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getDefaultPort()	0	int	0	9870
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:jsonParse(java.net.HttpURLConnection,boolean)	0	null	0	null
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:supportsSymlinks()	0	int	0	1
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)	0	null	0	null
org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getCurrentBlock()	0	null	0	null
org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:getNumBytes()	0	long	0	0
org.apache.hadoop.hdfs.DataStreamer$BlockToWrite:toString()	0	java.lang.String	0	null
org.apache.hadoop.hdfs.PeerCache$Key:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.PeerCache$Key:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.DFSClient$Renewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem$38:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$38:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSUtilClient:getPercentUsed(long,long)	0	float	0	100.0
org.apache.hadoop.hdfs.DFSUtilClient:getPercentRemaining(long,long)	0	float	0	0.0
org.apache.hadoop.hdfs.DFSUtilClient:concatSuffixes(java.lang.String[])	0	null	0	null
org.apache.hadoop.hdfs.DFSUtilClient:isValidName(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.DFSUtilClient:isValidName(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.DFSUtilClient:isHDFSEncryptionEnabled(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.hdfs.DFSUtilClient:isHDFSEncryptionEnabled(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:isStandbyException(java.lang.Exception)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.WrappedFailoverProxyProvider:useLogicalURI()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.IPFailoverProxyProvider:useLogicalURI()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:isRead(java.lang.reflect.Method)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:isRead(java.lang.reflect.Method)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:shouldFindObserver()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:shouldFindObserver()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:useLogicalURI()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider:useLogicalURI()	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.DataNodeUsageReport:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.DataNodeUsageReport:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.DatanodeStorage:isValidStorageId(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.DatanodeStorage:isValidStorageId(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.DatanodeStorage:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.DatanodeStorage:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.OutlierMetrics:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.OutlierMetrics:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.SlowPeerReports:haveSlowPeers()	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.SlowPeerReports:haveSlowPeers()	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.SlowPeerReports:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.SlowPeerReports:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports:haveSlowDisks()	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports:haveSlowDisks()	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$30:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$30:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$48:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$48:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSClientFaultInjector:corruptPacket()	0	int	0	0
org.apache.hadoop.hdfs.DFSClientFaultInjector:uncorruptPacket()	0	int	0	0
org.apache.hadoop.hdfs.DFSClientFaultInjector:failPacket()	0	int	0	0
org.apache.hadoop.hdfs.DFSClientFaultInjector:skipRollingRestartWait()	0	int	0	0
org.apache.hadoop.hdfs.ClientContext:getNetworkDistance(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	0
org.apache.hadoop.hdfs.ClientContext:getNetworkDistance(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	2147483647
org.apache.hadoop.hdfs.DistributedFileSystem$65:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$65:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$13:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$13:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$35:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$35:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$44:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$63:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$63:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$23:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$41:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$41:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.net.DomainPeer:getTcpNoDelay()	0	int	0	0
org.apache.hadoop.hdfs.net.DomainPeer:isClosed()	0	int	0	1
org.apache.hadoop.hdfs.net.DomainPeer:isClosed()	1	int	0	0
org.apache.hadoop.hdfs.net.DomainPeer:getLocalAddressString()	0	java.lang.String	0	<local>
org.apache.hadoop.hdfs.net.DomainPeer:isLocal()	0	int	0	1
org.apache.hadoop.hdfs.net.DomainPeer:hasSecureChannel()	0	int	0	1
org.apache.hadoop.hdfs.net.NioInetPeer:getRemoteAddressString()	0	null	0	null
org.apache.hadoop.hdfs.net.NioInetPeer:getDomainSocket()	0	null	0	null
org.apache.hadoop.hdfs.net.NioInetPeer:hasSecureChannel()	0	int	0	0
org.apache.hadoop.hdfs.net.EncryptedPeer:hasSecureChannel()	0	int	0	1
org.apache.hadoop.hdfs.net.BasicInetPeer:getInputStreamChannel()	0	null	0	null
org.apache.hadoop.hdfs.net.BasicInetPeer:getRemoteAddressString()	0	null	0	null
org.apache.hadoop.hdfs.net.BasicInetPeer:getDomainSocket()	0	null	0	null
org.apache.hadoop.hdfs.net.BasicInetPeer:hasSecureChannel()	0	int	0	0
org.apache.hadoop.hdfs.LocatedBlocksRefresher:waitForInterval()	0	int	0	1
org.apache.hadoop.hdfs.LocatedBlocksRefresher:waitForInterval()	1	int	0	0
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB$1:get(long,java.util.concurrent.TimeUnit)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:getAclStatus(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertDatanodeInfo(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertStorageTypes(org.apache.hadoop.fs.StorageType[],int)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlockProto(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlocks(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlocks(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlocks2(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlock(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.fs.FileEncryptionInfo)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.DatanodeID[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.fs.QuotaUsage)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertHdfsFileStatus(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.fs.StorageType[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertLocatedBlock(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.LocatedBlocks)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.fs.FsServerDefaults)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.HdfsFileStatus)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.HdfsFileStatus[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.DirectoryListing)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.CorruptFileBlocks)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.fs.ContentSummary)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus[])	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffReportEntry)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.SnapshotDiffReport)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convertPerFileEncInfo(org.apache.hadoop.fs.FileEncryptionInfo)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,java.lang.String,org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto,org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(java.lang.String,java.lang.Long,boolean,long,long,java.lang.Long,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelperClient:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto)	0	null	0	null
org.apache.hadoop.hdfs.DataStreamer$RefetchEncryptionKeyPolicy:continueRetryingOrThrow()	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem:getScheme()	0	java.lang.String	0	hdfs
org.apache.hadoop.hdfs.DistributedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem:supportsSymlinks()	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem:getDefaultPort()	0	int	0	8020
org.apache.hadoop.hdfs.DistributedFileSystem:isValidSnapshotName(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem:isValidSnapshotName(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.PeerCache:isDaemonStarted()	0	int	0	1
org.apache.hadoop.hdfs.PeerCache:isDaemonStarted()	1	int	0	0
org.apache.hadoop.hdfs.PeerCache:get(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean)	0	null	0	null
org.apache.hadoop.hdfs.PeerCache:getInternal(org.apache.hadoop.hdfs.protocol.DatanodeID,boolean)	0	null	0	null
org.apache.hadoop.hdfs.StripeReader$ReaderRetryPolicy:shouldRefetchEncryptionKey()	0	int	0	1
org.apache.hadoop.hdfs.StripeReader$ReaderRetryPolicy:shouldRefetchEncryptionKey()	1	int	0	0
org.apache.hadoop.hdfs.StripeReader$ReaderRetryPolicy:shouldRefetchToken()	0	int	0	1
org.apache.hadoop.hdfs.StripeReader$ReaderRetryPolicy:shouldRefetchToken()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator:hasNextNoFilter()	0	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$DirListingIterator:hasNextNoFilter()	1	int	0	1
org.apache.hadoop.hdfs.ViewDistributedFileSystem$2:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.ViewDistributedFileSystem$2:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$22:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.NameNodeProxiesClient:createFailoverProxyProvider(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,boolean,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)	0	null	0	null
org.apache.hadoop.hdfs.NameNodeProxiesClient:getFailoverProxyProviderClass(org.apache.hadoop.conf.Configuration,java.net.URI)	0	null	0	null
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheCleaner:getRateInMs()	0	long	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:removeEvictable(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)	0	java.lang.String	0	evictableMmapped
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:removeEvictable(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica)	1	java.lang.String	0	evictable
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:isFull()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:isFull()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:isStale()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:isStale()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:addNoChecksumAnchor()	0	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:hasMmap()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:hasMmap()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$ShmId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlotFromExistingShm(org.apache.hadoop.hdfs.ExtendedBlockId)	0	null	0	null
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager:allocSlot(org.apache.hadoop.hdfs.net.DomainPeer,org.apache.commons.lang3.mutable.MutableBoolean,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)	0	null	0	null
org.apache.hadoop.hdfs.shortcircuit.DfsClientShm:handle(org.apache.hadoop.net.unix.DomainSocket)	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:isSet(long)	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:isSet(long)	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:isAnchored()	0	int	0	1
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:isAnchored()	1	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:addAnchor()	0	int	0	0
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot:addAnchor()	1	int	0	1
org.apache.hadoop.hdfs.DistributedFileSystem$36:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$36:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$42:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$42:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSInputStream:getLastBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlocks)	0	long	0	0
org.apache.hadoop.hdfs.DFSInputStream:getLastBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlocks)	1	long	0	-1
org.apache.hadoop.hdfs.DFSInputStream:readBlockLength(org.apache.hadoop.hdfs.protocol.LocatedBlock)	0	long	0	0
org.apache.hadoop.hdfs.DFSInputStream:getFileLength()	0	long	0	0
org.apache.hadoop.hdfs.DFSInputStream:getCurrentBlock()	0	null	0	null
org.apache.hadoop.hdfs.DFSInputStream:read()	0	int	0	-1
org.apache.hadoop.hdfs.DFSInputStream:readWithStrategy(org.apache.hadoop.hdfs.ReaderStrategy)	0	int	0	-1
org.apache.hadoop.hdfs.DFSInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:getBestNodeDNAddrPair(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)	0	null	0	null
org.apache.hadoop.hdfs.DFSInputStream:isValidNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.Collection)	0	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:isValidNode(org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.Collection)	1	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:tokenRefetchNeeded(java.io.IOException,java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:pread(long,java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.hdfs.DFSInputStream:skip(long)	0	long	0	-1
org.apache.hadoop.hdfs.DFSInputStream:skip(long)	1	long	0	0
org.apache.hadoop.hdfs.DFSInputStream:seekToBlockSource(long)	0	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:seekToNewSource(long)	0	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:seekToNewSource(long)	1	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:available()	0	int	0	2147483647
org.apache.hadoop.hdfs.DFSInputStream:markSupported()	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:read(long,java.nio.ByteBuffer)	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:tryReadZeroCopy(int,java.util.EnumSet)	0	null	0	null
org.apache.hadoop.hdfs.DFSInputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:refreshBlockLocations(java.util.Map)	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:refreshBlockLocations(java.util.Map)	1	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:allBlocksLocal(org.apache.hadoop.hdfs.protocol.LocatedBlocks,java.util.Map)	0	int	0	0
org.apache.hadoop.hdfs.DFSInputStream:allBlocksLocal(org.apache.hadoop.hdfs.protocol.LocatedBlocks,java.util.Map)	1	int	0	1
org.apache.hadoop.hdfs.DFSInputStream:isResolveableAndLocal(java.net.InetSocketAddress)	0	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$66:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$66:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$PartialListingIterator:hasNext()	0	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$PartialListingIterator:hasNext()	1	int	0	1
org.apache.hadoop.hdfs.KeyProviderCache:get(org.apache.hadoop.conf.Configuration,java.net.URI)	0	null	0	null
org.apache.hadoop.hdfs.KeyProviderCache:createKeyProviderURI(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.DFSPacket:isHeartbeatPacket()	0	int	0	1
org.apache.hadoop.hdfs.DFSPacket:isHeartbeatPacket()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$58:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$58:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.security.token.block.BlockTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.ClientGSIContext:receiveRequestState(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,long)	0	long	0	0
org.apache.hadoop.hdfs.DataStreamer:isLazyPersist(org.apache.hadoop.hdfs.protocol.HdfsFileStatus)	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer:isLazyPersist(org.apache.hadoop.hdfs.protocol.HdfsFileStatus)	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer:shouldStop()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer:shouldStop()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer:shouldWaitForRestart(int)	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer:shouldWaitForRestart(int)	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer:shouldHandleExternalError()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()	0	int	0	0
org.apache.hadoop.hdfs.DataStreamer:processDatanodeOrExternalError()	1	int	0	1
org.apache.hadoop.hdfs.DataStreamer:handleRestartingDatanode()	0	int	0	1
org.apache.hadoop.hdfs.DataStreamer:handleRestartingDatanode()	1	int	0	0
org.apache.hadoop.hdfs.DataStreamer:handleBadDatanode()	0	int	0	0
org.apache.hadoop.hdfs.DataStreamer:handleBadDatanode()	1	int	0	1
org.apache.hadoop.hdfs.DataStreamer:createBlockOutputStream(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[],long,boolean)	0	int	0	0
org.apache.hadoop.hdfs.DataStreamer:getPinnings(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])	0	null	0	null
org.apache.hadoop.hdfs.DataStreamer:toString()	0	java.lang.String	0	block==null
org.apache.hadoop.hdfs.StripedDataStreamer:isHealthy()	0	int	0	1
org.apache.hadoop.hdfs.StripedDataStreamer:isHealthy()	1	int	0	0
org.apache.hadoop.hdfs.ViewDistributedFileSystem$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.ViewDistributedFileSystem$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripeRange:include(long)	0	int	0	1
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripeRange:include(long)	1	int	0	0
org.apache.hadoop.hdfs.util.StripedBlockUtil$VerticalRange:include(long)	0	int	0	1
org.apache.hadoop.hdfs.util.StripedBlockUtil$VerticalRange:include(long)	1	int	0	0
org.apache.hadoop.hdfs.util.ByteArrayManager$NewByteArrayWithoutLimit:release(byte[])	0	int	0	0
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripingChunk:useByteBuffer()	0	int	0	1
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripingChunk:useByteBuffer()	1	int	0	0
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripingChunk:useChunkBuffer()	0	int	0	1
org.apache.hadoop.hdfs.util.StripedBlockUtil$StripingChunk:useChunkBuffer()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$64:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$64:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$39:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$39:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSStripedOutputStream:shouldEndBlockGroup()	0	int	0	1
org.apache.hadoop.hdfs.DFSStripedOutputStream:shouldEndBlockGroup()	1	int	0	0
org.apache.hadoop.hdfs.DFSStripedOutputStream:isStreamerWriting(int)	0	int	0	0
org.apache.hadoop.hdfs.DFSStripedOutputStream:isStreamerWriting(int)	1	int	0	1
org.apache.hadoop.hdfs.DFSStripedOutputStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.DFSStripedOutputStream:isClosed()	0	int	0	1
org.apache.hadoop.hdfs.DFSStripedOutputStream:isClosed()	1	int	0	0
org.apache.hadoop.hdfs.DFSStripedOutputStream:generateParityCellsForLastStripe()	0	int	0	0
org.apache.hadoop.hdfs.DFSStripedOutputStream:generateParityCellsForLastStripe()	1	int	0	1
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkAnyParityStreamerIsHealthy()	0	int	0	1
org.apache.hadoop.hdfs.DFSStripedOutputStream:checkAnyParityStreamerIsHealthy()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$51:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$51:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSOutputStream:getPipeline()	0	null	0	null
org.apache.hadoop.hdfs.DFSOutputStream:isClosed()	0	int	0	1
org.apache.hadoop.hdfs.DFSOutputStream:isClosed()	1	int	0	0
org.apache.hadoop.hdfs.DFSOutputStream:exceptionNeedsCleanup(java.io.IOException)	0	int	0	1
org.apache.hadoop.hdfs.DFSOutputStream:exceptionNeedsCleanup(java.io.IOException)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasModificationLabel()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasModificationLabel()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:hasPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:hasPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto$Builder:hasTrg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto$Builder:hasTrg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto$Builder:hasLocalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto$Builder:hasLocalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:hasDropBehind()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:hasDropBehind()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:hasReadahead()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:hasReadahead()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto$Builder:hasDatanodeInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto$Builder:hasDatanodeInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasCursor()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto$Builder:hasCursor()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasDelHint()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasDelHint()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasSource()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasSource()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasStorageId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto$Builder:hasStorageId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto$Builder:hasResponse()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto$Builder:hasResponse()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasDst()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasDst()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasOverwriteDest()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasOverwriteDest()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasMoveToTrash()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:hasMoveToTrash()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasPayload()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasPayload()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasHandshakeSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasHandshakeSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasAccessTokenError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:hasAccessTokenError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasMaxVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasMaxVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasSlotId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasSlotId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasSupportsReceiptVerification()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto$Builder:hasSupportsReceiptVerification()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:hasSlotId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:hasSlotId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasConfigVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasConfigVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasUptime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:hasUptime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasAtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto$Builder:hasAtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasNamespaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasNamespaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasStoragespaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasStoragespaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasAtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasAtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasPerms()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasPerms()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasXAttrsRemoved()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto$Builder:hasXAttrsRemoved()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasOldValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasOldValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasNewValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasNewValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasErrorMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:hasErrorMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasScope()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasScope()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasPermissions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasPermissions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:hasResultMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:hasResultMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:hasIsSupported()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:hasIsSupported()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:hasZone()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:hasZone()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:hasPrevPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:hasPrevPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasNamespace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasNamespace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:hasStartPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:hasStartPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:hasIndex()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:hasIndex()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:hasStat()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto$Builder:hasStat()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:hasXAttr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto$Builder:hasXAttr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto$Builder:hasDirList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto$Builder:hasDirList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:hasMillis()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:hasMillis()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:hasIsRelative()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto$Builder:hasIsRelative()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto$Builder:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto$Builder:hasTimeWindow()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto$Builder:hasTimeWindow()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto$Builder:hasTxGap()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto$Builder:hasTxGap()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasSymlink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasSymlink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasBlockReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasBlockReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasBlocksize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasBlocksize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasLocations()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasLocations()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasChildrenNum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasChildrenNum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileEncryptionInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFileEncryptionInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:hasFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:hasError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:hasError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:hasSeqno()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:hasSeqno()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:hasDownstreamAckTimeNanos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto$Builder:hasDownstreamAckTimeNanos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto$Builder:hasBsize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto$Builder:hasBsize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasUsername()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasUsername()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasGroupname()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto$Builder:hasGroupname()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:hasRemainingEntries()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:hasRemainingEntries()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto$Builder:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasFileLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasFileLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasUnderConstruction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasUnderConstruction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasLastBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasLastBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasIsLastBlockComplete()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasIsLastBlockComplete()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasFileEncryptionInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasFileEncryptionInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto$Builder:hasZone()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathResponseProto$Builder:hasZone()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasClient()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasClient()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasLastBlockLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasLastBlockLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:hasIncremental()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:hasIncremental()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:hasNnAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto$Builder:hasNnAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasUserId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasUserId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasHandshakeSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:hasHandshakeSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:hasBytesPerChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:hasBytesPerChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasCtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasCtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasPerms()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasPerms()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasSymlinkTarget()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasSymlinkTarget()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasOverwrite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasOverwrite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasDefaultBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasDefaultBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasErasureCoded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto$Builder:hasErasureCoded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasSubmissionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasSubmissionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasCanceled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasCanceled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasNumReencrypted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasNumReencrypted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasNumFailures()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasNumFailures()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasCompletionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasCompletionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasLastFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:hasLastFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:hasLocalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:hasLocalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:hasClassName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:hasClassName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto$Builder:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:hasFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:hasCacheFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:hasCacheFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasOldBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasOldBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasNewBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:hasNewBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:hasCookie()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto$Builder:hasCookie()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:hasRollingUpgradeInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:hasRollingUpgradeInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasPolicyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasPolicyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasCreationPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasCreationPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasCreationFallbackPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasCreationFallbackPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasReplicationFallbackPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:hasReplicationFallbackPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto$Builder:hasDropBehind()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto$Builder:hasDropBehind()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto$Builder:hasReadahead()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$CachingStrategyProto$Builder:hasReadahead()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DNTransferAckProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasNamespaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasNamespaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasStoragespaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasStoragespaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaRequestProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:hasParentIdx()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:hasParentIdx()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:hasException()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto$Builder:hasException()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto$Builder:hasSummary()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto$Builder:hasSummary()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasFileLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasFileLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasUnderConstruction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasUnderConstruction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasLastBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasLastBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasIsLastBlockComplete()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasIsLastBlockComplete()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasFileEncryptionInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasFileEncryptionInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlocksProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto$Builder:hasSnapshottableDirList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto$Builder:hasSnapshottableDirList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsPathHandleProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfosProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasLowRedundancy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasLowRedundancy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasMissingReplOneBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasMissingReplOneBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasHighestPrioLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:hasHighestPrioLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventBatchProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasUnderReplicated()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasUnderReplicated()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasMissingReplOneBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasMissingReplOneBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasFirstBadLink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasFirstBadLink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasChecksumResponse()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasChecksumResponse()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasReadOpChecksumInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasReadOpChecksumInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasShortCircuitAccessVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:hasShortCircuitAccessVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto$Builder:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto$Builder:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasUnderReplicated()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasUnderReplicated()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasMissingReplOneBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasMissingReplOneBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatsResponseProto$Builder:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:hasFilename()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:hasFilename()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasXAttr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasXAttr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto$Builder:hasFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto$Builder:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasInKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasInKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasInIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasInIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasOutKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasOutKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasOutIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:hasOutIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:hasDiffReport()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:hasDiffReport()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto$Builder:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto$Builder:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto$Builder:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto$Builder:hasFilename()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto$Builder:hasFilename()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:hasStartPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:hasStartPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:hasIndex()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportCursorProto$Builder:hasIndex()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:hasChecked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto$Builder:hasChecked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto$Builder:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoResponseProto$Builder:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:hasContents()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto$Builder:hasContents()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasMasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasMasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasCreateFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasCreateFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasUnmasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasUnmasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto$Builder:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasCreateTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:hasCreateTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasDirStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasDirStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasSnapshotQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasSnapshotQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasSnapshotNumber()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasSnapshotNumber()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasParentFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:hasParentFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasAtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:hasAtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto$Builder:hasSnapshotPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto$Builder:hasSnapshotPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasB()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasB()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasCorrupt()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasCorrupt()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasBlockToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasBlockToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto$Builder:hasBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:hasBytesPerChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto$Builder:hasBytesPerChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:hasDiffReport()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:hasDiffReport()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:hasChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:hasChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:hasChunkOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:hasChunkOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasSource()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasSource()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasPipelineSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasPipelineSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasMinBytesRcvd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasMinBytesRcvd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasMaxBytesRcvd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasMaxBytesRcvd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasLatestGenerationStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasLatestGenerationStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasRequestedChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasRequestedChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasCachingStrategy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasCachingStrategy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasAllowLazyPersist()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasAllowLazyPersist()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasPinning()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasPinning()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStorageId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:hasStorageId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasUsedSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasUsedSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasFreeSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasFreeSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasReservedSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasReservedSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasReservedSpaceForReplicas()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasReservedSpaceForReplicas()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasNumBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto$Builder:hasNumBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasDirId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasDirId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasIsReference()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasIsReference()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto$Builder:hasCookie()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto$Builder:hasCookie()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:hasEventsList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:hasEventsList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:hasNewBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto$Builder:hasNewBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasDirId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasDirId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasIsReference()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasIsReference()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingEntryProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetTimesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto$Builder:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateResponseProto$Builder:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasSchema()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasSchema()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasCellSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasCellSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:hasDataEncryptionKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:hasDataEncryptionKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasCurrentStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasCurrentStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasPlanFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:hasPlanFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:hasPrevId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:hasPrevId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:hasFilter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:hasFilter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasLowRedundancy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasLowRedundancy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasMissingReplOneBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasMissingReplOneBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasHighestPrioLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsResponseProto$Builder:hasHighestPrioLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:hasCookie()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:hasCookie()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasXAttr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasXAttr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:hasFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCopyBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:hasBlockPool()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:hasBlockPool()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:hasForce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto$Builder:hasForce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasBlockPoolUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasBlockPoolUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastUpdate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastUpdate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasXceiverCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasXceiverCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasNonDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasNonDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasAdminState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasAdminState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCacheCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCacheCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCacheUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasCacheUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastUpdateMonotonic()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastUpdateMonotonic()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasUpgradeDomain()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasUpgradeDomain()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastBlockReportTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastBlockReportTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastBlockReportMonotonic()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasLastBlockReportMonotonic()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasNumBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:hasNumBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasSubmissionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasSubmissionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasCanceled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasCanceled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasNumReencrypted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasNumReencrypted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasNumFailures()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasNumFailures()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasCompletionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasCompletionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasLastFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:hasLastFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:hasClassName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:hasClassName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RemoteExceptionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasMasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasMasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasUnmasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto$Builder:hasUnmasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendRequestProto$Builder:hasFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasFinalizeTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasFinalizeTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasCreatedRollbackImages()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto$Builder:hasCreatedRollbackImages()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthResponseProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:hasFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto$Builder:hasFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:hasChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:hasChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:hasChunkOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReadOpChecksumInfoProto$Builder:hasChunkOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasPathIndex()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasPathIndex()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasPathIndex()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasPathIndex()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedListingKeyProto$Builder:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:hasUpgradeFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:hasUpgradeFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasCodecName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasCodecName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasDataUnits()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasDataUnits()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasParityUnits()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto$Builder:hasParityUnits()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasLast()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasLast()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportRequestProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasUsedSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasUsedSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasFreeSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasFreeSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasReservedSpace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasReservedSpace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasReservedSpaceForReplicas()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasReservedSpaceForReplicas()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasNumBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:hasNumBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeVolumeInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:hasSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:hasSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:hasBpid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:hasBpid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasSource()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasSource()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasPipelineSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasPipelineSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasMinBytesRcvd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasMinBytesRcvd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasMaxBytesRcvd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasMaxBytesRcvd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasLatestGenerationStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasLatestGenerationStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasRequestedChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasRequestedChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasCachingStrategy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasCachingStrategy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasAllowLazyPersist()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasAllowLazyPersist()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasPinning()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasPinning()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStorageId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$Builder:hasStorageId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasModificationLabel()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasModificationLabel()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportEntryProto$Builder:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto$Builder:hasRollingUpgradeInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeResponseProto$Builder:hasRollingUpgradeInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpTransferBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasMasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasMasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasUnmasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:hasUnmasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:hasHi()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:hasHi()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:hasLo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:hasLo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:hasNeedBlockToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:hasNeedBlockToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto$Builder:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanRequestProto$Builder:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:hasNewBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:hasNewBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$AppendEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:hasCacheFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveRequestProto$Builder:hasCacheFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:hasStartTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:hasStartTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:hasEndTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:hasEndTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto$Builder:hasCustomId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto$Builder:hasCustomId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BaseHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotRequestProto$Builder:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:hasSeqno()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:hasSeqno()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:hasDownstreamAckTimeNanos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:hasDownstreamAckTimeNanos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:hasArg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:hasArg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasBlk()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasBlk()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasNumAdditionalNodes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasNumAdditionalNodes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:hasShmId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:hasShmId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:hasSlotIdx()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:hasSlotIdx()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto$Builder:hasEventsList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidResponseProto$Builder:hasEventsList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto$Builder:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyResponseProto$Builder:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasDst()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasDst()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasOverwriteDest()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasOverwriteDest()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasMoveToTrash()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2RequestProto$Builder:hasMoveToTrash()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MkdirsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasStartTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasStartTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasFinalizeTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasFinalizeTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasCreatedRollbackImages()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:hasCreatedRollbackImages()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasRequestedNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasRequestedNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto$Builder:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetBalancerBandwidthRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:hasChecked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:hasChecked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetSafeModeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:hasCustomId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:hasCustomId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpCustomProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsReplicatedBlockStatsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:hasHi()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:hasHi()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:hasLo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmIdProto$Builder:hasLo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:hasDirList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:hasDirList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto$Builder:hasCorrupt()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto$Builder:hasCorrupt()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:hasPerm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:hasPerm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:hasZone()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:hasZone()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:hasLocations()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:hasLocations()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasFileAndDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasFileAndDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasSpaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasSpaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasTypeQuotaInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto$Builder:hasTypeQuotaInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:hasDst()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:hasDst()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasBytesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasBytesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasBytesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasBytesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasFilesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasFilesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasFilesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasFilesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasHasExpired()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:hasHasExpired()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:hasResponse()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:hasResponse()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasIpAddr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasIpAddr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasHostName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasHostName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasDatanodeUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasDatanodeUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasXferPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasXferPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasInfoPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasInfoPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasIpcPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasIpcPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasInfoSecurePort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:hasInfoSecurePort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasDestPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasDestPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto$Builder:hasConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasOldBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasOldBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasNewBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineRequestProto$Builder:hasNewBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasMtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasMtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasAtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasAtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasPerms()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasPerms()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasXAttrsRemoved()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:hasXAttrsRemoved()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasDirStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasDirStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasSnapshotQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasSnapshotQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasSnapshotNumber()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasSnapshotNumber()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasParentFullpath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryStatusProto$Builder:hasParentFullpath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto$Builder:hasForUpgrade()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto$Builder:hasForUpgrade()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasBlockPoolUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasBlockPoolUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastUpdate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastUpdate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasXceiverCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasXceiverCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasNonDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasNonDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasAdminState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasAdminState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCacheCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCacheCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCacheUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasCacheUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastUpdateMonotonic()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastUpdateMonotonic()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasUpgradeDomain()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasUpgradeDomain()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastBlockReportTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastBlockReportTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastBlockReportMonotonic()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasLastBlockReportMonotonic()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasNumBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$Builder:hasNumBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenewLeaseResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:hasFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:hasFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$RollingUpgradeStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:hasPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:hasPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:hasEndTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusResponseProto$Builder:hasEndTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasPolicyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasPolicyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasCreationPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasCreationPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasCreationFallbackPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasCreationFallbackPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasReplicationFallbackPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockStoragePolicyProto$Builder:hasReplicationFallbackPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmResponseProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetECTopologyResultForPoliciesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DisallowSnapshotResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasLocalPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasLocalPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasLocalMetaPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:hasLocalMetaPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasBytesPerChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasBytesPerChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasWritePacketSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasWritePacketSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasFileBufferSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasFileBufferSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasEncryptDataTransfer()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasEncryptDataTransfer()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasTrashInterval()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasTrashInterval()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasChecksumType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasChecksumType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasKeyProviderUri()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasKeyProviderUri()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasPolicyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:hasPolicyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasFailed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasFailed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasBlockPoolUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasBlockPoolUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasNonDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto$Builder:hasNonDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:hasIsFromEarlier()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:hasIsFromEarlier()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:hasCursor()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:hasCursor()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$PerFileEncryptionInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:hasContents()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:hasContents()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasDestPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasDestPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$RenameEventProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:hasTimeWindow()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:hasTimeWindow()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:hasTxGap()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:hasTxGap()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasLast()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasLast()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CompleteRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasClientMachine()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto$Builder:hasClientMachine()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$UnlinkEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveDefaultAclResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto$Builder:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto$Builder:hasRemainingEntries()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DirectoryListingProto$Builder:hasRemainingEntries()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasSymlink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasSymlink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasBlockReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasBlockReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasBlocksize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasBlocksize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasLocations()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasLocations()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasChildrenNum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasChildrenNum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileEncryptionInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFileEncryptionInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Builder:hasFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$RefreshNamenodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlan()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlan()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasIgnoreDateCheck()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasIgnoreDateCheck()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto$Builder:hasPlanFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:hasNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$EnableErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:hasNeedLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto$Builder:hasNeedLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto$Builder:hasPerm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$FsPermissionProto$Builder:hasPerm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBytesPerCrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBytesPerCrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasCrcPerBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasCrcPerBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBlockChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBlockChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasCrcType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasCrcType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasSchema()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasSchema()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasCellSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasCellSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto$Builder:hasNewSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto$Builder:hasNewSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasLowRedundancy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasLowRedundancy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasHighestPrioLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto$Builder:hasHighestPrioLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasPayload()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasPayload()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasHandshakeSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasHandshakeSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasAccessTokenError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$Builder:hasAccessTokenError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotOldName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotOldName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotNewName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto$Builder:hasSnapshotNewName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto$Builder:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthResponseProto$Builder:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasFailed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasFailed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasRemaining()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasRemaining()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasBlockPoolUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasBlockPoolUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasNonDfsUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:hasNonDfsUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasOffsetInBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasOffsetInBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasSeqno()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasSeqno()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasLastPacketInBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasLastPacketInBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasDataLen()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasDataLen()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasSyncBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:hasSyncBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasFirstBadLink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasFirstBadLink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasChecksumResponse()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasChecksumResponse()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasReadOpChecksumInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasReadOpChecksumInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasShortCircuitAccessVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$BlockOpResponseProto$Builder:hasShortCircuitAccessVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasSticky()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasSticky()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FileEncryptionInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto$Builder:hasDataEncryptionKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyResponseProto$Builder:hasDataEncryptionKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:hasStat()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:hasStat()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AppendResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasBytesPerChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasBytesPerChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasWritePacketSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasWritePacketSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasFileBufferSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasFileBufferSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasEncryptDataTransfer()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasEncryptDataTransfer()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasTrashInterval()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasTrashInterval()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasChecksumType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasChecksumType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasKeyProviderUri()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasKeyProviderUri()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasPolicyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$FsServerDefaultsProto$Builder:hasPolicyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:hasConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeQuotaInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasLocalPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasLocalPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasLocalMetaPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoResponseProto$Builder:hasLocalMetaPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasClientMachine()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:hasClientMachine()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesBatchResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:hasDatanodeInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:hasDatanodeInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeStorageReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasInKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasInKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasInIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasInIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasOutKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasOutKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasOutIv()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherOptionProto$Builder:hasOutIv()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto$Builder:hasServerDefaults()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto$Builder:hasServerDefaults()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCacheDirectiveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasTarget()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasTarget()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasLink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasLink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasDirPerm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasDirPerm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto$Builder:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto$Builder:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasIpAddr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasIpAddr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasHostName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasHostName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasDatanodeUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasDatanodeUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasXferPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasXferPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasInfoPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasInfoPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasIpcPort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasIpcPort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasInfoSecurePort()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeIDProto$Builder:hasInfoSecurePort()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto$Builder:hasPrevPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsRequestProto$Builder:hasPrevPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RecoverLeaseRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDataEncryptionKeyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto$Builder:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileLinkInfoResponseProto$Builder:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasNonce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasNonce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasEncryptionKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasEncryptionKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasEncryptionAlgorithm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto$Builder:hasEncryptionAlgorithm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasLen()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasLen()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasSendChecksums()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasSendChecksums()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasCachingStrategy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto$Builder:hasCachingStrategy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2ResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasSubmissionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasSubmissionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasCanceled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasCanceled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasNumReencrypted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasNumReencrypted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasNumFailures()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasNumFailures()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasCompletionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasCompletionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasLastFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto$Builder:hasLastFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:hasRecursive()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto$Builder:hasRecursive()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasReencryptionProto()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto$Builder:hasReencryptionProto()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasPrevious()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasPrevious()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto$Builder:hasArg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageRequestProto$Builder:hasArg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto$Builder:hasDiffReport()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingResponseProto$Builder:hasDiffReport()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:hasBlockPool()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:hasBlockPool()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:hasForce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:hasForce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:hasBaseHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:hasBaseHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2ResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2ResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2ResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$Rename2ResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:hasResultMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:hasResultMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:hasIsSupported()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECTopologyVerifierResultProto$Builder:hasIsSupported()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasFileAndDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasFileAndDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasSpaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasSpaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasTypeQuotaInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:hasTypeQuotaInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$QuotaUsageProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasLimit()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasLimit()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasMaxRelativeExpiry()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasMaxRelativeExpiry()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasDefaultReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:hasDefaultReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasFileCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasFileCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSpaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSpaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasTypeQuotaInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasTypeQuotaInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotFileCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotFileCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasSnapshotSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasErasureCodingPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:hasErasureCodingPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasUsername()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasUsername()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasGroupname()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:hasGroupname()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetOwnerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasReencryptionProto()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:hasReencryptionProto()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ZoneEncryptionInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:hasCacheFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:hasCacheFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$EvictWritersResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasEzKeyVersionName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasEzKeyVersionName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasSubmissionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasSubmissionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasCanceled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasCanceled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasNumReencrypted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasNumReencrypted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasNumFailures()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasNumFailures()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasCompletionTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasCompletionTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasLastFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ZoneReencryptionStatusProto$Builder:hasLastFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasOffsetInBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasOffsetInBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasSeqno()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasSeqno()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasLastPacketInBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasLastPacketInBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasDataLen()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasDataLen()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasSyncBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PacketHeaderProto$Builder:hasSyncBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:hasIsFromEarlier()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:hasIsFromEarlier()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:hasCursor()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportListingProto$Builder:hasCursor()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasLen()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasLen()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasSendChecksums()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasSendChecksums()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasCachingStrategy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:hasCachingStrategy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReadBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:hasCodec()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:hasCodec()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:hasCoders()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto$Builder:hasCoders()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasUserId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasUserId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasHandshakeSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTokenSecretProto$Builder:hasHandshakeSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasB()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasB()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasCorrupt()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasCorrupt()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasBlockToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasBlockToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:hasBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$LocatedBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPoliciesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasNonce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto$Builder:hasNonce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasGenerationStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasGenerationStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto$Builder:hasNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CloseEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:hasSlotId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:hasSlotId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessRequestProto$Builder:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DeleteBlockPoolResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasFirstTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasFirstTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasLastTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasLastTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasSyncTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto$Builder:hasSyncTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotOldName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotOldName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotNewName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:hasSnapshotNewName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameSnapshotRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasDelHint()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasDelHint()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasSource()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasSource()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasStorageId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:hasStorageId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpReplaceBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasCursor()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:hasCursor()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportListingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetPermissionResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$SetXAttrResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetEditsFromTxidRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:hasError()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ReleaseShortCircuitAccessResponseProto$Builder:hasError()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto$Builder:hasNeedBlockToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLocatedFileInfoRequestProto$Builder:hasNeedBlockToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto$Builder:hasLocations()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsResponseProto$Builder:hasLocations()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasCtime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasCtime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasPerms()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasPerms()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasSymlinkTarget()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasSymlinkTarget()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasOverwrite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasOverwrite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasDefaultBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasDefaultBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasErasureCoded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:hasErasureCoded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$CreateEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:hasSaved()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:hasSaved()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:hasPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetStoragePolicyRequestProto$Builder:hasPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetQuotaResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:hasNeedLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:hasNeedLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBatchedListingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasSucceed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasSucceed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasErrorMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:hasErrorMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MsyncResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockProto$Builder:hasNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdateBlockForPipelineResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:hasShmId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:hasShmId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:hasSlotIdx()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmSlotProto$Builder:hasSlotIdx()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto$Builder:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto$Builder:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasNonce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:hasNonce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ProvidedStorageLocationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:hasBlockChecksumType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:hasBlockChecksumType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:hasStripeLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:hasStripeLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$RemoveErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetReplicaVisibleLengthRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$CancelPlanResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasFirstTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasFirstTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasLastTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasLastTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasSyncTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:hasSyncTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventsListProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasOldValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasOldValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasNewValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasNewValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasErrorMessage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$GetReconfigurationStatusConfigChangeProto$Builder:hasErrorMessage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasSucceed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasSucceed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasErrorMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AddErasureCodingPolicyResponseProto$Builder:hasErrorMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesOverlimit()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasBytesOverlimit()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasFilesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasFilesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasFilesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:hasFilesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotDiffReportProto$Builder:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCachePoolsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:hasCorrupt()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:hasCorrupt()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCorruptFileBlocksResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:hasFilename()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:hasFilename()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasCurrentStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasCurrentStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasPlanFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusResponseProto$Builder:hasPlanFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:hasMillis()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:hasMillis()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:hasIsRelative()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:hasIsRelative()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoExpirationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumProto$Builder:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:hasUsage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:hasUsage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasPool()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasPool()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasExpiration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:hasExpiration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasTraceId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasTraceId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasParentId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasParentId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasSpanContext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto$Builder:hasSpanContext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:hasPrevId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:hasPrevId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:hasFilter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListCacheDirectivesRequestProto$Builder:hasFilter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:hasCacheFlags()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCacheDirectiveRequestProto$Builder:hasCacheFlags()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$TruncateEventProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$TruncateResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$AddErasureCodingPoliciesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ReportBadBlocksResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetStoragePoliciesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasPool()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasPool()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasExpiration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveInfoProto$Builder:hasExpiration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:hasDst()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RenameRequestProto$Builder:hasDst()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$IsFileClosedRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto$Builder:hasDiffReport()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportResponseProto$Builder:hasDiffReport()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$ListReconfigurablePropertiesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasScope()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasScope()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasPermissions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasPermissions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:hasZone()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneRequestProto$Builder:hasZone()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:hasNewSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:hasNewSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollEditsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetDatanodeInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasSourceDnInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasSourceDnInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetDnInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetDnInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetStorageUuids()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetStorageUuids()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetStorageTypes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasTargetStorageTypes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasLiveBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasLiveBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasExcludeReconstructedIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:hasExcludeReconstructedIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasCreateTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshotInfoProto$Builder:hasCreateTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:hasCookie()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:hasCookie()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CorruptFileBlocksProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:hasRecursive()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:hasRecursive()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$DisableErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBalancerBandwidthRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto$Builder:hasUpgradeFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusResponseProto$Builder:hasUpgradeFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListReencryptionStatusRequestProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RestoreFailedStorageResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfosProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto$Builder:hasFilename()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeRequestProto$Builder:hasFilename()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasLimit()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasLimit()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasMaxRelativeExpiry()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasMaxRelativeExpiry()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasDefaultReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolInfoProto$Builder:hasDefaultReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$GetAclStatusRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpgradeStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaOptionEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasGenerationStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasGenerationStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:hasNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ExtendedBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:hasSummary()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:hasSummary()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetContentSummaryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasClient()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasClient()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasLastBlockLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasLastBlockLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FsyncRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:hasServerDefaults()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:hasServerDefaults()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetServerDefaultsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$SetErasureCodingPolicyRequestProto$Builder:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$GetEZForPathRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:hasParentIdx()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:hasParentIdx()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:hasException()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:hasException()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BatchedDirectoryListingProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasMasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasMasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasCreateFlag()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasCreateFlag()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasUnmasked()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasUnmasked()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasEcPolicyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasEcPolicyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasStoragePolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:hasStoragePolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasB()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasB()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasHolder()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasHolder()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:hasBaseHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:hasBaseHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientOperationHeaderProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto$Builder:hasSaved()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SaveNamespaceResponseProto$Builder:hasSaved()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasB()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasB()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasHolder()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasHolder()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AbandonBlockRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasNonce()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasNonce()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasEncryptionKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasEncryptionKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasEncryptionAlgorithm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:hasEncryptionAlgorithm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DataEncryptionKeyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesOverlimit()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasBytesOverlimit()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasFilesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasFilesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasFilesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CachePoolStatsProto$Builder:hasFilesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasTraceId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasTraceId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasParentId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasParentId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasSpanContext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:hasSpanContext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferTraceInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:hasCodec()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:hasCodec()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:hasCoders()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:hasCoders()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$CodecProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasGroup()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasGroup()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasSticky()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasSticky()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasNeedLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:hasNeedLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:hasToken()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:hasToken()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetBlockLocalPathInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:hasForUpgrade()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:hasForUpgrade()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ShutdownDatanodeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RefreshNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasMaxVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasMaxVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasSlotId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasSlotId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasSupportsReceiptVerification()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:hasSupportsReceiptVerification()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpRequestShortCircuitAccessProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto$Builder:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneRequestProto$Builder:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasSuite()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasSuite()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasCryptoProtocolVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasCryptoProtocolVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasKeyName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$EncryptionZoneProto$Builder:hasKeyName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:hasTargetPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:hasTargetPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetLinkTargetResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto$Builder:hasBlockChecksumType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto$Builder:hasBlockChecksumType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto$Builder:hasStripeLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumOptionsProto$Builder:hasStripeLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto$Builder:hasUsage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetQuotaUsageResponseProto$Builder:hasUsage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$QueryPlanStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasCodecName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasCodecName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasDataUnits()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasDataUnits()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasParityUnits()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:hasParityUnits()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ECSchemaProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ListEncryptionZonesRequestProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingRequestProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:hasIncremental()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:hasIncremental()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:hasNnAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:hasNnAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBytesPerCrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBytesPerCrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasCrcPerBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasCrcPerBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBlockChecksum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBlockChecksum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasCrcType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasCrcType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockChecksumResponseProto$Builder:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasNamespace()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasNamespace()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$HAServiceStateResponseProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasStartAfter()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasStartAfter()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasNeedLocation()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetListingRequestProto$Builder:hasNeedLocation()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$GetErasureCodingCodecsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UnsetStoragePolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$SnapshottableDirectoryListingProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$ListXAttrsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$ModifyAclEntriesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:hasHasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:hasHasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ListOpenFilesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasLowRedundancy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasLowRedundancy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasCorruptBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasCorruptBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasMissingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasMissingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasBlocksInFuture()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasBlocksInFuture()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasPendingDeletionBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasPendingDeletionBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasHighestPrioLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:hasHighestPrioLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFsECBlockGroupStatsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddCachePoolRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$CreateEncryptionZoneResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasFromSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasFromSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasToSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:hasToSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshotDiffReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ModifyCachePoolRequestProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeRequestProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:hasBsize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:hasBsize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetPreferredBlockSizeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$UpdatePipelineResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:hasSnapshotPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:hasSnapshotPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSnapshotResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:hasXAttr()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:hasXAttr()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasConfigVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasConfigVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasUptime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeLocalInfoProto$Builder:hasUptime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ClientReadStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:hasTraceInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitShmRequestProto$Builder:hasTraceInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasSourceDnInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasSourceDnInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetDnInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetDnInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetStorageUuids()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetStorageUuids()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetStorageTypes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasTargetStorageTypes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasLiveBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasLiveBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasExcludeReconstructedIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$BlockECReconstructionInfoProto$Builder:hasExcludeReconstructedIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SetReplicationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:hasSnapshottableDirList()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:hasSnapshottableDirList()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSnapshottableDirListingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlan()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlan()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasIgnoreDateCheck()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasIgnoreDateCheck()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:hasPlanFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$SubmitDiskBalancerPlanRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasBytesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasBytesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasBytesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasBytesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasFilesNeeded()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasFilesNeeded()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasFilesCached()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasFilesCached()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasHasExpired()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveStatsProto$Builder:hasHasExpired()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetCurrentEditLogTxidRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$GetXAttrsRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$FinalizeUpgradeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasTarget()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasTarget()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasLink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasLink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasDirPerm()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasDirPerm()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasCreateParent()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateSymlinkRequestProto$Builder:hasCreateParent()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AllowSnapshotRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SatisfyStoragePolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ErasureCodingProtos$UnsetErasureCodingPolicyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeReportRequestProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageUuidsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheDirectiveEntryProto$Builder:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$RemoveAclEntriesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:hasTrg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:hasTrg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ConcatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ReconfigurationProtocolProtos$StartReconfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasPrevious()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasPrevious()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$GetVolumeReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:hasSecret()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:hasSecret()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:hasBpid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$HandshakeSecretProto$Builder:hasBpid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto$Builder:hasPoolName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCachePoolRequestProto$Builder:hasPoolName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CheckAccessRequestProto$Builder:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasBlk()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasBlk()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasNumAdditionalNodes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasNumAdditionalNodes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$DiskBalancerSettingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$SetAclResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:hasFs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:hasFs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasHeader()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasHeader()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasRequestedNumBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasRequestedNumBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasBlockChecksumOptions()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:hasBlockChecksumOptions()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpBlockGroupChecksumProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetDatanodeStorageReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptEncryptionZoneResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RemoveCacheDirectiveResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasFileCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasFileCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSpaceQuota()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSpaceQuota()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasTypeQuotaInfos()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasTypeQuotaInfos()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotFileCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotFileCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotDirectoryCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotDirectoryCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotSpaceConsumed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasSnapshotSpaceConsumed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasErasureCodingPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ContentSummaryProto$Builder:hasErasureCodingPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$RemoveXAttrResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$MetaSaveResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:hasSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:hasSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:hasSnapshotName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DeleteSnapshotRequestProto$Builder:hasSnapshotName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetAdditionalDatanodeResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetSlowDatanodeReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$TriggerBlockReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.LocatedBlocks$1:compare(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock)	0	int	0	0
org.apache.hadoop.hdfs.protocol.LocatedBlocks$1:compare(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock)	1	int	0	-1
org.apache.hadoop.hdfs.protocol.LocatedBlocks$1:compare(org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock)	2	int	0	1
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:hasRunningZone(java.lang.Long)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:hasRunningZone(java.lang.Long)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:addZoneIfNecessary(java.lang.Long,java.lang.String,org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:addZoneIfNecessary(java.lang.Long,java.lang.String,org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ReencryptionInfoProto)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:removeZone(java.lang.Long)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:removeZone(java.lang.Long)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ExtendedBlock:getLocalBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	null	0	null
org.apache.hadoop.hdfs.protocol.ExtendedBlock:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ExtendedBlock:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.LocatedBlocks:locatedBlockCount()	0	int	0	0
org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus:isSymlink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus:isSymlink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.RollingUpgradeStatus:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.RollingUpgradeStatus:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],boolean,boolean)	1	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$2:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,boolean,boolean)	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$3:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil:fromProto(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ChecksumProto)	0	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.Op:valueOf(byte)	0	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck:isSuccess()	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck:isSuccess()	1	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck:getOOBStatus()	0	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader:sanityCheck(long)	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader:sanityCheck(long)	1	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.PacketHeader:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted()	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.TrustedChannelResolver:isTrusted(java.net.InetAddress)	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant:isNegotiatedQopPrivacy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslParticipant:isNegotiatedQopPrivacy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:getSaslPropertiesResolver(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:negotiateCipherOption(org.apache.hadoop.conf.Configuration,java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$1:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$1:satisfy(short,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,boolean,boolean)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ProvidedStorageLocation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ProvidedStorageLocation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.LocatedBlock$ProvidedLastComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage,org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage)	0	int	0	1
org.apache.hadoop.hdfs.protocol.LocatedBlock$ProvidedLastComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage,org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage)	1	int	0	-1
org.apache.hadoop.hdfs.protocol.LocatedBlock$ProvidedLastComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage,org.apache.hadoop.hdfs.protocol.DatanodeInfoWithStorage)	2	int	0	0
org.apache.hadoop.hdfs.protocol.HdfsPathHandle:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isEnabled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isEnabled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isDisabled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isDisabled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isRemoved()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:isRemoved()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:isStarted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:isStarted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:isFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:isFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.HdfsFileStatus:isEmptyLocalName()	0	int	0	1
org.apache.hadoop.hdfs.protocol.HdfsFileStatus:isEmptyLocalName()	1	int	0	0
org.apache.hadoop.hdfs.protocol.HdfsNamedFileStatus:isSymlink()	0	int	0	1
org.apache.hadoop.hdfs.protocol.HdfsNamedFileStatus:isSymlink()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ReplicatedBlockStats:hasHighestPriorityLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ReplicatedBlockStats:hasHighestPriorityLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.EncryptionZone:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.EncryptionZone:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffReportEntry:getPathString(byte[])	0	java.lang.String	0	.
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffReportEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffReportEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator$SingleEntry:get(int)	0	null	0	null
org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator$SingleEntry:size()	0	int	0	1
org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator$SingleEntry:hasMore()	0	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isDecommissionInProgress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isDecommissionInProgress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isDecommissioned()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isDecommissioned()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:maintenanceNotExpired(long)	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:maintenanceNotExpired(long)	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isEnteringMaintenance()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isEnteringMaintenance()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isInMaintenance()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isInMaintenance()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isMaintenance()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isMaintenance()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:maintenanceExpired()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:maintenanceExpired()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isInService()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isInService()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isStale(long)	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:isStale(long)	1	int	0	0
org.apache.hadoop.hdfs.protocol.DatanodeInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ECBlockGroupStats:hasHighestPriorityLowRedundancyBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ECBlockGroupStats:hasHighestPriorityLowRedundancyBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ECBlockGroupStats:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.ECBlockGroupStats:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.CachePoolInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.CachePoolInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.protocol.LocatedStripedBlock:isStriped()	0	int	0	1
org.apache.hadoop.hdfs.protocol.CorruptFileBlocks:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.CorruptFileBlocks:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:isReplicationPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:isReplicationPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:isSystemPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:isSystemPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeID:checkDatanodeUuid(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.protocol.DatanodeID:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.DatanodeID:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.Block:filename2id(java.lang.String)	0	long	0	0
org.apache.hadoop.hdfs.protocol.Block:getGenerationStamp(java.lang.String)	0	long	0	0
org.apache.hadoop.hdfs.protocol.Block:getBlockId(java.lang.String)	0	long	0	0
org.apache.hadoop.hdfs.protocol.Block:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.Block:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.Block:matchingIdAndGenStamp(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.protocol.Block:matchingIdAndGenStamp(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.protocol.LocatedBlock:isStriped()	0	int	0	0
org.apache.hadoop.hdfs.protocol.DirectoryListing:hasMore()	0	int	0	1
org.apache.hadoop.hdfs.protocol.DirectoryListing:hasMore()	1	int	0	0
org.apache.hadoop.hdfs.protocol.DirectoryListing:getLastName()	0	null	0	null
org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.DeadNodeDetector$UniqueQueue:offer(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.DeadNodeDetector$UniqueQueue:offer(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.DFSStripedInputStream:createBlockReader(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,org.apache.hadoop.hdfs.protocol.LocatedBlock[],org.apache.hadoop.hdfs.StripeReader$BlockReaderInfo[],int,long)	0	int	0	0
org.apache.hadoop.hdfs.DFSStripedInputStream:createBlockReader(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,org.apache.hadoop.hdfs.protocol.LocatedBlock[],org.apache.hadoop.hdfs.StripeReader$BlockReaderInfo[],int,long)	1	int	0	1
org.apache.hadoop.hdfs.DFSStripedInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.hdfs.DFSStripedInputStream:readWithStrategy(org.apache.hadoop.hdfs.ReaderStrategy)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.CorruptFileBlockIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.CorruptFileBlockIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.ExternalBlockReader:skip(long)	0	long	0	0
org.apache.hadoop.hdfs.client.impl.ExternalBlockReader:available()	0	int	0	2147483647
org.apache.hadoop.hdfs.client.impl.ExternalBlockReader:getClientMmap(java.util.EnumSet)	0	null	0	null
org.apache.hadoop.hdfs.client.impl.ExternalBlockReader:getDataChecksum()	0	null	0	null
org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator$1:compare(org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry,org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator$1:compare(org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry,org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry)	1	int	0	1
org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator$1:compare(org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry,org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing$DiffReportListingEntry)	2	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:doByteBufferRead(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:skip(long)	0	long	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:available()	0	int	0	2147483647
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:isShortCircuit()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:getClientMmap(java.util.EnumSet)	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:getNetworkDistance()	0	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:read(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:available()	0	int	0	131072
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:isShortCircuit()	0	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:getClientMmap(java.util.EnumSet)	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy$LocalDatanodeInfo$1:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy$LocalDatanodeInfo$1:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator$RenameEntry:isRename()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.SnapshotDiffReportGenerator$RenameEntry:isRename()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:drainDataBuf(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:drainDataBuf(java.nio.ByteBuffer)	1	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:createNoChecksumContext()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:createNoChecksumContext()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:readWithoutBounceBuffer(java.nio.ByteBuffer)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:fillDataBuf(boolean)	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:fillDataBuf(boolean)	1	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:readWithBounceBuffer(java.nio.ByteBuffer,boolean)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:readWithoutBounceBuffer(byte[],int,int)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:readWithBounceBuffer(byte[],int,int,boolean)	0	int	0	-1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:available()	0	int	0	2147483647
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:isShortCircuit()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:getClientMmap(java.util.EnumSet)	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:getNetworkDistance()	0	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer$Factory$Key:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.client.impl.LeaseRenewer$Factory$Key:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getLegacyBlockReaderLocal()	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getBlockReaderLocal()	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:getRemoteBlockReaderFromDomain()	0	null	0	null
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:isSecurityException(java.io.IOException)	0	int	0	1
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:isSecurityException(java.io.IOException)	1	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:clientsRunning()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:clientsRunning()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:isRunning()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:isRunning()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:isRenewerExpired()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:isRenewerExpired()	1	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:put(org.apache.hadoop.hdfs.DFSClient)	0	int	0	0
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:put(org.apache.hadoop.hdfs.DFSClient)	1	int	0	1
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:clientsString()	0	java.lang.String	0	[]
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory$FailureInjector:getSupportsReceiptVerification()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.DfsClientConf:loadWriteByteArrayManagerConf(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.client.impl.DfsClientConf:isLocatedBlocksRefresherEnabled()	0	int	0	1
org.apache.hadoop.hdfs.client.impl.DfsClientConf:isLocatedBlocksRefresherEnabled()	1	int	0	0
org.apache.hadoop.hdfs.DistributedFileSystem$17:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DFSStripedOutputStream$1:call()	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$68:doCall(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.DistributedFileSystem$68:next(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getScheme()	0	java.lang.String	0	swebhdfs
org.apache.hadoop.fs.http.client.HttpFSFileSystem:getScheme()	0	java.lang.String	0	webhdfs
org.apache.hadoop.fs.http.client.HttpFSFileSystem:getDefaultPort()	0	int	0	9870
org.apache.hadoop.fs.http.client.HttpFSFileSystem:getRenewToken()	0	null	0	null
org.apache.hadoop.fs.http.client.HttpFSFileSystem:toStorageTypes(org.json.simple.JSONArray)	0	null	0	null
org.apache.hadoop.fs.http.client.HttpFSFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.http.client.HttpFSFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.http.server.FSOperations$FSRemoveAcl:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSUnsetStoragePolicy:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetErasureCodingPolicy:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSCreate:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetTimes:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetStoragePolicy:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSAppend:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSGetSnapshotDiff:execute(org.apache.hadoop.fs.FileSystem)	0	java.lang.String	0	
org.apache.hadoop.fs.http.server.FSOperations$FSDisallowSnapshot:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetXAttr:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSConcat:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSRemoveXAttr:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSAccess:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSUnSetErasureCodingPolicy:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSModifyAclEntries:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSDeleteSnapshot:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSListStatus:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.http.server.FSOperations$FSSetPermission:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetOwner:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:getUrl()	0	null	0	null
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:isRandomSecret(javax.servlet.FilterConfig)	0	int	0	0
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:isRandomSecret(javax.servlet.FilterConfig)	1	int	0	1
org.apache.hadoop.fs.http.server.FSOperations$FSRenameSnapshot:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSetAcl:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSRemoveAclEntries:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSAllowSnapshot:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSRemoveDefaultAcl:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.fs.http.server.FSOperations$FSSatisyStoragePolicy:execute(org.apache.hadoop.fs.FileSystem)	0	null	0	null
org.apache.hadoop.lib.lang.RunnableCallable:call()	0	null	0	null
org.apache.hadoop.lib.wsrs.LongParam:getDomain()	0	java.lang.String	0	a long
org.apache.hadoop.lib.wsrs.BooleanParam:getDomain()	0	java.lang.String	0	a boolean
org.apache.hadoop.lib.wsrs.JSONProvider:getSize(org.json.simple.JSONStreamAware,java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	0	long	0	-1
org.apache.hadoop.lib.wsrs.Param:toString()	0	java.lang.String	0	NULL
org.apache.hadoop.lib.wsrs.JSONMapProvider:getSize(java.util.Map,java.lang.Class,java.lang.reflect.Type,java.lang.annotation.Annotation[],javax.ws.rs.core.MediaType)	0	long	0	-1
org.apache.hadoop.lib.wsrs.IntegerParam:getDomain()	0	java.lang.String	0	an integer
org.apache.hadoop.lib.wsrs.EnumSetParam:toString(java.util.EnumSet)	0	java.lang.String	0	
org.apache.hadoop.lib.wsrs.ShortParam:getDomain()	0	java.lang.String	0	a short
org.apache.hadoop.lib.wsrs.ByteParam:getDomain()	0	java.lang.String	0	a byte
org.apache.hadoop.lib.wsrs.StringParam:getDomain()	0	java.lang.String	0	a string
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:isIdempotent(org.apache.hadoop.oncrpc.RpcCall)	0	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$DfsClientKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$DfsClientKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:dumpData(java.io.FileOutputStream,java.io.RandomAccessFile)	0	long	0	0
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mapErrorStatus(java.io.IOException)	0	int	0	70
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mapErrorStatus(java.io.IOException)	1	int	0	13
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:mapErrorStatus(java.io.IOException)	2	int	0	5
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:isIdempotent(org.apache.hadoop.oncrpc.RpcCall)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:isIdempotent(org.apache.hadoop.oncrpc.RpcCall)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:checkAccessPrivilege(java.net.SocketAddress,org.apache.hadoop.nfs.AccessPrivilege)	0	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:checkAccessPrivilege(java.net.SocketAddress,org.apache.hadoop.nfs.AccessPrivilege)	1	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange$1:compare(org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange,org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange$1:compare(org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange,org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange)	1	int	0	-1
org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange$1:compare(org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange,org.apache.hadoop.hdfs.nfs.nfs3.OffsetRange)	2	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:getFileAttr(org.apache.hadoop.hdfs.DFSClient,java.lang.String,org.apache.hadoop.security.IdMappingServiceProvider)	0	null	0	null
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:getWccAttr(org.apache.hadoop.hdfs.DFSClient,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:isSet(int,int)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:isSet(int,int)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$DFSInputStreamCacheKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$DFSInputStreamCacheKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:put(org.apache.hadoop.nfs.nfs3.FileHandle,org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)	0	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:put(org.apache.hadoop.nfs.nfs3.FileHandle,org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)	1	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkStreamTimeout(long)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkStreamTimeout(long)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:hasPendingWork()	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:hasPendingWork()	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkRepeatedWriteRequest(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int)	0	null	0	null
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:addWritesToCache(org.apache.hadoop.nfs.nfs3.request.WRITE3Request,io.netty.channel.Channel,int)	0	null	0	null
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkAndStartWrite(org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService,org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx)	1	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkSequential(long,long)	0	int	0	0
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:checkSequential(long,long)	1	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:streamCleanup(org.apache.hadoop.nfs.nfs3.FileHandle,long)	0	int	0	1
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:offerNextToWrite()	0	null	0	null
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasStateStoreVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasStateStoreVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasCompileInfo()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasCompileInfo()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateStarted()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasDateStarted()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasAdminAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto$Builder:hasAdminAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto$Builder:hasNamenodeMembership()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto$Builder:hasNamenodeMembership()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto$Builder:hasRouterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto$Builder:hasRouterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasClusterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasClusterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto$Builder:hasEntry()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto$Builder:hasEntry()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasReadOnly()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasReadOnly()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDestOrder()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasDestOrder()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasFaultTolerant()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:hasFaultTolerant()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto$Builder:hasIsInSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto$Builder:hasIsInSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto$Builder:hasRouter()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto$Builder:hasRouter()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:hasMembership()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:hasMembership()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto$Builder:hasMembership()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsRequestProto$Builder:hasMembership()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateNamenodeRegistrationRequestProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasTotalSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasTotalSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasAvailableSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasAvailableSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasProvidedSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasProvidedSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfFiles()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfFiles()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocks()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocks()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksMissing()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksMissing()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksPendingReplication()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksPendingReplication()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksUnderReplicated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksUnderReplicated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksPendingDeletion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfBlocksPendingDeletion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfActiveDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfActiveDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDeadDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDeadDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecommissioningDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecommissioningDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecomActiveDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecomActiveDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecomDeadDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfDecomDeadDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfStaleDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfStaleDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfInMaintenanceLiveDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfInMaintenanceLiveDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfInMaintenanceDeadDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfInMaintenanceDeadDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfEnteringMaintenanceDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto$Builder:hasNumOfEnteringMaintenanceDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:hasRouter()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:hasRouter()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto$Builder:hasMembershipVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto$Builder:hasMembershipVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto$Builder:hasMountTableVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto$Builder:hasMountTableVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:hasNamenodeMembership()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:hasNamenodeMembership()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetExpiredRegistrationsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetExpiredRegistrationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetExpiredRegistrationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetExpiredRegistrationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetExpiredRegistrationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:hasEntry()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:hasEntry()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasTotalSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasTotalSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasAvailableSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasAvailableSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasProvidedSpace()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasProvidedSpace()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfFiles()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfFiles()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocks()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocks()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksMissing()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksMissing()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksPendingReplication()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksPendingReplication()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksUnderReplicated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksUnderReplicated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksPendingDeletion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfBlocksPendingDeletion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfActiveDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfActiveDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDeadDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDeadDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecommissioningDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecommissioningDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecomActiveDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecomActiveDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecomDeadDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfDecomDeadDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfStaleDatanodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfStaleDatanodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfInMaintenanceLiveDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfInMaintenanceLiveDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfInMaintenanceDeadDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfInMaintenanceDeadDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfEnteringMaintenanceDataNodes()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:hasNumOfEnteringMaintenanceDataNodes()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipStatsRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasStateStoreVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasStateStoreVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasCompileInfo()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasCompileInfo()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateStarted()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasDateStarted()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasAdminAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:hasAdminAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto$Builder:hasEntry()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto$Builder:hasEntry()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationRequestProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:hasEntry()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:hasEntry()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasLastContact()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasLastContact()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasRouterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasRouterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasClusterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasClusterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasWebAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasWebAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasRpcAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasRpcAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasServiceAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasServiceAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasLifelineAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasLifelineAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasIsSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasIsSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasWebScheme()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:hasWebScheme()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$AddMountTableEntryResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnableNameserviceResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:hasMembershipVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:hasMembershipVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:hasMountTableVersion()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:hasMountTableVersion()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$StateStoreVersionRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto$Builder:hasResult()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RefreshMountTableEntriesResponseProto$Builder:hasResult()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$EnterSafeModeResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterFederatedStateProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterFederatedStateProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterFederatedStateProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterFederatedStateProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:hasIsInSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:hasIsInSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDestinationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryRequestProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterFederatedStateProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetMountTableEntriesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisableNameserviceResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasClusterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasClusterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$FederationNamespaceInfoProto$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$UpdateMountTableEntryResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:hasRouterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:hasRouterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$LeaveSafeModeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$DisabledNameserviceRecordProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamenodeRegistrationsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetDisabledNameservicesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:hasRouter()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:hasRouter()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoveMountTableEntryResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeHeartbeatResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasLastContact()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasLastContact()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasRouterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasRouterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasClusterId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasClusterId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasWebAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasWebAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasRpcAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasRpcAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasServiceAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasServiceAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasLifelineAddress()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasLifelineAddress()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasIsSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasIsSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasStats()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasStats()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasWebScheme()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$NamenodeMembershipRecordProto$Builder:hasWebScheme()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetSafeModeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetNamespaceInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto$Builder:hasRouter()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationResponseProto$Builder:hasRouter()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:hasPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:hasPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RemoteLocationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$RouterHeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDateCreated()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDateCreated()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDateModified()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDateModified()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasReadOnly()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasReadOnly()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDestOrder()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasDestOrder()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasOwnerName()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasOwnerName()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasGroupName()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasGroupName()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasMode()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasMode()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasFaultTolerant()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$Builder:hasFaultTolerant()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$GetRouterRegistrationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.RouterProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNamenodes()	0	java.lang.String	0	{}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameservices()	0	java.lang.String	0	{}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getMountTable()	0	java.lang.String	0	[]
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getRouters()	0	java.lang.String	0	{}
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumNameservices()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumNamenodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNumExpiredNamenodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getHostAndPort()	0	java.lang.String	0	Unknown
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getClusterId()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getBlockPoolId()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getCurrentTokensCount()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getSafemode()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getSafeModeTip()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameserviceAggregatedInt(java.util.function.ToIntFunction)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getNameserviceAggregatedLong(java.util.function.ToLongFunction)	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getDateString(long)	0	java.lang.String	0	-
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:getSecondsSince(long)	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getReadOps()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getReadAvg()	0	double	0	-1.0
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getWriteOps()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getWriteAvg()	0	double	0	-1.0
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getFailureOps()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getFailureAvg()	0	double	0	-1.0
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getRemoveOps()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NullStateStoreMetrics:getRemoveAvg()	0	double	0	-1.0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getUsed()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFree()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotal()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getProvidedCapacity()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getSafemode()	0	java.lang.String	0	Failed to get safemode status. Please check routerlog for more detail.
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:isUpgradeFinalized()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getRollingUpgradeStatus()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNonDfsUsedSpace()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getCacheUsed()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getCacheCapacity()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getBlockPoolUsedSpace()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPercentBlockPoolUsed()	0	float	0	0.0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotalBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumberOfMissingBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingReplicationBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingReconstructionBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getUnderReplicatedBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getLowRedundancyBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getPendingDeletionBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getScheduledReplicationBlocks()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumberOfMissingBlocksWithReplicationFactorOne()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getHighestPriorityLowRedundancyReplicatedBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getHighestPriorityLowRedundancyECBlocks()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getCorruptFiles()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getCorruptFilesCount()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNodes(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)	0	java.lang.String	0	{}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getClusterId()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getBlockPoolId()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNameDirStatuses()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNodeUsage()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNameJournalStatus()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getJournalTransactionInfo()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNNStartedTimeInMillis()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getDistinctVersionCount()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getDistinctVersions()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFSState()	0	java.lang.String	0	Operational
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFilesTotal()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotalLoad()	0	int	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumLiveDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDeadDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumStaleDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecomLiveDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecomDeadDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumDecommissioningDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumInMaintenanceLiveDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumInMaintenanceDeadDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumEnteringMaintenanceDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumInServiceLiveDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getVolumeFailuresTotal()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getEstimatedCapacityLostTotal()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getSnapshotStats()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getMaxObjects()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getBlockDeletionStartTime()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumStaleStorages()	0	int	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTopUserOpCounts()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getFsLockQueueLength()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotalSyncCount()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getTotalSyncTimes()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:isSecurityEnabled()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getLastHATransitionTime()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getBytesWithFutureGenerationStamps()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getSlowPeersReport()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getSlowDisksReport()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumberOfSnapshottableDirs()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getEnteringMaintenanceNodes()	0	java.lang.String	0	{}
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNameDirSize()	0	java.lang.String	0	N/A
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getNumEncryptionZones()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getVerifyECWithTopologyResult()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:getCurrentTokensCount()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor:getProcessingTime()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor:getProxyTime()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.resolver.FederationNamespaceInfo:getSrc()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.FederationNamespaceInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.resolver.FederationNamespaceInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.resolver.PathLocation:hasMultipleDestinations()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.resolver.PathLocation:hasMultipleDestinations()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getDatanodesSubcluster()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterSpaceComparator:compare(org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterAvailableSpace,org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterAvailableSpace)	0	int	0	-1
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterSpaceComparator:compare(org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterAvailableSpace,org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver$SubclusterAvailableSpace)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.resolver.order.RandomResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver:getRpcServer()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver:getMembershipStore()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:loadCache(boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:loadCache(boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:getNamenodesForBlockPoolId(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:registerNamenode(org.apache.hadoop.hdfs.server.federation.resolver.NamenodeStatusReport)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:loadCache(boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:loadCache(boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:buildLocation(java.lang.String,org.apache.hadoop.hdfs.server.federation.store.records.MountTable)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RemoteLocationContext:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RemoteLocationContext:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:getJvmMetrics()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:isQuotaUpdated(org.apache.hadoop.hdfs.server.federation.store.protocol.UpdateMountTableEntryRequest,org.apache.hadoop.hdfs.server.federation.store.records.MountTable)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:isQuotaUpdated(org.apache.hadoop.hdfs.server.federation.store.protocol.UpdateMountTableEntryRequest,org.apache.hadoop.hdfs.server.federation.store.records.MountTable)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:isQuotaSyncRequired(long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:isQuotaSyncRequired(long,long)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:verifySafeMode(boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:verifySafeMode(boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:getPermissionChecker()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:refreshSuperUserGroupsConfiguration()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isUnavailableException(java.io.IOException)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isUnavailableException(java.io.IOException)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isClusterUnAvailable(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isClusterUnAvailable(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isExpectedClass(java.lang.Class,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isExpectedClass(java.lang.Class,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isExpectedValue(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:isExpectedValue(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.ConnectionContext:isActive()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.ConnectionContext:isActive()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.ConnectionContext:isUsable()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.ConnectionContext:isUsable()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:rollEditLog()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:registerSubordinateNamenode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:getEditLogManifest(long)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:isUpgradeFinalized()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:isRollingUpgrade()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterNamenodeProtocol:getNextSPSPath()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.IsRouterActiveServlet:isActive()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.IsRouterActiveServlet:isActive()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.Router:isQuotaEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.Router:isQuotaEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RemoteParam:getParameterForContext(org.apache.hadoop.hdfs.server.federation.router.RemoteLocationContext)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationTokens(org.apache.hadoop.io.Text)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:isUnavailableSubclusterException(java.io.IOException)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:isUnavailableSubclusterException(java.io.IOException)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setReplication(java.lang.String,short)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getListing(java.lang.String,byte[],boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:listEncryptionZones(long)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:listReencryptionStatus(long)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getEditsFromTxid(long)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDataEncryptionKey()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:listOpenFiles(long,java.util.EnumSet,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:shouldAddMountPoint(java.lang.String,java.lang.String,byte[],int)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:shouldAddMountPoint(java.lang.String,java.lang.String,byte[],int)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:isMultiDestDirectory(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.Quota:lambda$getGlobalQuota$1(long[],org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.Quota:lambda$getGlobalQuota$1(long[],org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RemoteResult:hasException()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RemoteResult:hasException()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:isStoreAvailable()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:isQuotaSet(org.apache.hadoop.hdfs.server.federation.store.records.MountTable)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isPathReadOnly(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isPathReadOnly(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isPathAll(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isPathFaultTolerant(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isInvokeConcurrent(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:isInvokeConcurrent(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.ConnectionPoolId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:getConnection(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.Class)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterFsckServlet:lambda$doGet$0(javax.servlet.ServletContext,java.util.Map,java.io.PrintWriter,java.net.InetAddress)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:isAllowedDelegationTokenOp()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:isAllowedDelegationTokenOp()	1	int	0	1
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:createCredentials(org.apache.hadoop.hdfs.server.federation.router.Router,org.apache.hadoop.security.UserGroupInformation,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaManager:isQuotaSet(org.apache.hadoop.fs.QuotaUsage)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaManager:isQuotaSet(org.apache.hadoop.fs.QuotaUsage)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaManager:lambda$isQuotaSet$0(org.apache.hadoop.fs.QuotaUsage,org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaManager:lambda$isQuotaSet$0(org.apache.hadoop.fs.QuotaUsage,org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreBaseImpl:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:rename(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:rename(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:exists(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:mkdir(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:rename(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:rename(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:remove(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initDriver()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initDriver()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initRecordStorage(java.lang.String,java.lang.Class)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:initRecordStorage(java.lang.String,java.lang.Class)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:isDriverReady()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:isDriverReady()	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:putAll(java.util.List,boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:writeNode(java.lang.String,byte[],boolean,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:writeNode(java.lang.String,byte[],boolean,boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initDriver()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initDriver()	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initRecordStorage(java.lang.String,java.lang.Class)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:initRecordStorage(java.lang.String,java.lang.Class)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:isOldTempRecord(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:isOldTempRecord(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:putAll(java.util.List,boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:putAll(java.util.List,boolean,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:loadCache(boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:loadCache(boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:isUpdateTime()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:isUpdateTime()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.Query:matches(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.DisabledNameservice:hasOtherFields()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.DisabledNameservice:getExpirationMs()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:compareTo(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:checkExpired(long)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:checkExpired(long)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:isExpired()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:isExpired()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:getDefaultLocation()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:getExpirationMs()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.StateStoreVersion:getExpirationMs()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.StateStoreVersion:getDateModified()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.store.records.StateStoreVersion:getDateCreated()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:isExpired()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:getDeletionMs()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:hasOtherFields()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:compareTo(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:checkExpired(long)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:checkExpired(long)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:shouldBeDeleted(long)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord:shouldBeDeleted(long)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:like(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isAvailable()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isAvailable()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isBadState()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isBadState()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:checkExpired(long)	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:checkExpired(long)	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isExpired()	0	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipState:isExpired()	1	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipStats:getExpirationMs()	0	long	0	-1
org.apache.hadoop.hdfs.server.federation.store.records.MembershipStats:getDateModified()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.store.records.MembershipStats:getDateCreated()	0	long	0	0
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:getSourcePath()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:getDestinations()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:addDestination(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:addDestination(java.lang.String,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:isReadOnly()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MountTablePBImpl:isFaultTolerant()	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.RouterStatePBImpl:getAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.RouterStatePBImpl:getStateStoreVersion()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.RouterStatePBImpl:getStatus()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.RouterStatePBImpl:getVersion()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.RouterStatePBImpl:getCompileInfo()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getRouterId()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getNameserviceId()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getNamenodeId()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getClusterId()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getBlockPoolId()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getRpcAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getServiceAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getWebAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getLifelineAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getState()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.records.impl.pb.MembershipStatePBImpl:getWebScheme()	0	null	0	null
org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getHostPortString(java.net.InetSocketAddress)	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:loadCache(boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:loadCache(boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.federation.store.protocol.impl.pb.GetNamenodeRegistrationsRequestPBImpl:getPartialMembership()	0	null	0	null
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	0	java.lang.String	1	CVstYWRkIDxzb3VyY2U+IDxuYW1lc2VydmljZTEsIG5hbWVzZXJ2aWNlMiwgLi4uPiA8ZGVzdGluYXRpb24+IFstcmVhZG9ubHldIFstZmF1bHR0b2xlcmFudF0gWy1vcmRlciBIQVNIfExPQ0FMfFJBTkRPTXxIQVNIX0FMTHxTUEFDRV0gLW93bmVyIDxvd25lcj4gLWdyb3VwIDxncm91cD4gLW1vZGUgPG1vZGU+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	1	java.lang.String	1	CVstdXBkYXRlIDxzb3VyY2U+IFs8bmFtZXNlcnZpY2UxLCBuYW1lc2VydmljZTIsIC4uLj4gPGRlc3RpbmF0aW9uPl0gWy1yZWFkb25seSB0cnVlfGZhbHNlXSBbLWZhdWx0dG9sZXJhbnQgdHJ1ZXxmYWxzZV0gWy1vcmRlciBIQVNIfExPQ0FMfFJBTkRPTXxIQVNIX0FMTHxTUEFDRV0gLW93bmVyIDxvd25lcj4gLWdyb3VwIDxncm91cD4gLW1vZGUgPG1vZGU+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	2	java.lang.String	1	CVstcm0gPHNvdXJjZT5d
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	3	java.lang.String	1	CVstbHMgWy1kXSA8cGF0aD5d
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	4	java.lang.String	1	CVstZ2V0RGVzdGluYXRpb24gPHBhdGg+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	5	java.lang.String	1	CVstc2V0UXVvdGEgPHBhdGg+IC1uc1F1b3RhIDxuc1F1b3RhPiAtc3NRdW90YSA8cXVvdGEgaW4gYnl0ZXMgb3IgcXVvdGEgc2l6ZSBzdHJpbmc+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	6	java.lang.String	1	CVstc2V0U3RvcmFnZVR5cGVRdW90YSA8cGF0aD4gLXN0b3JhZ2VUeXBlIDxzdG9yYWdlIHR5cGU+IDxxdW90YSBpbiBieXRlcyBvciBxdW90YSBzaXplIHN0cmluZz5d
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	7	java.lang.String	1	CVstY2xyUXVvdGEgPHBhdGg+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	8	java.lang.String	1	CVstY2xyU3RvcmFnZVR5cGVRdW90YSA8cGF0aD5d
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	9	java.lang.String	1	CVstc2FmZW1vZGUgZW50ZXIgfCBsZWF2ZSB8IGdldF0=
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	10	java.lang.String	1	CVstbmFtZXNlcnZpY2UgZW5hYmxlIHwgZGlzYWJsZSA8bmFtZXNlcnZpY2U+XQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	11	java.lang.String	1	CVstZ2V0RGlzYWJsZWROYW1lc2VydmljZXNd
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	12	java.lang.String	1	CVstcmVmcmVzaF0=
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	13	java.lang.String	1	CVstcmVmcmVzaFJvdXRlckFyZ3MgPGhvc3Q6aXBjX3BvcnQ+IDxrZXk+IFthcmcxLi5hcmduXV0=
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	14	java.lang.String	1	CVstcmVmcmVzaFN1cGVyVXNlckdyb3Vwc0NvbmZpZ3VyYXRpb25d
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	15	java.lang.String	1	CVstcmVmcmVzaENhbGxRdWV1ZV0=
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getUsage(java.lang.String)	16	java.lang.String	1	CVstZHVtcFN0YXRlXQ==
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:validateMin(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:validateMin(java.lang.String[])	1	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:refreshSuperUserGroupsConfiguration()	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:refreshSuperUserGroupsConfiguration()	1	int	0	-1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:addMount(java.lang.String,java.lang.String[],java.lang.String,boolean,boolean,org.apache.hadoop.hdfs.server.federation.resolver.order.DestinationOrder,org.apache.hadoop.hdfs.tools.federation.RouterAdmin$ACLEntity)	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:updateMount(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getBooleanValue(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:getBooleanValue(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:genericRefresh(java.lang.String[],int)	0	int	0	-1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:dumpStateStore(org.apache.hadoop.conf.Configuration,java.io.PrintStream)	0	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:dumpStateStore(org.apache.hadoop.conf.Configuration,java.io.PrintStream)	1	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$updateStorageTypeQuota$5(long[],org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$updateStorageTypeQuota$5(long[],org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$setStorageTypeQuota$2(long[],org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$setStorageTypeQuota$2(long[],org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$setStorageTypeQuota$1(long[],org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:lambda$setStorageTypeQuota$1(long[],org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean:getHostAndPort()	0	java.lang.String	0	
org.apache.hadoop.hdfs.qjournal.server.JournalNodeMXBean:getVersion()	0	java.lang.String	0	
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:createEditsSyncDir()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeProxies()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeProxies()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:getOtherJournalNodeAddrs()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:downloadMissingLogSegment(java.net.URL,org.apache.hadoop.hdfs.server.protocol.RemoteEditLog)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:lambda$getJournalAddrList$1(java.net.InetSocketAddress,java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:lambda$getJournalAddrList$1(java.net.InetSocketAddress,java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNode:getJournalSyncerStatus(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNode:isStarted()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNode:isStarted()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JournalNode:lambda$getClusterIds$1(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalNode:lambda$getClusterIds$1(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.Journal:scanStorageForLatestEdits()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.server.Journal:getCurrentLagTxns()	0	long	0	0
org.apache.hadoop.hdfs.qjournal.server.Journal:getSegmentInfo(long)	0	null	0	null
org.apache.hadoop.hdfs.qjournal.server.Journal:getPersistedPaxosData(long)	0	null	0	null
org.apache.hadoop.hdfs.qjournal.server.Journal:moveTmpSegmentToCurrent(java.io.File,java.io.File,long)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JNStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.JNStorage:isFormatted()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JNStorage:isFormatted()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:isValidRequestor(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkRequestorOrSendError(org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	1	int	0	1
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:checkStorageInfoOrSendError(org.apache.hadoop.hdfs.qjournal.server.JNStorage,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)	1	int	0	1
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getLastWriterEpoch()	0	long	0	-1
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getLastPromisedEpoch()	0	long	0	-1
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getCurrentLagTxns()	0	long	0	-1
org.apache.hadoop.hdfs.qjournal.server.Journal$1:run()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:hasTxnCount()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:hasTxnCount()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:hasEditLog()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:hasEditLog()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasNsInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasNsInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasSegmentState()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasSegmentState()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasAcceptedInEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasAcceptedInEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasLastWriterEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasLastWriterEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasLastCommittedTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto$Builder:hasLastCommittedTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasManifest()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasManifest()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasHttpPort()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasHttpPort()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto$Builder:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder:hasResultCTime()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto$Builder:hasResultCTime()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasMaxTxns()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasMaxTxns()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasJournalId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasJournalId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasIpcSerialNumber()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasIpcSerialNumber()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasCommittedTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasCommittedTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:hasSegmentState()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:hasSegmentState()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:hasAcceptedInEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData$Builder:hasAcceptedInEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasNsInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasNsInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasForce()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto$Builder:hasForce()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:hasSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:hasSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasPrevStorage()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasPrevStorage()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasTargetLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasTargetLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasSegmentState()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasSegmentState()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasAcceptedInEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasAcceptedInEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasLastWriterEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasLastWriterEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasLastCommittedTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:hasLastCommittedTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:hasMinTxIdToKeep()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto$Builder:hasMinTxIdToKeep()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasFirstTxnId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasFirstTxnId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasNumTxns()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasNumTxns()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasRecords()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasRecords()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasSegmentTxnId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasSegmentTxnId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasJournalId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasJournalId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasIpcSerialNumber()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasIpcSerialNumber()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasCommittedTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasCommittedTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$RequestInfoProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.InterQJournalProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:hasSInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:hasSInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasFirstTxnId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasFirstTxnId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasNumTxns()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasNumTxns()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasRecords()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasRecords()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasSegmentTxnId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasSegmentTxnId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:hasIsFormatted()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:hasIsFormatted()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:hasResultCTime()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:hasResultCTime()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackRequestProto$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasIsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:hasIsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasIsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$SegmentStateProto$Builder:hasIsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasManifest()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasManifest()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasHttpPort()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasHttpPort()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasInProgressOk()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasInProgressOk()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasStateToAccept()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasStateToAccept()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto$Builder:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasLastPromisedEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasLastPromisedEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasHttpPort()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasHttpPort()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasNsInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasNsInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasInProgressOk()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasInProgressOk()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetEditLogManifestRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoRollbackResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:hasCanRollBack()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:hasCanRollBack()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasNsInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasNsInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasForce()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:hasForce()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$HeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo:hasCommittedTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.RequestInfo:hasCommittedTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder:hasLastSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto$Builder:hasLastSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasPrevStorage()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasPrevStorage()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasTargetLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasTargetLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasMaxTxns()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasMaxTxns()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:hasSInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeRequestProto$Builder:hasSInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FormatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentRequestProto$Builder:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasLastPromisedEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasLastPromisedEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasHttpPort()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasHttpPort()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateResponseProto$Builder:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:hasTxnCount()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:hasTxnCount()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:hasEditLog()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournaledEditsResponseProto$Builder:hasEditLog()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$FinalizeLogSegmentResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:hasLastSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:hasLastSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$NewEpochResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder:hasCanRollBack()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$CanRollBackResponseProto$Builder:hasCanRollBack()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoFinalizeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasStateToAccept()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasStateToAccept()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasFromURL()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:hasFromURL()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$AcceptRecoveryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:hasSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PrepareRecoveryRequestProto$Builder:hasSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder:hasIdentifier()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto$Builder:hasIdentifier()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalCTimeRequestProto$Builder:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:hasIdentifier()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:hasIdentifier()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$JournalIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:hasSegmentState()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:hasSegmentState()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:hasAcceptedInEpoch()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:hasAcceptedInEpoch()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PersistedRecoveryPaxosData:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder:hasIsFormatted()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$IsFormattedResponseProto$Builder:hasIsFormatted()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DiscardSegmentsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:hasNameServiceId()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:hasNameServiceId()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$GetJournalStateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:hasJid()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:hasJid()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$DoPreUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:hasReqInfo()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:hasReqInfo()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:hasMinTxIdToKeep()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:hasMinTxIdToKeep()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$PurgeLogsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$19:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$18:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$17:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.QuorumCall:getQuorumTimeoutIncreaseMillis(long,int)	0	long	0	-1
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$16:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:hasSomeData()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:hasSomeData()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.client.SegmentRecoveryComparator:compare(java.util.Map$Entry,java.util.Map$Entry)	0	int	0	0
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$12:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$22:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$21:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$11:call()	0	null	0	null
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel:hasHttpServerEndPoint()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel:hasHttpServerEndPoint()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet:isEpochEstablished()	0	int	0	1
org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet:isEpochEstablished()	1	int	0	0
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$9:call()	0	null	0	null
org.apache.hadoop.hdfs.tools.DFSAdmin:triggerBlockReport(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:triggerBlockReport(java.lang.String[])	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:saveNamespace(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:rollEdits()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:listOpenFiles(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:getBalancerBandwidth(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:fetchImage(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:finalizeUpgrade()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:getUpgradeStatus()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:upgrade(java.lang.String)	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:metaSave(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:printTopology()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshServiceAcl()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshUserToGroupsMappings()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshSuperUserGroupsConfiguration()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshCallQueue()	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:reconfig(java.lang.String[],int)	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfigurationUtil(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfigurationUtil(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfiguration(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfigurationDispatch(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfigurationDispatch(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	1	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurationStatusUtil(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurationStatusUtil(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurationStatus(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurationStatus(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurableProperties(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin:getReconfigurableProperties(java.lang.String,java.lang.String,java.io.PrintStream,java.io.PrintStream)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:genericRefresh(java.lang.String[],int)	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:getVolumeReport(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:deleteBlockPool(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:deleteBlockPool(java.lang.String[],int)	1	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:refreshNamenodes(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:shutdownDatanode(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:shutdownDatanode(java.lang.String[],int)	1	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin:evictWriters(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSAdmin:getDatanodeInfo(java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.GetConf$SecondaryNameNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.GetConf$CommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.GetConf$CommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.GetConf$CommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.ECAdmin$EnableECPolicyCommand:getName()	0	java.lang.String	0	-enablePolicy
org.apache.hadoop.hdfs.tools.ECAdmin$EnableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$EnableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$EnableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.JMXGet:getValue(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyECCommand:run(java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.GetConf$JournalNodeCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.GetConf$NameNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.DebugAdmin$VerifyMetaCommand:run(java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$ListStoragePoliciesCommand:getName()	0	java.lang.String	0	-listPolicies
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$ListStoragePoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$ListStoragePoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.ECAdmin$GetECPolicyCommand:getName()	0	java.lang.String	0	-getPolicy
org.apache.hadoop.hdfs.tools.ECAdmin$GetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$GetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$GetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DFSck:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSck:getCurrentNamenodeAddress(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.hdfs.tools.DFSck:doWork(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSck:doWork(java.lang.String[])	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCachePoolCommand:getName()	0	java.lang.String	0	-removePool
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand:getName()	0	java.lang.String	0	-setPolicy
org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$SetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	3	int	0	3
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCacheDirectiveInfoCommand:getName()	0	java.lang.String	0	-addDirective
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DebugAdmin$HelpCommand:run(java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.tools.DFSHAAdmin:getUsageString()	0	java.lang.String	0	Usage: haadmin [-ns <nameserviceId>]
org.apache.hadoop.hdfs.tools.DFSHAAdmin:runCmd(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSHAAdmin:checkSupportObserver(org.apache.hadoop.ha.HAServiceTarget)	0	int	0	1
org.apache.hadoop.hdfs.tools.DFSHAAdmin:checkSupportObserver(org.apache.hadoop.ha.HAServiceTarget)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSHAAdmin:transitionToObserver(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSHAAdmin:transitionToObserver(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine)	0	int	0	-1
org.apache.hadoop.hdfs.tools.DFSHAAdmin:failover(org.apache.commons.cli.CommandLine)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand:getName()	0	java.lang.String	0	-listCodecs
org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$ListECCodecsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff:getSnapshotName(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.GetConf$NNRpcAddressesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.GetConf$NNRpcAddressesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:getCommandName()	0	java.lang.String	0	clrQuota
org.apache.hadoop.hdfs.tools.ECAdmin$UnsetECPolicyCommand:getName()	0	java.lang.String	0	-unsetPolicy
org.apache.hadoop.hdfs.tools.ECAdmin$UnsetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$UnsetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$UnsetECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$DirPathCache:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:getStoragePolicy(org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto)	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap:getParentPath(long)	0	java.lang.String	0	/
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageXmlWriter$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	1	int	0	-1
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$1:compare(byte[],byte[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$1:compare(byte[],byte[])	1	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$1:compare(byte[],byte[])	2	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:getEntry(java.lang.String,org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode)	0	java.lang.String	0	
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChild(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChildStr(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChildInt(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChildLong(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChildBool(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:removeChildBool(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor$Node:getRemainingKeyNames()	0	java.lang.String	0	
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	1	int	0	-1
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$Dir:getPath()	0	java.lang.String	0	/
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$Dir:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$Dir:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$2:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader$2:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	1	int	0	-1
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector$CorruptionChecker:getTypeOfId(long)	0	java.lang.String	0	Node
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector$CorruptionChecker:getTypeOfId(long)	1	java.lang.String	0	Ref
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector$CorruptionChecker:getTypeOfId(long)	2	java.lang.String	0	Unknown
org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent:canLoadVersion(int)	0	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent:canLoadVersion(int)	1	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB:getParentPath(long)	0	java.lang.String	0	/
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:isHelpOption(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:isHelpOption(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1:run()	0	null	0	null
org.apache.hadoop.hdfs.tools.DebugAdmin:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.DebugAdmin:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.hdfs.tools.DFSAdmin$RollingUpgradeCommand:run(org.apache.hadoop.hdfs.DistributedFileSystem,java.lang.String[],int)	0	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ProvisionTrashCommand:getName()	0	java.lang.String	0	-provisionTrash
org.apache.hadoop.hdfs.tools.CryptoAdmin$ProvisionTrashCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin$ProvisionTrashCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ProvisionTrashCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCachePoolCommand:getName()	0	java.lang.String	0	-modifyPool
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:getName()	0	java.lang.String	0	-setStoragePolicy
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListZonesCommand:getName()	0	java.lang.String	0	-listZones
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListZonesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListZonesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListZonesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SatisfyStoragePolicyCommand:getName()	0	java.lang.String	0	-satisfyStoragePolicy
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SatisfyStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SatisfyStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$SatisfyStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListReencryptionStatusCommand:getName()	0	java.lang.String	0	-listReencryptionStatus
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListReencryptionStatusCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ListReencryptionStatusCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCacheDirectiveInfoCommand:getName()	0	java.lang.String	0	-listDirectives
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfosCommand:getName()	0	java.lang.String	0	-removeDirectives
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfosCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand:getName()	0	java.lang.String	0	-addPool
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand:getShortUsage()	0	java.lang.String	1	Wy1hZGRQb29sIDxuYW1lPiBbLW93bmVyIDxvd25lcj5dIFstZ3JvdXAgPGdyb3VwPl0gWy1tb2RlIDxtb2RlPl0gWy1saW1pdCA8bGltaXQ+XSBbLWRlZmF1bHRSZXBsaWNhdGlvbiA8ZGVmYXVsdFJlcGxpY2F0aW9uPl0gWy1tYXhUdGwgPG1heFR0bD5dXQo=
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$AddCachePoolCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:getCommandName()	0	java.lang.String	0	setSpaceQuota
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfoCommand:getName()	0	java.lang.String	0	-removeDirective
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$RemoveCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$UnsetStoragePolicyCommand:getName()	0	java.lang.String	0	-unsetStoragePolicy
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$UnsetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$UnsetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$UnsetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:go(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer$Flags,org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsVisitor)	0	int	0	-1
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:go(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer$Flags,org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsVisitor)	1	int	0	0
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:isHelpOption(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:isHelpOption(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:getName()	0	java.lang.String	0	-reencryptZone
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	3
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	3	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$ReencryptZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	4	int	0	4
org.apache.hadoop.hdfs.tools.ECAdmin$DisableECPolicyCommand:getName()	0	java.lang.String	0	-disablePolicy
org.apache.hadoop.hdfs.tools.ECAdmin$DisableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$DisableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$DisableECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DFSAdmin$1:run()	0	null	0	null
org.apache.hadoop.hdfs.tools.GetConf:doWork(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.tools.ECAdmin:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:getCommandName()	0	java.lang.String	0	setQuota
org.apache.hadoop.hdfs.tools.AdminHelper$HelpCommand:getName()	0	java.lang.String	0	-help
org.apache.hadoop.hdfs.tools.AdminHelper$HelpCommand:getShortUsage()	0	java.lang.String	1	Wy1oZWxwIDxjb21tYW5kLW5hbWU+XQo=
org.apache.hadoop.hdfs.tools.AdminHelper$HelpCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.AdminHelper$HelpCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand:getName()	0	java.lang.String	0	-listPools
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$ListCachePoolsCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.ECAdmin$ListECPoliciesCommand:getName()	0	java.lang.String	0	-listPolicies
org.apache.hadoop.hdfs.tools.ECAdmin$ListECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$ListECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$ListECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.ECAdmin$VerifyClusterSetupCommand:getName()	0	java.lang.String	0	-verifyClusterSetup
org.apache.hadoop.hdfs.tools.ECAdmin$VerifyClusterSetupCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$VerifyClusterSetupCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.GetConf$BackupNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])	0	int	0	0
org.apache.hadoop.hdfs.tools.NNHAServiceTarget:supportObserver()	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$RemoveECPolicyCommand:getName()	0	java.lang.String	0	-removePolicy
org.apache.hadoop.hdfs.tools.ECAdmin$RemoveECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$RemoveECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$RemoveECPolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCacheDirectiveInfoCommand:getName()	0	java.lang.String	0	-modifyDirective
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CacheAdmin$ModifyCacheDirectiveInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DebugAdmin$RecoverLeaseCommand:run(java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.DebugAdmin$RecoverLeaseCommand:run(java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:getName()	0	java.lang.String	0	-addPolicies
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.ECAdmin$AddECPoliciesCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List)	2	int	0	3
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List)	3	int	0	4
org.apache.hadoop.hdfs.tools.DebugAdmin$ComputeMetaCommand:run(java.util.List)	4	int	0	5
org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand:getName()	0	java.lang.String	0	-getFileEncryptionInfo
org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	2
org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$GetFileEncryptionInfoCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	3	int	0	3
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:getCommandName()	0	java.lang.String	0	clrSpaceQuota
org.apache.hadoop.hdfs.tools.CryptoAdmin$CreateZoneCommand:getName()	0	java.lang.String	0	-createZone
org.apache.hadoop.hdfs.tools.CryptoAdmin$CreateZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.CryptoAdmin$CreateZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.CryptoAdmin$CreateZoneCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:getName()	0	java.lang.String	0	-getStoragePolicy
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin$GetStoragePolicyCommand:run(org.apache.hadoop.conf.Configuration,java.util.List)	2	int	0	2
org.apache.hadoop.hdfs.web.resources.NamenodeAddressParam:getName()	0	java.lang.String	0	namenoderpcaddress
org.apache.hadoop.hdfs.web.resources.TokenServiceParam:getName()	0	java.lang.String	0	service
org.apache.hadoop.hdfs.web.resources.UriFsPathParam:getName()	0	java.lang.String	0	path
org.apache.hadoop.hdfs.web.resources.UriFsPathParam:getAbsolutePath()	0	null	0	null
org.apache.hadoop.hdfs.web.resources.TokenKindParam:getName()	0	java.lang.String	0	kind
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.security.token.Token)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.hdfs.protocol.HdfsFileStatus,boolean)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonArray(org.apache.hadoop.hdfs.protocol.DatanodeInfo[])	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonArray(org.apache.hadoop.fs.StorageType[])	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.hdfs.protocol.LocatedBlock)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.hdfs.protocol.DirectoryListing)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.hdfs.protocol.HdfsFileStatus[])	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonArray(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.hdfs.protocol.LocatedBlocks)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.fs.ContentSummary)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.fs.QuotaUsage)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.fs.MD5MD5CRC32FileChecksum)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.fs.permission.AclStatus)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.fs.XAttr,org.apache.hadoop.fs.XAttrCodec)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonArray(java.util.List,org.apache.hadoop.fs.XAttrCodec)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonMap(org.apache.hadoop.fs.BlockLocation)	0	null	0	null
org.apache.hadoop.hdfs.web.JsonUtil:toJsonString(org.apache.hadoop.fs.BlockLocation[])	0	null	0	null
org.apache.hadoop.hdfs.web.ParamFilter:getResponseFilter()	0	null	0	null
org.apache.hadoop.hdfs.web.ParamFilter:containsUpperCase(java.lang.Iterable)	0	int	0	1
org.apache.hadoop.hdfs.web.ParamFilter:containsUpperCase(java.lang.Iterable)	1	int	0	0
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:getDefaultTop()	0	int	0	100
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet:isBalancingNeeded(double)	0	int	0	0
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet:isBalancingNeeded(double)	1	int	0	1
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster:computePoolSize(int)	0	int	0	100
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:compareTo(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)	0	int	0	-1
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:compareTo(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)	1	int	0	0
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:compareTo(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)	2	int	0	1
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:isBalancingNeeded(double)	0	int	0	1
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode:isBalancingNeeded(double)	1	int	0	0
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbReader$LevelDbIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbReader$LevelDbIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBReader$FRIterator:next()	0	null	0	null
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap:blockPoolIDFromFileName(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader$FRIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader$FRIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap$LevelDBReader:iterator()	0	null	0	null
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap$TextReader:nextInternal(java.util.Iterator)	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getBlockPoolCurrentDir(java.lang.String,org.apache.hadoop.hdfs.server.datanode.StorageLocation)	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getStorageLocationFile(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getDirecorySize()	0	long	0	0
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getCurrentDir()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getVersionFile()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getPreviousVersionFile()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getPreviousDir()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getPreviousTmp()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getRemovedTmp()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getFinalizedTmp()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getLastCheckpointTmp()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:getPreviousCheckpoint()	0	null	0	null
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:hasSomeData()	0	int	0	0
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:hasSomeData()	1	int	0	1
org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory:isLockSupported()	0	int	0	0
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(java.io.File)	0	int	0	1
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(java.io.File)	1	int	0	0
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	0	int	0	1
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	1	int	0	0
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.common.Storage:containsStorageDir(org.apache.hadoop.hdfs.server.datanode.StorageLocation,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.common.Storage:confirmFormat(java.lang.Iterable,boolean,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.common.Storage:confirmFormat(java.lang.Iterable,boolean,boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.common.Storage:is203LayoutVersion(int)	0	int	0	1
org.apache.hadoop.hdfs.server.common.Storage:is203LayoutVersion(int)	1	int	0	0
org.apache.hadoop.hdfs.server.common.Storage$DirIterator:hasNext()	0	int	0	0
org.apache.hadoop.hdfs.server.common.Storage$DirIterator:hasNext()	1	int	0	1
org.apache.hadoop.hdfs.server.common.Storage$DirIterator:shouldReturnNextDir()	0	int	0	1
org.apache.hadoop.hdfs.server.common.Storage$DirIterator:shouldReturnNextDir()	1	int	0	0
org.apache.hadoop.hdfs.server.common.FileRegion:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.common.FileRegion:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:createRecoveryContext()	0	null	0	null
org.apache.hadoop.hdfs.server.common.MetricsLoggerTask:hasAppenders(org.apache.commons.logging.Log)	0	int	0	1
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:matchRule(java.lang.String,java.lang.String,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:lambda$loadRuleMap$4(java.util.regex.Pattern,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:lambda$loadRuleMap$4(java.util.regex.Pattern,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:lambda$static$0(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:lambda$static$0(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$RollingUpgradeStartupOption:matches(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)	0	int	0	1
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$RollingUpgradeStartupOption:matches(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)	1	int	0	0
org.apache.hadoop.hdfs.server.common.StorageInfo:readPropertiesFile(java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.server.mover.Mover$Processor:isSnapshotPathInCurrent(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$Processor:isSnapshotPathInCurrent(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoves4Block(org.apache.hadoop.hdfs.server.mover.Mover$StorageTypeDiff,org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy)	0	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoves4Block(org.apache.hadoop.hdfs.server.mover.Mover$StorageTypeDiff,org.apache.hadoop.hdfs.protocol.LocatedBlock,org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy)	1	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoveReplica(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.mover.Mover$MLocation,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoveReplica(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoveReplica(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$Processor:chooseTargetInSameNode(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$Processor:chooseTargetInSameNode(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List)	1	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$Processor:chooseTarget(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List,org.apache.hadoop.hdfs.server.balancer.Matcher)	0	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$Processor:chooseTarget(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source,java.util.List,org.apache.hadoop.hdfs.server.balancer.Matcher)	1	int	0	0
org.apache.hadoop.hdfs.server.mover.Mover$StorageTypeDiff:removeOverlap(boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.mover.Mover$StorageTypeDiff:removeOverlap(boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerLatencyWithReportingNode:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerLatencyWithReportingNode:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped$1$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped$1$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isInSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isInSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isSafeModeTrackingBlocks()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isSafeModeTrackingBlocks()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:leaveSafeMode(boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:timeToLeaveExtension()	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isInRollBackMode(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:isInRollBackMode(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:areThresholdsMet()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:areThresholdsMet()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusDefault:isPlacementPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusDefault:isPlacementPolicySatisfied()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusDefault:getErrorDescription()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusDefault:getAdditionalReplicasRequired()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:registerNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:requestLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:pruneIfExpired(long,org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:pruneIfExpired(long,org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager$NodeData)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:checkLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,long)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:removeLease(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:contains(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:contains(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.Block)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.InvalidateBlocks:invalidateWork(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:isPlacementPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:isPlacementPolicySatisfied()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:isNodeGroupPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:isNodeGroupPolicySatisfied()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:getErrorDescription()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithNodeGroup:getAdditionalReplicasRequired()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:toDatanodeInfos(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:toStorageIDs(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:toStorageTypes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlocksOnFailedStorage()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:areBlocksOnFailedStorage()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo:getDatanodeStorageInfo(java.lang.Iterable,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo$BlockIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker:isSlowPeerTrackerEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$StorageIterator:hasNext()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$StorageIterator:hasNext()	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks:getNumReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks:getTimedOutBlocks()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:isSufficient(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,boolean,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:getCapacity()	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:isProvidedStorage(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:isProvidedStorage(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isDeleted()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:getDatanode(int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:findStorageInfo(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:listRemove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo:isCompleteOrCommitted()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo:toString()	0	java.lang.String	0	PROVIDED-STORAGE
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager:parseEntry(java.lang.String,java.lang.String,int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2:isPlacementPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2:getErrorDescription()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$2:getAdditionalReplicasRequired()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.HostSet:matchedBy(java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.HostSet:matchedBy(java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.HostSet:match(java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.HostSet:match(java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:hasStaleStorages()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:hasStaleStorages()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:getLeaseRecoveryCommand(int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:getInvalidateBlocks(int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:chooseStorage4Block(org.apache.hadoop.fs.StorageType,long)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:checkBlockReportReceived()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:checkBlockReportReceived()	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:isRegistered()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:isRegistered()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:hasStorageType(org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:hasStorageType(org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:isPlacementPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:isPlacementPolicySatisfied()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:isUpgradeDomainPolicySatisfied()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:isUpgradeDomainPolicySatisfied()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:getErrorDescription()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementStatusWithUpgradeDomain:getAdditionalReplicasRequired()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:checkToDeleteIterator()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber:checkToDeleteIterator()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ErasureCodingWork:hasAllInternalBlocks()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ErasureCodingWork:hasAllInternalBlocks()	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockIdGenerator:isValidBlock(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockIdGenerator:isValidBlock(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite:isStoragePolicyXAttr(org.apache.hadoop.fs.XAttr)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite:isStoragePolicyXAttr(org.apache.hadoop.fs.XAttr)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockQueue:poll(int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedBlockList:getNumberOfBlocks()	0	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:addStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:removeStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:removeStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:isProvided()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:isProvided()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:numNodes()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:isStriped()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:hasNoStorage()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous:hasNoStorage()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:useDelHint(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,java.util.List,java.util.Collection,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:isMovableBasedOnUpgradeDomain(java.util.Collection,java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:isMovableBasedOnUpgradeDomain(java.util.Collection,java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:isMovable(java.util.Collection,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:nextBlockAddedToPending(java.util.Iterator,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:nextBlockAddedToPending(java.util.Iterator,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:getPendingCount()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:getYetToBeProcessedCount()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:isBlockReplicatedOk(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,java.lang.Boolean,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor$BlockStats)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:isBlockReplicatedOk(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,java.lang.Boolean,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor$BlockStats)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:lambda$check$2(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:lambda$check$2(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:lambda$run$0(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:lambda$run$0(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:getSize4Testing(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:contains(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:contains(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ExcessRedundancyMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createBlockTokenSecretManager(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isBlockTokenEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isBlockTokenEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:shouldUpdateBlockKey(long)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:shouldUpdateBlockKey(long)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasMinStorage(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasMinStorage(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasMinStorage(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasMinStorage(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:commitBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:commitBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.protocol.Block)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:commitOrCompleteLastBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:convertLastBlockToUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockCollection,long)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createLocatedBlocks(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],long,boolean,long,long,boolean,boolean,org.apache.hadoop.fs.FileEncryptionInfo,org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isSufficientlyReplicated(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isSufficientlyReplicated(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:invalidateBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasEnoughEffectiveReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:hasEnoughEffectiveReplicas(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:scheduleReconstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:validateReconstructionWork(org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:validateReconstructionWork(org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:requestBlockReportLeaseId(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:checkBlockReportLease(org.apache.hadoop.hdfs.server.protocol.BlockReportContext,org.apache.hadoop.hdfs.protocol.DatanodeID)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReport(org.apache.hadoop.hdfs.protocol.DatanodeID,org.apache.hadoop.hdfs.server.protocol.DatanodeStorage,org.apache.hadoop.hdfs.protocol.BlockListAsLongs,org.apache.hadoop.hdfs.server.protocol.BlockReportContext)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,java.util.Collection,java.util.Collection,java.util.Collection,java.util.Collection)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isBlockUnderConstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$BlockUCState,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:shouldProcessExtraRedundancy(org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:shouldProcessExtraRedundancy(org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:addBlock(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,java.util.List)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:processAndHandleReportedBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNodeHealthyForDecommissionOrMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNodeHealthyForDecommissionOrMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNeededReconstructionForMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNeededReconstructionForMaintenance(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNeededReconstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isNeededReconstruction(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.NumberReplicas,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:computeDatanodeWork()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:isPopulatingReplQueues()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:shouldPopulateReplQueues()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createSPSManager(org.apache.hadoop.conf.Configuration,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:createSPSManager(org.apache.hadoop.conf.Configuration,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:lambda$getBlocksWithLocations$0(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:lambda$getBlocksWithLocations$0(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)	1	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,boolean)	2	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap:numNodes(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap:removeNode(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap:removeBlock(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap:size()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:addToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:addToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:useDelHint(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,java.util.List,java.util.Collection,java.util.List)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:notReduceNumOfGroups(java.util.List,java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault:notReduceNumOfGroups(java.util.List,java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$3:call()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks:isUnderRecovery(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks:isUnderRecovery(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerJsonReport:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerJsonReport:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:isIncluded(java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:isIncluded(java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:getMaintenanceExpireTimeInMS(java.net.InetSocketAddress)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$getMaintenanceExpireTimeInMS$5(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$getMaintenanceExpireTimeInMS$5(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$getUpgradeDomain$2(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$getUpgradeDomain$2(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$isExcluded$1(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$isExcluded$1(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$isIncluded$0(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager$HostProperties:lambda$isIncluded$0(java.net.InetSocketAddress,org.apache.hadoop.hdfs.protocol.DatanodeAdminProperties)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:parseEntry(java.lang.String,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:isIncluded(org.apache.hadoop.hdfs.protocol.DatanodeID)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:isIncluded(org.apache.hadoop.hdfs.protocol.DatanodeID)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:getUpgradeDomain(org.apache.hadoop.hdfs.protocol.DatanodeID)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:getMaintenanceExpirationTimeInMS(org.apache.hadoop.hdfs.protocol.DatanodeID)	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:exceededNumBlocksPerCheck()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:exceededNumBlocksPerCheck()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:getPendingRepLimit()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:getBlocksPerLock()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt:isCorruptedDuringWrite()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockToMarkCorrupt:isCorruptedDuringWrite()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap:contains(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap:remove(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap:getDatanodeByHost(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap:getDatanodeByXferAddr(java.lang.String,int)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap:getDataNodeByHostName(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:compareDataNode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	2	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:contains(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:contains(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriority(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)	0	int	0	3
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityContiguous(int,int,int,int)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityContiguous(int,int,int,int)	1	int	0	4
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityContiguous(int,int,int,int)	2	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityContiguous(int,int,int,int)	3	int	0	2
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityStriped(int,int,short,short)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityStriped(int,int,short,short)	1	int	0	4
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityStriped(int,int,short,short)	2	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:getPriorityStriped(int,int,short,short)	3	int	0	2
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int,int,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:add(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:remove(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,int,int)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:shouldAbortHeartbeatCheck(long)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:shouldAbortHeartbeatCheck(long)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$BlockIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:addStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:findStorageInfoFromEnd(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getStorageBlockIndex(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:getBlockOnStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:removeStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:removeStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:isStriped()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:hasNoStorage()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:hasNoStorage()	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped:isProvided()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockGroupIdGenerator:hasValidBlockInRange(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SequentialBlockGroupIdGenerator:hasValidBlockInRange(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$LeavingServiceStatus:getUnderReplicatedBlocks()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$LeavingServiceStatus:getOutOfServiceOnlyReplicas()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$LeavingServiceStatus:getUnderReplicatedInOpenFiles()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$LeavingServiceStatus:getStartTime()	0	long	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:findReasonForNotCaching(org.apache.hadoop.hdfs.server.namenode.CachedBlock,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	java.lang.String	0	not tracked by the BlockManager
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:findReasonForNotCaching(org.apache.hadoop.hdfs.server.namenode.CachedBlock,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	java.lang.String	0	not complete
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:findReasonForNotCaching(org.apache.hadoop.hdfs.server.namenode.CachedBlock,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	2	java.lang.String	0	not needed by any directives
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:findReasonForNotCaching(org.apache.hadoop.hdfs.server.namenode.CachedBlock,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	3	java.lang.String	0	no longer needed by any directives
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:addDependentNodesToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:isMovable(java.util.Collection,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:isMovable(java.util.Collection,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker:getSlowDiskReportAsJsonString()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.ReplicaUnderConstruction:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isInactive(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isInactive(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isSlowNode(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isSlowNode(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanode(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanode(org.apache.hadoop.hdfs.protocol.DatanodeID)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getDatanodeStorageInfos(org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[],java.lang.String,java.lang.Object[])	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isDatanodeDead(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isDatanodeDead(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:shouldCountVersion(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:shouldCountVersion(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:shouldAvoidStaleDataNodesForWrite()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:shouldAvoidStaleDataNodesForWrite()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isNameResolved(java.net.InetAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:isNameResolved(java.net.InetAddress)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getBlockRecoveryCommand(java.lang.String,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:getCacheCommand(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$CachedBlocksList,int,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$SafeModeMonitor:canLeave()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode$SafeModeMonitor:canLeave()	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LocatedBlockBuilder:isBlockMax()	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.LocatedBlockBuilder:isBlockMax()	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks$BlockRecoveryAttempt:hasTimedOut(long)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks$BlockRecoveryAttempt:hasTimedOut(long)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.PendingRecoveryBlocks$BlockRecoveryAttempt:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:toString()	0	java.lang.String	0	PROVIDED-LOCATION
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:getNetworkLocation()	0	java.lang.String	0	/REMOTE
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:getName()	0	java.lang.String	0	PROVIDED
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:isSlowPeerTrackerEnabled()	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getJson()	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:removeFromCorruptReplicasMap(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:removeFromCorruptReplicasMap(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap$Reason)	1	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:getNodes(org.apache.hadoop.hdfs.protocol.Block)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:isReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:isReplicaCorrupt(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:numCorruptReplicas(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:getCorruptBlockIdsForTesting(org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager,org.apache.hadoop.hdfs.protocol.BlockType,int,java.lang.Long)	0	null	0	null
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:lambda$getCorruptBlockIdsForTesting$0(org.apache.hadoop.hdfs.protocol.BlockType,org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager,long,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.CorruptReplicasMap:lambda$getCorruptBlockIdsForTesting$0(org.apache.hadoop.hdfs.protocol.BlockType,org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager,long,org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isLegacyBlock(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isLegacyBlock(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isGenStampInFuture(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isGenStampInFuture(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isStripedBlock(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isStripedBlock(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isStripedBlockID(long)	0	int	0	1
org.apache.hadoop.hdfs.server.blockmanagement.BlockIdManager:isStripedBlockID(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.LocalReplica:breakHardLinksIfNeeded()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.LocalReplica:renameFile(java.io.File,java.io.File)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:getMetadataURI()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:getMetadataOutputStream(boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:blockDataExists()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:metadataExists()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:metadataExists()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:getMetadataLength()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:getPinning(org.apache.hadoop.fs.LocalFileSystem)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:breakHardLinksIfNeeded()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:compareWith(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$VolumePair:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.LocalReplicaInPipeline:getVisibleLength()	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.BlockSender:isLongRead()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockSender:isLongRead()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:getBytesReserved()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:getOriginalBytesReserved()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents:beforeMetadataOp(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.FileIoProvider$OPERATION)	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents:beforeFileIo(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.FileIoProvider$OPERATION,long)	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm:handle(org.apache.hadoop.net.unix.DomainSocket)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataStorage:createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,boolean,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataStorage:createStorageID(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,boolean,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:getDomainPeerServer(org.apache.hadoop.conf.Configuration,int)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.DataNode:getXceiverCount()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:getActiveTransferThreadCount()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:isDatanodeUp()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:isDatanodeUp()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter$SecureResources)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.DataNode:parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:parseArguments(java.lang.String[],org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:getDiskBalancerStatus()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.datanode.DataNode:isConnectedToNN(java.net.InetSocketAddress)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:isBPServiceAlive(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:isDatanodeFullyStarted(boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:isDatanodeFullyStarted(boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:getDatanodeUuid()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.DataNode:getSlowDisks()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.DataNode:isTransfer(org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:isTransfer(org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNode:isWrite(org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode:isWrite(org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer:waitAllPeers(long,java.util.concurrent.TimeUnit)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector:dropHeartbeatPacket()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:shouldVerifyChecksum()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:shouldVerifyChecksum()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:receivePacket()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:receivePacket()	1	int	0	-1
org.apache.hadoop.hdfs.server.datanode.BlockReceiver:getVolumeBaseUri()	0	java.lang.String	0	unavailable
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:processBlockMunlockRequest(org.apache.hadoop.hdfs.ExtendedBlockId)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:getClientNames(org.apache.hadoop.hdfs.ExtendedBlockId)	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer:getFilterHandlers(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser:unmaskedPermission()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser:param(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.ParameterParser:decodeHexNibble(char)	0	int	0	65535
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler:initializeState(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:doTransition(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,java.util.List,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:restoreBlockFilesFromTrash(java.io.File)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:isTrashAllowed(java.io.File)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:isTrashAllowed(java.io.File)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:trashEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage:trashEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager$PerStorageIBR:removeAll()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:positiveMsToHours(long)	0	double	0	0.0
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter()	0	long	0	9223372036854775807
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:findNextUsableBlockIter()	1	long	0	0
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:scanBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:getNextBlockToScan()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:popNextSuspectBlock()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BlockScanner:isEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockScanner:isEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockScanner:hasAnyRegisteredScanner()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockScanner:hasAnyRegisteredScanner()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockScanner:getVolumeStats(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:isRunning()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder:isRunning()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getVisibleLength()	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[])	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread:processCommand(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand[])	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.StorageLocation:matchesStorageDirectory(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.StorageLocation:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.StorageLocation:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.StorageLocation:compareTo(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.StorageLocation:compareTo(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	1	int	0	-1
org.apache.hadoop.hdfs.server.datanode.StorageLocation:compareTo(org.apache.hadoop.hdfs.server.datanode.StorageLocation)	2	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casWaiters(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Waiter,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Waiter)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casWaiters(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Waiter,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Waiter)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casListeners(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Listener,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Listener)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casListeners(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Listener,org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$Listener)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casValue(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$SynchronizedHelper:casValue(org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture,java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:getDoneValue(java.lang.Object)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:wasInterrupted()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:wasInterrupted()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:set(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:set(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:setException(java.lang.Throwable)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:setException(java.lang.Throwable)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:setFuture(org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableFuture)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:setFuture(org.apache.hadoop.thirdparty.com.google.common.util.concurrent.ListenableFuture)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$DirectExecutor:toString()	0	java.lang.String	0	MoreExecutors.directExecutor()
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$Callback)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:checkVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$Callback)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataStorage$3:call()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy$AvailableSpaceVolumeList:areAllVolumesWithinFreeSpaceThreshold()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy$AvailableSpaceVolumeList:areAllVolumesWithinFreeSpaceThreshold()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:getBlockFile()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:getMetaFile()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:getGenStamp()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$ScanInfo:fullMetaFile()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$Factory:isSimulated()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockDirFilter:accept(java.io.File,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockDirFilter:accept(java.io.File,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$State:shouldAdvertise()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$State:shouldAdvertise()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader:isTransientCache()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader:getRecoveredMappableBlock(java.io.File,java.lang.String,byte)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader:isNativeLoader()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.AddBlockPoolException:hasExceptions()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.AddBlockPoolException:hasExceptions()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:checkVolumesRemoved()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:checkVolumesRemoved()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:lambda$chooseVolume$0(java.util.List,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList:lambda$chooseVolume$0(java.util.List,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappedBlock:getAddress()	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappedBlock:getKey()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:getReplicaCachePath(java.lang.String,long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:getCacheAddress(java.lang.String,long)	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:isCached(java.lang.String,long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:isCached(java.lang.String,long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker:getReplica(java.lang.String,long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:deleteFiles()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:deleteFiles()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:moveFiles()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService$ReplicaFileDeleteTask:moveFiles()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getNextSubDir(java.lang.String,java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getNextFinalizedSubDir()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:getSubdirEntries()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl:nextBlock()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:reserve(org.apache.hadoop.hdfs.ExtendedBlockId,long)	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:getCachePath(org.apache.hadoop.hdfs.ExtendedBlockId)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockFileFilter:accept(java.io.File,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockFileFilter:accept(java.io.File,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:fetchReplicaInfo(java.lang.String,long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getMetaDataInputStream(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:hasEnoughResource()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:hasEnoughResource()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getVolumeFailureSummary()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isReplicaProvided(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isReplicaProvided(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:delBlockFromDisk(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:delBlockFromDisk(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isValidBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isValidRbw(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isValid(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isValid(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:initReplicaRecoveryImpl(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.protocol.Block,long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:ramDiskConfigured()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:ramDiskConfigured()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:getPinning(org.apache.hadoop.hdfs.protocol.ExtendedBlock)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:isDeletingBlock(java.lang.String,long)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:reserveLockedMemory(long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:reserveLockedMemory(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:queryVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:queryVolume(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappedBlock:getAddress()	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:isTransientCache()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:isNativeLoader()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:isNativeLoader()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:nextBlock()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:atEnd()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:atEnd()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:shouldDefer()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache$UncachingTask:shouldDefer()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager$UsedBytesCount:reserve(long)	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockPoolSlice:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProvidedBlockPoolSlice:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.CacheStats$UsedBytesCount:reserve(long)	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$1:accept(java.io.File,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$1:accept(java.io.File,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getCapacity()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getAvailable()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getActualNonDfsUsed()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getNonDfsUsed()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getFinalizedDir(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:containsBlock(java.net.URI,java.net.URI)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:containsBlock(java.net.URI,java.net.URI)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:initializeCacheExecutor(java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:checkClosed()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:checkClosed()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:getRemainingReserved()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:getReserved()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:nextSorted(java.util.List,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:toString()	0	java.lang.String	0	NULL
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:isBPDirEmpty(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:isBPDirEmpty(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:loadLastPartialChunkChecksum(java.io.File,java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:isBlockMetaFile(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:isBlockMetaFile(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:compareTo(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:compareTo(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica)	1	int	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica:compareTo(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica)	2	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil:getGenerationStampFromFile(java.io.File[],java.io.File,int)	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:loadDfsUsed()	0	long	0	-1
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:recoverTempUnlinkedBlock(java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:selectReplicaToDelete(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:validateIntegrityAndSetLength(java.io.File,long)	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:readReplicasFromCache(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:getConf()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi$FsVolumeReferences$FsVolumeSpiIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.ErrorReportAction:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.ErrorReportAction:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DatanodeUtil:dirNoFilesRecursive(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,org.apache.hadoop.hdfs.server.datanode.FileIoProvider)	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.DatanodeUtil:dirNoFilesRecursive(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi,java.io.File,org.apache.hadoop.hdfs.server.datanode.FileIoProvider)	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataNode$2:call()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:isAlive()	0	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:isAlive()	1	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:blockReport(long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:getRpcMetricSuffix()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:cacheReport()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:shouldRetryInit()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:shouldRetryInit()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:shouldRun()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:shouldRun()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2:run()	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isHeartbeatDue(long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isHeartbeatDue(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isLifelineDue(long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isLifelineDue(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isBlockReportDue(long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isBlockReportDue(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isOutliersReportDue(long)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:isOutliersReportDue(long)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$Scheduler:getLifelineWaitTime()	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReader:scheduleNewRead(java.util.BitSet,int,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks)	0	int	0	-1
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReader:createBlockReader(long)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:isLessThanNeeded(long,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:isLessThanNeeded(long,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:isCloseEnough(org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:isCloseEnough(org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:computeDelay(long,long,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	0	long	0	0
org.apache.hadoop.hdfs.server.datanode.DiskBalancer$DiskBalancerMover:getBlockToCopy(org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi$BlockIterator,org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkItem)	0	null	0	null
org.apache.hadoop.hdfs.server.datanode.BPOfferService:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:isAlive()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:isAlive()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:containsNN(java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:containsNN(java.net.InetSocketAddress)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActor(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,org.apache.hadoop.hdfs.server.datanode.BPServiceActor)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromActive(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:processCommandFromStandby(org.apache.hadoop.hdfs.server.protocol.DatanodeCommand,java.net.InetSocketAddress)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.BPOfferService:shouldRetryInit()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int)	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.DataXceiverServer$BlockBalanceThrottler:setMaxConcurrentMovers(int,int)	1	int	0	0
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager:sendImmediately()	0	int	0	1
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager:sendImmediately()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer:shouldForceSync()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer:shouldForceSync()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer:isFlushed()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer:isFlushed()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.QuotaByStorageTypeEntry:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp:shouldCompleteLastBlock()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextOp()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser:traverseDirInt(long,org.apache.hadoop.hdfs.server.namenode.INode,java.util.List,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder:hasLastInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder:hasLastInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder:hasNumInodes()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$Builder:hasNumInodes()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes$SnapshotCopy:isDirectory()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes$SnapshotCopy:getErasureCodingPolicyID()	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes$SnapshotCopy:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes$SnapshotCopy:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LegacyReader:decodeOp()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LegacyReader:scanOp()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder:hasStr()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry$Builder:hasStr()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder:hasParent()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$Builder:hasParent()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:hasRoot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:hasRoot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:addToParent(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader:addToParent(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.BackupImage:tryConvergeJournalSpool()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:getReplicaInfo(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:hdfsPathExists(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:hdfsPathExists(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.LeaseManager:isMaxLockHoldToReleaseLease(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.LeaseManager:isMaxLockHoldToReleaseLease(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature:isQuotaSet()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature:isQuotaSet()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature:isQuotaByStorageTypeSet(org.apache.hadoop.fs.StorageType)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature:isQuotaByStorageTypeSet(org.apache.hadoop.fs.StorageType)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,byte,boolean,int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:getStoragePolicyID()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:getLocalStoragePolicyID()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageCompression:toString()	0	java.lang.String	0	no compression
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeDirType:isOfType(org.apache.hadoop.hdfs.server.common.Storage$StorageDirType)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeDirType:isOfType(org.apache.hadoop.hdfs.server.common.Storage$StorageDirType)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$1:logEdit()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasRenewer()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasRenewer()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasRealUser()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasRealUser()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasIssueDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasIssueDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasMaxDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasMaxDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasSequenceNumber()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasSequenceNumber()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeFeatureProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasOndiskVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasOndiskVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasCodec()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Builder:hasCodec()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:unprotectedRemoveBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:unprotectedRemoveBlock(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.protocol.Block)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:validateAddBlock(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String,long,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.LocatedBlock[])	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:addFile(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus,short,long,java.lang.String,java.lang.String,boolean,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:completeFileInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp:completeFileInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.hdfs.protocol.Block,long)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasReferredId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasReferredId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasDstSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasDstSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasLastSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:hasLastSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$DelegationKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.SerialNumberMap:get(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.SerialNumberMap:get(int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp:isUserVisible(org.apache.hadoop.fs.XAttr)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirXAttrOp:isUserVisible(org.apache.hadoop.fs.XAttr)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:isInAnEZ(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:isInAnEZ(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getFullPathName(java.lang.Long)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getKeyName(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getEncryptionZoneForPath(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getParentEncryptionZoneForPath(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getEZINodeForPath(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:pathResolvesToId(long,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:pathResolvesToId(long,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:isEncryptionZoneRoot(org.apache.hadoop.hdfs.server.namenode.INode,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:isEncryptionZoneRoot(org.apache.hadoop.hdfs.server.namenode.INode,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getNumEncryptionZones()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:hasCreatedEncryptionZone()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:hasCreatedEncryptionZone()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isDirectory()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:getLocalStoragePolicyID()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:getStoragePolicyID()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isWithQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isWithQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:searchChildren(byte[])	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isWithSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isWithSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isSnapshottable()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isSnapshottable()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isDescendantOfSnapshotRoot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:isDescendantOfSnapshotRoot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:getChild(byte[],int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:searchChild(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:searchChild(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:nextChild(org.apache.hadoop.hdfs.util.ReadOnlyList,byte[])	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:removeChild(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:removeChild(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addChild(org.apache.hadoop.hdfs.server.namenode.INode,boolean,int)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addChild(org.apache.hadoop.hdfs.server.namenode.INode,boolean,int)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addChild(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addChild(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addChildAtLoading(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume:isResourceAvailable()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume:isResourceAvailable()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasTarget()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasTarget()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink$Builder:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasSnapshotCopy()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:hasSnapshotCopy()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager$EncryptionZoneInt:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp:shouldCompleteLastBlock()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto$Builder:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream:shouldForceSync()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream:shouldForceSync()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream:getNumSync()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameOperation:removeSrc4OldRename()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp$RenameOperation:removeSrc4OldRename()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:getSnapshot(byte[])	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeDiff(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeDiff(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INodeDirectory,java.lang.String,java.lang.String,byte[],int,int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeDiffRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode,java.util.List,org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo,byte[][],int,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeDiffRecursively(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode,java.util.List,org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo,byte[][],int,boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList:findEarlierSnapshotBlocks(int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList:findLaterSnapshotBlocks(int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat:loadFileDiffList(java.io.DataInput,org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature:cloneDiffList(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature:computeDiffBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff,org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature:computeDiffBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff,org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$1:compare(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$1:compare(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	-1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$1:compare(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	2	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager:isDescendantOfSnapshotRoot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager:isDescendantOfSnapshotRoot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager:getSnapshottableDirListing(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager:getMaxSnapshotID()	0	int	0	268435455
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:addDirDiff(long,byte[][],org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:addDirDiff(long,byte[][],org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:addFileDiff(org.apache.hadoop.hdfs.server.namenode.INodeFile,byte[][])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:addFileDiff(org.apache.hadoop.hdfs.server.namenode.INodeFile,byte[][])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:isFromEarlier()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffListingInfo:isFromEarlier()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList:replaceCreatedChild(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList:replaceCreatedChild(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList:removeDeletedChild(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList:removeDeletedChild(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiffList:findSnapshotDeleted(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:getLast()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:getLastSnapshotId()	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:getPrior(int,boolean)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:getDiffById(int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:getSnapshotById(int)	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:changedBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.AbstractINodeDiffList:toString()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff:replaceCreated(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff:replaceCreated(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo:isFromEarlier()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo:isFromEarlier()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$SkipListNode:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$SkipListNode:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList$SkipListNode:toString()	0	java.lang.String	0	?
org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshotFeature:changedBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.FileWithSnapshotFeature:changedBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList:childrenDiff2String(org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff)	0	java.lang.String	0	null
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat$ReferenceMap:toProcessSubtree(long)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotFSImageFormat$ReferenceMap:toProcessSubtree(long)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature$1$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature$1$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:lambda$new$0(org.apache.hadoop.hdfs.server.namenode.INode$Feature)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:lambda$new$0(org.apache.hadoop.hdfs.server.namenode.INode$Feature)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot:getSnapshotName(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot)	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot:getSnapshotId(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot)	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$RenameEntry:isRename()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotDiffInfo$RenameEntry:isRename()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeUtils:getClientNamenodeAddress(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker:isCompleted()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater$ZoneSubmissionTracker:isCompleted()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile:containsTxId(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile:containsTxId(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNode:getProtocolVersion(java.lang.String,long)	0	long	0	69
org.apache.hadoop.hdfs.server.namenode.NameNode:getProtocolVersion(java.lang.String,long)	1	long	0	28
org.apache.hadoop.hdfs.server.namenode.NameNode:getProtocolVersion(java.lang.String,long)	2	long	0	6
org.apache.hadoop.hdfs.server.namenode.NameNode:getProtocolVersion(java.lang.String,long)	3	long	0	1
org.apache.hadoop.hdfs.server.namenode.NameNode:getLifelineRpcServerAddress(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NameNode:getTrimmedOrNull(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NameNode:getNNAuxiliaryRpcAddress()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration,boolean,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration,boolean,boolean)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNode:doRollback(org.apache.hadoop.conf.Configuration,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNode:doRollback(org.apache.hadoop.conf.Configuration,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNode:parseArguments(java.lang.String[])	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NameNode:createNameNode(java.lang.String[],org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp:addSymlink(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean,boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.BackupState:shouldPopulateReplQueues()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasRenewer()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasRenewer()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasRealUser()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasRealUser()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasIssueDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasIssueDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasMaxDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasMaxDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasSequenceNumber()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasSequenceNumber()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$PersistToken$Builder:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.AclTransformation$ValidatedAclSpec:containsKey(org.apache.hadoop.fs.permission.AclEntry)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.AclTransformation$ValidatedAclSpec:containsKey(org.apache.hadoop.fs.permission.AclEntry)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream:isActive()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream:isActive()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream:isResourceAvailable()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream:isResourceAvailable()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:deleteAllowed(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,long)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp:unprotectedDelete(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,long)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:getByID(byte)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:getByName(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:getNextAvailablePolicyID()	0	int	0	64
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:disablePolicy(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:disablePolicy(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:enablePolicy(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:enablePolicy(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$1:getProtocol()	0	java.lang.String	0	webhdfs
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:generateDelegationToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:getParent(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$5$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasNamespaceId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasNamespaceId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV1()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV1()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV2()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV2()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV1Limit()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasGenstampV1Limit()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasLastAllocatedBlockId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasLastAllocatedBlockId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasTransactionId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasTransactionId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasRollingUpgradeStartTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasRollingUpgradeStartTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasLastAllocatedStripedBlockId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection$Builder:hasLastAllocatedStripedBlockId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:getXAttr(byte[],java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:toBytes(java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:hasClientMachine()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:hasClientMachine()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasReferredId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasReferredId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasDstSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasDstSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasLastSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection$INodeReference$Builder:hasLastSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NNStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.NNStorage:isPreUpgradableLayout(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.AclStorage:copyINodeDefaultAcl(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.AclStorage:copyINodeDefaultAcl(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:reencryptEdeks()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$EDEKReencryptCallable:reencryptEdeks()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$UpdateBlocksOp:shouldCompleteLastBlock()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder:hasSnapshotCounter()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder:hasSnapshotCounter()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder:hasNumSnapshots()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Builder:hasNumSnapshots()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:getSelfSnapshot(int)	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.Quota:isViolated(long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.Quota:isViolated(long,long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.Quota:isViolated(long,long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.Quota:isViolated(long,long,long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForWrite()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForWrite()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForWriteWithoutLock()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForWriteWithoutLock()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isSegmentOpen()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isSegmentOpen()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isSegmentOpenWithoutLock()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isSegmentOpenWithoutLock()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForRead()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLog:isOpenForRead()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLog:getTotalSyncCount()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider$AuthorizationContext:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider$AuthorizationContext:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider$AuthorizationContext:hashCode()	0	int	0	42
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:hasParent()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:hasParent()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasCurrentId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasCurrentId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasTokenSequenceNumber()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasTokenSequenceNumber()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasNumKeys()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasNumKeys()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasNumTokens()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:hasNumTokens()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.StoragePolicySummary$StorageTypeAllocation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.StoragePolicySummary$StorageTypeAllocation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:isOpen()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:isOpen()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasPreferredBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasPreferredBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasFileUC()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasFileUC()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasAcl()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasAcl()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasXAttrs()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasXAttrs()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasStoragePolicyID()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasStoragePolicyID()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasBlockType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasBlockType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasErasureCodingPolicyID()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:hasErasureCodingPolicyID()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredHardLimit()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredHardLimit()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredHardLimit(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredHardLimit(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredSoftLimit()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:expiredSoftLimit()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:hasFiles()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease:hasFiles()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder:hasClientMachine()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$FileUnderConstructionFeature$Builder:hasClientMachine()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp:verifyQuotaForUCBlock(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasNumOfDiff()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Builder:hasNumOfDiff()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:yield()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:yield()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:getErasureCodingPolicyName(org.apache.hadoop.hdfs.server.namenode.INode)	0	java.lang.String	0	Replicated
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:getErasureCodingPolicyName(org.apache.hadoop.hdfs.server.namenode.INode)	1	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ImageServlet:isValidRequestor(javax.servlet.ServletContext,java.lang.String,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ImageServlet:getParamStringForMostRecentImage()	0	java.lang.String	0	getimage=1&txid=latest
org.apache.hadoop.hdfs.server.namenode.INodesInPath:isDotSnapshotDir(byte[])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodesInPath:isDotSnapshotDir(byte[])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodesInPath:shouldUpdateLatestId(int,int)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodesInPath:shouldUpdateLatestId(int,int)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodesInPath:getPathSnapshotId()	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.INodesInPath:isDescendant(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodesInPath:isDescendant(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodesInPath:toString(org.apache.hadoop.hdfs.server.namenode.INode)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrCompactProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNextDirectiveId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNextDirectiveId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNumPools()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNumPools()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNumDirectives()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:hasNumDirectives()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeReference:tryRemoveReference(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeReference:removeReference(org.apache.hadoop.hdfs.server.namenode.INodeReference)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeReference:getPriorSnapshot(org.apache.hadoop.hdfs.server.namenode.INodeReference)	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeReference:isReference()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeReference:getDstSnapshotId()	0	int	0	2147483646
org.apache.hadoop.hdfs.server.namenode.FsckServlet:lambda$doGet$0(javax.servlet.ServletContext,org.apache.hadoop.conf.Configuration,java.util.Map,java.io.PrintWriter,java.net.InetAddress)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasReplication()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasReplication()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasPreferredBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasPreferredBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasFileUC()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasFileUC()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasAcl()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasAcl()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasXAttrs()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasXAttrs()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasStoragePolicyID()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasStoragePolicyID()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasBlockType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasBlockType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasErasureCodingPolicyID()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeFile$Builder:hasErasureCodingPolicyID()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.BackupJournalManager:toString()	0	java.lang.String	0	BackupJournalManager
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:getFirstTxId()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:getLastTxId()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:isInProgress()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:isLocalLog()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector:shouldSendShortFile(java.io.File)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointFaultInjector:shouldCorruptAByte(java.io.File)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream:getLastJournalledTxId()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream:shouldForceSync()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$AclFeatureProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageSerialization:readString_EmptyAsNull(java.io.DataInput)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:scanNextOp()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:skipUntil(long)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditLogInputStream:skipUntil(long)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeReferenceSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:isStaleInProgressLog(long,org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:isStaleInProgressLog(long,org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:getLogFile(java.io.File,long,boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:hasMergeError()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:hasMergeError()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:hasPermission(org.apache.hadoop.hdfs.server.namenode.INodeAttributes,org.apache.hadoop.fs.permission.FsAction)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:hasAclPermission(org.apache.hadoop.hdfs.server.namenode.INodeAttributes,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.hdfs.server.namenode.AclFeature)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:hasAclPermission(org.apache.hadoop.hdfs.server.namenode.INodeAttributes,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.hdfs.server.namenode.AclFeature)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:isStickyBitViolated(org.apache.hadoop.hdfs.server.namenode.INodeAttributes,org.apache.hadoop.hdfs.server.namenode.INodeAttributes)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:isStickyBitViolated(org.apache.hadoop.hdfs.server.namenode.INodeAttributes,org.apache.hadoop.hdfs.server.namenode.INodeAttributes)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:isSymlink()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasFile()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasFile()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasDirectory()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasDirectory()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasSymlink()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Builder:hasSymlink()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isRoot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isRoot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isInCurrentState()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isInCurrentState()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isInLatestSnapshot(int)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isInLatestSnapshot(int)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isAncestorDirectory(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isAncestorDirectory(org.apache.hadoop.hdfs.server.namenode.INodeDirectory)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:shouldRecordInSrcSnapshot(int)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:shouldRecordInSrcSnapshot(int)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isReference()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isFile()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isSetStoragePolicy()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isSetStoragePolicy()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isDirectory()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isSymlink()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isQuotaSet()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isQuotaSet()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:getLocalName()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INode:getFullPathName()	0	java.lang.String	0	/
org.apache.hadoop.hdfs.server.namenode.INode:isDeleted()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isDeleted()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:getParentString()	0	java.lang.String	0	parent=null
org.apache.hadoop.hdfs.server.namenode.INode:getParent()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INode:getParentReference()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INode:isLastReference()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:isLastReference()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isValidAbsolutePath(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:isValidAbsolutePath(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INode:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INode:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:getTransactionIdStr()	0	java.lang.String	0	(none)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:hasTransactionId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:hasTransactionId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:hasRpcIds()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:hasRpcIds()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:readXAttrsFromEditLog(java.io.DataInputStream,int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:readAclEntriesFromXml(org.apache.hadoop.hdfs.util.XMLUtils$Stanza)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:readXAttrsFromXml(org.apache.hadoop.hdfs.util.XMLUtils$Stanza)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasNsQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasNsQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasDsQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasDsQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasAcl()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasAcl()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasXAttrs()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasXAttrs()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasTypeQuotas()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:hasTypeQuotas()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.BackupNode:getServiceRpcServerAddress(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.BackupNode:shouldCheckpointAtStartup()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.BackupNode:shouldCheckpointAtStartup()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:getReplication(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:isStriped(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:isStriped(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:hasSnapshotCounter()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:hasSnapshotCounter()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:hasNumSnapshots()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:hasNumSnapshots()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:isSyncThreadAlive()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:isSyncThreadAlive()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isAuditEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isAuditEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isClientPortInfoAbsent(org.apache.hadoop.ipc.CallerContext)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isClientPortInfoAbsent(org.apache.hadoop.ipc.CallerContext)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getLazyPersistFileScrubberTS()	0	long	0	-1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:hasRetryCache()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:hasRetryCache()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCallerContextEnabled()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:inActiveState()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:inActiveState()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:inTransitionToActive()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:inTransitionToActive()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:shouldUseDelegationTokens()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:shouldUseDelegationTokens()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:hasReadLock()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:hasReadLock()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCTime()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInStandbyState()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInStandbyState()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:recoverLeaseInternal(org.apache.hadoop.hdfs.server.namenode.FSNamesystem$RecoverLeaseOp,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String,java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkFileProgress(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkFileProgress(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkBlocksComplete(java.lang.String,boolean,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[])	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:checkBlocksComplete(java.lang.String,boolean,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[])	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.PermissionStatus,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:internalReleaseLease(org.apache.hadoop.hdfs.server.namenode.LeaseManager$Lease,java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInSnapshot(long)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInSnapshot(long)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getBlockCollection(long)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getTransactionsSinceLastLogRoll()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCorrectTransactionsSinceLastLogRoll()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getCurrentTokensCount()	0	long	0	-1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInStartupSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInStartupSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInManualOrResourceLowSafeMode()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isInManualOrResourceLowSafeMode()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getMillisSinceLastLoadedEdits()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getState()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFSState()	0	java.lang.String	0	safeMode
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFSState()	1	java.lang.String	0	Operational
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getTopUserOpCounts()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isFileDeleted(org.apache.hadoop.hdfs.server.namenode.INodeFile)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isFileDeleted(org.apache.hadoop.hdfs.server.namenode.INodeFile)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isAllowedDelegationTokenOp()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isAllowedDelegationTokenOp()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getSafemode()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getRollingUpgradeStatus()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isRollingUpgrade()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isRollingUpgrade()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getTotalSyncTimes()	0	java.lang.String	0	
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getQuotaCommand(long,long)	0	java.lang.String	0	clearQuota
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getQuotaCommand(long,long)	1	java.lang.String	0	clearSpaceQuota
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getQuotaCommand(long,long)	2	java.lang.String	0	setQuota
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getQuotaCommand(long,long)	3	java.lang.String	0	setSpaceQuota
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFailedStorageCommand(java.lang.String)	0	java.lang.String	0	checkRestoreFailedStorage
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFailedStorageCommand(java.lang.String)	1	java.lang.String	0	enableRestoreFailedStorage
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:getFailedStorageCommand(java.lang.String)	2	java.lang.String	0	disableRestoreFailedStorage
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isObserver()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:isObserver()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.XAttrStorage:readINodeXAttrByPrefixedName(org.apache.hadoop.hdfs.server.namenode.INode,int,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasChildrenSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasChildrenSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasIsSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasIsSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasSnapshotCopy()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasSnapshotCopy()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasCreatedListSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff$Builder:hasCreatedListSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result:isHealthy()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result:isHealthy()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck$Result:getReplicationFactor()	0	float	0	0.0
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:getImageTxIdToRetain(org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector)	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getCount(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step)	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getPercentComplete()	0	float	0	1.0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getPercentComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase)	0	float	0	1.0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getPercentComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase)	1	float	0	0.0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getPercentComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step)	0	float	0	1.0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getPercentComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step)	1	float	0	0.0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgressView:getTotal(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase,org.apache.hadoop.hdfs.server.namenode.startupprogress.Step)	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.startupprogress.Step:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:isComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:isComplete(org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasNsQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasNsQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasDsQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasDsQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasAcl()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasAcl()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasXAttrs()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasXAttrs()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasTypeQuotas()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeDirectory$Builder:hasTypeQuotas()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:getParentINodeDirectory(byte[][])	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:isRoot(byte[][])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:isRoot(byte[][])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader:isParent(byte[][],byte[][])	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp:isFileClosed(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp:isFileClosed(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NetworkTopologyServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)	0	java.lang.String	0	json
org.apache.hadoop.hdfs.server.namenode.NetworkTopologyServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)	1	java.lang.String	0	text
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasFile()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasFile()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasDirectory()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasDirectory()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasSymlink()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:hasSymlink()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.AclFeature:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.AclFeature:decrementAndGetRefCount()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:processStartupCommand(org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CommandLineOpts)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:shouldCheckpointBasedOnCount()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:shouldCheckpointBasedOnCount()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:getLastCheckpointDeltaMs()	0	long	0	-1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$ErasureCodingSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:storageVersionMatches(org.apache.hadoop.hdfs.server.common.StorageInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:storageVersionMatches(org.apache.hadoop.hdfs.server.common.StorageInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:isSameCluster(org.apache.hadoop.hdfs.server.namenode.FSImage)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:isSameCluster(org.apache.hadoop.hdfs.server.namenode.FSImage)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:namespaceIdMatches(org.apache.hadoop.hdfs.server.namenode.FSImage)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:namespaceIdMatches(org.apache.hadoop.hdfs.server.namenode.FSImage)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.CheckpointSignature:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SaverContext$DeduplicationMap:getId(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp:createSingleDirectory(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,byte[],org.apache.hadoop.fs.permission.PermissionStatus)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:generateEncryptedDataEncryptionKey(org.apache.hadoop.hdfs.server.namenode.FSDirectory,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:getFileEncryptionInfo(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:isInAnEZ(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp:getEncryptionKeyInfo(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.crypto.CryptoProtocolVersion[])	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ImageServlet$ImageUploadRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ImageServlet$ImageUploadRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:rename(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:delete(java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:checkPathLength(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:checkPathLength(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:saveNamespace(long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer:getCurrentEditLogTxid()	0	long	0	-1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:hasFullPath()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:hasFullPath()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasNumOfDiff()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:hasNumOfDiff()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:getPosition()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:hasId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:hasId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:hasStr()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:hasStr()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Entry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section$Builder:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetPermission(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetPermission(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.fs.permission.FsPermission)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetOwner(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetOwner(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetQuota(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,long,org.apache.hadoop.fs.StorageType)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp:unprotectedSetReplication(org.apache.hadoop.hdfs.server.namenode.FSDirectory,org.apache.hadoop.hdfs.server.namenode.INodesInPath,short)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:hasNumEntry()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:hasNumEntry()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:hasMaskBits()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:hasMaskBits()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder:hasInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder:hasInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder:hasFullPath()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FilesUnderConstructionSection$FileUnderConstructionEntry$Builder:hasFullPath()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:needToSave()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:needToSave()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:getMaxSeenTxId()	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.GlobalStateIdContext:isCoordinatedCall(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.GlobalStateIdContext:isCoordinatedCall(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasFileSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasFileSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasSnapshotCopy()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$FileDiff$Builder:hasSnapshotCopy()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet:isOpen()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.JournalSet:isOpen()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.JournalSet:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.JournalSet:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes$SnapshotCopy:isDirectory()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes$SnapshotCopy:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes$SnapshotCopy:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeDirectoryAttributes)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:valueOf(org.apache.hadoop.hdfs.server.namenode.INode,java.lang.String,boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:isFile()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile:metadataEquals(org.apache.hadoop.hdfs.server.namenode.INodeFileAttributes)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:isUnderConstruction()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile:isUnderConstruction()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:checkBlockComplete(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo[],int,int,short)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:removeLastBlock(org.apache.hadoop.hdfs.protocol.Block)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:isWithSnapshot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile:isWithSnapshot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:getFileReplication()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile:getErasureCodingPolicyID()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:computeFileSize(boolean,boolean)	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:getPenultimateBlock()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:getLastBlock()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:collectBlocksBeyondMax(long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,java.util.Set)	0	long	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:getSnapshotBlocksToRetain(int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile:isBlockInLatestSnapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile:isBlockInLatestSnapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasOndiskVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasOndiskVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasCodec()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:hasCodec()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder:hasNumEntry()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder:hasNumEntry()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder:hasMaskBits()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$StringTableSection$Builder:hasMaskBits()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:hasLastInodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:hasLastInodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:hasNumInodes()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:hasNumInodes()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,long,org.apache.hadoop.hdfs.server.namenode.QuotaCounts)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:unprotectedTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath,long,org.apache.hadoop.hdfs.server.namenode.INode$BlocksMapUpdateInfo,long,org.apache.hadoop.hdfs.server.namenode.QuotaCounts)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirTruncateOp:shouldCopyOnTruncate(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodeFile,org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasPermission()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasPermission()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasTarget()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasTarget()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasModificationTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasModificationTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasAccessTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:hasAccessTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INodeSymlink:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasNamespaceId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasNamespaceId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV1()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV1()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV2()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV2()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV1Limit()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasGenstampV1Limit()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasLastAllocatedBlockId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasLastAllocatedBlockId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasTransactionId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasTransactionId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasRollingUpgradeStartTime()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasRollingUpgradeStartTime()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasLastAllocatedStripedBlockId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:hasLastAllocatedStripedBlockId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$NameSystemSection:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ImageServlet$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader:decodeOp()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader:decodeOpFrame()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isUserBypassingExtAttrProvider(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isUserBypassingExtAttrProvider(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:getUserFilteredAttributeProvider(org.apache.hadoop.security.UserGroupInformation)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isAccessTimeSupported()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isAccessTimeSupported()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isValidToCreate(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isValidToCreate(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:addLastINode(org.apache.hadoop.hdfs.server.namenode.INodesInPath,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.fs.permission.FsPermission,boolean)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirectory:removeLastINode(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	long	0	-1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:removeLastINode(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	1	long	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:removeLastINode(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	2	long	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedName(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedName(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isExactReservedName(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isExactReservedName(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isExactReservedName(byte[][])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isExactReservedName(byte[][])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedName(byte[][])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedName(byte[][])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedRawName(byte[][])	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirectory:isReservedRawName(byte[][])	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirectory:getAuditFileInfo(org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ImageServlet$2:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:canRollBack(org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.common.StorageInfo,org.apache.hadoop.hdfs.server.common.StorageInfo,int)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasLength()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasLength()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasOffset()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:hasOffset()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeResourcePolicy:areResourcesAvailable(java.util.Collection,int)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeResourcePolicy:areResourcesAvailable(java.util.Collection,int)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasCurrentId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasCurrentId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasTokenSequenceNumber()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasTokenSequenceNumber()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasNumKeys()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasNumKeys()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasNumTokens()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SecretManagerSection$Builder:hasNumTokens()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:processFileInode(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:processFileInode(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:shouldSubmitCurrentBatch()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:shouldSubmitCurrentBatch()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:canTraverseDir(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:canTraverseDir(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp:removeErasureCodingPolicyXAttr(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INodesInPath)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp:getErasureCodingPolicy(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp:getErasureCodingPolicyXAttrForINode(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.INode)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:hasStorageType()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:hasStorageType()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:hasQuota()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:hasQuota()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$QuotaByStorageTypeEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:hasRoot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotSection$Snapshot$Builder:hasRoot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CachedBlock:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.CachedBlock:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.CachedBlock:getMark()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CachedBlock:getMark()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CachedBlock:isPresent(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$CachedBlocksList)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CachedBlock:isPresent(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor$CachedBlocksList)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.CachedBlock:isInList(org.apache.hadoop.util.IntrusiveCollection)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.CachedBlock:isInList(org.apache.hadoop.util.IntrusiveCollection)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker:lambda$new$0(java.net.URI)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker:lambda$new$0(java.net.URI)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageUtil:checkFileFormat(java.io.RandomAccessFile)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageUtil:checkFileFormat(java.io.RandomAccessFile)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ChecksummedReader:decodeOp()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ChecksummedReader:scanOp()	0	long	0	-12345
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasHostname()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasHostname()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasPort()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasPort()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasZkfcPort()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:hasZkfcPort()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasNameserviceId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasNameserviceId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasNamenodeId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasNamenodeId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasHostname()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasHostname()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasPort()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasPort()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasZkfcPort()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.proto.HAZKInfoProtos$ActiveNodeInfo$Builder:hasZkfcPort()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:tooLongSinceLastLoad()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:tooLongSinceLastLoad()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.ActiveState:shouldPopulateReplQueues()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun()	0	int	0	2
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun()	1	int	0	3
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun()	2	int	0	5
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doRun()	3	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:format(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:format(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:doPreUpgrade(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:downloadImage(org.apache.hadoop.hdfs.server.namenode.NNStorage,org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol,org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:checkLogsAvailableForRead(org.apache.hadoop.hdfs.server.namenode.FSImage,long,long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:checkLayoutVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:checkLayoutVersion(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:formatAndDownloadAliasMap(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo)	0	int	0	5
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:formatAndDownloadAliasMap(java.lang.String,org.apache.hadoop.hdfs.server.namenode.ha.RemoteNameNodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1:run()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState:shouldPopulateReplQueues()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState:toString()	0	java.lang.String	0	observer
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState:toString()	1	java.lang.String	0	standby
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:checkAddress(java.net.URL)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:checkAddress(java.net.URL)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2:doWork()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy:getActiveNodeProxy()	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$UserCounts:add(org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$User)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$UserCounts:addAll(java.util.Collection)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$Op:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$Op:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow$Bucket:isStaleNow(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow$Bucket:isStaleNow(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$User:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager$User:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext:allowStaleReads()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:anyNsSsCountGreaterOrEqual(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:anyNsSsCountGreaterOrEqual(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:anyTypeSpaceCountGreaterOrEqual(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:anyTypeSpaceCountGreaterOrEqual(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.QuotaCounts:hashCode()	0	int	0	42
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNextDirectiveId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNextDirectiveId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNumPools()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNumPools()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNumDirectives()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$CacheManagerSection$Builder:hasNumDirectives()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier$DatanodeWithStorage$StorageDetails:hasSpaceForScheduling(long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier$DatanodeWithStorage$StorageDetails:hasSpaceForScheduling(long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:isEnabled()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:isEnabled()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:checkIfAlreadyChosen(java.util.List,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:checkIfAlreadyChosen(java.util.List,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:checkSourceAndTargetTypeExists(org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.List,java.util.List,org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier$DatanodeMap)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:checkSourceAndTargetTypeExists(org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.util.List,java.util.List,org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier$DatanodeMap)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:removeOverlapBetweenStorageTypes(java.util.List,java.util.List,boolean)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:removeOverlapBetweenStorageTypes(java.util.List,java.util.List,boolean)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded$DirPendingWorkInfo:isDirWorkDone()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded$DirPendingWorkInfo:isDirWorkDone()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.sps.ItemInfo:isDir()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.sps.ItemInfo:isDir()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AclEditLogUtil:read(java.io.DataInputStream,int)	0	null	0	null
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$CreatedListEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:unprotectedSatisfyStoragePolicy(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSDirectory)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:unprotectedSatisfyStoragePolicy(org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.FSDirectory)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:inodeHasSatisfyXAttr(org.apache.hadoop.hdfs.server.namenode.INode)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSDirSatisfyStoragePolicyOp:inodeHasSatisfyXAttr(org.apache.hadoop.hdfs.server.namenode.INode)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasSnapshotId()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasSnapshotId()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasChildrenSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasChildrenSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasIsSnapshotRoot()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasIsSnapshotRoot()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasName()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasName()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasSnapshotCopy()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasSnapshotCopy()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasCreatedListSize()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:hasCreatedListSize()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DirectoryDiff:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.XAttrFeature:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$XAttrFeatureProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImage:recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImage:recoverStorageDirs(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.NNStorage,java.util.Map)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImage:hasRollbackFSImage()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImage:hasRollbackFSImage()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImage:needsResaveBasedOnStaleCheckpoint(java.io.File,long)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImage:needsResaveBasedOnStaleCheckpoint(java.io.File,long)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(long,long,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImage:saveNamespace(long,long,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader$1:compare(org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section,org.apache.hadoop.hdfs.server.namenode.FsImageProto$FileSummary$Section)	1	int	0	-1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode:isDelayActive()	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode:isDelayActive()	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode:addPendingBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Pool:getName()	0	java.lang.String	0	blockpool
org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Pool:getUtilization(org.apache.hadoop.hdfs.server.protocol.DatanodeStorageReport,org.apache.hadoop.fs.StorageType)	0	null	0	null
org.apache.hadoop.hdfs.server.balancer.Matcher$3:match(org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Matcher$3:match(org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Matcher$3:toString()	0	java.lang.String	0	ANY_OTHER
org.apache.hadoop.hdfs.server.balancer.Balancer:choose4One(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Matcher)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Balancer:choose4One(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.Matcher)	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.Balancer:matchStorageGroups(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Matcher)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Balancer:matchStorageGroups(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Matcher)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.MovedBlocks:contains(org.apache.hadoop.hdfs.protocol.Block)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.MovedBlocks:contains(org.apache.hadoop.hdfs.protocol.Block)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Matcher$2:toString()	0	java.lang.String	0	SAME_RACK
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:isIterationOver()	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:isIterationOver()	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:isGoodBlockCandidate(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:isGoodBlockCandidate(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:shouldFetchMoreBlocks()	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Source:shouldFetchMoreBlocks()	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Matcher$1:toString()	0	java.lang.String	0	SAME_NODE_GROUP
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIncluded(java.util.Set,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIncluded(java.util.Set,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIn(java.util.Set,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIn(java.util.Set,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIn(java.util.Set,java.lang.String,int)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Util:isIn(java.util.Set,java.lang.String,int)	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup:hasSpaceForScheduling(long)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup:hasSpaceForScheduling(long)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$Allocator:allocate(int)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped:getInternalBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)	0	null	0	null
org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlockStriped:getNumBytes(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)	0	long	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher:shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher:shouldIgnore(org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher:isGoodBlockCandidate(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.fs.StorageType,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher:isGoodBlockCandidate(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup,org.apache.hadoop.fs.StorageType,org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock)	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Node:getName()	0	java.lang.String	0	datanode
org.apache.hadoop.hdfs.server.balancer.BalancingPolicy$Node:getUtilization(org.apache.hadoop.hdfs.server.protocol.DatanodeStorageReport,org.apache.hadoop.fs.StorageType)	0	null	0	null
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:chooseBlockAndProxy()	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:chooseBlockAndProxy()	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType)	0	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:markMovedIfGoodBlock(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.fs.StorageType)	1	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:chooseProxySource()	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:chooseProxySource()	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:addTo(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:addTo(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DDatanode$StorageGroup)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:stopWaitingForResponse(long)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove:stopWaitingForResponse(long)	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:isUpgrading()	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:isUpgrading()	1	int	0	0
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:shouldContinue(long)	0	int	0	1
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:shouldContinue(long)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringStripedBlock:isStriped()	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo:hashCode()	0	int	0	0
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo:isDeletedBlock()	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo:isDeletedBlock()	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.JournalInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.protocol.JournalInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.protocol.NamespaceInfo:isCapabilitySupported(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo$Capability)	0	int	0	1
org.apache.hadoop.hdfs.server.protocol.NamespaceInfo:isCapabilitySupported(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo$Capability)	1	int	0	0
org.apache.hadoop.hdfs.server.protocol.RemoteEditLog:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.protocol.RemoteEditLog:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:isInSafeMode()	0	int	0	0
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:isFileExist(long)	0	int	0	0
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:getNumLiveDataNodes()	0	int	0	0
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:getSubtreeStorageCount(org.apache.hadoop.fs.StorageType)	0	int	0	0
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:add(org.apache.hadoop.net.Node)	0	int	0	0
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:add(org.apache.hadoop.net.Node)	1	int	0	1
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:remove(org.apache.hadoop.net.Node)	0	int	0	1
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:remove(org.apache.hadoop.net.Node)	1	int	0	0
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandomWithStorageType(java.lang.String,java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)	0	null	0	null
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandomWithStorageTypeAndExcludeRoot(org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl,org.apache.hadoop.net.Node,org.apache.hadoop.fs.StorageType,java.util.Collection)	0	null	0	null
org.apache.hadoop.hdfs.DFSUtil$ServiceComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	0	int	0	0
org.apache.hadoop.hdfs.DFSUtil$ServiceComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	1	int	0	1
org.apache.hadoop.hdfs.DFSUtil$ServiceComparator:compare(org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)	2	int	0	-1
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.PBHelper:convert(org.apache.hadoop.hdfs.server.protocol.NNHAStatusHeartbeat)	0	null	0	null
org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB:initReplicaRecovery(org.apache.hadoop.hdfs.server.protocol.BlockRecoveryCommand$RecoveringBlock)	0	null	0	null
org.apache.hadoop.hdfs.DFSUtil:isValidNameForComponent(java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.DFSUtil:isValidNameForComponent(java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.DFSUtil:isReservedPathComponent(java.lang.String)	0	int	0	1
org.apache.hadoop.hdfs.DFSUtil:isReservedPathComponent(java.lang.String)	1	int	0	0
org.apache.hadoop.hdfs.DFSUtil:byteArray2PathString(byte[][],int,int)	0	java.lang.String	0	
org.apache.hadoop.hdfs.DFSUtil:byteArray2PathString(byte[][],int,int)	1	java.lang.String	0	/
org.apache.hadoop.hdfs.DFSUtil:strings2PathString(java.lang.String[])	0	java.lang.String	0	
org.apache.hadoop.hdfs.DFSUtil:strings2PathString(java.lang.String[])	1	java.lang.String	0	/
org.apache.hadoop.hdfs.DFSUtil:path2String(java.lang.Object)	0	null	0	null
org.apache.hadoop.hdfs.DFSUtil:getHttpClientScheme(org.apache.hadoop.conf.Configuration)	0	java.lang.String	0	https
org.apache.hadoop.hdfs.DFSUtil:getHttpClientScheme(org.apache.hadoop.conf.Configuration)	1	java.lang.String	0	http
org.apache.hadoop.hdfs.DFSUtil:parseHelpArgument(java.lang.String[],java.lang.String,java.io.PrintStream,boolean)	0	int	0	1
org.apache.hadoop.hdfs.DFSUtil:parseHelpArgument(java.lang.String[],java.lang.String,java.io.PrintStream,boolean)	1	int	0	0
org.apache.hadoop.hdfs.DFSUtil:getSpnegoKeytabKey(org.apache.hadoop.conf.Configuration,java.lang.String)	0	java.lang.String	0	dfs.web.authentication.kerberos.keytab
org.apache.hadoop.hdfs.DFSUtil:createKeyProviderCryptoExtension(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.hdfs.DFSUtil:isParentEntry(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.DFSUtil:isParentEntry(java.lang.String,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:exportKeys()	0	null	0	null
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:updateKeys(long)	0	int	0	0
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:updateKeys()	0	int	0	0
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:updateKeys()	1	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:isExpired(long)	0	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:isExpired(long)	1	int	0	0
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:hasKey(int)	0	int	0	1
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:hasKey(int)	1	int	0	0
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:createCredentials(org.apache.hadoop.hdfs.server.namenode.NameNode,org.apache.hadoop.security.UserGroupInformation,java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.util.XMLUtils:codePointMustBeMangled(int)	0	int	0	1
org.apache.hadoop.hdfs.util.XMLUtils:codePointMustBeMangled(int)	1	int	0	0
org.apache.hadoop.hdfs.util.XMLUtils:codePointToEntityRef(int)	0	java.lang.String	0	&quot;
org.apache.hadoop.hdfs.util.XMLUtils:codePointToEntityRef(int)	1	java.lang.String	0	&amp;
org.apache.hadoop.hdfs.util.XMLUtils:codePointToEntityRef(int)	2	java.lang.String	0	&apos;
org.apache.hadoop.hdfs.util.XMLUtils:codePointToEntityRef(int)	3	java.lang.String	0	&lt;
org.apache.hadoop.hdfs.util.XMLUtils:codePointToEntityRef(int)	4	java.lang.String	0	&gt;
org.apache.hadoop.hdfs.util.ByteArray:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.util.LightWeightLinkedSet:addElem(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.util.LightWeightLinkedSet:addElem(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.util.LightWeightLinkedSet:removeElem(java.lang.Object)	0	null	0	null
org.apache.hadoop.hdfs.util.LightWeightLinkedSet:pollFirst()	0	null	0	null
org.apache.hadoop.hdfs.util.EnumDoubles:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.util.EnumDoubles:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.util.LightWeightLinkedSet$LinkedSetIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.util.LightWeightLinkedSet$LinkedSetIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.util.Diff:search(java.util.List,java.lang.Object)	0	int	0	-1
org.apache.hadoop.hdfs.util.Diff:removeCreated(org.apache.hadoop.hdfs.util.Diff$Element)	0	int	0	1
org.apache.hadoop.hdfs.util.Diff:removeCreated(org.apache.hadoop.hdfs.util.Diff$Element)	1	int	0	0
org.apache.hadoop.hdfs.util.Diff:containsDeleted(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.util.Diff:containsDeleted(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.util.Diff:containsDeleted(org.apache.hadoop.hdfs.util.Diff$Element)	0	int	0	1
org.apache.hadoop.hdfs.util.Diff:containsDeleted(org.apache.hadoop.hdfs.util.Diff$Element)	1	int	0	0
org.apache.hadoop.hdfs.util.Diff:removeDeleted(org.apache.hadoop.hdfs.util.Diff$Element)	0	int	0	1
org.apache.hadoop.hdfs.util.Diff:removeDeleted(org.apache.hadoop.hdfs.util.Diff$Element)	1	int	0	0
org.apache.hadoop.hdfs.util.Diff:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.util.Diff:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.util.Canceler:isCancelled()	0	int	0	1
org.apache.hadoop.hdfs.util.Canceler:isCancelled()	1	int	0	0
org.apache.hadoop.hdfs.util.ReferenceCountMap:getReferenceCount(org.apache.hadoop.hdfs.util.ReferenceCountMap$ReferenceCounter)	0	long	0	0
org.apache.hadoop.hdfs.util.XMLUtils$Stanza:getValueOrNull(java.lang.String)	0	null	0	null
org.apache.hadoop.hdfs.util.LightWeightHashSet:isEmpty()	0	int	0	1
org.apache.hadoop.hdfs.util.LightWeightHashSet:isEmpty()	1	int	0	0
org.apache.hadoop.hdfs.util.LightWeightHashSet:contains(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.util.LightWeightHashSet:contains(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.util.LightWeightHashSet:addElem(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.util.LightWeightHashSet:addElem(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.util.LightWeightHashSet:remove(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.util.LightWeightHashSet:remove(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.util.LightWeightHashSet:removeElem(java.lang.Object)	0	null	0	null
org.apache.hadoop.hdfs.util.LightWeightHashSet:computeCapacity(int)	0	int	0	16
org.apache.hadoop.hdfs.util.LightWeightHashSet:computeCapacity(int)	1	int	0	1073741824
org.apache.hadoop.hdfs.util.LightWeightHashSet:containsAll(java.util.Collection)	0	int	0	0
org.apache.hadoop.hdfs.util.LightWeightHashSet:containsAll(java.util.Collection)	1	int	0	1
org.apache.hadoop.hdfs.util.EnumCounters:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.util.EnumCounters:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.util.EnumCounters:allLessOrEqual(long)	0	int	0	0
org.apache.hadoop.hdfs.util.EnumCounters:allLessOrEqual(long)	1	int	0	1
org.apache.hadoop.hdfs.util.EnumCounters:anyGreaterOrEqual(long)	0	int	0	1
org.apache.hadoop.hdfs.util.EnumCounters:anyGreaterOrEqual(long)	1	int	0	0
org.apache.hadoop.hdfs.util.MD5FileUtils:readStoredMd5ForFile(java.io.File)	0	null	0	null
org.apache.hadoop.hdfs.util.ReadOnlyList$Util$2:toString()	0	java.lang.String	0	[]
org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedSetIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.util.LightWeightHashSet$LinkedSetIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.BlockListAsLongs$BufferDecoder$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasIsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:hasIsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasXmitsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasXmitsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasXceiverCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasXceiverCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasFailedVolumes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasFailedVolumes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasCacheCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasCacheCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasCacheUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasCacheUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasVolumeFailureSummary()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasVolumeFailureSummary()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasRequestFullBlockReportLease()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto$Builder:hasRequestFullBlockReportLease()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasReplicaFound()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasReplicaFound()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasDataBlockNum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasDataBlockNum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasCellSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:hasCellSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto$Builder:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasBuildVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasBuildVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasUnused()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasUnused()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasBlockPoolID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasBlockPoolID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasCapabilities()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasCapabilities()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasTotalRpcs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasTotalRpcs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasCurRpc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasCurRpc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasLeaseId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto$Builder:hasLeaseId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasClusterID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasClusterID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasNamespaceID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto$Builder:hasNamespaceID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasPreviousEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasPreviousEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasLastTransactionId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasLastTransactionId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasInSync()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:hasInSync()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$Builder:hasBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto$Builder:hasBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasNewGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasNewGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasTruncateBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasTruncateBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder:hasBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$Builder:hasNextMarker()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto$Builder:hasNextMarker()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasDatanodeID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasDatanodeID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto$Builder:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto$Builder:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasMostRecentCheckpointTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasMostRecentCheckpointTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasCurSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasCurSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:hasCommand()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:hasCommand()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$Builder:hasMarker()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto$Builder:hasMarker()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasFencerInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto$Builder:hasFencerInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$Builder:hasCommand()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointResponseProto$Builder:hasCommand()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasPreviousEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasPreviousEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasLastTransactionId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasLastTransactionId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasInSync()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceResponseProto$Builder:hasInSync()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasDataNodeId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasDataNodeId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasAggregateLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasAggregateLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasMedian()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasMedian()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasMad()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasMad()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasUpperLimitLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto$Builder:hasUpperLimitLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasDataBlockNum()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasDataBlockNum()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasCellSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder:hasCellSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasRecoveryId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasRecoveryId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasNewBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto$Builder:hasNewBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:hasNextMarker()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:hasNextMarker()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasRegistartion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasRegistartion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasErrorCode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasErrorCode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:hasMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasHaStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasHaStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasRollingUpgradeStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasRollingUpgradeStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasRollingUpgradeStatusV2()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasRollingUpgradeStatusV2()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasFullBlockReportLeaseId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto$Builder:hasFullBlockReportLeaseId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto$Builder:hasSpsPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto$Builder:hasSpsPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$FinalizeCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto$Builder:hasSrcPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathResponseProto$Builder:hasSrcPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto$Builder:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentRequestProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasRecoveryId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasRecoveryId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasNewBlockId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:hasNewBlockId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasMostRecentCheckpointTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasMostRecentCheckpointTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasCurSegmentTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasCurSegmentTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasCheckpointCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Builder:hasCheckpointCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto$Builder:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasCmdType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasCmdType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBalancerCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBalancerCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasRecoveryCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasRecoveryCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasFinalizeCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasFinalizeCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasKeyUpdateCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasKeyUpdateCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasRegisterCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasRegisterCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkIdCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkIdCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkECReconstructionCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:hasBlkECReconstructionCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasCheckpointCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:hasCheckpointCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:hasTxid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:hasTxid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$LifelineResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasIsBlockTokenEnabled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasIsBlockTokenEnabled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasKeyUpdateInterval()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasKeyUpdateInterval()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasTokenLifeTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasTokenLifeTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasCurrentKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder:hasCurrentKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointRequestProto$Builder:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasRegistartion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasRegistartion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasErrorCode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasErrorCode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$Builder:hasMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageReceivedDeletedBlocksProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportRequestProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:hasBandwidth()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:hasBandwidth()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BalancerBandwidthCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:hasNeedToReturnImage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:hasNeedToReturnImage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:hasLastVolumeFailureDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:hasLastVolumeFailureDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:hasEstimatedCapacityLostTotal()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:hasEstimatedCapacityLostTotal()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasCmdType()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasCmdType()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBalancerCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBalancerCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasRecoveryCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasRecoveryCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasFinalizeCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasFinalizeCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasKeyUpdateCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasKeyUpdateCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasRegisterCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasRegisterCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkIdCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkIdCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkECReconstructionCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Builder:hasBlkECReconstructionCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasDataNodeId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasDataNodeId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasAggregateLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasAggregateLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasMedian()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasMedian()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasMad()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasMad()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasUpperLimitLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:hasUpperLimitLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowPeerReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasReplicaFound()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasReplicaFound()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryResponseProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$Builder:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto$Builder:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:hasTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:hasTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:hasNumberOfBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:hasNumberOfBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:hasMarker()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:hasMarker()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ListRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:hasNeedToReturnImage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder:hasNeedToReturnImage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$XAttrEditLogProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasNewGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasNewGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasCloseFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasCloseFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasDeleteBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto$Builder:hasDeleteBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasBasePath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasBasePath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanMetadataOpLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanMetadataOpLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanReadIoLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanReadIoLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanWriteIoLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto$Builder:hasMeanWriteIoLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:hasCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:hasCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:hasCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:hasCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasDeleteHint()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$Builder:hasDeleteHint()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$Builder:hasCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportResponseProto$Builder:hasCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$InitReplicaRecoveryRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasDatanode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasDatanode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasMinBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:hasMinBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$Builder:hasCmd()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CacheReportResponseProto$Builder:hasCmd()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:hasKeyValuePair()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:hasKeyValuePair()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasStartTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasStartTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasEndTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasEndTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasIsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder:hasIsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasHaStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasHaStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasRollingUpgradeStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasRollingUpgradeStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasRollingUpgradeStatusV2()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasRollingUpgradeStatusV2()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasFullBlockReportLeaseId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:hasFullBlockReportLeaseId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:hasSrc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:hasSrc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EditLogProtos$AclEditLogProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasFencerInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:hasFencerInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$FenceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:hasSinceTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:hasSinceTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$Builder:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto$Builder:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:hasIsRollingUpgrade()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:hasIsRollingUpgrade()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:hasFileId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:hasFileId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetFilePathRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasClusterID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasClusterID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasNamespaceID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:hasNamespaceID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder:hasTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto$Builder:hasTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$KeyUpdateCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder:hasSignature()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogResponseProto$Builder:hasSignature()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$Builder:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlockKeysResponseProto$Builder:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:hasLastVolumeFailureDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:hasLastVolumeFailureDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:hasEstimatedCapacityLostTotal()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$VolumeFailureSummaryProto$Builder:hasEstimatedCapacityLostTotal()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasErrorCode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasErrorCode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto$Builder:hasMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasStatus()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasStatus()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasDeleteHint()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:hasDeleteHint()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:hasInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:hasInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder:hasStorageUuid()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$UpdateReplicaUnderRecoveryResponseProto$Builder:hasStorageUuid()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$EndCheckpointResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasRpcAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasRpcAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasHttpAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasHttpAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasRole()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:hasRole()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:hasManifest()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:hasManifest()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder:hasIsUpgradeFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto$Builder:hasIsUpgradeFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RollEditLogRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasXmitsInProgress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasXmitsInProgress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasXceiverCount()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasXceiverCount()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasFailedVolumes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasFailedVolumes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasCacheCapacity()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasCacheCapacity()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasCacheUsed()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasCacheUsed()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasVolumeFailureSummary()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasVolumeFailureSummary()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasRequestFullBlockReportLease()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:hasRequestFullBlockReportLease()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$HeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeLifelineProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:hasSpsPath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:hasSpsPath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:hasAction()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:hasAction()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasErrorCode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasErrorCode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasMsg()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:hasMsg()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasKeyBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:hasKeyBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$Builder:hasCommittedTxnId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto$Builder:hasCommittedTxnId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasContext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:hasContext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReceivedAndDeletedResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasDatanodeID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasDatanodeID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasKeys()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasKeys()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeRegistrationProto$Builder:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder:hasManifest()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetEditLogManifestResponseProto$Builder:hasManifest()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockRecoveryCommandProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasBuildVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasBuildVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasUnused()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasUnused()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasBlockPoolID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasBlockPoolID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasSoftwareVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasSoftwareVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasCapabilities()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasCapabilities()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamespaceInfoProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasIsBlockTokenEnabled()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasIsBlockTokenEnabled()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasKeyUpdateInterval()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasKeyUpdateInterval()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasTokenLifeTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasTokenLifeTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasCurrentKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:hasCurrentKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder:hasTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdResponseProto$Builder:hasTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$StartLogSegmentResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeResponseProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasNewGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasNewGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasNewLength()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasNewLength()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasCloseFile()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasCloseFile()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasDeleteBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:hasDeleteBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$CommitBlockSynchronizationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:hasBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:hasBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:hasCommittedTxnId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:hasCommittedTxnId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogManifestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$StartCheckpointRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasExpiryDate()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasExpiryDate()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasKeyBytes()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockKeyProto$Builder:hasKeyBytes()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:hasStorage()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:hasStorage()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:hasNumberOfBlocks()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$StorageBlockReportProto$Builder:hasNumberOfBlocks()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetTransactionIdRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasNewGenStamp()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasNewGenStamp()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasTruncateBlock()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasTruncateBlock()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasEcPolicy()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasEcPolicy()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasBlockIndices()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:hasBlockIndices()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder:hasIsRollingUpgrade()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeResponseProto$Builder:hasIsRollingUpgrade()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:hasIsUpgradeFinalized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:hasIsUpgradeFinalized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsUpgradeFinalizedResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$IsRollingUpgradeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasNamespceID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasNamespceID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasClusterID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasClusterID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasCTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:hasCTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasFirstTxnId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasFirstTxnId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasNumTxns()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasNumTxns()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasRecords()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasRecords()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:hasTxId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:hasTxId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetMostRecentCheckpointTxIdResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasJournalInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasJournalInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasFirstTxnId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasFirstTxnId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasNumTxns()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasNumTxns()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasRecords()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasRecords()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasEpoch()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:hasEpoch()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.JournalProtocolProtos$JournalRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockECReconstructionCommandProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$BlockPoolRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$RegisterResponseProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasRpcAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasRpcAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasHttpAddress()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasHttpAddress()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasStorageInfo()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasStorageInfo()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasRole()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$Builder:hasRole()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasLayoutVersion()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasLayoutVersion()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasNamespceID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasNamespceID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasClusterID()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasClusterID()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasCTime()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder:hasCTime()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$Builder:hasKeyValuePair()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$WriteRequestProto$Builder:hasKeyValuePair()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasTotalRpcs()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasTotalRpcs()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasCurRpc()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasCurRpc()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasLeaseId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:hasLeaseId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReportBadBlocksResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetNextSPSPathRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasBasePath()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasBasePath()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanMetadataOpLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanMetadataOpLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanReadIoLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanReadIoLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanWriteIoLatency()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:hasMeanWriteIoLatency()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$SlowDiskReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$ErrorReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$RegisterDatanodeRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InterDatanodeProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:hasKey()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:hasKey()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$KeyValueProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AliasMapProtocolProtos$ReadResponseProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasRegistration()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasRegistration()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasBlockPoolId()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasBlockPoolId()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasContext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockReportRequestProto$Builder:hasContext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasDatanode()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasDatanode()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasMinBlockSize()	0	int	0	1
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$GetBlocksRequestProto$Builder:hasMinBlockSize()	1	int	0	0
org.apache.hadoop.hdfs.protocol.LayoutVersion:supports(java.util.Map,org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeature,int)	0	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion:supports(java.util.Map,org.apache.hadoop.hdfs.protocol.LayoutVersion$LayoutFeature,int)	1	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver:isTrusted()	0	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted()	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted()	1	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted(java.net.InetAddress)	0	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted(java.net.InetAddress)	1	int	0	0
org.apache.hadoop.hdfs.protocol.BlockListAsLongs$1:getNumberOfBlocks()	0	int	0	0
org.apache.hadoop.hdfs.protocol.BlockListAsLongs$LongsDecoder$1:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.protocol.BlockListAsLongs$LongsDecoder$1:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.protocol.CacheDirective:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.protocol.CacheDirective:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.protocol.CacheDirective:getPrev(org.apache.hadoop.util.IntrusiveCollection)	0	null	0	null
org.apache.hadoop.hdfs.protocol.CacheDirective:getNext(org.apache.hadoop.util.IntrusiveCollection)	0	null	0	null
org.apache.hadoop.hdfs.protocol.CacheDirective:isInList(org.apache.hadoop.util.IntrusiveCollection)	0	int	0	0
org.apache.hadoop.hdfs.protocol.CacheDirective:isInList(org.apache.hadoop.util.IntrusiveCollection)	1	int	0	1
org.apache.hadoop.hdfs.HAUtil:isHAEnabled(org.apache.hadoop.conf.Configuration,java.lang.String)	0	int	0	0
org.apache.hadoop.hdfs.HAUtil:isHAEnabled(org.apache.hadoop.conf.Configuration,java.lang.String)	1	int	0	1
org.apache.hadoop.hdfs.HAUtil:usesSharedEditsDir(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.hdfs.HAUtil:usesSharedEditsDir(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.hdfs.HAUtil:useLogicalUri(org.apache.hadoop.conf.Configuration,java.net.URI)	0	int	0	0
org.apache.hadoop.hdfs.HAUtil:isAtLeastOneActive(java.util.List)	0	int	0	1
org.apache.hadoop.hdfs.HAUtil:isAtLeastOneActive(java.util.List)	1	int	0	0
org.apache.hadoop.CustomOutputCommitter:needsTaskCommit(org.apache.hadoop.mapred.TaskAttemptContext)	0	int	0	1
org.apache.hadoop.mapred.YarnChild$3:run()	0	null	0	null
org.apache.hadoop.mapred.TaskAttemptListenerImpl:canCommit(org.apache.hadoop.mapred.TaskAttemptID)	0	int	0	0
org.apache.hadoop.mapred.TaskAttemptListenerImpl:getProtocolVersion(java.lang.String,long)	0	long	0	21
org.apache.hadoop.mapred.YarnChild$2:run()	0	null	0	null
org.apache.hadoop.mapred.MapReduceChildJVM:getChildEnvProp(org.apache.hadoop.mapred.JobConf,boolean)	0	java.lang.String	0	mapreduce.map.env
org.apache.hadoop.mapred.MapReduceChildJVM:getChildEnvProp(org.apache.hadoop.mapred.JobConf,boolean)	1	java.lang.String	0	mapreduce.reduce.env
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6:run()	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:isJobNamePatternMatch(org.apache.hadoop.mapred.JobConf,java.lang.String)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:keepJobFiles(org.apache.hadoop.mapred.JobConf,java.lang.String)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:keepJobFiles(org.apache.hadoop.mapred.JobConf,java.lang.String)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:isFirstAttempt()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:isFirstAttempt()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery()	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:shouldAttemptRecovery()	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:recovered()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:recovered()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isDataStagnated(long)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isDataStagnated(long)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:getForecast()	0	double	0	-1.0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isDefaultForecast(double)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isDefaultForecast(double)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:getSSE()	0	double	0	-1.0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isErrorWithinBound(double)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:isErrorWithinBound(double)	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:getRawData()	0	double	0	-1.0
org.apache.hadoop.mapreduce.v2.app.speculate.forecast.SimpleExponentialSmoothing:getTimeStamp()	0	long	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$TaskAttemptHistoryStatistics:notHeartbeatedInAWhile(long)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$TaskAttemptHistoryStatistics:notHeartbeatedInAWhile(long)	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.speculate.NullTaskRuntimesEngine:attemptEnrolledTime(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	9223372036854775807
org.apache.hadoop.mapreduce.v2.app.speculate.NullTaskRuntimesEngine:thresholdRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	long	0	9223372036854775807
org.apache.hadoop.mapreduce.v2.app.speculate.NullTaskRuntimesEngine:estimatedRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.NullTaskRuntimesEngine:estimatedNewAttemptRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.NullTaskRuntimesEngine:runtimeEstimateVariance(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.StartEndTimesBase:attemptEnrolledTime(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	9223372036854775807
org.apache.hadoop.mapreduce.v2.app.speculate.StartEndTimesBase:dataStatisticsForTask(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.speculate.StartEndTimesBase:thresholdRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	long	0	9223372036854775807
org.apache.hadoop.mapreduce.v2.app.speculate.StartEndTimesBase:estimatedNewAttemptRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.TaskRuntimeEstimator:hasStagnatedProgress(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,long)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:getForecastEntry(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:estimatedRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:estimatedNewAttemptRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:hasStagnatedProgress(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId,long)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:runtimeEstimateVariance(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:runtimeEstimateVariance(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	1	long	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.TaskSpeculationPredicate:canSpeculate(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.speculate.TaskSpeculationPredicate:canSpeculate(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.mapreduce.v2.api.records.TaskId)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	0	long	0	-9223372036854775808
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	1	long	0	-9223372036854775807
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	2	long	0	-9223372036854775806
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	3	long	0	-9223372036854775805
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	4	long	0	-9223372036854775803
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:speculationValue(org.apache.hadoop.mapreduce.v2.api.records.TaskId,long)	5	long	0	-9223372036854775804
org.apache.hadoop.mapreduce.v2.app.speculate.LegacyTaskRuntimeEstimator:storedPerAttemptValue(java.util.Map,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.ExponentiallySmoothedTaskRuntimeEstimator:getEstimateVector(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.speculate.ExponentiallySmoothedTaskRuntimeEstimator:estimatedRuntime(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.ExponentiallySmoothedTaskRuntimeEstimator:runtimeEstimateVariance(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	long	0	-1
org.apache.hadoop.mapreduce.v2.app.speculate.DataStatistics:mean()	0	double	0	0.0
org.apache.hadoop.mapreduce.v2.app.speculate.DataStatistics:var()	0	double	0	0.0
org.apache.hadoop.mapreduce.v2.app.speculate.DataStatistics:outlier(float)	0	double	0	0.0
org.apache.hadoop.mapreduce.v2.app.speculate.DataStatistics:meanCI()	0	double	0	0.0
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:isCompletelyDone()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container:isCompletelyDone()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherEvent:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:preemptReducesIfNeeded()	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:preemptReducesIfNeeded()	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:preemptReducersForHangingMapRequests(long)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:preemptReducersForHangingMapRequests(long)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:canAssignMaps()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:canAssignMaps()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:canAssignReduces()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:canAssignReduces()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:isNodeBlacklisted(java.lang.String)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests:remove(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests:remove(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests:get(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:remove(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests:remove(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils:divideAndCeil(long,long)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils:calculateRatioOrMaxValue(long,long)	0	int	0	2147483647
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:isPreempted(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:isPreempted(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.preemption.NoopAMPreemptionPolicy:isPreempted(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.preemption.NoopAMPreemptionPolicy:getCheckpointID(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy:isPreempted(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy:getCheckpointID(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.job.impl.MapTaskImpl:getSplitsAsString()	0	java.lang.String	0	
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:getFinishTime()	0	long	0	0
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:getSplitsAsString()	0	java.lang.String	0	
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$1:compare(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$1:compare(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo)	1	int	0	-1
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$1:compare(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo)	2	int	0	1
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getCpuVcoresKey(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	0	java.lang.String	0	mapreduce.map.cpu.vcores
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getCpuVcoresKey(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	1	java.lang.String	0	mapreduce.reduce.cpu.vcores
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getMemoryKey(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	0	java.lang.String	0	mapreduce.map.memory.mb
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getMemoryKey(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	1	java.lang.String	0	mapreduce.reduce.memory.mb
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getResourceTypePrefix(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	0	java.lang.String	0	mapreduce.map.resource.
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:getResourceTypePrefix(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	1	java.lang.String	0	mapreduce.reduce.resource.
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:isContainerAssigned()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:isContainerAssigned()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL)	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:allReducersComplete()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:allReducersComplete()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:getWorkflowAdjacencies(org.apache.hadoop.conf.Configuration)	0	java.lang.String	0	
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:checkAccess(org.apache.hadoop.mapreduce.v2.app.job.Job)	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:checkAccess(org.apache.hadoop.mapreduce.v2.app.job.Job)	1	int	0	1
org.apache.hadoop.mapreduce.v2.app.webapp.JobConfPage:confPostTableInit()	0	java.lang.String	1	dmFyIGNvbmZJbml0VmFscyA9IG5ldyBBcnJheSgpOwokKCd0Zm9vdCBpbnB1dCcpLmtleXVwKCBmdW5jdGlvbiAoKSAKeyAgY29uZkRhdGFUYWJsZS5mbkZpbHRlciggdGhpcy52YWx1ZSwgJCgndGZvb3QgaW5wdXQnKS5pbmRleCh0aGlzKSApOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZWFjaCggZnVuY3Rpb24gKGkpIHsKICBjb25mSW5pdFZhbHNbaV0gPSB0aGlzLnZhbHVlOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZm9jdXMoIGZ1bmN0aW9uICgpIHsKICBpZiAoIHRoaXMuY2xhc3NOYW1lID09ICdzZWFyY2hfaW5pdCcgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJyc7CiAgICB0aGlzLnZhbHVlID0gJyc7CiAgfQp9ICk7CiQoJ3Rmb290IGlucHV0JykuYmx1ciggZnVuY3Rpb24gKGkpIHsKICBpZiAoIHRoaXMudmFsdWUgPT0gJycgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJ3NlYXJjaF9pbml0JzsKICAgIHRoaXMudmFsdWUgPSBjb25mSW5pdFZhbHNbJCgndGZvb3QgaW5wdXQnKS5pbmRleCh0aGlzKV07CiAgfQp9ICk7Cg==
org.apache.hadoop.mapreduce.v2.app.webapp.TaskPage$AttemptsBlock:isValidRequest()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.webapp.TaskPage$AttemptsBlock:isValidRequest()	1	int	0	0
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:isValidRequest()	0	int	0	1
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$SpeculatorEventDispatcher$1:call(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.JobEndNotifier:notifyViaCustomNotifier()	0	int	0	0
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1:call(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.MRClientSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.MRClientSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.v2.app.MRClientSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:isJobCompletionEvent(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent)	0	int	0	1
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:isJobCompletionEvent(org.apache.hadoop.mapreduce.jobhistory.HistoryEvent)	1	int	0	0
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:isWriterActive()	0	int	0	1
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo:isWriterActive()	1	int	0	0
org.apache.hadoop.mapred.LocalJobRunner:getProtocolVersion(java.lang.String,long)	0	long	0	37
org.apache.hadoop.mapred.LocalJobRunner:getTaskTrackerExpiryInterval()	0	long	0	0
org.apache.hadoop.mapred.LocalJobRunner:getAllJobs()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getJobHistoryDir()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getChildQueues(java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getRootQueues()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getQueues()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getQueueAclsForCurrentUser()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:getDelegationToken(org.apache.hadoop.io.Text)	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner:renewDelegationToken(org.apache.hadoop.security.token.Token)	0	long	0	0
org.apache.hadoop.mapred.LocalClientProtocolProvider:create(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapred.LocalClientProtocolProvider:create(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapred.LocalDistributedCacheManager$2:run()	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner$Job:getProtocolVersion(java.lang.String,long)	0	long	0	21
org.apache.hadoop.mapred.LocalJobRunner$Job:getTask(org.apache.hadoop.mapred.JvmContext)	0	null	0	null
org.apache.hadoop.mapred.LocalJobRunner$Job:canCommit(org.apache.hadoop.mapred.TaskAttemptID)	0	int	0	1
org.apache.hadoop.mapred.LocalJobRunner$Job:getCheckpointID(org.apache.hadoop.mapred.TaskID)	0	null	0	null
org.apache.hadoop.mapred.LocalDistributedCacheManager:hasLocalClasspaths()	0	int	0	1
org.apache.hadoop.mapred.LocalDistributedCacheManager:hasLocalClasspaths()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto$Builder:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto$Builder:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto$Builder:hasAppId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto$Builder:hasAppId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto$Builder:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto$Builder:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasTaskAttemptState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasTaskAttemptState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasDiagnosticInfo()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasDiagnosticInfo()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasStateString()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasStateString()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasPhase()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasPhase()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasShuffleFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasShuffleFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasSortFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasSortFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerHost()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerHost()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerHttpPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasNodeManagerHttpPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasContainerId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:hasContainerId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasMapProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasMapProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasReduceProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasReduceProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasCleanupProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasCleanupProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasSetupProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasSetupProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasUser()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasUser()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobFile()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobFile()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasSubmitTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasSubmitTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasIsUber()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasIsUber()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobPriority()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasJobPriority()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasHistoryFile()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:hasHistoryFile()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:hasTaskAttemptReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:hasTaskAttemptReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto$Builder:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto$Builder:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasTaskType()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasTaskType()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CountersProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:hasName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:hasName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:hasDisplayName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:hasDisplayName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasFromEventId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasFromEventId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasMaxEvents()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:hasMaxEvents()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:hasTaskReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:hasTaskReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:hasKey()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:hasKey()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto$Builder:hasDisplayName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterGroupProto$Builder:hasDisplayName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto$Builder:hasJobReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto$Builder:hasJobReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasTaskState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasTaskState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasSuccessfulAttempt()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto$Builder:hasSuccessfulAttempt()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasTaskState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasTaskState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasSuccessfulAttempt()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:hasSuccessfulAttempt()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasMapProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasMapProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasReduceProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasReduceProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasCleanupProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasCleanupProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasSetupProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasSetupProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobFile()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobFile()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasSubmitTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasSubmitTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasIsUber()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasIsUber()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobPriority()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasJobPriority()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasHistoryFile()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobReportProto$Builder:hasHistoryFile()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:hasJobReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:hasJobReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasDisplayName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasDisplayName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasTaskType()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasTaskType()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterGroupMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasMapOutputServerAddress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasMapOutputServerAddress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasAttemptRunTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasAttemptRunTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasEventId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto$Builder:hasEventId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto$Builder:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto$Builder:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasTaskAttemptState()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasTaskAttemptState()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasProgress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasProgress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasDiagnosticInfo()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasDiagnosticInfo()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasStateString()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasStateString()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasPhase()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasPhase()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasShuffleFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasShuffleFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasSortFinishTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasSortFinishTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerHost()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerHost()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerHttpPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasNodeManagerHttpPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptReportProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto$Builder:hasTaskType()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto$Builder:hasTaskType()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto$Builder:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto$Builder:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto$Builder:hasTaskAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto$Builder:hasTaskAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto$Builder:hasTaskReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportResponseProto$Builder:hasTaskReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:hasAppId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:hasAppId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:hasTaskType()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:hasTaskType()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasFromEventId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasFromEventId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasMaxEvents()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto$Builder:hasMaxEvents()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasContainerId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasContainerId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerHost()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerHost()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerHttpPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:hasNodeManagerHttpPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:hasKey()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:hasKey()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$StringCounterMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CountersProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CountersProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CountersProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CountersProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasDisplayName()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasDisplayName()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasValue()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:hasValue()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$CounterProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:hasId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:hasId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto$Builder:hasTaskId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto$Builder:hasTaskId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto$Builder:hasJobId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto$Builder:hasJobId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasStatus()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasStatus()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasMapOutputServerAddress()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasMapOutputServerAddress()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasAttemptRunTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasAttemptRunTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasEventId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:hasEventId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto$Builder:hasTaskAttemptReport()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportResponseProto$Builder:hasTaskAttemptReport()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto$Builder:hasCounters()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersResponseProto$Builder:hasCounters()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerHost()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerHost()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerHttpPort()	0	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$AMInfoProto$Builder:hasNodeManagerHttpPort()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLoadedJobCacheResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshLogRetentionSettingsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshAdminAclsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.proto.HSAdminRefreshProtocolProtos$RefreshJobRetentionSettingsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetTaskReportsRequestPBImpl:getTaskType()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.TaskId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.api.records.TaskId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.api.records.JobId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.v2.api.records.JobId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptReportPBImpl:getTaskAttemptState()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptReportPBImpl:getDiagnosticInfo()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptReportPBImpl:getStateString()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptReportPBImpl:getPhase()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptReportPBImpl:getNodeManagerHost()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskReportPBImpl:getTaskState()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.CountersPBImpl:getCounter(java.lang.Enum)	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.CounterPBImpl:getName()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.CounterPBImpl:getDisplayName()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.JobReportPBImpl:getJobState()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.AMInfoPBImpl:getNodeManagerHost()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.CounterGroupPBImpl:getName()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.CounterGroupPBImpl:getDisplayName()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptCompletionEventPBImpl:getStatus()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskAttemptCompletionEventPBImpl:getMapOutputServerAddress()	0	null	0	null
org.apache.hadoop.mapreduce.v2.api.records.impl.pb.TaskIdPBImpl:getTaskType()	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	mapreduce.jobhistory.principal
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.v2.security.client.ClientHSSecurityInfo$2:annotationType()	0	null	0	null
org.apache.hadoop.mapreduce.v2.util.MRApps:taskSymbol(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	0	java.lang.String	0	m
org.apache.hadoop.mapreduce.v2.util.MRApps:taskSymbol(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	1	java.lang.String	0	r
org.apache.hadoop.mapreduce.v2.util.MRApps:getFileSizes(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.mapreduce.v2.util.LocalResourceBuilder:getResourceDescription(org.apache.hadoop.yarn.api.records.LocalResourceType)	0	java.lang.String	0	cache archive (mapreduce.job.cache.archives) 
org.apache.hadoop.mapreduce.v2.util.LocalResourceBuilder:getResourceDescription(org.apache.hadoop.yarn.api.records.LocalResourceType)	1	java.lang.String	0	cache file (mapreduce.job.cache.files) 
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getYARNWebappScheme()	0	java.lang.String	0	https://
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getYARNWebappScheme()	1	java.lang.String	0	http://
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getJHSWebappScheme(org.apache.hadoop.conf.Configuration)	0	java.lang.String	0	https://
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getJHSWebappScheme(org.apache.hadoop.conf.Configuration)	1	java.lang.String	0	http://
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getDefaultJHSWebappPort()	0	int	0	19890
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getDefaultJHSWebappPort()	1	int	0	19888
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getDefaultJHSWebappURLWithoutScheme()	0	java.lang.String	0	0.0.0.0:19890
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getDefaultJHSWebappURLWithoutScheme()	1	java.lang.String	0	0.0.0.0:19888
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getAMWebappScheme(org.apache.hadoop.conf.Configuration)	0	java.lang.String	0	https://
org.apache.hadoop.mapreduce.v2.util.MRWebAppUtil:getAMWebappScheme(org.apache.hadoop.conf.Configuration)	1	java.lang.String	0	http://
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	0	int	0	5
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	1	int	0	4
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	2	int	0	3
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	3	int	0	2
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	4	int	0	1
org.apache.hadoop.mapreduce.TypeConverter:toYarnApplicationPriority(java.lang.String)	5	int	0	0
org.apache.hadoop.mapreduce.TypeConverter:fromYarn(org.apache.hadoop.mapreduce.v2.api.records.Counters)	0	null	0	null
org.apache.hadoop.mapreduce.TypeConverter:toYarn(org.apache.hadoop.mapred.Counters)	0	null	0	null
org.apache.hadoop.mapreduce.TypeConverter:toYarn(org.apache.hadoop.mapreduce.Counters)	0	null	0	null
org.apache.hadoop.yarn.proto.MRClientProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	0	int	0	0
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	1	int	0	1
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	2	int	0	2
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	3	int	0	3
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	4	int	0	4
org.apache.hadoop.mapred.jobcontrol.JobControl:getState()	5	int	0	-1
org.apache.hadoop.mapred.jobcontrol.Job:getAssignedJobID()	0	null	0	null
org.apache.hadoop.mapred.jobcontrol.Job:getState()	0	int	0	0
org.apache.hadoop.mapred.jobcontrol.Job:getState()	1	int	0	1
org.apache.hadoop.mapred.jobcontrol.Job:getState()	2	int	0	2
org.apache.hadoop.mapred.jobcontrol.Job:getState()	3	int	0	3
org.apache.hadoop.mapred.jobcontrol.Job:getState()	4	int	0	4
org.apache.hadoop.mapred.jobcontrol.Job:getState()	5	int	0	5
org.apache.hadoop.mapred.jobcontrol.Job:getState()	6	int	0	-1
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:next(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)	0	int	0	0
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:next(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)	1	int	0	1
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapred.OutputCommitter:isRecoverySupported()	0	int	0	0
org.apache.hadoop.mapred.OutputCommitter:isCommitJobRepeatable(org.apache.hadoop.mapred.JobContext)	0	int	0	0
org.apache.hadoop.mapred.MapTask$DirectMapOutputCollector:getOutputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.MapTaskStatus:getIsMap()	0	int	0	1
org.apache.hadoop.mapred.FixedLengthInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.FixedLengthInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.FileInputFormat$2:compare(org.apache.hadoop.mapred.FileInputFormat$NodeInfo,org.apache.hadoop.mapred.FileInputFormat$NodeInfo)	0	int	0	-1
org.apache.hadoop.mapred.FileInputFormat$2:compare(org.apache.hadoop.mapred.FileInputFormat$NodeInfo,org.apache.hadoop.mapred.FileInputFormat$NodeInfo)	1	int	0	0
org.apache.hadoop.mapred.FileInputFormat$2:compare(org.apache.hadoop.mapred.FileInputFormat$NodeInfo,org.apache.hadoop.mapred.FileInputFormat$NodeInfo)	2	int	0	1
org.apache.hadoop.mapred.FileOutputCommitter:getJobAttemptPath(org.apache.hadoop.mapred.JobContext)	0	null	0	null
org.apache.hadoop.mapred.FileOutputCommitter:getTaskAttemptPath(org.apache.hadoop.mapred.TaskAttemptContext)	0	null	0	null
org.apache.hadoop.mapred.FileOutputCommitter:getCommittedTaskPath(org.apache.hadoop.mapred.TaskAttemptContext)	0	null	0	null
org.apache.hadoop.mapred.FileOutputCommitter:getWorkPath(org.apache.hadoop.mapred.TaskAttemptContext,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.mapred.FileOutputCommitter:isRecoverySupported()	0	int	0	1
org.apache.hadoop.mapred.Merger$MergeQueue:next()	0	int	0	0
org.apache.hadoop.mapred.Merger$MergeQueue:next()	1	int	0	1
org.apache.hadoop.mapred.Merger$MergeQueue:lessThan(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.Merger$MergeQueue:lessThan(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.Task:isJobAbortTask()	0	int	0	1
org.apache.hadoop.mapred.Task:isJobAbortTask()	1	int	0	0
org.apache.hadoop.mapred.Task:isMapOrReduce()	0	int	0	1
org.apache.hadoop.mapred.Task:isMapOrReduce()	1	int	0	0
org.apache.hadoop.mapred.Task:calculateOutputSize()	0	long	0	-1
org.apache.hadoop.mapred.Task:keepTaskFiles(org.apache.hadoop.mapred.JobConf)	0	int	0	1
org.apache.hadoop.mapred.Task:keepTaskFiles(org.apache.hadoop.mapred.JobConf)	1	int	0	0
org.apache.hadoop.mapred.Utils$OutputFileUtils$OutputLogFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.Utils$OutputFileUtils$OutputLogFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.IFileInputStream:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.mapred.IFileInputStream:readWithChecksum(byte[],int,int)	0	int	0	-1
org.apache.hadoop.mapred.SequenceFileAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.mapred.SequenceFileAsTextRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	1	int	0	1
org.apache.hadoop.mapred.IndexCache:isUnderConstruction(org.apache.hadoop.mapred.IndexCache$IndexInformation)	0	int	0	1
org.apache.hadoop.mapred.IndexCache:isUnderConstruction(org.apache.hadoop.mapred.IndexCache$IndexInformation)	1	int	0	0
org.apache.hadoop.mapred.IndexCache:checkTotalMemoryUsed()	0	int	0	1
org.apache.hadoop.mapred.IndexCache:checkTotalMemoryUsed()	1	int	0	0
org.apache.hadoop.mapred.MapTask$TrackedRecordReader:getInputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader:getInputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.FileInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.FileInputFormat$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.FileInputFormat$1:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.MapTask$SkippingRecordReader:next(java.lang.Object,java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.Queue:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.Queue:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue)	0	int	0	0
org.apache.hadoop.mapred.Queue:isHierarchySameAs(org.apache.hadoop.mapred.Queue)	1	int	0	1
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int,java.io.InputStream)	0	int	0	0
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int)	0	int	0	0
org.apache.hadoop.mapred.BackupStore$BackupRamManager:reserve(int,int)	0	int	0	0
org.apache.hadoop.mapred.ReduceTask$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	0	int	0	-1
org.apache.hadoop.mapred.ReduceTask$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.mapred.ReduceTask$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	2	int	0	1
org.apache.hadoop.mapred.JobClient:isJobDirValid(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)	0	int	0	1
org.apache.hadoop.mapred.JobClient:isJobDirValid(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)	1	int	0	0
org.apache.hadoop.mapred.JobClient:getJobsFromQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.Task$TaskReporter:getCounter(java.lang.Enum)	0	null	0	null
org.apache.hadoop.mapred.MergeSorter:sort()	0	null	0	null
org.apache.hadoop.mapred.ReduceTask:isMapTask()	0	int	0	0
org.apache.hadoop.mapred.SkipBadRecords:getSkipOutputPath(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapred.StatisticsCollector$TimeWindow:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.StatisticsCollector$TimeWindow:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.SplitLocationInfo:isOnDisk()	0	int	0	1
org.apache.hadoop.mapred.Merger$MergeQueue$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	0	int	0	0
org.apache.hadoop.mapred.Merger$MergeQueue$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	1	int	0	-1
org.apache.hadoop.mapred.Merger$MergeQueue$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	2	int	0	1
org.apache.hadoop.mapred.MRSortResultIterator:getProgress()	0	null	0	null
org.apache.hadoop.mapred.MRSortResultIterator:next()	0	int	0	0
org.apache.hadoop.mapred.MRSortResultIterator:next()	1	int	0	1
org.apache.hadoop.mapred.JobACLsManager:isMRAdmin(org.apache.hadoop.security.UserGroupInformation)	0	int	0	1
org.apache.hadoop.mapred.JobACLsManager:isMRAdmin(org.apache.hadoop.security.UserGroupInformation)	1	int	0	0
org.apache.hadoop.mapred.JobACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList)	0	int	0	1
org.apache.hadoop.mapred.JobACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL,java.lang.String,org.apache.hadoop.security.authorize.AccessControlList)	1	int	0	0
org.apache.hadoop.mapred.MapTask:isMapTask()	0	int	0	1
org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat$PipesDummyRecordReader:createKey()	0	null	0	null
org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat$PipesDummyRecordReader:createValue()	0	null	0	null
org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat$PipesDummyRecordReader:getPos()	0	long	0	0
org.apache.hadoop.mapred.pipes.PipesNonJavaInputFormat$PipesDummyRecordReader:next(org.apache.hadoop.io.FloatWritable,org.apache.hadoop.io.NullWritable)	0	int	0	1
org.apache.hadoop.mapred.pipes.Submitter:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.mapred.pipes.Submitter:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.mapred.Merger$Segment:inMemory()	0	int	0	1
org.apache.hadoop.mapred.Merger$Segment:inMemory()	1	int	0	0
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:next(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:next(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.KeyValueTextInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:deprecatedConf(org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:deprecatedConf(org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.mapred.SequenceFileRecordReader:next(java.lang.Object,java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.SequenceFileRecordReader:next(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.SequenceFileRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapred.MapTask$MapOutputBuffer$MRResultIterator:next()	0	int	0	1
org.apache.hadoop.mapred.MapTask$MapOutputBuffer$MRResultIterator:next()	1	int	0	0
org.apache.hadoop.mapred.MapTask$MapOutputBuffer$MRResultIterator:getProgress()	0	null	0	null
org.apache.hadoop.mapred.FileOutputFormat:getOutputPath(org.apache.hadoop.mapred.JobConf)	0	null	0	null
org.apache.hadoop.mapred.FileOutputFormat:getWorkOutputPath(org.apache.hadoop.mapred.JobConf)	0	null	0	null
org.apache.hadoop.mapred.JVMId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.JVMId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.JVMId:compareTo(org.apache.hadoop.mapred.JVMId)	0	int	0	-1
org.apache.hadoop.mapred.JVMId:compareTo(org.apache.hadoop.mapred.JVMId)	1	int	0	1
org.apache.hadoop.mapred.JVMId:forName(java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:hasNext()	0	int	0	1
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:hasNext()	1	int	0	0
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:skippedAllRanges()	0	int	0	1
org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:skippedAllRanges()	1	int	0	0
org.apache.hadoop.mapred.JobStatus:getJobRunState(int)	0	java.lang.String	0	UNKNOWN
org.apache.hadoop.mapred.IndexCache$IndexInformation:getSize()	0	int	0	0
org.apache.hadoop.mapred.Utils$OutputFileUtils$OutputFilesFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.Utils$OutputFileUtils$OutputFilesFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.IFile$Reader:positionToNextRecord(java.io.DataInput)	0	int	0	0
org.apache.hadoop.mapred.IFile$Reader:positionToNextRecord(java.io.DataInput)	1	int	0	1
org.apache.hadoop.mapred.IFile$Reader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	0	int	0	0
org.apache.hadoop.mapred.IFile$Reader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	1	int	0	1
org.apache.hadoop.mapred.LineRecordReader:isCompressedInput()	0	int	0	1
org.apache.hadoop.mapred.LineRecordReader:isCompressedInput()	1	int	0	0
org.apache.hadoop.mapred.LineRecordReader:maxBytesToConsume(long)	0	int	0	2147483647
org.apache.hadoop.mapred.LineRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.mapred.LineRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.Text)	1	int	0	1
org.apache.hadoop.mapred.LineRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapred.CleanupQueue:deletePath(org.apache.hadoop.mapred.CleanupQueue$PathDeletionContext)	0	int	0	1
org.apache.hadoop.mapred.CleanupQueue:isQueueEmpty()	0	int	0	1
org.apache.hadoop.mapred.CleanupQueue:isQueueEmpty()	1	int	0	0
org.apache.hadoop.mapred.ReduceTask$SkippingReduceValuesIterator:more()	0	int	0	1
org.apache.hadoop.mapred.ReduceTask$SkippingReduceValuesIterator:more()	1	int	0	0
org.apache.hadoop.mapred.KeyValueLineRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.mapred.KeyValueLineRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	1	int	0	1
org.apache.hadoop.mapred.QueueConfigurationParser:createHierarchy(java.lang.String,org.w3c.dom.Element)	0	null	0	null
org.apache.hadoop.mapred.Counters$Counter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.Counters$Counter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.SortedRanges$Range:isEmpty()	0	int	0	1
org.apache.hadoop.mapred.SortedRanges$Range:isEmpty()	1	int	0	0
org.apache.hadoop.mapred.SortedRanges$Range:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.SortedRanges$Range:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.SortedRanges$Range:compareTo(org.apache.hadoop.mapred.SortedRanges$Range)	0	int	0	-1
org.apache.hadoop.mapred.SortedRanges$Range:compareTo(org.apache.hadoop.mapred.SortedRanges$Range)	1	int	0	1
org.apache.hadoop.mapred.SortedRanges$Range:compareTo(org.apache.hadoop.mapred.SortedRanges$Range)	2	int	0	0
org.apache.hadoop.mapred.JobEndNotifier$JobEndStatusInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.JobEndNotifier$JobEndStatusInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter:getOutputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.ReduceTaskStatus:getIsMap()	0	int	0	0
org.apache.hadoop.mapred.JobConf:getSpeculativeExecution()	0	int	0	1
org.apache.hadoop.mapred.JobConf:getSpeculativeExecution()	1	int	0	0
org.apache.hadoop.mapred.JobConf:getJobPriorityAsInteger()	0	int	0	0
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	0	int	0	5
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	1	int	0	4
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	2	int	0	3
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	3	int	0	2
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	4	int	0	1
org.apache.hadoop.mapred.JobConf:convertPriorityToInteger(java.lang.String)	5	int	0	0
org.apache.hadoop.mapred.JobConf:getMaxPhysicalMemoryForTask()	0	long	0	-1
org.apache.hadoop.mapred.JobConf:parseMaximumHeapSizeMB(java.lang.String)	0	int	0	-1
org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter:getOutputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.BackupStore:hasNext()	0	int	0	0
org.apache.hadoop.mapred.BackupStore:hasNext()	1	int	0	1
org.apache.hadoop.mapred.lib.MultithreadedMapRunner$BlockingArrayQueue:add(java.lang.Runnable)	0	int	0	1
org.apache.hadoop.mapred.lib.CombineFileInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)	0	null	0	null
org.apache.hadoop.mapred.lib.CombineFileInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJobBase:getValueAggregatorDescriptor(java.lang.String,org.apache.hadoop.mapred.JobConf)	0	null	0	null
org.apache.hadoop.mapred.lib.MultipleOutputs:lambda$close$1(org.apache.hadoop.mapred.RecordWriter,java.util.concurrent.atomic.AtomicBoolean)	0	null	0	null
org.apache.hadoop.mapred.lib.CombineFileRecordReader:next(java.lang.Object,java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.lib.CombineFileRecordReader:next(java.lang.Object,java.lang.Object)	1	int	0	1
org.apache.hadoop.mapred.lib.CombineFileRecordReader:initNextRecordReader()	0	int	0	0
org.apache.hadoop.mapred.lib.CombineFileRecordReader:initNextRecordReader()	1	int	0	1
org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector:getOutputBytes(java.util.List)	0	long	0	0
org.apache.hadoop.mapred.TextInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.Counters:getCounterValue(org.apache.hadoop.mapreduce.counters.CounterGroupBase,java.lang.String)	0	long	0	0
org.apache.hadoop.mapred.Counters$Group:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapred.ClusterStatus:getMaxMemory()	0	long	0	-1
org.apache.hadoop.mapred.ClusterStatus:getUsedMemory()	0	long	0	-1
org.apache.hadoop.mapred.join.OuterJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:hasNext()	0	int	0	1
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:hasNext()	1	int	0	0
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:next(org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	0
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:next(org.apache.hadoop.mapred.join.TupleWritable)	1	int	0	1
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:flush(org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector:flush(org.apache.hadoop.mapred.join.TupleWritable)	1	int	0	0
org.apache.hadoop.mapred.join.MultiFilterRecordReader$MultiFilterDelegationIterator:replay(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapred.join.InnerJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	0
org.apache.hadoop.mapred.join.InnerJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapred.join.TupleWritable)	1	int	0	1
org.apache.hadoop.mapred.join.MultiFilterRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapred.join.MultiFilterRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapred.join.MultiFilterRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)	1	int	0	0
org.apache.hadoop.mapred.join.JoinRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.mapred.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapred.join.JoinRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.mapred.join.TupleWritable)	1	int	0	0
org.apache.hadoop.mapred.join.CompositeRecordReader:hasNext()	0	int	0	1
org.apache.hadoop.mapred.join.CompositeRecordReader:hasNext()	1	int	0	0
org.apache.hadoop.mapred.join.CompositeRecordReader:getPos()	0	long	0	0
org.apache.hadoop.mapred.join.WrappedRecordReader:hasNext()	0	int	0	1
org.apache.hadoop.mapred.join.WrappedRecordReader:hasNext()	1	int	0	0
org.apache.hadoop.mapred.join.WrappedRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapred.join.WrappedRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)	1	int	0	0
org.apache.hadoop.mapred.join.WrappedRecordReader:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.join.WrappedRecordReader:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.join.WrappedRecordReader:hashCode()	0	int	0	42
org.apache.hadoop.mapred.FileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapred.FileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.mapred.Reporter$1:getCounter(java.lang.Enum)	0	null	0	null
org.apache.hadoop.mapred.Reporter$1:getCounter(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.Reporter$1:getProgress()	0	float	0	0.0
org.apache.hadoop.mapred.QueueManager:hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)	0	int	0	0
org.apache.hadoop.mapred.QueueManager:hasAccess(java.lang.String,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.security.UserGroupInformation)	1	int	0	1
org.apache.hadoop.mapred.QueueManager:isRunning(java.lang.String)	0	int	0	0
org.apache.hadoop.mapred.BackupStore$MemoryCache:reserveSpace(int)	0	int	0	1
org.apache.hadoop.mapred.BackupStore$MemoryCache:reserveSpace(int)	1	int	0	0
org.apache.hadoop.mapred.TaskStatus:getMaxStringSize()	0	int	0	1024
org.apache.hadoop.mapred.TaskStatus:getShuffleFinishTime()	0	long	0	0
org.apache.hadoop.mapred.TaskStatus:getMapFinishTime()	0	long	0	0
org.apache.hadoop.mapred.TaskStatus:getSortFinishTime()	0	long	0	0
org.apache.hadoop.mapred.TaskStatus:inTaskCleanupPhase()	0	int	0	1
org.apache.hadoop.mapred.TaskStatus:inTaskCleanupPhase()	1	int	0	0
org.apache.hadoop.mapred.TaskStatus:getFetchFailedMaps()	0	null	0	null
org.apache.hadoop.mapred.BasicTypeSorterBase:getMemoryUtilized()	0	long	0	0
org.apache.hadoop.mapred.ClusterStatus$BlackListInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.ClusterStatus$BlackListInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.OutputCommitter:isRecoverySupported()	0	int	0	0
org.apache.hadoop.mapreduce.OutputCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext)	0	int	0	0
org.apache.hadoop.mapreduce.counters.AbstractCounters:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.counters.CounterGroupFactory:version()	0	int	0	1
org.apache.hadoop.mapreduce.counters.CounterGroupFactory:isFrameworkGroup(java.lang.String)	0	int	0	1
org.apache.hadoop.mapreduce.counters.CounterGroupFactory:isFrameworkGroup(java.lang.String)	1	int	0	0
org.apache.hadoop.mapreduce.counters.AbstractCounterGroup:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.counters.AbstractCounter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.counters.AbstractCounter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.JobID:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.JobID:forName(java.lang.String)	0	null	0	null
org.apache.hadoop.mapreduce.TaskCompletionEvent:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.TaskCompletionEvent:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.mapreduce.Job$4:run()	0	null	0	null
org.apache.hadoop.mapreduce.Job$5:run()	0	null	0	null
org.apache.hadoop.mapreduce.TaskID:isMap()	0	int	0	1
org.apache.hadoop.mapreduce.TaskID:isMap()	1	int	0	0
org.apache.hadoop.mapreduce.TaskID:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.TaskID:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.mapreduce.TaskID:forName(java.lang.String)	0	null	0	null
org.apache.hadoop.mapreduce.TaskReport:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.TaskReport:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.mapreduce.InputSplit:getLocationInfo()	0	null	0	null
org.apache.hadoop.mapreduce.checkpoint.TaskCheckpointID:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.checkpoint.TaskCheckpointID:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.checkpoint.FSCheckpointID:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.checkpoint.FSCheckpointID:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.checkpoint.FSCheckpointService:delete(org.apache.hadoop.mapreduce.checkpoint.CheckpointID)	0	int	0	1
org.apache.hadoop.mapreduce.CryptoUtils:cryptoPadding(org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$RawKVIteratorReader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	0	int	0	1
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$RawKVIteratorReader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	1	int	0	0
org.apache.hadoop.mapreduce.task.reduce.InMemoryReader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.InMemoryReader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)	1	int	0	1
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:waitUntilDone(int)	0	int	0	1
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:waitUntilDone(int)	1	int	0	0
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl$Penalty:compareTo(java.util.concurrent.Delayed)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl$Penalty:compareTo(java.util.concurrent.Delayed)	1	int	0	-1
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl$Penalty:compareTo(java.util.concurrent.Delayed)	2	int	0	1
org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:getDescription()	0	java.lang.String	0	MEMORY
org.apache.hadoop.mapreduce.task.reduce.MapOutput$MapOutputComparator:compare(org.apache.hadoop.mapreduce.task.reduce.MapOutput,org.apache.hadoop.mapreduce.task.reduce.MapOutput)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.MapOutput$MapOutputComparator:compare(org.apache.hadoop.mapreduce.task.reduce.MapOutput,org.apache.hadoop.mapreduce.task.reduce.MapOutput)	1	int	0	-1
org.apache.hadoop.mapreduce.task.reduce.MapOutput$MapOutputComparator:compare(org.apache.hadoop.mapreduce.task.reduce.MapOutput,org.apache.hadoop.mapreduce.task.reduce.MapOutput)	2	int	0	1
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:copyMapOutput(org.apache.hadoop.mapreduce.TaskAttemptID)	1	int	0	1
org.apache.hadoop.mapreduce.task.reduce.Fetcher:verifySanity(long,long,int,java.util.Set,org.apache.hadoop.mapreduce.TaskAttemptID)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.Fetcher:verifySanity(long,long,int,java.util.Set,org.apache.hadoop.mapreduce.TaskAttemptID)	1	int	0	1
org.apache.hadoop.mapreduce.task.reduce.MapOutput:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.task.reduce.MapOutput:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	0	int	0	0
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	1	int	0	-1
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$1:compare(org.apache.hadoop.mapred.Merger$Segment,org.apache.hadoop.mapred.Merger$Segment)	2	int	0	1
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:getDescription()	0	java.lang.String	0	DISK
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKey()	0	int	0	0
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.task.ReduceContextImpl:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.task.JobContextImpl:getArchiveClassPaths(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.task.JobContextImpl:parseTimestamps(java.lang.String[])	0	null	0	null
org.apache.hadoop.mapreduce.task.JobContextImpl:getFileClassPaths(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.task.JobContextImpl:toTimestampStrs(long[])	0	null	0	null
org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl$DummyReporter:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.QueueInfo:getSchedulingInfo()	0	java.lang.String	0	N/A
org.apache.hadoop.mapreduce.ID:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.ID:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.JobResourceUploader$LimitChecker:hasLimits()	0	int	0	1
org.apache.hadoop.mapreduce.JobResourceUploader$LimitChecker:hasLimits()	1	int	0	0
org.apache.hadoop.mapreduce.JobStatus:isJobComplete()	0	int	0	1
org.apache.hadoop.mapreduce.JobStatus:isJobComplete()	1	int	0	0
org.apache.hadoop.mapreduce.SharedCacheConfig:isSharedCacheEnabled()	0	int	0	1
org.apache.hadoop.mapreduce.SharedCacheConfig:isSharedCacheEnabled()	1	int	0	0
org.apache.hadoop.mapreduce.security.SecureShuffleUtils:verifyHash(byte[],byte[],javax.crypto.SecretKey)	0	int	0	1
org.apache.hadoop.mapreduce.security.SecureShuffleUtils:verifyHash(byte[],byte[],javax.crypto.SecretKey)	1	int	0	0
org.apache.hadoop.mapreduce.security.token.JobTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier:getUser()	0	null	0	null
org.apache.hadoop.mapreduce.security.TokenCache:getSecretKey(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)	0	null	0	null
org.apache.hadoop.mapreduce.security.TokenCache:isTokenRenewalExcluded(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapreduce.security.TokenCache:isTokenRenewalExcluded(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.mapreduce.security.SpillCallBackInjector:getSpilledFileReport()	0	null	0	null
org.apache.hadoop.mapreduce.Job:getTaskFailureEventString()	0	java.lang.String	0	There are no failed tasks for the job. Job is failed due to some other reason and reason can be found in the logs.
org.apache.hadoop.mapreduce.Job:isSuccessful()	0	int	0	1
org.apache.hadoop.mapreduce.Job:isSuccessful()	1	int	0	0
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	0	int	0	5
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	1	int	0	4
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	2	int	0	3
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	3	int	0	2
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	4	int	0	1
org.apache.hadoop.mapreduce.Job:convertPriorityToInteger(org.apache.hadoop.mapreduce.JobPriority)	5	int	0	0
org.apache.hadoop.mapreduce.Job:addFileToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapreduce.Job:addFileToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.mapreduce.Job:addFileToSharedCacheAndClasspath(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapreduce.Job:addFileToSharedCacheAndClasspath(java.net.URI,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.mapreduce.Job:addArchiveToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapreduce.Job:addArchiveToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.mapreduce.Job:isConnected()	0	int	0	1
org.apache.hadoop.mapreduce.Job:isConnected()	1	int	0	0
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:addDependingJob(org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob)	0	int	0	0
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:isCompleted()	0	int	0	1
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:isCompleted()	1	int	0	0
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:isReady()	0	int	0	1
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:isReady()	1	int	0	0
org.apache.hadoop.mapreduce.lib.db.DateSplitter:resultSetColToLong(java.sql.ResultSet,int,int)	0	long	0	-9223372036854775808
org.apache.hadoop.mapreduce.lib.db.DBRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.db.DBRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat$DataDrivenDBInputSplit:getLength()	0	long	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldHelper:getStartOffset(byte[],int,int,int[],org.apache.hadoop.mapreduce.lib.partition.KeyFieldHelper$KeyDescription)	0	int	0	-1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldHelper:parseKey(java.lang.String,java.util.StringTokenizer)	0	null	0	null
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner:getPartition(java.lang.Object,java.lang.Object,int)	0	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:compare(byte[],int,int,byte[],int,int)	0	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:compareByteSequence(byte[],int,int,byte[],int,int,org.apache.hadoop.mapreduce.lib.partition.KeyFieldHelper$KeyDescription)	0	int	0	1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:compareByteSequence(byte[],int,int,byte[],int,int,org.apache.hadoop.mapreduce.lib.partition.KeyFieldHelper$KeyDescription)	1	int	0	-1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:isdigit(byte)	0	int	0	1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:isdigit(byte)	1	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:decimalCompare(byte[],int,int,byte[],int,int)	0	int	0	1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:decimalCompare(byte[],int,int,byte[],int,int)	1	int	0	-1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:decimalCompare(byte[],int,int,byte[],int,int)	2	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:decimalCompare1(byte[],int,int)	0	int	0	1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:decimalCompare1(byte[],int,int)	1	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:oneNegativeCompare(byte[],int,int,byte[],int,int)	0	int	0	-1
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:oneNegativeCompare(byte[],int,int,byte[],int,int)	1	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:isZero(byte[],int,int)	0	int	0	0
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:isZero(byte[],int,int)	1	int	0	1
org.apache.hadoop.mapreduce.lib.partition.InputSampler:printUsage()	0	int	0	-1
org.apache.hadoop.mapreduce.lib.partition.InputSampler:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorJobBase:getValueAggregatorDescriptor(java.lang.String,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$SubMapRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$SubMapRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$SubMapRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.output.BindingPathOutputCommitter:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:isRecoverySupported()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:getStageName(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData)	0	java.lang.String	0	committer_commit_job
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:lambda$coalesceDirectories$2(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:lambda$coalesceDirectories$2(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Result:wasExecuted()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Result:wasExecuted()	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)	0	null	0	null
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isRecoverySupported(org.apache.hadoop.mapreduce.JobContext)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterConfig,org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData,java.lang.Throwable,boolean,boolean)	0	null	0	null
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.DirEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData:joinMap(java.util.Map,java.lang.String,java.lang.String,java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestPrinter:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestPrinter:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryIterator:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryIterator:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryIterator:fetchNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryIterator:fetchNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperations:isFile(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperations:storePreservesEtagsThroughRenames(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperations:storeSupportsResilientCommit()	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO$EntryWriter:enqueue(java.util.List)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperationsThroughFileSystem:storePreservesEtagsThroughRenames(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitter:hasOutputPath()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitter:hasOutputPath()	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputPath(org.apache.hadoop.mapreduce.JobContext)	0	null	0	null
org.apache.hadoop.mapreduce.lib.output.NullOutputFormat$2:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.NullOutputFormat$2:isRecoverySupported()	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat$2:accept(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat$2:accept(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter$CommittedTaskFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter$CommittedTaskFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:lambda$close$1(org.apache.hadoop.mapreduce.RecordWriter,java.util.concurrent.atomic.AtomicBoolean)	0	null	0	null
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapreduce.lib.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:next(org.apache.hadoop.mapreduce.lib.join.TupleWritable)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:next(org.apache.hadoop.mapreduce.lib.join.TupleWritable)	1	int	0	1
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:flush(org.apache.hadoop.mapreduce.lib.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector:flush(org.apache.hadoop.mapreduce.lib.join.TupleWritable)	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.MultiFilterRecordReader$MultiFilterDelegationIterator:replay(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.ArrayListBackedIterator:next(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.ArrayListBackedIterator:next(org.apache.hadoop.io.Writable)	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.ArrayListBackedIterator:replay(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:next(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:next(org.apache.hadoop.io.Writable)	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:replay(org.apache.hadoop.io.Writable)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.StreamBackedIterator:replay(org.apache.hadoop.io.Writable)	1	int	0	1
org.apache.hadoop.mapreduce.lib.join.TupleWritable$1:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.TupleWritable$1:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapreduce.lib.join.TupleWritable)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapreduce.lib.join.TupleWritable)	1	int	0	1
org.apache.hadoop.mapreduce.lib.join.MultiFilterRecordReader:combine(java.lang.Object[],org.apache.hadoop.mapreduce.lib.join.TupleWritable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.MultiFilterRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.MultiFilterRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.JoinRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.JoinRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.ResetableIterator$EMPTY:hasNext()	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.ResetableIterator$EMPTY:next(org.apache.hadoop.io.Writable)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.ResetableIterator$EMPTY:replay(org.apache.hadoop.io.Writable)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.TupleWritable:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.lib.join.TupleWritable:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:hasNext()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:hasNext()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:next()	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:next()	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.lib.join.WrappedRecordReader:hashCode()	0	int	0	42
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat$SequenceFileAsBinaryRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:getFormatMinSplitSize()	0	long	0	1
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.FileInputFormat$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.FileInputFormat$1:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper:fileSplitIsValid(org.apache.hadoop.mapreduce.TaskAttemptContext)	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper:fileSplitIsValid(org.apache.hadoop.mapreduce.TaskAttemptContext)	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$MD5Filter:accept(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$MD5Filter:accept(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat:getFormatMinSplitSize()	0	long	0	102400
org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$FilterRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$FilterRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.input.SplitLineReader:needAdditionalRecordAfterSplit()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.LineRecordReader:maxBytesToConsume(long)	0	int	0	2147483647
org.apache.hadoop.mapreduce.lib.input.LineRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.LineRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.LineRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader:findSeparator(byte[],int,int,byte)	0	int	0	-1
org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.KeyValueLineRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader:initNextRecordReader()	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader:initNextRecordReader()	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.TextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader:needAdditionalRecordAfterSplit()	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader:needAdditionalRecordAfterSplit()	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader:didReadAfterSplit()	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader:didReadAfterSplit()	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.FileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.mapreduce.lib.input.FileInputFormat$MultiPathFilter:accept(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader:needAdditionalRecordAfterSplit()	0	int	0	1
org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader:needAdditionalRecordAfterSplit()	1	int	0	0
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.chain.Chain$ChainRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapreduce.lib.chain.Chain$ChainRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapreduce.lib.chain.Chain$ChainRecordReader:readFromQueue()	0	int	0	0
org.apache.hadoop.mapreduce.lib.chain.Chain$ChainRecordReader:readFromQueue()	1	int	0	1
org.apache.hadoop.mapreduce.lib.chain.Chain$ChainRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.mapreduce.lib.chain.Chain:setIfUnsetThrowable(java.lang.Throwable)	0	int	0	1
org.apache.hadoop.mapreduce.lib.chain.Chain:setIfUnsetThrowable(java.lang.Throwable)	1	int	0	0
org.apache.hadoop.mapreduce.lib.chain.Chain:getPrefix(boolean)	0	java.lang.String	0	mapreduce.chain.mapper
org.apache.hadoop.mapreduce.lib.chain.Chain:getPrefix(boolean)	1	java.lang.String	0	mapreduce.chain.reducer
org.apache.hadoop.mapreduce.JobResourceUploader:useSharedCache(java.net.URI,java.lang.String,java.util.Map,org.apache.hadoop.conf.Configuration,boolean)	0	null	0	null
org.apache.hadoop.mapreduce.JobResourceUploader:validateFilePath(java.lang.String,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.mapreduce.util.ProcessTree:isAlive(java.lang.String)	0	int	0	1
org.apache.hadoop.mapreduce.util.ProcessTree:isAlive(java.lang.String)	1	int	0	0
org.apache.hadoop.mapreduce.util.ProcessTree:isProcessGroupAlive(java.lang.String)	0	int	0	1
org.apache.hadoop.mapreduce.util.ProcessTree:isProcessGroupAlive(java.lang.String)	1	int	0	0
org.apache.hadoop.mapreduce.util.MRJobConfUtil:getTaskProgressMinDeltaThreshold()	0	double	0	5.0
org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent:getDiagnostics()	0	java.lang.String	0	
org.apache.hadoop.mapreduce.jobhistory.JobUnsuccessfulCompletionEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.TaskAttemptFinishedEvent:getRackName()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobSubmittedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.HistoryViewer:getTaskLogsUrl(java.lang.String,org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo)	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobInitedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.NormalizedResourceEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.ReduceAttemptFinishedEvent:getRackName()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.TaskUpdatedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobPriorityChangeEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.MapAttemptFinishedEvent:getRackName()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.TaskAttemptStartedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobStatusChangedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.TaskAttemptUnsuccessfulCompletionEvent:getRackName()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobQueueChangeEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.TaskStartedEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.jobhistory.JobInfoChangeEvent:getTimelineMetrics()	0	null	0	null
org.apache.hadoop.mapreduce.JobSubmitter$1:compare(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.InputSplit)	0	int	0	0
org.apache.hadoop.mapreduce.JobSubmitter$1:compare(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.InputSplit)	1	int	0	1
org.apache.hadoop.mapreduce.JobSubmitter$1:compare(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.InputSplit)	2	int	0	-1
org.apache.hadoop.mapreduce.JobSubmitter$SplitComparator:compare(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.InputSplit)	0	int	0	1
org.apache.hadoop.mapreduce.JobSubmitter$SplitComparator:compare(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.InputSplit)	1	int	0	0
org.apache.hadoop.mapreduce.JobSubmitter$SplitComparator:compare(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.InputSplit)	2	int	0	-1
org.apache.hadoop.mapreduce.TaskAttemptID:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.mapreduce.TaskAttemptID:forName(java.lang.String)	0	null	0	null
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:isPublic(org.apache.hadoop.conf.Configuration,java.net.URI,java.util.Map)	0	int	0	1
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:isPublic(org.apache.hadoop.conf.Configuration,java.net.URI,java.util.Map)	1	int	0	0
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:ancestorsHaveExecutePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)	0	int	0	0
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:ancestorsHaveExecutePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Map)	1	int	0	1
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:checkPermissionOfOther(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction,java.util.Map)	0	int	0	1
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:checkPermissionOfOther(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction,java.util.Map)	1	int	0	0
org.apache.hadoop.mapreduce.filecache.DistributedCache:getSymlink(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapreduce.filecache.DistributedCache:parseBooleans(java.lang.String[])	0	null	0	null
org.apache.hadoop.mapreduce.filecache.DistributedCache:checkURIs(java.net.URI[],java.net.URI[])	0	int	0	1
org.apache.hadoop.mapreduce.filecache.DistributedCache:checkURIs(java.net.URI[],java.net.URI[])	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:isNameNodeStillNotStarted(java.lang.Exception)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:isNameNodeStillNotStarted(java.lang.Exception)	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:scanOldDirsForJob(org.apache.hadoop.mapreduce.v2.api.records.JobId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getTask(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getCompletedMaps()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getCompletedReduces()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getProgress()	0	float	0	1.0
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:isUber()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getFailedMaps()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getFailedReduces()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getKilledMaps()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.UnparsedJob:getKilledReduces()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler:isAllowedDelegationTokenOp()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:getProgress()	0	float	0	1.0
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getInitDelaySecs()	0	int	0	30
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getApplicationName()	0	java.lang.String	0	Job History Server
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getEventHandler()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getClock()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getClusterInfo()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getBlacklistedNodes()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getClientToAMTokenSecretManager()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:isLastAMRetry()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.JobHistory:hasSuccessfullyUnregistered()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getNMHostname()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getTaskAttemptFinishingMonitor()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getHistoryUrl()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer$1:run()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer$2:run()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.CompletedTask:canCommit(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.CompletedTask:getProgress()	0	float	0	1.0
org.apache.hadoop.mapreduce.v2.hs.CompletedTask:isFinished()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.CompletedTaskAttempt:getProgress()	0	float	0	1.0
org.apache.hadoop.mapreduce.v2.hs.CompletedTaskAttempt:isFinished()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isMovePending()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isMovePending()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:didMoveFail()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:didMoveFail()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isDeleted()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isDeleted()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isOversized()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo:isOversized()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:isValidRequest()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:isValidRequest()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:hasAccess(org.apache.hadoop.mapreduce.v2.app.job.Job,javax.servlet.http.HttpServletRequest)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.webapp.HsView:jobsPostTableInit()	0	java.lang.String	1	dmFyIGFzSW5pdFZhbHMgPSBuZXcgQXJyYXkoKTsKJCgndGZvb3QgaW5wdXQnKS5rZXl1cCggZnVuY3Rpb24gKCkgCnsgIGpvYnNEYXRhVGFibGUuZm5GaWx0ZXIoIHRoaXMudmFsdWUsICQoJ3Rmb290IGlucHV0JykuaW5kZXgodGhpcykgKTsKfSApOwokKCd0Zm9vdCBpbnB1dCcpLmVhY2goIGZ1bmN0aW9uIChpKSB7CiAgYXNJbml0VmFsc1tpXSA9IHRoaXMudmFsdWU7Cn0gKTsKJCgndGZvb3QgaW5wdXQnKS5mb2N1cyggZnVuY3Rpb24gKCkgewogIGlmICggdGhpcy5jbGFzc05hbWUgPT0gJ3NlYXJjaF9pbml0JyApCiAgewogICAgdGhpcy5jbGFzc05hbWUgPSAnJzsKICAgIHRoaXMudmFsdWUgPSAnJzsKICB9Cn0gKTsKJCgndGZvb3QgaW5wdXQnKS5ibHVyKCBmdW5jdGlvbiAoaSkgewogIGlmICggdGhpcy52YWx1ZSA9PSAnJyApCiAgewogICAgdGhpcy5jbGFzc05hbWUgPSAnc2VhcmNoX2luaXQnOwogICAgdGhpcy52YWx1ZSA9IGFzSW5pdFZhbHNbJCgndGZvb3QgaW5wdXQnKS5pbmRleCh0aGlzKV07CiAgfQp9ICk7Cg==
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTaskPage$AttemptsBlock:isValidRequest()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTaskPage$AttemptsBlock:isValidRequest()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.webapp.HsConfPage:confPostTableInit()	0	java.lang.String	1	dmFyIGNvbmZJbml0VmFscyA9IG5ldyBBcnJheSgpOwokKCd0Zm9vdCBpbnB1dCcpLmtleXVwKCBmdW5jdGlvbiAoKSAKeyAgY29uZkRhdGFUYWJsZS5mbkZpbHRlciggdGhpcy52YWx1ZSwgJCgndGZvb3QgaW5wdXQnKS5pbmRleCh0aGlzKSApOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZWFjaCggZnVuY3Rpb24gKGkpIHsKICBjb25mSW5pdFZhbHNbaV0gPSB0aGlzLnZhbHVlOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZm9jdXMoIGZ1bmN0aW9uICgpIHsKICBpZiAoIHRoaXMuY2xhc3NOYW1lID09ICdzZWFyY2hfaW5pdCcgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJyc7CiAgICB0aGlzLnZhbHVlID0gJyc7CiAgfQp9ICk7CiQoJ3Rmb290IGlucHV0JykuYmx1ciggZnVuY3Rpb24gKGkpIHsKICBpZiAoIHRoaXMudmFsdWUgPT0gJycgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJ3NlYXJjaF9pbml0JzsKICAgIHRoaXMudmFsdWUgPSBjb25mSW5pdFZhbHNbJCgndGZvb3QgaW5wdXQnKS5pbmRleCh0aGlzKV07CiAgfQp9ICk7Cg==
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTaskPage:attemptsPostTableInit()	0	java.lang.String	1	dmFyIGFzSW5pdFZhbHMgPSBuZXcgQXJyYXkoKTsKJCgndGZvb3QgaW5wdXQnKS5rZXl1cCggZnVuY3Rpb24gKCkgCnsgIGF0dGVtcHRzRGF0YVRhYmxlLmZuRmlsdGVyKCB0aGlzLnZhbHVlLCAkKCd0Zm9vdCBpbnB1dCcpLmluZGV4KHRoaXMpICk7Cn0gKTsKJCgndGZvb3QgaW5wdXQnKS5lYWNoKCBmdW5jdGlvbiAoaSkgewogIGFzSW5pdFZhbHNbaV0gPSB0aGlzLnZhbHVlOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZm9jdXMoIGZ1bmN0aW9uICgpIHsKICBpZiAoIHRoaXMuY2xhc3NOYW1lID09ICdzZWFyY2hfaW5pdCcgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJyc7CiAgICB0aGlzLnZhbHVlID0gJyc7CiAgfQp9ICk7CiQoJ3Rmb290IGlucHV0JykuYmx1ciggZnVuY3Rpb24gKGkpIHsKICBpZiAoIHRoaXMudmFsdWUgPT0gJycgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJ3NlYXJjaF9pbml0JzsKICAgIHRoaXMudmFsdWUgPSBhc0luaXRWYWxzWyQoJ3Rmb290IGlucHV0JykuaW5kZXgodGhpcyldOwogIH0KfSApOwo=
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksPage:jobsPostTableInit()	0	java.lang.String	1	dmFyIGFzSW5pdFZhbHMgPSBuZXcgQXJyYXkoKTsKJCgndGZvb3QgaW5wdXQnKS5rZXl1cCggZnVuY3Rpb24gKCkgCnsgICQoJy5kdC10YXNrcycpLmRhdGFUYWJsZSgpLmZuRmlsdGVyKCB0aGlzLnZhbHVlLCAkKCd0Zm9vdCBpbnB1dCcpLmluZGV4KHRoaXMpICk7Cn0gKTsKJCgndGZvb3QgaW5wdXQnKS5lYWNoKCBmdW5jdGlvbiAoaSkgewogIGFzSW5pdFZhbHNbaV0gPSB0aGlzLnZhbHVlOwp9ICk7CiQoJ3Rmb290IGlucHV0JykuZm9jdXMoIGZ1bmN0aW9uICgpIHsKICBpZiAoIHRoaXMuY2xhc3NOYW1lID09ICdzZWFyY2hfaW5pdCcgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJyc7CiAgICB0aGlzLnZhbHVlID0gJyc7CiAgfQp9ICk7CiQoJ3Rmb290IGlucHV0JykuYmx1ciggZnVuY3Rpb24gKGkpIHsKICBpZiAoIHRoaXMudmFsdWUgPT0gJycgKQogIHsKICAgIHRoaXMuY2xhc3NOYW1lID0gJ3NlYXJjaF9pbml0JzsKICAgIHRoaXMudmFsdWUgPSBhc0luaXRWYWxzWyQoJ3Rmb290IGlucHV0JykuaW5kZXgodGhpcyldOwogIH0KfSApOwo=
org.apache.hadoop.mapreduce.v2.hs.CompletedJob$1:compare(org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt,org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt)	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.CompletedJob$1:compare(org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt,org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt)	1	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.CompletedJob$1:compare(org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt,org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt)	2	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:isFull()	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$JobListCache:isFull()	1	int	0	0
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getProgress()	0	float	0	1.0
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getAllCounters()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getTasks()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getTasks(org.apache.hadoop.mapreduce.v2.api.records.TaskType)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getTask(org.apache.hadoop.mapreduce.v2.api.records.TaskId)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getDiagnostics()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:isUber()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getTaskAttemptCompletionEvents(int,int)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getMapAttemptCompletionEvents(int,int)	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.mapreduce.JobACL)	0	int	0	1
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getAMInfos()	0	null	0	null
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getFailedMaps()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getFailedReduces()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getKilledMaps()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.PartialJob:getKilledReduces()	0	int	0	-1
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:getGroups(java.lang.String[])	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshUserToGroupsMappings()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshSuperUserGroupsConfiguration()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshAdminAcls()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshLoadedJobCache()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshJobRetentionSettings()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:refreshLogRetentionSettings()	0	int	0	0
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.mapred.ClientCache:instantiateHistoryProxy()	0	null	0	null
org.apache.hadoop.mapred.ResourceMgrDelegate:getQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.mapred.ResourceMgrDelegate:getTaskTrackerExpiryInterval()	0	long	0	0
org.apache.hadoop.mapred.ResourceMgrDelegate:getProtocolVersion(java.lang.String,long)	0	long	0	0
org.apache.hadoop.mapred.YARNRunner:isJobInTerminalState(org.apache.hadoop.mapreduce.JobStatus)	0	int	0	1
org.apache.hadoop.mapred.YARNRunner:isJobInTerminalState(org.apache.hadoop.mapreduce.JobStatus)	1	int	0	0
org.apache.hadoop.mapred.ClientServiceDelegate:killTask(org.apache.hadoop.mapreduce.TaskAttemptID,boolean)	0	int	0	1
org.apache.hadoop.mapred.ClientServiceDelegate:killJob(org.apache.hadoop.mapreduce.JobID)	0	int	0	1
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:length()	0	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:remaining()	0	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:position()	0	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:position(int)	0	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:capacity()	0	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.InputBuffer:array()	0	null	0	null
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataReader:readBoolean()	0	int	0	1
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataReader:readBoolean()	1	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataReader:hasUnReadData()	0	int	0	1
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataReader:hasUnReadData()	1	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataWriter:shortOfSpace(int)	0	int	0	1
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataWriter:shortOfSpace(int)	1	int	0	0
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataWriter:hasUnFlushedData()	0	int	0	1
org.apache.hadoop.mapred.nativetask.buffer.ByteBufferDataWriter:hasUnFlushedData()	1	int	0	0
org.apache.hadoop.mapred.nativetask.Command:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.nativetask.Command:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.nativetask.serde.NullWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	0
org.apache.hadoop.mapred.nativetask.serde.ByteWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapred.nativetask.serde.KVSerializer:deserializeKV(org.apache.hadoop.mapred.nativetask.buffer.DataInputStream,org.apache.hadoop.mapred.nativetask.util.SizedWritable,org.apache.hadoop.mapred.nativetask.util.SizedWritable)	0	int	0	0
org.apache.hadoop.mapred.nativetask.serde.IntWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	4
org.apache.hadoop.mapred.nativetask.serde.LongWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	8
org.apache.hadoop.mapred.nativetask.serde.NativeSerialization:getSerializer(java.lang.Class)	0	null	0	null
org.apache.hadoop.mapred.nativetask.serde.BoolWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	1
org.apache.hadoop.mapred.nativetask.serde.FloatWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	4
org.apache.hadoop.mapred.nativetask.serde.DoubleWritableSerializer:getLength(org.apache.hadoop.io.Writable)	0	int	0	8
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:collect(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:collect(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	1	int	0	1
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:write(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:write(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	1	int	0	1
org.apache.hadoop.mapred.nativetask.handlers.CombinerHandler:create(org.apache.hadoop.mapred.nativetask.TaskContext)	0	null	0	null
org.apache.hadoop.mapred.nativetask.handlers.CombinerHandler:onCall(org.apache.hadoop.mapred.nativetask.Command,org.apache.hadoop.mapred.nativetask.util.ReadWriteBuffer)	0	null	0	null
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:next()	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:nextKeyValue(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:nextKeyValue(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)	1	int	0	1
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:receiveData()	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:receiveData()	1	int	0	1
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:getProgress()	0	null	0	null
org.apache.hadoop.mapred.nativetask.handlers.BufferPullee:load()	0	int	0	0
org.apache.hadoop.mapred.nativetask.handlers.NativeCollectorOnlyHandler:onCall(org.apache.hadoop.mapred.nativetask.Command,org.apache.hadoop.mapred.nativetask.util.ReadWriteBuffer)	0	null	0	null
org.apache.hadoop.mapred.nativetask.Platforms:support(java.lang.String,org.apache.hadoop.mapred.nativetask.serde.INativeSerializer,org.apache.hadoop.mapred.JobConf)	0	int	0	1
org.apache.hadoop.mapred.nativetask.Platforms:support(java.lang.String,org.apache.hadoop.mapred.nativetask.serde.INativeSerializer,org.apache.hadoop.mapred.JobConf)	1	int	0	0
org.apache.hadoop.mapred.nativetask.Platforms:define(java.lang.Class)	0	int	0	1
org.apache.hadoop.mapred.nativetask.Platforms:define(java.lang.Class)	1	int	0	0
org.apache.hadoop.mapred.nativetask.HadoopPlatform:support(java.lang.String,org.apache.hadoop.mapred.nativetask.serde.INativeSerializer,org.apache.hadoop.mapred.JobConf)	0	int	0	1
org.apache.hadoop.mapred.nativetask.HadoopPlatform:support(java.lang.String,org.apache.hadoop.mapred.nativetask.serde.INativeSerializer,org.apache.hadoop.mapred.JobConf)	1	int	0	0
org.apache.hadoop.mapred.nativetask.HadoopPlatform:define(java.lang.Class)	0	int	0	0
org.apache.hadoop.mapred.nativetask.HadoopPlatform:name()	0	java.lang.String	0	Hadoop
org.apache.hadoop.mapred.nativetask.util.BytesUtil:toStringBinary(byte[])	0	java.lang.String	0	null
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:hasUser()	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:hasUser()	1	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:hasJobToken()	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:hasJobToken()	1	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:hasJobToken()	0	int	0	1
org.apache.hadoop.mapred.proto.ShuffleHandlerRecoveryProtos$JobShuffleInfoProto$Builder:hasJobToken()	1	int	0	0
org.apache.hadoop.mapred.FadvisedFileRegion:customShuffleTransfer(java.nio.channels.WritableByteChannel,long)	0	long	0	0
org.apache.hadoop.mapred.ShuffleHandler$AttemptPathIdentifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.ShuffleHandler$AttemptPathIdentifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:splitMaps(java.util.List)	0	null	0	null
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:sendMap(org.apache.hadoop.mapred.ShuffleHandler$ReduceContext)	0	null	0	null
org.apache.hadoop.mapred.uploader.FrameworkUploader:checkSymlink(java.io.File)	0	int	0	1
org.apache.hadoop.mapred.uploader.FrameworkUploader:checkSymlink(java.io.File)	1	int	0	0
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseArguments(java.lang.String[])	0	int	0	0
org.apache.hadoop.mapred.uploader.FrameworkUploader:parseArguments(java.lang.String[])	1	int	0	1
org.apache.hadoop.examples.MultiFileWordCount$CombineFileLineRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.examples.MultiFileWordCount$CombineFileLineRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.examples.MultiFileWordCount$CombineFileLineRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.examples.SecondarySort$IntPair:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.SecondarySort$IntPair:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.SecondarySort$IntPair:compareTo(org.apache.hadoop.examples.SecondarySort$IntPair)	0	int	0	-1
org.apache.hadoop.examples.SecondarySort$IntPair:compareTo(org.apache.hadoop.examples.SecondarySort$IntPair)	1	int	0	1
org.apache.hadoop.examples.SecondarySort$IntPair:compareTo(org.apache.hadoop.examples.SecondarySort$IntPair)	2	int	0	0
org.apache.hadoop.examples.WordMean:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.examples.WordMean:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.examples.MultiFileWordCount$WordOffset:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.MultiFileWordCount$WordOffset:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.MultiFileWordCount$WordOffset:hashCode()	0	int	0	42
org.apache.hadoop.examples.dancing.DistributedPentomino:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.dancing.Pentomino:isSide(int,int,int)	0	int	0	1
org.apache.hadoop.examples.dancing.Pentomino:isSide(int,int,int)	1	int	0	0
org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.examples.terasort.TeraSort:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.terasort.TeraSort:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.examples.terasort.TeraGen$RangeInputFormat$RangeInputSplit:getLength()	0	long	0	0
org.apache.hadoop.examples.terasort.TeraGen:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.terasort.TeraGen:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.terasort.TeraGen:run(java.lang.String[])	2	int	0	1
org.apache.hadoop.examples.terasort.Unsigned16:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.terasort.Unsigned16:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.terasort.Unsigned16:getByte(int)	0	int	0	0
org.apache.hadoop.examples.terasort.TeraChecksum:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.terasort.TeraChecksum:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.terasort.TeraChecksum:run(java.lang.String[])	2	int	0	1
org.apache.hadoop.examples.terasort.TeraGen$RangeInputFormat$RangeRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.examples.terasort.TeraGen$RangeInputFormat$RangeRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.examples.terasort.TeraValidate:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.examples.terasort.TeraValidate:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.RandomWriter$RandomInputFormat$RandomRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.examples.RandomWriter$RandomInputFormat$RandomRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.examples.RandomWriter$RandomInputFormat$RandomRecordReader:getProgress()	0	float	0	0.0
org.apache.hadoop.examples.WordStandardDeviation:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.examples.WordStandardDeviation:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.examples.SecondarySort$FirstGroupingComparator:compare(org.apache.hadoop.examples.SecondarySort$IntPair,org.apache.hadoop.examples.SecondarySort$IntPair)	0	int	0	0
org.apache.hadoop.examples.SecondarySort$FirstGroupingComparator:compare(org.apache.hadoop.examples.SecondarySort$IntPair,org.apache.hadoop.examples.SecondarySort$IntPair)	1	int	0	-1
org.apache.hadoop.examples.SecondarySort$FirstGroupingComparator:compare(org.apache.hadoop.examples.SecondarySort$IntPair,org.apache.hadoop.examples.SecondarySort$IntPair)	2	int	0	1
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpReducer$1:hasNext()	0	int	0	1
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpReducer$1:hasNext()	1	int	0	0
org.apache.hadoop.examples.RandomWriter:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.RandomWriter:run(java.lang.String[])	1	int	0	-2
org.apache.hadoop.examples.Sort:printUsage()	0	int	0	2
org.apache.hadoop.examples.BaileyBorweinPlouffe:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.examples.BaileyBorweinPlouffe:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.Join:printUsage()	0	int	0	2
org.apache.hadoop.examples.QuasiMonteCarlo:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.QuasiMonteCarlo:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.MultiFileWordCount:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.MultiFileWordCount:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.MultiFileWordCount:run(java.lang.String[])	2	int	0	1
org.apache.hadoop.examples.RandomTextWriter:printUsage()	0	int	0	2
org.apache.hadoop.examples.RandomTextWriter:run(java.lang.String[])	0	int	0	-2
org.apache.hadoop.examples.WordMedian:readAndFindMedian(java.lang.String,int,int,org.apache.hadoop.conf.Configuration)	0	double	0	-1.0
org.apache.hadoop.examples.WordMedian:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.examples.WordMedian:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.examples.Grep:run(java.lang.String[])	0	int	0	2
org.apache.hadoop.examples.Grep:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.examples.pi.DistSum:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.examples.pi.DistSum:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.examples.pi.DistSum$Machine$AbstractInputFormat$1:nextKeyValue()	0	int	0	1
org.apache.hadoop.examples.pi.DistSum$Machine$AbstractInputFormat$1:nextKeyValue()	1	int	0	0
org.apache.hadoop.examples.pi.DistSum$Machine$AbstractInputFormat$1:getProgress()	0	float	0	1.0
org.apache.hadoop.examples.pi.DistSum$Machine$AbstractInputFormat$1:getProgress()	1	float	0	0.0
org.apache.hadoop.examples.pi.DistSum$Machine$SummationSplit:getLength()	0	long	0	1
org.apache.hadoop.examples.pi.Util:printUsage(java.lang.String[],java.lang.String)	0	int	0	-1
org.apache.hadoop.examples.pi.Util:createNonexistingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.examples.pi.Util:createNonexistingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.examples.pi.SummationWritable:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.pi.SummationWritable:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.pi.math.Modular:modInverse(long,long)	0	long	0	1
org.apache.hadoop.examples.pi.math.Summation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.pi.math.Summation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.pi.math.Summation:contains(org.apache.hadoop.examples.pi.math.Summation)	0	int	0	1
org.apache.hadoop.examples.pi.math.Summation:contains(org.apache.hadoop.examples.pi.math.Summation)	1	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Sum$1:hasNext()	0	int	0	1
org.apache.hadoop.examples.pi.math.Bellard$Sum$1:hasNext()	1	int	0	0
org.apache.hadoop.examples.pi.math.ArithmeticProgression:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.pi.math.ArithmeticProgression:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.pi.math.ArithmeticProgression:compareTo(org.apache.hadoop.examples.pi.math.ArithmeticProgression)	0	int	0	1
org.apache.hadoop.examples.pi.math.ArithmeticProgression:compareTo(org.apache.hadoop.examples.pi.math.ArithmeticProgression)	1	int	0	0
org.apache.hadoop.examples.pi.math.ArithmeticProgression:compareTo(org.apache.hadoop.examples.pi.math.ArithmeticProgression)	2	int	0	-1
org.apache.hadoop.examples.pi.math.ArithmeticProgression:contains(org.apache.hadoop.examples.pi.math.ArithmeticProgression)	0	int	0	1
org.apache.hadoop.examples.pi.math.ArithmeticProgression:contains(org.apache.hadoop.examples.pi.math.ArithmeticProgression)	1	int	0	0
org.apache.hadoop.examples.pi.DistBbp:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.examples.pi.TaskResult:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.examples.pi.TaskResult:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.examples.pi.TaskResult:combine(org.apache.hadoop.examples.pi.TaskResult)	0	null	0	null
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpInputFormat$1:nextKeyValue()	0	int	0	1
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpInputFormat$1:nextKeyValue()	1	int	0	0
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpInputFormat$1:getProgress()	0	float	0	1.0
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpInputFormat$1:getProgress()	1	float	0	0.0
org.apache.hadoop.maven.plugin.cmakebuilder.TestMojo:isTruthy(java.lang.String)	0	int	0	0
org.apache.hadoop.maven.plugin.cmakebuilder.TestMojo:isTruthy(java.lang.String)	1	int	0	1
org.apache.hadoop.maven.plugin.cmakebuilder.TestMojo:shouldRunTest()	0	int	0	0
org.apache.hadoop.maven.plugin.cmakebuilder.TestMojo:shouldRunTest()	1	int	0	1
org.apache.hadoop.maven.plugin.protoc.ProtocRunner$ChecksumComparator:hasDirectoryChanged(java.io.File)	0	int	0	0
org.apache.hadoop.maven.plugin.protoc.ProtocRunner$ChecksumComparator:hasFileChanged(java.io.File)	0	int	0	1
org.apache.hadoop.maven.plugin.protoc.ProtocRunner$ChecksumComparator:hasFileChanged(java.io.File)	1	int	0	0
org.apache.hadoop.maven.plugin.resourcegz.ResourceGzMojo:lambda$execute$0(java.util.List,java.nio.file.Path)	0	int	0	1
org.apache.hadoop.maven.plugin.resourcegz.ResourceGzMojo:lambda$execute$0(java.util.List,java.nio.file.Path)	1	int	0	0
org.apache.hadoop.maven.plugin.shade.resource.ServicesResourceTransformer:canTransformResource(java.lang.String)	0	int	0	1
org.apache.hadoop.maven.plugin.shade.resource.ServicesResourceTransformer:canTransformResource(java.lang.String)	1	int	0	0
org.apache.hadoop.maven.plugin.shade.resource.ServicesResourceTransformer:hasTransformedResource()	0	int	0	1
org.apache.hadoop.maven.plugin.shade.resource.ServicesResourceTransformer:hasTransformedResource()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:singleCopy(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:multipartCopy(java.lang.String,long,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:multipartCopy(java.lang.String,long,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$1:hasNext()	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$1:hasNext()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:innerDelete(org.apache.hadoop.fs.FileStatus,boolean)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rejectRootDirectoryDelete(boolean,boolean)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rejectRootDirectoryDelete(boolean,boolean)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getScheme()	0	java.lang.String	0	oss
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getCanonicalServiceName()	0	null	0	null
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:mkdir(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,com.aliyun.oss.model.OSSObjectSummary)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,com.aliyun.oss.model.OSSObjectSummary)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem$2:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem$2:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.OSS:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptAllButSelf:accept(org.apache.hadoop.fs.Path,com.aliyun.oss.model.OSSObjectSummary)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptAllButSelf:accept(org.apache.hadoop.fs.Path,com.aliyun.oss.model.OSSObjectSummary)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptAllButSelf:accept(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.FileStatusAcceptor$AcceptAllButSelf:accept(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$PartNumberAscendComparator:compare(com.aliyun.oss.model.PartETag,com.aliyun.oss.model.PartETag)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$PartNumberAscendComparator:compare(com.aliyun.oss.model.PartETag,com.aliyun.oss.model.PartETag)	1	int	0	-1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:hasNext()	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:hasNext()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:requestNextBatch()	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:requestNextBatch()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore$2:continueListStatus()	0	int	0	0
org.apache.hadoop.fs.aliyun.oss.OSSListResult:isV1()	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.OSSListResult:isV1()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.OSSListRequest:isV1()	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.OSSListRequest:isV1()	1	int	0	0
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:getValueWithKey(org.apache.hadoop.conf.Configuration,java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:objectRepresentsDirectory(java.lang.String,long)	0	int	0	1
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:objectRepresentsDirectory(java.lang.String,long)	1	int	0	0
org.apache.hadoop.tools.HadoopArchiveLogs$AppInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tools.HadoopArchiveLogs$AppInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tools.HadoopArchiveLogs:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.HadoopArchiveLogs:prepareWorkingDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.HadoopArchiveLogs:prepareWorkingDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal()	0	int	0	-1
org.apache.hadoop.tools.HadoopArchiveLogsRunner:runInternal()	1	int	0	0
org.apache.hadoop.tools.HadoopArchiveLogsRunner$2:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.HadoopArchiveLogsRunner$2:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.tools.HadoopArchives$HarEntry:isDir()	0	int	0	1
org.apache.hadoop.tools.HadoopArchives$HarEntry:isDir()	1	int	0	0
org.apache.hadoop.tools.HadoopArchives:checkValidName(java.lang.String)	0	int	0	0
org.apache.hadoop.tools.HadoopArchives:checkValidName(java.lang.String)	1	int	0	1
org.apache.hadoop.tools.HadoopArchives:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.HadoopArchives:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:hasActiveBlock()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:hasActiveBlock()	1	int	0	0
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:putObject()	0	long	0	0
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:sourceHasNext()	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:sourceHasNext()	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:requestNextBatch()	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$FileStatusListingIterator:requestNextBatch()	1	int	0	0
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:getDeltaComponent(java.util.concurrent.TimeUnit,java.lang.String)	0	long	0	0
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:getName()	0	java.lang.String	0	bucket-info
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:getUsage()	0	java.lang.String	1	YnVja2V0LWluZm8gW09QVElPTlNdIHMzYTovL0JVQ0tFVAoJcHJvdmlkZS9jaGVjayBpbmZvcm1hdGlvbiBhYm91dCBhIHNwZWNpZmljIGJ1Y2tldAoKQ29tbW9uIG9wdGlvbnM6CiAgLWF1dGggLSBSZXF1aXJlIHRoZSBTM0d1YXJkIG1vZGUgdG8gYmUgImF1dGhvcml0YXRpdmUiCiAgLW5vbmF1dGggLSBSZXF1aXJlIHRoZSBTM0d1YXJkIG1vZGUgdG8gYmUgIm5vbi1hdXRob3JpdGF0aXZlIgogIC1tYWdpYyAtIFJlcXVpcmUgdGhlIFMzIGZpbGVzeXN0ZW0gdG8gYmUgc3VwcG9ydCB0aGUgIm1hZ2ljIiBjb21taXR0ZXIKICAtZW5jcnlwdGlvbiAobm9uZSwgc3NlLXMzLCBzc2Uta21zKSAtIFJlcXVpcmUgZW5jcnlwdGlvbiBwb2xpY3kKICAtbWFya2VycyAoYXdhcmUsIGtlZXAsIGRlbGV0ZSwgYXV0aG9yaXRhdGl2ZSkgLSBkaXJlY3RvcnkgbWFya2VycyBwb2xpY3kKICAtZ3VhcmRlZCAtIFJlcXVpcmUgUzNHdWFyZC4gV2lsbCBhbHdheXMgZmFpbC4KICAtdW5ndWFyZGVkIC0gRm9yY2UgUzNHdWFyZCB0byBiZSBkaXNhYmxlZCAoYWx3YXlzIHRydWUpCg==
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:run(java.lang.String[],java.io.PrintStream)	0	int	0	0
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:getName()	0	java.lang.String	0	uploads
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:getUsage()	0	java.lang.String	1	dXBsb2FkcyBbT1BUSU9OU10gczNhOi8vQlVDS0VUWy9wYXRoXQoJbGlzdCBvciBhYm9ydCBwZW5kaW5nIG11bHRpcGFydCB1cGxvYWRzCgpDb21tb24gb3B0aW9uczoKICgtbGlzdCB8IC1leHBlY3QgPG51bS11cGxvYWRzPiB8IC1hYm9ydCkgWy12ZXJib3NlXSBbPGFnZS1vcHRpb25zPl0gWy1mb3JjZV0KCSAtIFVuZGVyIGdpdmVuIHBhdGgsIGxpc3Qgb3IgZGVsZXRlIGFsbCB1cGxvYWRzLCBvciBvbmx5IHRob3NlIApvbGRlciB0aGFuIHNwZWNpZmllZCBieSA8YWdlLW9wdGlvbnM+CjxhZ2Utb3B0aW9ucz4gYXJlIGFueSBjb21iaW5hdGlvbiBvZiB0aGUgaW50ZWdlci12YWx1ZWQgb3B0aW9uczoKCVstZGF5cyA8ZGF5cz5dIFstaG91cnMgPGhvdXJzPl0gWy1taW51dGVzIDxtaW51dGVzPl0gWy1zZWNvbmRzIDxzZWNvbmRzPl0KLWV4cGVjdCBpcyBzaW1pbGFyIHRvIGxpc3QsIGV4Y2VwdCBubyBvdXRwdXQgaXMgcHJpbnRlZCwKCWJ1dCB0aGUgZXhpdCBjb2RlIHdpbGwgYmUgYW4gZXJyb3IgaWYgdGhlIHByb3ZpZGVkIG51bWJlcgoJaXMgZGlmZmVyZW50IHRoYXQgdGhlIG51bWJlciBvZiB1cGxvYWRzIGZvdW5kIGJ5IHRoZSBjb21tYW5kLgotZm9yY2Ugb3B0aW9uIHByZXZlbnRzIHRoZSAiQXJlIHlvdSBzdXJlIiBwcm9tcHQgd2hlbgoJdXNpbmcgLWFib3J0
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:run(java.lang.String[],java.io.PrintStream)	0	int	0	0
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:olderThan(com.amazonaws.services.s3.model.MultipartUpload,long)	0	int	0	1
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:olderThan(com.amazonaws.services.s3.model.MultipartUpload,long)	1	int	0	0
org.apache.hadoop.fs.s3a.s3guard.S3Guard:checkNoS3Guard(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.fs.s3a.s3guard.S3Guard:checkNoS3Guard(java.net.URI,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.fs.s3a.s3guard.S3Guard:allowAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.Collection)	0	int	0	1
org.apache.hadoop.fs.s3a.s3guard.S3Guard:allowAuthoritative(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.S3AFileSystem,java.util.Collection)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem$CopyFromLocalCallbacksImpl:lambda$copyLocalFileFromTo$1(org.apache.hadoop.fs.Path,java.io.File)	0	null	0	null
org.apache.hadoop.fs.s3a.tools.MarkerTool:getUsage()	0	java.lang.String	1	bWFya2VycyAoLWF1ZGl0IHwgLWNsZWFuKSBbLW1pbiA8Y291bnQ+XSBbLW1heCA8Y291bnQ+XSBbLW91dCA8ZmlsZW5hbWU+XSBbLWxpbWl0IDxsaW1pdD5dIFstbm9uYXV0aF0gWy12ZXJib3NlXSA8UEFUSD4KCVZpZXcgYW5kIG1hbmlwdWxhdGUgUzMgZGlyZWN0b3J5IG1hcmtlcnMKCg==
org.apache.hadoop.fs.s3a.tools.MarkerTool:getName()	0	java.lang.String	0	markers
org.apache.hadoop.fs.s3a.tools.MarkerTool:suffix(int)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.tools.MarkerTool:suffix(int)	1	java.lang.String	0	s
org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanResult:finish()	0	int	0	0
org.apache.hadoop.fs.s3a.FailureInjectionPolicy:trueWithProbability(float)	0	int	0	1
org.apache.hadoop.fs.s3a.FailureInjectionPolicy:trueWithProbability(float)	1	int	0	0
org.apache.hadoop.fs.s3a.Invoker:lambda$maybeRetry$3(org.apache.hadoop.util.functional.InvocationRaisingIOE)	0	null	0	null
org.apache.hadoop.fs.s3a.Invoker:lambda$retry$2(org.apache.hadoop.util.functional.InvocationRaisingIOE)	0	null	0	null
org.apache.hadoop.fs.s3a.Invoker:lambda$ignoreIOExceptions$1(org.apache.hadoop.util.functional.InvocationRaisingIOE)	0	null	0	null
org.apache.hadoop.fs.s3a.Invoker:lambda$once$0(org.apache.hadoop.util.functional.InvocationRaisingIOE)	0	null	0	null
org.apache.hadoop.fs.s3a.S3AInstrumentation$OutputStreamStatistics:effectiveBandwidth()	0	double	0	0.0
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButSelfAndS3nDirs:accept(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:remainingCapacity()	0	long	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:hasData()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:hasData()	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:write(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:startUpload()	0	null	0	null
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:enterClosedState()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock:enterClosedState()	1	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:ensureCurrentBuffer()	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteObjectReader:read(java.nio.ByteBuffer,long,int)	0	int	0	-1
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:available()	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:toString()	0	java.lang.String	0	closed
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:isClosed()	0	int	0	1
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:isClosed()	1	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:ensureCurrentBuffer()	0	int	0	0
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:ensureCurrentBuffer()	1	int	0	1
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:toString()	0	java.lang.String	0	closed
org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$ObjectListingIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.s3a.select.SelectInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.s3a.select.SelectInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.s3a.select.SelectInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.s3a.select.SelectInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.s3a.select.SelectInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.s3a.select.SelectTool:getName()	0	java.lang.String	0	select
org.apache.hadoop.fs.s3a.select.SelectTool:getUsage()	0	java.lang.String	1	c2VsZWN0IFtPUFRJT05TXSBbLWxpbWl0IHJvd3NdIFstaGVhZGVyICh1c2V8bm9uZXxpZ25vcmUpXSBbLW91dCBwYXRoXSBbLWV4cGVjdGVkIHJvd3NdIFstY29tcHJlc3Npb24gKGd6aXB8YnppcDJ8bm9uZSldIFstaW5wdXRmb3JtYXQgY3N2XSBbLW91dHB1dGZvcm1hdCBjc3ZdIDxQQVRIPiA8U0VMRUNUIFFVRVJZPgoJbWFrZSBhbiBTMyBTZWxlY3QgY2FsbAoK
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[],java.io.PrintStream)	0	int	0	0
org.apache.hadoop.fs.s3a.select.SelectTool:bandwidthMBs(long,long)	0	double	0	0.0
org.apache.hadoop.fs.s3a.S3AUtils$1:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils$1:accept(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils$1:toString()	0	java.lang.String	0	HIDDEN_FILE_FILTER
org.apache.hadoop.fs.s3a.S3ListResult:isV1()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ListResult:isV1()	1	int	0	0
org.apache.hadoop.fs.s3a.S3ListResult:hasPrefixesOrObjects()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ListResult:hasPrefixesOrObjects()	1	int	0	0
org.apache.hadoop.fs.s3a.S3ListResult:representsEmptyDirectory(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3ListResult:representsEmptyDirectory(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:containsInterruptedException(java.lang.Throwable)	0	null	0	null
org.apache.hadoop.fs.s3a.S3AUtils:isThrottleException(java.lang.Exception)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils:isThrottleException(java.lang.Exception)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:isMessageTranslatableToEOF(com.amazonaws.SdkBaseException)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils:isMessageTranslatableToEOF(com.amazonaws.SdkBaseException)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:objectRepresentsDirectory(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils:objectRepresentsDirectory(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:dateToLong(java.util.Date)	0	long	0	0
org.apache.hadoop.fs.s3a.S3AUtils:setIfDefined(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils:setIfDefined(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:checkDiskBuffer(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils:checkDiskBuffer(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AUtils:ensureOutputParameterInRange(java.lang.String,long)	0	int	0	2147483647
org.apache.hadoop.fs.s3a.S3AUtils:getS3EncryptionKey(java.lang.String,org.apache.hadoop.conf.Configuration,boolean)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.S3AInstrumentation:hasMetricSystem()	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInstrumentation:hasMetricSystem()	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInstrumentation:getCounterValue(java.lang.String)	0	long	0	0
org.apache.hadoop.fs.s3a.S3AInstrumentation:lookupCounter(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.s3a.S3AInstrumentation:incrementNamedCounter(java.lang.String,long)	0	long	0	0
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$4(org.apache.hadoop.fs.s3a.Statistic)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$4(org.apache.hadoop.fs.s3a.Statistic)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$2(org.apache.hadoop.fs.s3a.Statistic)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$2(org.apache.hadoop.fs.s3a.Statistic)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$0(org.apache.hadoop.fs.s3a.Statistic)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInstrumentation:lambda$new$0(org.apache.hadoop.fs.s3a.Statistic)	1	int	0	0
org.apache.hadoop.fs.s3a.MultipartUtils$UploadIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.s3a.MultipartUtils$UploadIterator:hasNext()	1	int	0	0
org.apache.hadoop.fs.s3a.MultipartUtils$UploadIterator:requestNextBatch()	0	int	0	0
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:createEndpointConfiguration(java.lang.String,com.amazonaws.ClientConfiguration,java.lang.String)	0	null	0	null
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptAllButS3nDirs:accept(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:initialize()	0	int	0	1
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:outputImmediatelyVisible()	0	int	0	0
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:aboutToComplete(java.lang.String,java.util.List,long,org.apache.hadoop.fs.statistics.IOStatistics)	0	int	0	0
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:getName()	0	java.lang.String	0	magic
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:requiresDelayedCommitOutputInFileSystem()	0	int	0	1
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isMagicCommitPath(java.util.List)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isMagicCommitPath(java.util.List)	1	int	0	0
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isMagicFile(java.util.List)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isMagicFile(java.util.List)	1	int	0	0
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isCommitMetadataFile(java.util.List)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isCommitMetadataFile(java.util.List)	1	int	0	0
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isUnderMagicPath(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:isUnderMagicPath(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:requiresDelayedCommitOutputInFileSystem()	0	int	0	0
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:maybeSaveSummary(java.lang.String,org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.files.SuccessData,java.lang.Throwable,boolean,boolean)	0	null	0	null
org.apache.hadoop.fs.s3a.commit.PutTracker:initialize()	0	int	0	0
org.apache.hadoop.fs.s3a.commit.PutTracker:outputImmediatelyVisible()	0	int	0	1
org.apache.hadoop.fs.s3a.commit.PutTracker:aboutToComplete(java.lang.String,java.util.List,long,org.apache.hadoop.fs.statistics.IOStatistics)	0	int	0	1
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:getName()	0	java.lang.String	0	staging
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getName()	0	java.lang.String	0	directory
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getName()	0	java.lang.String	0	partitioned
org.apache.hadoop.fs.s3a.commit.files.SuccessData:joinMap(java.util.Map,java.lang.String,java.lang.String,java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.commit.files.PendingSet:size()	0	int	0	0
org.apache.hadoop.fs.s3a.commit.impl.CommitContext:toString()	0	java.lang.String	0	CommitContext{}
org.apache.hadoop.fs.s3a.commit.impl.CommitUtilsWithMR:jobIdString(org.apache.hadoop.mapreduce.JobContext)	0	java.lang.String	0	(no job ID)
org.apache.hadoop.fs.s3a.commit.impl.CommitUtilsWithMR:jobName(org.apache.hadoop.mapreduce.JobContext)	0	java.lang.String	0	(anonymous)
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations$MaybeIOE:hasException()	0	int	0	1
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations$MaybeIOE:hasException()	1	int	0	0
org.apache.hadoop.fs.s3a.CredentialInitializationException:isRetryable()	0	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockUploadData:hasFile()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockUploadData:hasFile()	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.S3ObjectSummary)	1	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptFilesOnly:accept(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.s3a.Listing$AcceptFilesOnly:accept(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.fs.s3a.Listing$AcceptFilesOnly:accept(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload:abort()	0	null	0	null
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialBinding:nullToEmptyString(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider:hasCredentials()	0	int	0	1
org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider:hasCredentials()	1	int	0	0
org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider$NoCredentials:getAWSAccessKeyId()	0	null	0	null
org.apache.hadoop.fs.s3a.auth.AbstractSessionCredentialsProvider$NoCredentials:getAWSSecretKey()	0	null	0	null
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:toString()	0	java.lang.String	0	Empty credentials
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:isEmpty()	0	int	0	1
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:isEmpty()	1	int	0	0
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:isValid(org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired)	0	int	0	0
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:isValid(org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired)	1	int	0	1
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:buildInvalidCredentialsError(org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired)	0	java.lang.String	0	 No AWS credentials
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:getAdditionalTokenIssuers()	0	null	0	null
org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding:getUserAgentField()	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets:toString()	0	java.lang.String	0	(no encryption)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:bindingName()	0	java.lang.String	0	Role
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:bindingName()	0	java.lang.String	0	Session
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:getUserAgentField()	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.auth.delegation.AbstractS3ATokenIdentifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3a.auth.delegation.AbstractS3ATokenIdentifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3a.auth.delegation.AbstractS3ATokenIdentifier:getExpiryTime()	0	long	0	0
org.apache.hadoop.fs.s3a.auth.RoleModel:pathToKey(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:streamOpened()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getCloseOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getClosed()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getAborted()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getForwardSeekOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBackwardSeekOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBytesRead()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getTotalBytesRead()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBytesSkippedOnSeek()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBytesBackwardsOnSeek()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBytesReadInClose()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getBytesDiscardedInAbort()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getOpenOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getSeekOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getReadExceptions()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getReadOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getReadFullyOperations()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getReadsIncomplete()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getPolicySetCount()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getVersionMismatches()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyInputStreamStatistics:getInputPolicy()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyCommitterStatistics:getIOStatistics()	0	null	0	null
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyBlockOutputStreamStatistics:getBytesPendingUpload()	0	long	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyBlockOutputStreamStatistics:getBlocksAllocated()	0	int	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyBlockOutputStreamStatistics:getBlocksReleased()	0	int	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyBlockOutputStreamStatistics:getBlocksActivelyAllocated()	0	int	0	0
org.apache.hadoop.fs.s3a.statistics.impl.EmptyS3AStatisticsContext$EmptyBlockOutputStreamStatistics:getBytesWritten()	0	long	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:getDefaultPort()	0	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:pathToKey(org.apache.hadoop.fs.Path)	0	java.lang.String	0	
org.apache.hadoop.fs.s3a.S3AFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteWithoutCloseCheck(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:s3Exists(org.apache.hadoop.fs.Path,java.util.Set)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:s3Exists(org.apache.hadoop.fs.Path,java.util.Set)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:getCanonicalServiceName()	0	null	0	null
org.apache.hadoop.fs.s3a.S3AFileSystem:keepDirectoryMarkers(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:keepDirectoryMarkers(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:isDirectory(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:isFile(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AFileSystem:lambda$deleteObject$15(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.s3a.S3AUtils$2:accept(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AUtils$2:toString()	0	java.lang.String	0	ACCEPT_ALL
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:markSupported()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.fs.s3a.S3ListRequest:isV1()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ListRequest:isV1()	1	int	0	0
org.apache.hadoop.fs.s3a.impl.ErrorTranslation:isUnknownBucket(com.amazonaws.AmazonServiceException)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.ErrorTranslation:isUnknownBucket(com.amazonaws.AmazonServiceException)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.ErrorTranslation:isObjectNotFound(com.amazonaws.AmazonServiceException)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.ErrorTranslation:isObjectNotFound(com.amazonaws.AmazonServiceException)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.HeaderProcessing:decodeBytes(byte[])	0	null	0	null
org.apache.hadoop.fs.s3a.impl.HeaderProcessing:lambda$cloneObjectMetadata$5(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.HeaderProcessing:lambda$cloneObjectMetadata$5(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$NoChangeDetection:getRevisionId(com.amazonaws.services.s3.model.ObjectMetadata,java.lang.String)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$NoChangeDetection:getRevisionId(org.apache.hadoop.fs.s3a.S3ObjectAttributes)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$NoChangeDetection:getRevisionId(com.amazonaws.services.s3.transfer.model.CopyResult)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$NoChangeDetection:toString()	0	java.lang.String	0	NoChangeDetection
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:keepDirectoryMarkers(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:keepDirectoryMarkers(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:lambda$static$1(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:lambda$static$0(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.s3a.impl.NetworkBinding:fixBucketRegion(java.lang.String)	0	java.lang.String	0	us-east-1
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:isSymptomOfBrokenConnection(java.lang.Exception)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:isSymptomOfBrokenConnection(java.lang.Exception)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream()	0	int	0	0
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:drainOrAbortHttpStream()	1	int	0	1
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.GetObjectRequest)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.GetObjectRequest)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.CopyObjectRequest)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.CopyObjectRequest)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.GetObjectMetadataRequest)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.ChangeTracker:maybeApplyConstraint(com.amazonaws.services.s3.model.GetObjectMetadataRequest)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploader:lambda$abort$3(org.apache.hadoop.fs.Path,java.lang.String)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:execute()	0	null	0	null
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:lambda$submitUpload$1(java.io.File,org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation$UploadEntry)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:lambda$submitCreateEmptyDir$0(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation$1:hasNext()	0	int	0	0
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation$1:hasNext()	1	int	0	1
org.apache.hadoop.fs.s3a.impl.RequestFactoryImpl$1:read()	0	int	0	-1
org.apache.hadoop.fs.s3a.impl.DeleteOperation:submitDelete(java.util.List)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.DeleteOperation:lambda$asyncDeleteAction$1(org.apache.hadoop.fs.s3a.impl.DeleteOperation$DeleteEntry)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.DeleteOperation:lambda$asyncDeleteAction$1(org.apache.hadoop.fs.s3a.impl.DeleteOperation$DeleteEntry)	1	int	0	0
org.apache.hadoop.fs.s3a.impl.DeleteOperation:lambda$submitDelete$0(java.util.List)	0	null	0	null
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:lambda$build$1(java.lang.String,int,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:lambda$build$1(java.lang.String,int,java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:getPos()	0	long	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.s3a.S3AInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.s3a.S3AInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.s3a.S3AInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:validateReadahead(java.lang.Long)	0	long	0	65536
org.apache.hadoop.fs.s3a.S3AInputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:isObjectStreamOpen()	0	int	0	1
org.apache.hadoop.fs.s3a.S3AInputStream:isObjectStreamOpen()	1	int	0	0
org.apache.hadoop.fs.s3a.S3AInputStream:lambda$populateBuffer$6(com.amazonaws.services.s3.model.S3ObjectInputStream,java.lang.Integer,byte[],java.lang.Integer,java.lang.Integer)	0	null	0	null
org.apache.hadoop.fs.s3a.audit.AWSRequestAnalyzer:isRequestNotAlwaysInSpan(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3a.audit.AWSRequestAnalyzer:isRequestNotAlwaysInSpan(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3a.audit.AWSRequestAnalyzer:isRequestMultipartIO(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3a.audit.AWSRequestAnalyzer:isRequestMultipartIO(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3a.audit.AWSRequestAnalyzer:toSafeLong(java.lang.Number)	0	long	0	0
org.apache.hadoop.fs.s3a.audit.OperationAuditor:checkAccess(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.S3AFileStatus,org.apache.hadoop.fs.permission.FsAction)	0	int	0	1
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$WrappingAuditSpan:isActive()	0	int	0	1
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$WrappingAuditSpan:isActive()	1	int	0	0
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$WrappingAuditSpan:isValidSpan()	0	int	0	1
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$WrappingAuditSpan:isValidSpan()	1	int	0	0
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:removeActiveSpanFromMap()	0	int	0	1
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:removeActiveSpanFromMap()	1	int	0	0
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:getSpanId()	0	java.lang.String	0	(auditor not yet created)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:getOperationName()	0	java.lang.String	0	AuditManagerS3A
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:isValidSpan()	0	int	0	0
org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:hasNext()	0	int	0	0
org.apache.hadoop.fs.s3a.MultipartUtils$ListingIterator:hasNext()	1	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:unlimited()	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:unlimited()	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:hasCapacity(long)	0	int	0	1
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:hasCapacity(long)	1	int	0	0
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:remainingCapacity()	0	long	0	2147483647
org.apache.hadoop.fs.s3native.S3xLoginHelper$Login:hasLogin()	0	int	0	1
org.apache.hadoop.fs.s3native.S3xLoginHelper$Login:hasLogin()	1	int	0	0
org.apache.hadoop.fs.s3native.S3xLoginHelper$Login:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.s3native.S3xLoginHelper$Login:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.s3native.S3xLoginHelper:toString(java.net.URI)	0	java.lang.String	0	(null URI)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getScheme()	0	java.lang.String	0	s3n
org.apache.hadoop.fs.adl.AdlFsInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.adl.AdlPermission:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.adl.AdlPermission:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.adl.AdlFileSystem:getScheme()	0	java.lang.String	0	adl
org.apache.hadoop.fs.adl.AdlFileSystem:getDefaultPort()	0	int	0	443
org.apache.hadoop.fs.adl.AdlFileSystem:supportsSymlinks()	0	int	0	0
org.apache.hadoop.fs.adl.AdlFileSystem:setReplication(org.apache.hadoop.fs.Path,short)	0	int	0	1
org.apache.hadoop.fs.adl.AdlFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.adl.AdlFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.adl.AdlFileSystem:getTransportScheme()	0	java.lang.String	0	https
org.apache.hadoop.fs.adl.AdlFileSystem:getDefaultBlockSize()	0	long	0	268435456
org.apache.hadoop.fs.adl.AdlFileSystem:getBlockSize(org.apache.hadoop.fs.Path)	0	long	0	268435456
org.apache.hadoop.fs.adl.AdlFileSystem:getReplication(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.adl.AdlFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.adl.Adl:getUriDefaultPort()	0	int	0	443
org.apache.hadoop.fs.azure.PageBlobOutputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azure.PageBlobOutputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azure.metrics.RollingWindowAverage:getCurrentAverage()	0	long	0	0
org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater:getRequestContentLength(java.net.HttpURLConnection)	0	long	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getScheme()	0	java.lang.String	0	wasbs
org.apache.hadoop.fs.azure.CachedSASKeyEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.azure.CachedSASKeyEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.azure.CachingAuthorizer:get(java.lang.Object)	0	null	0	null
org.apache.hadoop.fs.azure.BlobOperationDescriptor:getQueryParameter(java.net.URL,java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:makeRenamePendingFileContents()	0	java.lang.String	0	exceeded maximum rename pending file size
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending:quote(java.lang.String)	0	java.lang.String	0	""
org.apache.hadoop.fs.azure.CachedAuthorizerEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.azure.CachedAuthorizerEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:authorize(java.lang.String,java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor:executeParallel(org.apache.hadoop.fs.azure.FileMetadata[],org.apache.hadoop.fs.azure.AzureFileSystemThreadTask)	0	int	0	0
org.apache.hadoop.fs.azure.BlockBlobInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.azure.BlockBlobInputStream:available()	0	int	0	0
org.apache.hadoop.fs.azure.BlockBlobInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azure.BlockBlobInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azure.BlockBlobInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.azure.BlockBlobInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.azure.Wasbs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getContainerFromAuthority(java.net.URI)	0	java.lang.String	0	$root
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getHTTPScheme()	0	java.lang.String	0	https
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getHTTPScheme()	1	java.lang.String	0	http
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isKeyForDirectorySet(java.lang.String,java.util.Set)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isKeyForDirectorySet(java.lang.String,java.util.Set)	1	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:matchAsteriskPattern(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:matchAsteriskPattern(java.lang.String,java.lang.String)	1	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:needToStampVersion(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:needToStampVersion(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	1	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:needToCreateContainer(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:needToCreateContainer(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	1	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isOkContainerState(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	0	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isOkContainerState(org.apache.hadoop.fs.azure.AzureNativeFileSystemStore$ContainerAccessType)	1	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:getMetadataAttribute(java.util.HashMap,java.lang.String[])	0	null	0	null
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:encodeMetadataAttribute(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:decodeMetadataAttribute(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveFolderAttribute(org.apache.hadoop.fs.azure.StorageInterface$CloudBlobWrapper)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveFolderAttribute(org.apache.hadoop.fs.azure.StorageInterface$CloudBlobWrapper)	1	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isAuthenticatedAccess()	0	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:isAuthenticatedAccess()	1	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveMetadata(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:retrieveAttribute(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String,org.apache.hadoop.fs.azure.SelfRenewingLease)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:delete(java.lang.String,org.apache.hadoop.fs.azure.SelfRenewingLease)	1	int	0	0
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:explicitFileExists(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:explicitFileExists(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper$2:getName()	0	java.lang.String	0	delegation
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem$2:execute(org.apache.hadoop.fs.azure.FileMetadata)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:getScheme()	0	java.lang.String	0	wasb
org.apache.hadoop.fs.azure.NativeAzureFileSystem:newMetricsSourceName()	0	java.lang.String	0	AzureFileSystemMetrics
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isWasbScheme(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isWasbScheme(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithAuthEnabled(org.apache.hadoop.fs.Path,boolean,boolean)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithAuthEnabled(org.apache.hadoop.fs.Path,boolean,boolean)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteWithoutAuth(org.apache.hadoop.fs.Path,boolean,boolean)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isStickyBitCheckViolated(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata,boolean)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isStickyBitCheckViolated(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isStickyBitCheckViolated(org.apache.hadoop.fs.azure.FileMetadata,org.apache.hadoop.fs.azure.FileMetadata)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteFile(java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:deleteFile(java.lang.String,boolean)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:existsInternal(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:existsInternal(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:conditionalRedoFolderRename(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:conditionalRedoFolderRename(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isAllowedUser(java.lang.String,java.util.List)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem:isAllowedUser(java.lang.String,java.util.List)	1	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azure.PageBlobInputStream:getPageBlobDataSize(org.apache.hadoop.fs.azure.StorageInterface$CloudPageBlobWrapper,com.microsoft.azure.storage.OperationContext)	0	long	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.azure.PageBlobInputStream:dataAvailableInBuffer()	0	int	0	1
org.apache.hadoop.fs.azure.PageBlobInputStream:dataAvailableInBuffer()	1	int	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:ensureDataInBuffer()	0	int	0	1
org.apache.hadoop.fs.azure.PageBlobInputStream:ensureDataInBuffer()	1	int	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:read(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.azure.PageBlobInputStream:skipImpl(long)	0	long	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:skipWithinBuffer(long)	0	long	0	0
org.apache.hadoop.fs.azure.PageBlobInputStream:getBytesRemainingInCurrentPage()	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem$1:execute(org.apache.hadoop.fs.azure.FileMetadata)	0	int	0	1
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[])	2	int	0	2
org.apache.hadoop.fs.azure.WasbFsck:run(java.lang.String[])	3	int	0	0
org.apache.hadoop.fs.azure.WasbFsck:recursiveCheckChildPathName(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.fs.azure.WasbFsck:recursiveCheckChildPathName(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.fs.azure.security.SpnegoToken:isTokenValid()	0	int	0	1
org.apache.hadoop.fs.azure.security.SpnegoToken:isTokenValid()	1	int	0	0
org.apache.hadoop.fs.azure.security.WasbTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.fs.azure.security.TokenUtils:toToken(java.util.Map)	0	null	0	null
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper$1:getName()	0	java.lang.String	0	doas
org.apache.hadoop.fs.azure.Wasb:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.azure.AzureLinkedStack:isEmpty()	0	int	0	1
org.apache.hadoop.fs.azure.AzureLinkedStack:isEmpty()	1	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:isFileNotFoundException(com.microsoft.azure.storage.StorageException)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:isFileNotFoundException(com.microsoft.azure.storage.StorageException)	1	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:isBlobAlreadyExistsConflict(com.microsoft.azure.storage.StorageException)	0	int	0	1
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:isBlobAlreadyExistsConflict(com.microsoft.azure.storage.StorageException)	1	int	0	0
org.apache.hadoop.fs.azure.WasbAuthorizationOperations:toString()	0	java.lang.String	0	read
org.apache.hadoop.fs.azure.WasbAuthorizationOperations:toString()	1	java.lang.String	0	write
org.apache.hadoop.fs.azure.BlockBlobAppendStream:hasCapability(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending$1:execute(org.apache.hadoop.fs.azure.FileMetadata)	0	int	0	1
org.apache.hadoop.fs.azurebfs.Abfss:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:isTokenAboutToExpire()	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:getUserAgentSuffix()	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider:isTokenAboutToExpire()	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:isShortUserName(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:isShortUserName(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:shouldUseShortUserName(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:shouldUseShortUserName(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:getShortName(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:shouldUseFullyQualifiedUserName(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:shouldUseFullyQualifiedUserName(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:isInSubstitutionList(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:isInSubstitutionList(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:isUuid(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:isRecoverableFailure(java.io.IOException)	0	int	0	1
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:isRecoverableFailure(java.io.IOException)	1	int	0	0
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:consumeInputStream(java.io.InputStream,int)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:isSecureScheme()	0	int	0	1
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getScheme()	0	java.lang.String	0	abfss
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$2:call()	0	null	0	null
org.apache.hadoop.fs.azurebfs.Abfs:getUriDefaultPort()	0	int	0	-1
org.apache.hadoop.fs.azurebfs.contracts.exceptions.AzureBlobFileSystemException:toString()	0	java.lang.String	0	AzureBlobFileSystemException
org.apache.hadoop.fs.azurebfs.AbfsCountersImpl:lookupCounter(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getBlock(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,byte[])	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:tryEvict()	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:evict(org.apache.hadoop.fs.azurebfs.services.ReadBuffer)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:isAlreadyQueued(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:isAlreadyQueued(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:isInList(java.util.Collection,org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:isInList(java.util.Collection,org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getBlockFromCompletedQueue(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,long,int,byte[])	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getNextBlockToRead()	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:getNumBuffers()	0	int	0	16
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:lambda$purgeBuffersForStream$0(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,org.apache.hadoop.fs.azurebfs.services.ReadBuffer)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:lambda$purgeBuffersForStream$0(org.apache.hadoop.fs.azurebfs.services.AbfsInputStream,org.apache.hadoop.fs.azurebfs.services.ReadBuffer)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsListResult:isFailedListing()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsListResult:isFailedListing()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials:safeDecode(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials:safeDecode(java.lang.String)	1	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.UnknownIOExceptionRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	IOE
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ConnectionTimeoutRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	CT
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.UnknownSocketExceptionRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	SE
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ServerErrorRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	ING
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ServerErrorRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	1	java.lang.String	0	EGR
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ServerErrorRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	2	java.lang.String	0	OPR
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ServerErrorRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	3	java.lang.String	0	503
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.UnknownHostRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	UH
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ReadTimeoutRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	RT
org.apache.hadoop.fs.azurebfs.services.retryReasonCategories.ConnectionResetRetryReason:getAbbreviation(java.lang.Integer,java.lang.String)	0	java.lang.String	0	CR
org.apache.hadoop.fs.azurebfs.services.AbfsListStatusRemoteIterator:hasNext()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:getLatencyInstant()	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:getClientLatency()	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:isValidInstant(java.time.Instant)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:isValidInstant(java.time.Instant)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:isNullInputStream(java.io.InputStream)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:isNullInputStream(java.io.InputStream)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsPermission:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsPermission:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsPermission:valueOf(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azurebfs.services.AbfsPermission:isExtendedAcl(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsPermission:isExtendedAcl(java.lang.String)	1	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:hasResult()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:hasResult()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:isARetriedRequest()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:isARetriedRequest()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:executeHttpOperation(int,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	1	int	0	1
org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy:shouldRetry(int,int)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.ExponentialRetryPolicy:shouldRetry(int,int)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasCapability(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasCapability(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasActiveBlock()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasActiveBlock()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasActiveBlockDataToUpload()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasActiveBlockDataToUpload()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:isLeaseFreed()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasLease()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:hasLease()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:processAclString(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isUpnFormatAclEntries(java.util.Map)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isUpnFormatAclEntries(java.util.Map)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isExtendAcl(java.util.Map,boolean)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isExtendAcl(java.util.Map,boolean)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:containsNamedAce(java.util.Map,boolean)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:containsNamedAce(java.util.Map,boolean)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isNamedAce(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsAclHelper:isNamedAce(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(long,byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read()	0	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:shouldReadFully()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:shouldReadFully()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:shouldReadLastBlock()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:shouldReadLastBlock()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readOneBlock(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readOneBlock(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readFileCompletely(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readFileCompletely(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readLastBlock(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readLastBlock(byte[],int,int)	1	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:optimisedRead(byte[],int,int,long,long)	0	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:validate(byte[],int,int)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:validate(byte[],int,int)	1	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readRemote(long,byte[],int,int,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	0	int	0	-1
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:available()	0	int	0	2147483647
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:getPos()	0	long	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:seekToNewSource(long)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:markSupported()	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation$AbfsHttpOperationWithFixedResult:getResponseHeader(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingIntercept:updateBytesTransferred(boolean,org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingIntercept:updateBytesTransferred(boolean,org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:timerOrchestrator(org.apache.hadoop.fs.azurebfs.services.TimerFunctionality,java.util.TimerTask)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:timerOrchestrator(org.apache.hadoop.fs.azurebfs.services.TimerFunctionality,java.util.TimerTask)	1	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:suspendIfNecessary()	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:suspendIfNecessary()	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClient:checkIsDir(org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClient:checkIsDir(org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renameIdempotencyCheckOp(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	0	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClient:checkUserError(int)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClient:checkUserError(int)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSuccessCheckOp(org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,long,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	0	int	0	1
org.apache.hadoop.fs.azurebfs.services.AbfsClient:appendSuccessCheckOp(org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation,java.lang.String,long,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	1	int	0	0
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStreamContext:getLeaseId()	0	null	0	null
org.apache.hadoop.fs.azurebfs.extensions.BoundDTExtension:getCanonicalServiceName()	0	null	0	null
org.apache.hadoop.fs.azurebfs.extensions.BoundDTExtension:getUserAgentSuffix()	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.extensions.ExtensionHelper:lambda$close$0(org.apache.hadoop.fs.azurebfs.extensions.BoundDTExtension)	0	null	0	null
org.apache.hadoop.fs.azurebfs.utils.DateTimeUtils:isRecentlyModified(java.lang.String,java.time.Instant)	0	int	0	1
org.apache.hadoop.fs.azurebfs.utils.DateTimeUtils:isRecentlyModified(java.lang.String,java.time.Instant)	1	int	0	0
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:lookupForLocalUserIdentity(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:lookupForLocalGroupIdentity(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.utils.UriUtils:containsAbfsUrl(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.utils.UriUtils:extractAccountNameFromHostName(java.lang.String)	0	null	0	null
org.apache.hadoop.fs.azurebfs.utils.UriUtils:generateUniqueTestPath()	0	java.lang.String	0	/test
org.apache.hadoop.fs.azurebfs.utils.UriUtils:encodedUrlStr(java.lang.String)	0	java.lang.String	0	https%3A%2F%2Ffailed%2Fto%2Fencode%2Furl
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:isNearExpiry(java.time.OffsetDateTime,long)	0	int	0	1
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:isNearExpiry(java.time.OffsetDateTime,long)	1	int	0	0
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:get()	0	null	0	null
org.apache.hadoop.fs.azurebfs.utils.Base64:validateIsBase64String(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.utils.Base64:validateIsBase64String(java.lang.String)	1	int	0	1
org.apache.hadoop.fs.azurebfs.utils.TracingContext:validateClientCorrelationID(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:storeSupportsResilientCommit()	0	int	0	1
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:storeSupportsResilientCommit()	1	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$VersionedFileStatus:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore$VersionedFileStatus:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:isInfiniteLeaseKey(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:parseContentLength(java.lang.String)	0	long	0	-1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:parseIsDirectory(java.lang.String)	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:parseIsDirectory(java.lang.String)	1	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:isKeyForDirectorySet(java.lang.String,java.util.Set)	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:isKeyForDirectorySet(java.lang.String,java.util.Set)	1	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:maybeCreateLease(java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)	0	null	0	null
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:areLeasesFreed()	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:areLeasesFreed()	1	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$FileSystemOperation:failed()	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$FileSystemOperation:failed()	1	int	0	0
org.apache.hadoop.fs.azurebfs.security.AbfssDtFetcher:getScheme()	0	java.lang.String	0	abfss
org.apache.hadoop.fs.azurebfs.security.AbfsDtFetcher:getScheme()	0	java.lang.String	0	abfs
org.apache.hadoop.fs.azurebfs.security.AbfsTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getTokenProviderClass(org.apache.hadoop.fs.azurebfs.services.AuthType,java.lang.String,java.lang.Class,java.lang.Class)	0	null	0	null
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getCreateRemoteFileSystemDuringInitialization()	0	int	0	1
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getCreateRemoteFileSystemDuringInitialization()	1	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem$1:call()	0	null	0	null
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:isSecureScheme()	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:delete(org.apache.hadoop.fs.Path,boolean)	1	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getScheme()	0	java.lang.String	0	abfs
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)	0	null	0	null
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:deleteRoot()	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:fileSystemExists()	0	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:fileSystemExists()	1	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:isAbfsScheme(java.lang.String)	0	int	0	0
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:isAbfsScheme(java.lang.String)	1	int	0	1
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)	0	int	0	1
org.apache.hadoop.tools.RegexpInConfigurationFilter:shouldCopy(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.RegexpInConfigurationFilter:shouldCopy(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.tools.DistCpContext:splitLargeFile()	0	int	0	1
org.apache.hadoop.tools.DistCpContext:splitLargeFile()	1	int	0	0
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.tools.CopyListingFileStatus)	0	java.lang.String	0	N/A
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.tools.CopyListingFileStatus)	1	java.lang.String	0	dir
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.tools.CopyListingFileStatus)	2	java.lang.String	0	file
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.fs.FileStatus)	0	java.lang.String	0	N/A
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.fs.FileStatus)	1	java.lang.String	0	dir
org.apache.hadoop.tools.mapred.CopyMapper:getFileType(org.apache.hadoop.fs.FileStatus)	2	java.lang.String	0	file
org.apache.hadoop.tools.mapred.CopyMapper:canSkip(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.tools.mapred.CopyMapper:canSkip(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.tools.mapred.CopyMapper:maybeUseModTimeToCompare(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.tools.mapred.CopyMapper:maybeUseModTimeToCompare(org.apache.hadoop.tools.CopyListingFileStatus,org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.tools.mapred.CopyOutputFormat:getWorkingDirectory(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.tools.mapred.CopyOutputFormat:getCommitDirectory(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.tools.mapred.DeletedDirTracker:isDirectoryOrAncestorDeleted(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.mapred.DeletedDirTracker:isDirectoryOrAncestorDeleted(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.tools.mapred.DeletedDirTracker:shouldDelete(org.apache.hadoop.tools.CopyListingFileStatus)	0	int	0	1
org.apache.hadoop.tools.mapred.DeletedDirTracker:shouldDelete(org.apache.hadoop.tools.CopyListingFileStatus)	1	int	0	0
org.apache.hadoop.tools.mapred.DeletedDirTracker:isContained(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.mapred.DeletedDirTracker:isContained(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:nextKeyValue()	0	int	0	0
org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:nextKeyValue()	1	int	0	1
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplitRatio(int,int,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.tools.mapred.CopyCommitter:isFileNotFoundException(java.io.IOException)	0	int	0	1
org.apache.hadoop.tools.mapred.CopyCommitter:isFileNotFoundException(java.io.IOException)	1	int	0	0
org.apache.hadoop.tools.TrueCopyFilter:shouldCopy(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.OptionsParser:getVal(org.apache.commons.cli.CommandLine,java.lang.String)	0	null	0	null
org.apache.hadoop.tools.RegexCopyFilter:shouldCopy(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.RegexCopyFilter:shouldCopy(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.tools.DistCpOptions:getSourcePaths()	0	null	0	null
org.apache.hadoop.tools.DistCpOptions:shouldUseSnapshotDiff()	0	int	0	1
org.apache.hadoop.tools.DistCpOptions:shouldUseSnapshotDiff()	1	int	0	0
org.apache.hadoop.tools.DistCpOptions:getPreserveAttributes()	0	null	0	null
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	2	int	0	-2
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	3	int	0	-3
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	4	int	0	-4
org.apache.hadoop.tools.DistCp:run(java.lang.String[])	5	int	0	-999
org.apache.hadoop.tools.DistCpSync:preSyncCheck()	0	int	0	0
org.apache.hadoop.tools.DistCpSync:preSyncCheck()	1	int	0	1
org.apache.hadoop.tools.DistCpSync:sync()	0	int	0	0
org.apache.hadoop.tools.DistCpSync:getAllDiffs()	0	int	0	1
org.apache.hadoop.tools.DistCpSync:getAllDiffs()	1	int	0	0
org.apache.hadoop.tools.DistCpSync:getSnapshotName(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.tools.DistCpSync:checkNoChange(org.apache.hadoop.hdfs.DistributedFileSystem,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.DistCpSync:checkNoChange(org.apache.hadoop.hdfs.DistributedFileSystem,org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.tools.DistCpSync:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.DistCpSync:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.tools.DistCpSync:isParentOrSelfMarkedDeleted(org.apache.hadoop.tools.DiffInfo,java.util.List)	0	int	0	1
org.apache.hadoop.tools.DistCpSync:isParentOrSelfMarkedDeleted(org.apache.hadoop.tools.DiffInfo,java.util.List)	1	int	0	0
org.apache.hadoop.tools.DistCpSync:getTraverseExcludeList(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.tools.DiffInfo$2:compare(org.apache.hadoop.tools.DiffInfo,org.apache.hadoop.tools.DiffInfo)	0	int	0	0
org.apache.hadoop.tools.DiffInfo$2:compare(org.apache.hadoop.tools.DiffInfo,org.apache.hadoop.tools.DiffInfo)	1	int	0	-1
org.apache.hadoop.tools.DiffInfo$2:compare(org.apache.hadoop.tools.DiffInfo,org.apache.hadoop.tools.DiffInfo)	2	int	0	1
org.apache.hadoop.tools.util.ProducerConsumer:hasWork()	0	int	0	1
org.apache.hadoop.tools.util.ProducerConsumer:hasWork()	1	int	0	0
org.apache.hadoop.tools.util.ThrottledInputStream:read(byte[],int,int)	0	int	0	0
org.apache.hadoop.tools.CopyListingFileStatus:isSplit()	0	int	0	1
org.apache.hadoop.tools.CopyListingFileStatus:isSplit()	1	int	0	0
org.apache.hadoop.tools.CopyListingFileStatus:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.tools.CopyListingFileStatus:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.tools.dynamometer.blockgenerator.BlockInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.tools.dynamometer.blockgenerator.BlockInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver$NoSplitTextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.tools.dynamometer.ApplicationMaster$RMCallbackHandler:getProgress()	0	float	0	0.0
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied$BlockPlacementStatusSatisfied:isPlacementPolicySatisfied()	0	int	0	1
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied$BlockPlacementStatusSatisfied:getErrorDescription()	0	null	0	null
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied$BlockPlacementStatusSatisfied:getAdditionalReplicasRequired()	0	int	0	0
org.apache.hadoop.tools.dynamometer.ApplicationMaster:init(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.dynamometer.ApplicationMaster:init(java.lang.String[])	1	int	0	1
org.apache.hadoop.tools.dynamometer.ApplicationMaster:run()	0	int	0	0
org.apache.hadoop.tools.dynamometer.ApplicationMaster:isNameNode(org.apache.hadoop.yarn.api.records.ContainerId)	0	int	0	1
org.apache.hadoop.tools.dynamometer.ApplicationMaster:isNameNode(org.apache.hadoop.yarn.api.records.ContainerId)	1	int	0	0
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[])	1	int	0	2
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[])	2	int	0	-1
org.apache.hadoop.tools.dynamometer.Client:run(java.lang.String[])	3	int	0	1
org.apache.hadoop.tools.dynamometer.Client:init(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.dynamometer.Client:init(java.lang.String[])	1	int	0	1
org.apache.hadoop.tools.dynamometer.Client:isCompleted(org.apache.hadoop.mapreduce.JobStatus$State)	0	int	0	1
org.apache.hadoop.tools.dynamometer.Client:isCompleted(org.apache.hadoop.mapreduce.JobStatus$State)	1	int	0	0
org.apache.hadoop.tools.dynamometer.SimulatedDataNodes:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.dynamometer.SimulatedDataNodes:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.tools.dynamometer.SimulatedDataNodes:lambda$run$0(org.apache.hadoop.hdfs.server.datanode.DataNode)	0	int	0	1
org.apache.hadoop.tools.dynamometer.SimulatedDataNodes:lambda$run$0(org.apache.hadoop.hdfs.server.datanode.DataNode)	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualRecordReader:nextKeyValue()	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualRecordReader:nextKeyValue()	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:getDescription()	0	java.lang.String	0	This mapper creates 1-byte files for the specified duration.
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:verifyConfigurations(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:verifyConfigurations(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:getSimpleUgi()	0	java.lang.String	0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:isPoison()	0	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.UserCommandKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.UserCommandKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand$PoisonPillCommand:isPoison()	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:getDescription()	0	java.lang.String	0	This mapper replays audit log files.
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:verifyConfigurations(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:verifyConfigurations(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:replayLog(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand)	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:replayLog(org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand)	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadDriver:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadDriver:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputSplit:getLength()	0	long	0	0
org.apache.hadoop.tools.DistCh:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.tools.DistCh:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.DistCh:run(java.lang.String[])	2	int	0	-2
org.apache.hadoop.tools.DistCh:setup(java.util.List,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.tools.DistCh:setup(java.util.List,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.tools.DistCh$FileOperation:isDifferent(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.tools.DistCh$FileOperation:isDifferent(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.TreePath:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.TreePath:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.hdfs.server.namenode.FixedBlockResolver:getReplication(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ImageWriter$DirEntryCache:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.ImageWriter$DirEntryCache:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator:hasNext()	0	int	0	1
org.apache.hadoop.hdfs.server.namenode.TreeWalk$TreeIterator:hasNext()	1	int	0	0
org.apache.hadoop.hdfs.server.namenode.NullBlockAliasMap$1$1:hasNext()	0	int	0	0
org.apache.hadoop.hdfs.server.namenode.FileSystemImage:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.hdfs.server.namenode.FileSystemImage:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.mapred.gridmix.SubmitterUserResolver:setTargetUsers(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.mapred.gridmix.SubmitterUserResolver:needsTargetUsersList()	0	int	0	0
org.apache.hadoop.mapred.gridmix.RandomAlgorithms$Selector:next()	0	int	0	-1
org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc$1:compare(org.apache.hadoop.mapred.gridmix.FilePool$Node,org.apache.hadoop.mapred.gridmix.FilePool$Node)	0	int	0	-1
org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc$1:compare(org.apache.hadoop.mapred.gridmix.FilePool$Node,org.apache.hadoop.mapred.gridmix.FilePool$Node)	1	int	0	1
org.apache.hadoop.mapred.gridmix.FilePool$InnerDesc$1:compare(org.apache.hadoop.mapred.gridmix.FilePool$Node,org.apache.hadoop.mapred.gridmix.FilePool$Node)	2	int	0	0
org.apache.hadoop.mapred.gridmix.JobCreator$1:canEmulateDistCacheLoad()	0	int	0	1
org.apache.hadoop.mapred.gridmix.ReadRecordFactory:next(org.apache.hadoop.mapred.gridmix.GridmixKey,org.apache.hadoop.mapred.gridmix.GridmixRecord)	0	int	0	0
org.apache.hadoop.mapred.gridmix.ReadRecordFactory:next(org.apache.hadoop.mapred.gridmix.GridmixKey,org.apache.hadoop.mapred.gridmix.GridmixRecord)	1	int	0	1
org.apache.hadoop.mapred.gridmix.AvgRecordFactory:next(org.apache.hadoop.mapred.gridmix.GridmixKey,org.apache.hadoop.mapred.gridmix.GridmixRecord)	0	int	0	0
org.apache.hadoop.mapred.gridmix.AvgRecordFactory:next(org.apache.hadoop.mapred.gridmix.GridmixKey,org.apache.hadoop.mapred.gridmix.GridmixRecord)	1	int	0	1
org.apache.hadoop.mapred.gridmix.SleepJob:canEmulateCompression()	0	int	0	0
org.apache.hadoop.mapred.gridmix.PseudoLocalFs$RandomInputStream:read()	0	int	0	-1
org.apache.hadoop.mapred.gridmix.PseudoLocalFs$RandomInputStream:read(byte[],int,int)	0	int	0	-1
org.apache.hadoop.mapred.gridmix.GridmixJob:checkMemoryUpperLimits(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,boolean)	0	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixJob:checkMemoryUpperLimits(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,boolean)	1	int	0	0
org.apache.hadoop.mapred.gridmix.GridmixJob:compareTo(java.util.concurrent.Delayed)	0	int	0	0
org.apache.hadoop.mapred.gridmix.GridmixJob:compareTo(java.util.concurrent.Delayed)	1	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixJob:compareTo(java.util.concurrent.Delayed)	2	int	0	-1
org.apache.hadoop.mapred.gridmix.GridmixJob:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixJob:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.gridmix.EchoUserResolver:setTargetUsers(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.mapred.gridmix.EchoUserResolver:needsTargetUsersList()	0	int	0	0
org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory:next(org.apache.hadoop.mapred.gridmix.GridmixKey,org.apache.hadoop.mapred.gridmix.GridmixRecord)	0	int	0	1
org.apache.hadoop.mapred.gridmix.JobFactory:getNextJobFiltered()	0	null	0	null
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData:canEmulateCompression()	0	int	0	0
org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver:setTargetUsers(java.net.URI,org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver:needsTargetUsersList()	0	int	0	1
org.apache.hadoop.mapred.gridmix.GenerateData:canEmulateCompression()	0	int	0	0
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:exists(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:exists(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter:done()	0	int	0	1
org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter:done()	1	int	0	0
org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter:accept(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.mapred.gridmix.FilePool$MinFileFilter:accept(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.mapred.gridmix.LoadJob:canEmulateCompression()	0	int	0	1
org.apache.hadoop.mapred.gridmix.ExecutionSummarizer:stringifyDataStatistics(org.apache.hadoop.mapred.gridmix.GenerateData$DataStatistics)	0	java.lang.String	0	N/A
org.apache.hadoop.mapred.gridmix.FileQueue:read()	0	int	0	-1
org.apache.hadoop.mapred.gridmix.GridmixRecord:fixedBytes()	0	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixRecord:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixRecord:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher:getProgress()	0	float	0	1.0
org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin:getProgress()	0	float	0	1.0
org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin:getProgress()	0	float	0	1.0
org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapred.gridmix.SleepJob$SleepInputFormat$1:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path)	0	int	0	2
org.apache.hadoop.mapred.gridmix.Gridmix:writeInputData(long,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])	0	int	0	1
org.apache.hadoop.mapred.gridmix.Gridmix:runJob(org.apache.hadoop.conf.Configuration,java.lang.String[])	1	int	0	2
org.apache.hadoop.mapred.gridmix.GridmixKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.mapred.gridmix.GridmixKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat$1:nextKeyValue()	0	int	0	1
org.apache.hadoop.mapred.gridmix.GenerateData$GenDataFormat$1:nextKeyValue()	1	int	0	0
org.apache.hadoop.mapred.gridmix.JobCreator$2:canEmulateDistCacheLoad()	0	int	0	0
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:isLocalDistCacheFile(java.lang.String,java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:isLocalDistCacheFile(java.lang.String,java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:writeDistCacheFilesList()	0	int	0	3
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:writeDistCacheFilesList()	1	int	0	0
org.apache.hadoop.resourceestimator.common.api.RecurrenceId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.resourceestimator.common.api.RecurrenceId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.resourceestimator.translator.validator.ParserValidator:validate(java.io.InputStream)	0	int	0	1
org.apache.hadoop.tools.rumen.Node:hasChildren()	0	int	0	1
org.apache.hadoop.tools.rumen.Node:hasChildren()	1	int	0	0
org.apache.hadoop.tools.rumen.Node:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.tools.rumen.Node:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.tools.rumen.JobHistoryUtils:applyParser(java.lang.String,java.util.regex.Pattern)	0	null	0	null
org.apache.hadoop.tools.rumen.JobHistoryUtils:isJobConfXml(java.lang.String)	0	int	0	1
org.apache.hadoop.tools.rumen.JobHistoryUtils:isJobConfXml(java.lang.String)	1	int	0	0
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobInitedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Anonymizer:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.tools.rumen.Anonymizer:run()	0	int	0	0
org.apache.hadoop.tools.rumen.Anonymizer:run()	1	int	0	-1
org.apache.hadoop.tools.rumen.state.StatePool:isUpdated()	0	int	0	1
org.apache.hadoop.tools.rumen.state.StatePool:reloadState(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.tools.rumen.state.StatePool:getVersion()	0	long	0	1
org.apache.hadoop.tools.rumen.TaskAttempt20LineEventEmitter$TaskAttemptFinishedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobPriorityChangeEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.TaskAttempt20LineEventEmitter$TaskAttemptUnsuccessfulCompletionEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:needsAnonymization(java.lang.String)	0	int	0	0
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:needsAnonymization(java.lang.String)	1	int	0	1
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:hasSuffix(java.lang.String,java.lang.String[])	0	int	0	1
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:hasSuffix(java.lang.String,java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:isKnownData(java.lang.String,java.lang.String[])	0	int	0	1
org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility:isKnownData(java.lang.String,java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.rumen.HistoryEventEmitter:parseCounters(java.lang.String)	0	null	0	null
org.apache.hadoop.tools.rumen.Folder$1JobEntryComparator:compare(org.apache.hadoop.tools.rumen.Pair,org.apache.hadoop.tools.rumen.Pair)	0	int	0	-1
org.apache.hadoop.tools.rumen.Folder$1JobEntryComparator:compare(org.apache.hadoop.tools.rumen.Pair,org.apache.hadoop.tools.rumen.Pair)	1	int	0	0
org.apache.hadoop.tools.rumen.Folder$1JobEntryComparator:compare(org.apache.hadoop.tools.rumen.Pair,org.apache.hadoop.tools.rumen.Pair)	2	int	0	1
org.apache.hadoop.tools.rumen.Folder:initialize(java.lang.String[])	0	int	0	2
org.apache.hadoop.tools.rumen.Folder:initialize(java.lang.String[])	1	int	0	0
org.apache.hadoop.tools.rumen.Folder:initialize(java.lang.String[])	2	int	0	1
org.apache.hadoop.tools.rumen.Folder:run()	0	int	0	0
org.apache.hadoop.tools.rumen.ReduceAttempt20LineHistoryEventEmitter$ReduceAttemptFinishedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.JobBuilder:extractMegabytes(java.util.Properties,java.lang.String[])	0	null	0	null
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:initializeHadoopLogsAnalyzer(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:setNextDirectoryInputStream()	0	int	0	0
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:setNextDirectoryInputStream()	1	int	0	1
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:readInputLine()	0	null	0	null
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:readBalancedLine()	0	null	0	null
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:parseCounter(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:run()	0	int	0	0
org.apache.hadoop.tools.rumen.ParsedHost:numberOfDistances()	0	int	0	3
org.apache.hadoop.tools.rumen.ParsedHost:parse(java.lang.String)	0	null	0	null
org.apache.hadoop.tools.rumen.ParsedHost:process(java.lang.String)	0	null	0	null
org.apache.hadoop.tools.rumen.ParsedHost:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.tools.rumen.ParsedHost:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.tools.rumen.ParsedHost:distance(org.apache.hadoop.tools.rumen.ParsedHost)	0	int	0	0
org.apache.hadoop.tools.rumen.ParsedHost:distance(org.apache.hadoop.tools.rumen.ParsedHost)	1	int	0	1
org.apache.hadoop.tools.rumen.ParsedHost:distance(org.apache.hadoop.tools.rumen.ParsedHost)	2	int	0	2
org.apache.hadoop.tools.rumen.datatypes.ClassName:getPrefix()	0	java.lang.String	0	class
org.apache.hadoop.tools.rumen.datatypes.ClassName:needsAnonymization(org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.tools.rumen.datatypes.ClassName:needsAnonymization(org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.tools.rumen.datatypes.NodeName$NodeNameState:isUpdated()	0	int	0	1
org.apache.hadoop.tools.rumen.datatypes.NodeName$NodeNameState:isUpdated()	1	int	0	0
org.apache.hadoop.tools.rumen.datatypes.NodeName$NodeNameState:getName()	0	java.lang.String	0	node
org.apache.hadoop.tools.rumen.datatypes.JobName:getPrefix()	0	java.lang.String	0	job
org.apache.hadoop.tools.rumen.datatypes.FileName:anonymize(java.lang.String,org.apache.hadoop.tools.rumen.anonymization.WordList)	0	null	0	null
org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType:getPrefix()	0	java.lang.String	0	data
org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType:needsAnonymization(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType:anonymize(java.lang.String,org.apache.hadoop.tools.rumen.anonymization.WordList)	0	null	0	null
org.apache.hadoop.tools.rumen.datatypes.UserName:getPrefix()	0	java.lang.String	0	user
org.apache.hadoop.tools.rumen.datatypes.QueueName:getPrefix()	0	java.lang.String	0	queue
org.apache.hadoop.tools.rumen.datatypes.NodeName:anonymize(java.lang.String,org.apache.hadoop.tools.rumen.anonymization.WordList)	0	null	0	null
org.apache.hadoop.tools.rumen.datatypes.FileName$FileNameState:isUpdated()	0	int	0	1
org.apache.hadoop.tools.rumen.datatypes.FileName$FileNameState:isUpdated()	1	int	0	0
org.apache.hadoop.tools.rumen.datatypes.FileName$FileNameState:getName()	0	java.lang.String	0	path
org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser:accept(java.lang.String)	0	int	0	1
org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser:accept(java.lang.String)	1	int	0	0
org.apache.hadoop.tools.rumen.ZombieJob:getName()	0	java.lang.String	0	(name unknown)
org.apache.hadoop.tools.rumen.ZombieJob:getQueueName()	0	java.lang.String	0	default
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeLoggedTask(org.apache.hadoop.tools.rumen.LoggedTask)	0	null	0	null
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeLoggedTaskAttempt(org.apache.hadoop.tools.rumen.LoggedTaskAttempt)	0	null	0	null
org.apache.hadoop.tools.rumen.ZombieJob:getUser()	0	java.lang.String	0	(unknown)
org.apache.hadoop.tools.rumen.ZombieJob:sanitizeTaskRuntime(long,org.apache.hadoop.mapreduce.ID)	0	long	0	100
org.apache.hadoop.tools.rumen.ZombieJob:getMachineNode(java.lang.String)	0	null	0	null
org.apache.hadoop.tools.rumen.ZombieJob:makeUpReduceRuntime(org.apache.hadoop.mapred.TaskStatus$State)	0	long	0	0
org.apache.hadoop.tools.rumen.ZombieJob:doMakeUpReduceRuntime(org.apache.hadoop.mapred.TaskStatus$State)	0	long	0	0
org.apache.hadoop.tools.rumen.ZombieJob:makeUpRuntime(java.util.List)	0	long	0	-1
org.apache.hadoop.tools.rumen.Task20LineHistoryEventEmitter$TaskFinishedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.CurrentJHParser:canParse(java.io.InputStream)	0	int	0	1
org.apache.hadoop.tools.rumen.CurrentJHParser:canParse(java.io.InputStream)	1	int	0	0
org.apache.hadoop.tools.rumen.Task20LineHistoryEventEmitter$TaskUpdatedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.MapAttempt20LineHistoryEventEmitter$MapAttemptFinishedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.TaskAttempt20LineEventEmitter$TaskAttemptStartedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Task20LineHistoryEventEmitter$TaskStartedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader$JobComparator:compare(org.apache.hadoop.tools.rumen.LoggedJob,org.apache.hadoop.tools.rumen.LoggedJob)	0	int	0	-1
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader$JobComparator:compare(org.apache.hadoop.tools.rumen.LoggedJob,org.apache.hadoop.tools.rumen.LoggedJob)	1	int	0	0
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader$JobComparator:compare(org.apache.hadoop.tools.rumen.LoggedJob,org.apache.hadoop.tools.rumen.LoggedJob)	2	int	0	1
org.apache.hadoop.tools.rumen.Task20LineHistoryEventEmitter$TaskFailedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobUnsuccessfulCompletionEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobSubmittedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobStatusChangedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobInfoChangeEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter$JobFinishedEventEmitter:maybeEmitEvent(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String,org.apache.hadoop.tools.rumen.HistoryEventEmitter)	0	null	0	null
org.apache.hadoop.tools.rumen.Hadoop20JHParser:canParse(java.io.InputStream)	0	int	0	1
org.apache.hadoop.tools.rumen.Hadoop20JHParser:canParse(java.io.InputStream)	1	int	0	0
org.apache.hadoop.tools.rumen.JobConfigurationParser:parse(java.io.InputStream)	0	null	0	null
org.apache.hadoop.tools.rumen.Histogram:get(long)	0	long	0	0
org.apache.hadoop.tools.rumen.Histogram:getCDF(int,int[])	0	null	0	null
org.apache.hadoop.tools.rumen.TraceBuilder:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.tools.rumen.ZombieJobProducer:getNextJob()	0	null	0	null
org.apache.hadoop.yarn.sls.SLSRunner:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.sls.SLSRunner$NodeDetails:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getHttpPort()	0	int	0	0
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getLastHealthReportTime()	0	long	0	0
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getLastNodeHeartBeatResponse()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getNodeManagerVersion()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:pullNewlyIncreasedContainers()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getOpportunisticContainersStatus()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getAggregatedContainersUtilization()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getNodeUtilization()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getUntrackedTimeStamp()	0	long	0	0
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getDecommissioningTimeout()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getAllocationTagsWithCount()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getRMContext()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:getPhysicalResource()	0	null	0	null
org.apache.hadoop.yarn.sls.nodemanager.NodeInfo$FakeRMNodeImpl:isUpdatedCapability()	0	int	0	0
org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper:getOpportunisticContainersStatus()	0	null	0	null
org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper:getUntrackedTimeStamp()	0	long	0	0
org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper:getDecommissioningTimeout()	0	null	0	null
org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper:getPhysicalResource()	0	null	0	null
org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper:isUpdatedCapability()	0	int	0	0
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics:getMemorySize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable,org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric)	0	long	0	0
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics:getVirtualCores(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable,org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric)	0	int	0	0
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics:getSchedulerAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId)	0	null	0	null
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$3:run()	0	null	0	null
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$2:run()	0	null	0	null
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$1:run()	0	null	0	null
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:getNextJob()	0	null	0	null
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:getNodesPerRack()	0	int	0	1
org.apache.hadoop.yarn.sls.synthetic.SynthJob:hasDeadline()	0	int	0	1
org.apache.hadoop.yarn.sls.synthetic.SynthJob:hasDeadline()	1	int	0	0
org.apache.hadoop.yarn.sls.synthetic.SynthJob:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.sls.synthetic.SynthJob:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.sls.synthetic.SynthUtils:getNormalDist(org.apache.commons.math3.random.JDKRandomGenerator,double,double)	0	null	0	null
org.apache.hadoop.yarn.sls.synthetic.SynthUtils:getLogNormalDist(org.apache.commons.math3.random.JDKRandomGenerator,double,double)	0	null	0	null
org.apache.hadoop.record.Utils:h2c(char)	0	int	0	0
org.apache.hadoop.record.Utils:utf8LenForCodePoint(int)	0	int	0	1
org.apache.hadoop.record.Utils:utf8LenForCodePoint(int)	1	int	0	2
org.apache.hadoop.record.Utils:utf8LenForCodePoint(int)	2	int	0	3
org.apache.hadoop.record.Utils:utf8LenForCodePoint(int)	3	int	0	4
org.apache.hadoop.record.Utils:writeUtf8(int,byte[],int)	0	int	0	1
org.apache.hadoop.record.Utils:writeUtf8(int,byte[],int)	1	int	0	2
org.apache.hadoop.record.Utils:writeUtf8(int,byte[],int)	2	int	0	3
org.apache.hadoop.record.Utils:writeUtf8(int,byte[],int)	3	int	0	4
org.apache.hadoop.record.Utils:isValidCodePoint(int)	0	int	0	1
org.apache.hadoop.record.Utils:isValidCodePoint(int)	1	int	0	0
org.apache.hadoop.record.BinaryRecordInput$BinaryIndex:done()	0	int	0	1
org.apache.hadoop.record.BinaryRecordInput$BinaryIndex:done()	1	int	0	0
org.apache.hadoop.record.Buffer:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.record.Buffer:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.streaming.DumpTypedBytes:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.streaming.DumpTypedBytes:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.streaming.DumpTypedBytes:dumpTypedBytes(java.util.List)	0	int	0	0
org.apache.hadoop.streaming.LoadTypedBytes:run(java.lang.String[])	0	int	0	1
org.apache.hadoop.streaming.LoadTypedBytes:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.streaming.LoadTypedBytes:run(java.lang.String[])	2	int	0	0
org.apache.hadoop.streaming.PathFinder:getAbsolutePath(java.lang.String)	0	null	0	null
org.apache.hadoop.streaming.PipeCombiner:getDoPipe()	0	int	0	1
org.apache.hadoop.streaming.PipeCombiner:getDoPipe()	1	int	0	0
org.apache.hadoop.streaming.PipeReducer:getDoPipe()	0	int	0	1
org.apache.hadoop.streaming.PipeReducer:getDoPipe()	1	int	0	0
org.apache.hadoop.streaming.StreamBaseRecordReader:getProgress()	0	float	0	1.0
org.apache.hadoop.streaming.PipeMapRed:safeDiv(long,long)	0	java.lang.String	0	NA
org.apache.hadoop.streaming.PipeMapper:getDoPipe()	0	int	0	1
org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader:getProgress()	0	float	0	1.0
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	1	int	0	1
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:slowReadUntilMatch(java.util.regex.Pattern,boolean,org.apache.hadoop.io.DataOutputBuffer)	0	int	0	0
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:nextState(int,int,int)	0	int	0	11
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:nextState(int,int,int)	1	int	0	10
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:nextState(int,int,int)	2	int	0	12
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:nextState(int,int,int)	3	int	0	13
org.apache.hadoop.streaming.StreamKeyValUtil:findTab(byte[],int,int)	0	int	0	-1
org.apache.hadoop.streaming.io.KeyOnlyTextOutputReader:readKeyValue()	0	int	0	0
org.apache.hadoop.streaming.io.KeyOnlyTextOutputReader:readKeyValue()	1	int	0	1
org.apache.hadoop.streaming.io.KeyOnlyTextOutputReader:getLastOutput()	0	java.lang.String	0	<undecodable>
org.apache.hadoop.streaming.io.TypedBytesOutputReader:readKeyValue()	0	int	0	0
org.apache.hadoop.streaming.io.TypedBytesOutputReader:readKeyValue()	1	int	0	1
org.apache.hadoop.streaming.io.TextOutputReader:readKeyValue()	0	int	0	0
org.apache.hadoop.streaming.io.TextOutputReader:readKeyValue()	1	int	0	1
org.apache.hadoop.streaming.io.TextOutputReader:getLastOutput()	0	java.lang.String	0	<undecodable>
org.apache.hadoop.streaming.io.RawBytesOutputReader:readKeyValue()	0	int	0	0
org.apache.hadoop.streaming.io.RawBytesOutputReader:readKeyValue()	1	int	0	1
org.apache.hadoop.streaming.io.RawBytesOutputReader:readLength()	0	int	0	-1
org.apache.hadoop.streaming.StreamXmlRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	0	int	0	0
org.apache.hadoop.streaming.StreamXmlRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)	1	int	0	1
org.apache.hadoop.streaming.StreamXmlRecordReader:slowReadUntilMatch(java.util.regex.Pattern,boolean,org.apache.hadoop.io.DataOutputBuffer)	0	int	0	0
org.apache.hadoop.streaming.StreamXmlRecordReader:nextState(int,int,int)	0	int	0	11
org.apache.hadoop.streaming.StreamXmlRecordReader:nextState(int,int,int)	1	int	0	10
org.apache.hadoop.streaming.StreamXmlRecordReader:nextState(int,int,int)	2	int	0	12
org.apache.hadoop.streaming.StreamXmlRecordReader:nextState(int,int,int)	3	int	0	13
org.apache.hadoop.streaming.JarBuilder:fileExtension(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.streaming.JarBuilder:getBasePathInJarOut(java.lang.String)	0	java.lang.String	0	classes/
org.apache.hadoop.streaming.JarBuilder:getBasePathInJarOut(java.lang.String)	1	java.lang.String	0	lib/
org.apache.hadoop.streaming.JarBuilder:getBasePathInJarOut(java.lang.String)	2	java.lang.String	0	
org.apache.hadoop.streaming.StreamJob:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.streaming.StreamJob:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.streaming.StreamJob:getClusterNick()	0	java.lang.String	0	default
org.apache.hadoop.streaming.StreamJob:packageJobJar()	0	null	0	null
org.apache.hadoop.streaming.StreamJob:submitAndMonitorJob()	0	int	0	0
org.apache.hadoop.typedbytes.TypedBytesWritable:getType()	0	null	0	null
org.apache.hadoop.typedbytes.TypedBytesRecordInput$TypedBytesIndex:done()	0	int	0	1
org.apache.hadoop.typedbytes.TypedBytesRecordInput$TypedBytesIndex:done()	1	int	0	0
org.apache.hadoop.typedbytes.TypedBytesInput:skipType()	0	int	0	1
org.apache.hadoop.typedbytes.TypedBytesInput:skipType()	1	int	0	0
org.apache.hadoop.typedbytes.TypedBytesWritableInput:read()	0	null	0	null
org.apache.hadoop.typedbytes.TypedBytesWritableInput:readType()	0	null	0	null
org.apache.hadoop.yarn.api.resource.PlacementConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TimedPlacementConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TimedPlacementConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$CardinalityConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$CardinalityConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$CompositeConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.resource.PlacementConstraint$CompositeConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraint$CompositeConstraint:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceProfilesResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceProfilesResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceTypeInfoResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceTypeInfoResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.GetResourceProfileRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.protocolrecords.GetResourceProfileRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.GetResourceProfileResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.protocolrecords.GetResourceProfileResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.UpdateContainerRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.UpdateContainerRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ReservationId:parseReservationId(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.api.records.ReservationId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ReservationId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ApplicationAttemptId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ApplicationAttemptId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ApplicationId:compareTo(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.api.records.ApplicationId:compareTo(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	-1
org.apache.hadoop.yarn.api.records.ApplicationId:compareTo(org.apache.hadoop.yarn.api.records.ApplicationId)	2	int	0	0
org.apache.hadoop.yarn.api.records.ApplicationId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ApplicationId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ExecutionTypeRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ExecutionTypeRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.NodeState:isUnusable()	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeState:isUnusable()	1	int	0	0
org.apache.hadoop.yarn.api.records.NodeState:isInactiveState()	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeState:isInactiveState()	1	int	0	0
org.apache.hadoop.yarn.api.records.NodeState:isActiveState()	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeState:isActiveState()	1	int	0	0
org.apache.hadoop.yarn.api.records.ResourceInformation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ResourceInformation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ResourceTypeInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ResourceTypeInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ResourceUtilization:getCustomResource(java.lang.String)	0	float	0	0.0
org.apache.hadoop.yarn.api.records.ResourceUtilization:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ResourceUtilization:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.NMToken:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.NMToken:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.UpdateContainerError:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.UpdateContainerError:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.Container:getVersion()	0	int	0	0
org.apache.hadoop.yarn.api.records.ContainerId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ContainerId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ResourceSizing:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ResourceSizing:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.Resource:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.Resource:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.Resource:compareTo(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.api.records.Resource:compareTo(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	-1
org.apache.hadoop.yarn.api.records.Resource:compareTo(org.apache.hadoop.yarn.api.records.Resource)	2	int	0	0
org.apache.hadoop.yarn.api.records.Resource:castToIntSafely(long)	0	int	0	2147483647
org.apache.hadoop.yarn.api.records.NodeReport:getDecommissioningTimeout()	0	null	0	null
org.apache.hadoop.yarn.api.records.ReservationRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ReservationRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.ResourceRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.ResourceRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetricCalculator:compare(java.lang.Number,java.lang.Number)	0	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetricCalculator:compare(java.lang.Number,java.lang.Number)	1	int	0	-1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetricCalculator:compare(java.lang.Number,java.lang.Number)	2	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:isValid()	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:isValid()	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:compareTo(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent)	0	int	0	-1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent:compareTo(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent)	1	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.ApplicationEntity:isApplicationEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.SubApplicationEntity:isSubApplicationEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.SubApplicationEntity:isSubApplicationEntity(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.FlowRunEntity:getRunId()	0	long	0	0
org.apache.hadoop.yarn.api.records.timelineservice.FlowRunEntity:getMaxEndTime()	0	long	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric:isValid()	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric:isValid()	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineMetric:getSingleDataValue()	0	null	0	null
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType:isParent(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType)	0	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType:isParent(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType)	1	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType:isChild(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType:isChild(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:isValid()	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:isValid()	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:compareTo(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	int	0	-1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity:compareTo(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	1	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity$Identifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity$Identifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.Priority:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.Priority:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timeline.TimelineEvent:compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEvent)	0	int	0	-1
org.apache.hadoop.yarn.api.records.timeline.TimelineEvent:compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEvent)	1	int	0	1
org.apache.hadoop.yarn.api.records.timeline.TimelineEvent:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timeline.TimelineEvent:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timeline.TimelineEntity:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.timeline.TimelineEntity:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.timeline.TimelineEntity:compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)	0	int	0	-1
org.apache.hadoop.yarn.api.records.timeline.TimelineEntity:compareTo(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)	1	int	0	1
org.apache.hadoop.yarn.api.records.NodeLabel:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeLabel:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.UpdatedContainer:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.UpdatedContainer:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:compareTo(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:compareTo(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	-1
org.apache.hadoop.yarn.api.records.NodeId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.NodeId:compareTo(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.api.records.NodeId:compareTo(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	-1
org.apache.hadoop.yarn.api.records.NodeId:compareTo(org.apache.hadoop.yarn.api.records.NodeId)	2	int	0	0
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	0	int	0	8032
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	1	int	0	8030
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	2	int	0	8088
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	3	int	0	8090
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	4	int	0	8031
org.apache.hadoop.yarn.conf.YarnConfiguration:getRMDefaultPortNumber(java.lang.String,org.apache.hadoop.conf.Configuration)	5	int	0	8033
org.apache.hadoop.yarn.conf.YarnConfiguration:useHttps(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.conf.YarnConfiguration:useHttps(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.yarn.conf.HAUtil:isAutomaticFailoverEnabledAndEmbedded(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.conf.HAUtil:isAutomaticFailoverEnabledAndEmbedded(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto$Builder:hasAccessType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto$Builder:hasAccessType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto$Builder:hasAcl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto$Builder:hasAcl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasStartBegin()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasStartBegin()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasStartEnd()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasStartEnd()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasFinishBegin()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasFinishBegin()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasFinishEnd()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasFinishEnd()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasScope()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasScope()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasExpireTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasExpireTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasRemainingTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto$Builder:hasRemainingTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasAmContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasAmContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasOriginalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasOriginalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto$Builder:hasIsUnregistered()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto$Builder:hasIsUnregistered()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasSchedulingDelay()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasSchedulingDelay()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasDelayUnit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$Builder:hasDelayUnit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:hasOverCommitTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto$Builder:hasOverCommitTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceBlacklistRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasResourceSizing()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasResourceSizing()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto$Builder:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationClientProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry$Builder:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasContainsGangs()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasContainsGangs()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasAcceptanceTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasAcceptanceTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasSchedulingDelay()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasSchedulingDelay()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasDelayUnit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:hasDelayUnit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StrictPreemptionContractProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasStagingTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasStagingTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasVolumeCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasVolumeCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasReadonly()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:hasReadonly()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:hasOperation()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:hasOperation()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:hasFailOnUnknownNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto$Builder:hasFailOnUnknownNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto$Builder:hasException()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto$Builder:hasException()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:hasApplicationSubmissionContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:hasApplicationSubmissionContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto$Builder:hasFailOnUnknownNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto$Builder:hasFailOnUnknownNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto$Builder:hasDecommissionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto$Builder:hasDecommissionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto$Builder:hasDecommissionTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto$Builder:hasDecommissionTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:hasTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest$Builder:hasTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasAmContainerSpec()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasAmContainerSpec()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasCancelTokensWhenComplete()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasCancelTokensWhenComplete()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasUnmanagedAm()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasUnmanagedAm()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasMaxAppAttempts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasMaxAppAttempts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasKeepContainersAcrossApplicationAttempts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasKeepContainersAcrossApplicationAttempts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasAttemptFailuresValidityInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasAttemptFailuresValidityInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto$Builder:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:hasReason()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:hasReason()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:hasRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:hasRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasScheme()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasScheme()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasFile()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasFile()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasUserInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:hasUserInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$CommitResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$CommitResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$CommitResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$CommitResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:hasClusterMetrics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:hasClusterMetrics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto$Builder:hasClusterMetrics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsResponseProto$Builder:hasClusterMetrics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasContainerSubState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto$Builder:hasContainerSubState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasReason()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasReason()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasUpdateRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasUpdateRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasCurrentContainerVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:hasCurrentContainerVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto$Builder:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto$Builder:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto$Builder:hasExpireTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto$Builder:hasExpireTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:hasEnforceExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:hasEnforceExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoRequest$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:hasVendorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse$Builder:hasVendorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:hasAttributeKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:hasAttributeKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:hasAttributeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto$Builder:hasAttributeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RestartContainerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RestartContainerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RestartContainerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RestartContainerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto$Builder:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasClientToAmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasClientToAmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAppResourceUsage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAppResourceUsage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasOriginalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasOriginalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasCurrentApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasCurrentApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasProgress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasProgress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAmRmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAmRmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasLogAggregationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasLogAggregationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasUnmanagedApplication()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasUnmanagedApplication()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAppNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAppNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAmNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasAmNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasLaunchTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasLaunchTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto$Builder:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasRetryPolicy()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasRetryPolicy()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasMaxRetries()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasMaxRetries()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasRetryInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasRetryInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasFailuresValidityInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:hasFailuresValidityInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasExpireTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasExpireTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasRemainingTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:hasRemainingTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodeIdToLabelsNameProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasLogUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasLogUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasNodeHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasNodeHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasExposedPorts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto$Builder:hasExposedPorts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasTokens()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasTokens()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasContainerRetryContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasContainerRetryContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasTokensConf()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:hasTokensConf()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:hasAttributePrefix()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:hasAttributePrefix()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:hasAttributeName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto$Builder:hasAttributeName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto$Builder:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:hasInterpreter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:hasInterpreter()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasMasterContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto$Builder:hasMasterContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasAbsoluteCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasAbsoluteCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasAbsoluteMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasAbsoluteMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasMaxAMPercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasMaxAMPercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasEffectiveMinCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasEffectiveMinCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasEffectiveMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasEffectiveMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasConfiguredMinCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasConfiguredMinCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasConfiguredMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:hasConfiguredMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasMaximumCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasMaximumCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasClientToAmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasClientToAmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasResourceProfiles()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:hasResourceProfiles()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsSubmitted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsSubmitted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsRunning()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsRunning()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsPending()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsPending()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsCompleted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsCompleted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsKilled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsKilled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsFailed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumAppsFailed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumActiveUsers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasNumActiveUsers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAvailableMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAvailableMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAvailableVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAvailableVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasAllocatedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasPendingContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:hasReservedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto$Builder:hasNode()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto$Builder:hasNode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLocalResourceMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasResourceName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasResourceName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasRelaxLocality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasRelaxLocality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasExecutionTypeRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasExecutionTypeRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto$Builder:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:hasTargetQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:hasTargetQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto$Builder:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintMapEntryProto$Builder:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StrictPreemptionContractProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StrictPreemptionContractProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StrictPreemptionContractProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StrictPreemptionContractProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:hasAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:hasAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:hasClusterId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:hasClusterId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:hasRmId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:hasRmId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasPmem()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasPmem()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasVmem()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasVmem()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasCpu()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:hasCpu()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:hasToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto$Builder:hasToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasAMCommand()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasAMCommand()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasNumClusterNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasNumClusterNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasPreempt()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasPreempt()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasAmRmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasAmRmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasCollectorInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:hasCollectorInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeResponse$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto$Builder:hasIsKillCompleted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto$Builder:hasIsKillCompleted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringStringMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeLabelsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasAbsoluteCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasAbsoluteCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasAbsoluteMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasAbsoluteMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasMaxAMPercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasMaxAMPercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasEffectiveMinCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasEffectiveMinCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasEffectiveMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasEffectiveMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasConfiguredMinCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasConfiguredMinCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasConfiguredMaxCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsProto$Builder:hasConfiguredMaxCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest$Builder:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesRequest$Builder:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.SCMAdminProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto$Builder:hasAccepted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto$Builder:hasAccepted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasNodeHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasNodeHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasContainerToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasContainerToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasExposedPorts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:hasExposedPorts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasAllocatedResource()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasAllocatedResource()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasAssignedNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasAssignedNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto$Builder:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:hasNodeAttribute()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:hasNodeAttribute()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:hasResourceOption()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto$Builder:hasResourceOption()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasScope()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasScope()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasMinCardinality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasMinCardinality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasMaxCardinality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasMaxCardinality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasAttributeOpCode()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto$Builder:hasAttributeOpCode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:hasFailOnUnknownNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:hasFailOnUnknownNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto$Builder:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LabelsToNodeIdsProto$Builder:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:hasContainerReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:hasContainerReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:hasReason()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:hasReason()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:hasRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectedSchedulingRequestProto$Builder:hasRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasUnits()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasUnits()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasNodeHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasNodeHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasContainerToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasContainerToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasExposedPorts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerProto$Builder:hasExposedPorts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfileEntry:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto$Builder:hasNodeAttribute()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AttributeToNodesProto$Builder:hasNodeAttribute()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto$Builder:hasClusterId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto$Builder:hasClusterId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto$Builder:hasRmId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ActiveRMInfoProto$Builder:hasRmId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasAmContainerSpec()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasAmContainerSpec()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasCancelTokensWhenComplete()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasCancelTokensWhenComplete()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasUnmanagedAm()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasUnmanagedAm()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasMaxAppAttempts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasMaxAppAttempts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasKeepContainersAcrossApplicationAttempts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasKeepContainersAcrossApplicationAttempts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasAttemptFailuresValidityInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasAttemptFailuresValidityInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationSubmissionContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumNodeManagers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumNodeManagers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumDecommissionedNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumDecommissionedNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumActiveNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumActiveNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumLostNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumLostNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumUnhealthyNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumUnhealthyNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumRebootedNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumRebootedNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumDecommissioningNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumDecommissioningNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumShutdownNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:hasNumShutdownNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasResourceName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasResourceName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasRelaxLocality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasRelaxLocality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasExecutionTypeRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasExecutionTypeRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasUnits()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasUnits()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersRequestProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:hasCommand()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto$Builder:hasCommand()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$Builder:hasCompositeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$Builder:hasCompositeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasMaximumCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasMaximumCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasCurrentCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasCurrentCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasDefaultNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasDefaultNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasQueueStatistics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasQueueStatistics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasPreemptionDisabled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasPreemptionDisabled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasIntraQueuePreemptionDisabled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto$Builder:hasIntraQueuePreemptionDisabled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto$Builder:hasEnforceExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeRequestProto$Builder:hasEnforceExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:hasClusterTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:hasClusterTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasLogUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasLogUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasNodeHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasNodeHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasExposedPorts()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:hasExposedPorts()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto$Builder:hasAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptIdProto$Builder:hasAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasScope()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasScope()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasMinCardinality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasMinCardinality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasMaxCardinality()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasMaxCardinality()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasAttributeOpCode()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:hasAttributeOpCode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SimplePlacementConstraintProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto$Builder:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileResponseProto$Builder:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:hasIsUnregistered()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:hasIsUnregistered()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:hasApplicationTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:hasApplicationTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainersRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto$Builder:hasIsExclusive()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto$Builder:hasIsExclusive()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasAllocatedResource()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasAllocatedResource()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasAssignedNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasAssignedNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:hasSupported()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:hasSupported()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:hasMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:hasMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:hasAttributeKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:hasAttributeKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:hasAttributeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:hasAttributeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasTrace()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasTrace()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasClassName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasClassName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasCause()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:hasCause()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasRackName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasRackName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasHealthReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasHealthReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasLastHealthReportTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasLastHealthReportTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasContainersUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasContainersUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasDecommissioningTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasDecommissioningTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:hasNodeUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:hasMemory()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:hasMemory()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:hasVirtualCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:hasVirtualCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:hasPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:hasPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:hasClusterTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:hasClusterTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasConcurrency()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasConcurrency()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasDuration()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto$Builder:hasDuration()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasMaximumCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasMaximumCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasCurrentCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasCurrentCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasDefaultNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasDefaultNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasQueueStatistics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasQueueStatistics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasPreemptionDisabled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasPreemptionDisabled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasIntraQueuePreemptionDisabled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:hasIntraQueuePreemptionDisabled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:hasCommand()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:hasCommand()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumNodeManagers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumNodeManagers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumDecommissionedNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumDecommissionedNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumActiveNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumActiveNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumLostNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumLostNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumUnhealthyNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumUnhealthyNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumRebootedNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumRebootedNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumDecommissioningNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumDecommissioningNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumShutdownNms()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnClusterMetricsProto$Builder:hasNumShutdownNms()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:hasCollectorAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:hasCollectorAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:hasCollectorToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto$Builder:hasCollectorToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:hasMaximumCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto$Builder:hasMaximumCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:hasTargetType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:hasTargetType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:hasTargetKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:hasTargetKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasScheme()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasScheme()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasFile()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasFile()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasUserInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$URLProto$Builder:hasUserInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto$Builder:hasClusterTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationIdProto$Builder:hasClusterTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SignalContainerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:hasStrictContract()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:hasStrictContract()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:hasContract()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto$Builder:hasContract()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:hasOverCommitTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:hasOverCommitTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceOptionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:hasPartitionName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:hasPartitionName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:hasQueueConfigurations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:hasQueueConfigurations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshClusterMaxPriorityResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasContainerSubState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:hasContainerSubState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto$Builder:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReleaseSharedCacheResourceRequestProto$Builder:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto$Builder:hasProfile()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto$Builder:hasProfile()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$FinishApplicationMasterRequestProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto$Builder:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StopContainerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:hasSimpleConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:hasSimpleConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:hasCompositeConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:hasCompositeConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:hasHostname()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:hasHostname()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:hasAttributeValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:hasAttributeValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$CommitResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:hasExpireTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:hasExpireTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationUpdateTimeoutMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto$Builder:hasResourceProfiles()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto$Builder:hasResourceProfiles()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto$Builder:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto$Builder:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasLocalizationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasLocalizationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasReason()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasReason()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasUpdateRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasUpdateRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasCurrentContainerVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerErrorProto$Builder:hasCurrentContainerVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:hasTargetType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:hasTargetType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:hasTargetKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$Builder:hasTargetKey()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasAllocatedResource()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasAllocatedResource()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasAssignedNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasAssignedNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerHistoryDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasResourceSizing()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasResourceSizing()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasPlacementConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:hasPlacementConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SchedulingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasBlacklistRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasBlacklistRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasProgress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasProgress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasTokens()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasTokens()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasContainerRetryContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasContainerRetryContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasTokensConf()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerLaunchContextProto$Builder:hasTokensConf()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasTrace()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasTrace()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasClassName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasClassName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasCause()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SerializedExceptionProto$Builder:hasCause()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:hasAttributePrefix()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:hasAttributePrefix()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:hasAttributeName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:hasAttributeName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeKeyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto$Builder:hasApplicationReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto$Builder:hasApplicationReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringLongMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasIncludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasIncludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasExcludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasExcludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasRolledLogsIncludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasRolledLogsIncludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasRolledLogsExcludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasRolledLogsExcludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasLogAggregationPolicyClassName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasLogAggregationPolicyClassName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasLogAggregationPolicyParameters()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:hasLogAggregationPolicyParameters()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasUnits()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasUnits()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceInformationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringBytesMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasLocalizationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasLocalizationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasMaximumCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasMaximumCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasClientToAmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasClientToAmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasResourceProfiles()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterResponseProto$Builder:hasResourceProfiles()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasReservationRequests()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasReservationRequests()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasArrival()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasArrival()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasDeadline()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasDeadline()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasReservationName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasReservationName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasRecurrenceExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasRecurrenceExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContainerProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:hasNode()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:hasNode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasAllocatedResource()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasAllocatedResource()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasAssignedNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasAssignedNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerStartDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:hasAttributeValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeResponse:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeResponse:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceBlacklistRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceBlacklistRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceBlacklistRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceBlacklistRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeResponse:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeResponse:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AddToClusterNodeLabelsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNumUsedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNumUsedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNumReservedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNumReservedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasUsedResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasUsedResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasReservedResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasReservedResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNeededResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasNeededResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasQueueUsagePercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasQueueUsagePercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasClusterUsagePercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasClusterUsagePercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasPreemptedMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasPreemptedMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasPreemptedVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto$Builder:hasPreemptedVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:hasTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:hasTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationStartDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasReservationRequests()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasReservationRequests()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasArrival()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasArrival()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasDeadline()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasDeadline()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasReservationName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasReservationName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasRecurrenceExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasRecurrenceExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationDefinitionProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:hasVolumeType()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:hasVolumeType()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:hasAccessMode()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$Builder:hasAccessMode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterMetricsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasStartBegin()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasStartBegin()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasStartEnd()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasStartEnd()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasFinishBegin()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasFinishBegin()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasFinishEnd()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasFinishEnd()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasScope()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasScope()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:hasCollectorAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:hasCollectorAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:hasCollectorToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:hasCollectorToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CollectorInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeResourceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto$Builder:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueUserACLInfoProto$Builder:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationSubmissionRequestProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasAMCommand()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasAMCommand()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasNumClusterNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasNumClusterNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasPreempt()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasPreempt()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasAmRmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasAmRmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasCollectorInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateResponseProto$Builder:hasCollectorInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$IncreaseContainersResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasPmem()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasPmem()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasVmem()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasVmem()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasCpu()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceUtilizationProto$Builder:hasCpu()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:hasCompositeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:hasCompositeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerLocalizationStatusesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteRequestProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$FailApplicationAttemptResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasContainerLaunchContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasContainerLaunchContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasAutoCommit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:hasAutoCommit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:hasOperation()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:hasOperation()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:hasFailOnUnknownNodes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:hasFailOnUnknownNodes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:hasAccepted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:hasAccepted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasIncludeApplications()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasIncludeApplications()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasIncludeChildQueues()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasIncludeChildQueues()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasRecursive()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto$Builder:hasRecursive()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:hasContainerLaunchContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:hasContainerLaunchContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:hasContainerToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto$Builder:hasContainerToken()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasStagingTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasStagingTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasVolumeCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasVolumeCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasReadonly()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodePublishVolumeRequest$Builder:hasReadonly()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto$Builder:hasApplicationSubmissionContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationRequestProto$Builder:hasApplicationSubmissionContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationTimeoutsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProfilesProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasMasterContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasMasterContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto$Builder:hasApplicationPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateApplicationPriorityResponseProto$Builder:hasApplicationPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainersResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasVisibility()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasVisibility()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasPattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasPattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasShouldBeUploadedToSharedCache()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto$Builder:hasShouldBeUploadedToSharedCache()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:hasApplicationReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:hasApplicationReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto$Builder:hasClusterTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationIdProto$Builder:hasClusterTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:hasSupported()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:hasSupported()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:hasMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$ValidateVolumeCapabilitiesResponse$Builder:hasMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasUnits()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasUnits()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypeInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ContainerFinishDataProto$Builder:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdToLabelsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto$Builder:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasIncludeResourceAllocations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto$Builder:hasIncludeResourceAllocations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:hasApplicationAttemptReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:hasApplicationAttemptReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshSuperUserGroupsConfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasTimestamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasTimestamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasVisibility()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasVisibility()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasPattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasPattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasShouldBeUploadedToSharedCache()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:hasShouldBeUploadedToSharedCache()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationName()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationName()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationHistoryDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto$Builder:hasApplicationAttemptReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptReportResponseProto$Builder:hasApplicationAttemptReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeAttributesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNumUsedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNumUsedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNumReservedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNumReservedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasUsedResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasUsedResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasReservedResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasReservedResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNeededResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasNeededResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasQueueUsagePercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasQueueUsagePercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasClusterUsagePercentage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasClusterUsagePercentage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasPreemptedMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasPreemptedMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasPreemptedVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:hasPreemptedVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationResourceUsageReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:hasPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:hasPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto$Builder:hasContainerReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerReportResponseProto$Builder:hasContainerReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasQueueName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasQueueName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasIncludeApplications()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasIncludeApplications()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasIncludeChildQueues()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasIncludeChildQueues()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasRecursive()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:hasRecursive()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateRequestProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RollbackResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RollbackResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RollbackResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RollbackResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:hasResourceOption()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:hasResourceOption()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeResourceMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptFinishDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto$Builder:hasPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UseSharedCacheResourceResponseProto$Builder:hasPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:hasPartitionName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:hasPartitionName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:hasQueueConfigurations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueConfigurationsMapProto$Builder:hasQueueConfigurations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$CheckForDecommissioningNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshAdminAclsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:hasMemory()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:hasMemory()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:hasVirtualCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceProto$Builder:hasVirtualCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationDeleteResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:hasTargetQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$MoveApplicationAcrossQueuesRequestProto$Builder:hasTargetQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RestartContainerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodeLabelsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RemoveFromClusterNodeLabelsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:hasSimpleConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:hasSimpleConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:hasCompositeConstraint()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintProto$Builder:hasCompositeConstraint()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$GetGroupsForUserRequestProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasBlacklistRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasBlacklistRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasProgress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasProgress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$AllocateRequestProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasContainerVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasContainerVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasContainerVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasContainerVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdateContainerRequestProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto$Builder:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLocalizationStatusesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:hasAccessType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:hasAccessType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:hasAcl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:hasAcl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationACLMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:hasNumAllocations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:hasNumAllocations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:hasHostname()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:hasHostname()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:hasAttributeValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeToAttributeValueProto$Builder:hasAttributeValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:hasUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:hasUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:hasContainer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:hasContainer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SubmitApplicationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientSCMProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:hasContainerLaunchContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:hasContainerLaunchContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:hasContainerToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:hasContainerToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto$Builder:hasQueueInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto$Builder:hasQueueInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationAttemptsRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasRetryPolicy()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasRetryPolicy()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasMaxRetries()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasMaxRetries()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasRetryInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasRetryInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasFailuresValidityInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryContextProto$Builder:hasFailuresValidityInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:hasException()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:hasException()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerExceptionMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshQueuesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasContainerLaunchContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasContainerLaunchContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasAutoCommit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReInitializeContainerRequestProto$Builder:hasAutoCommit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshServiceAclsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceAllocationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasAmContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasAmContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasOriginalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasOriginalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAttemptReportProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToAttributesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder:hasPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeIdProto$Builder:hasPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$NodesToAttributesMappingResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto$Builder:hasInterpreter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestsProto$Builder:hasInterpreter()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:hasVolumeId()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:hasVolumeId()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:hasTargetPath()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:hasTargetPath()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RunSharedCacheCleanerTaskRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RollbackResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto$Builder:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewReservationResponseProto$Builder:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:hasNumAllocations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:hasNumAllocations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:hasResources()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceSizingProto$Builder:hasResources()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$RegisterApplicationMasterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:hasToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:hasToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$NMTokenProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueUserAclsInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:hasApplicationTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AppTimeoutsMapProto$Builder:hasApplicationTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoRequest:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoRequest:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:hasVolumeType()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:hasVolumeType()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:hasAccessMode()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:hasAccessMode()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationFinishDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:hasVendorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:hasVendorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$GetPluginInfoResponse:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasConcurrency()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasConcurrency()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasDuration()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:hasDuration()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesResourcesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:hasQueueInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:hasQueueInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetQueueInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeProto$Builder:hasAttributeValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationUpdateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionContractProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:hasUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:hasUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:hasContainer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$UpdatedContainerProto$Builder:hasContainer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceTypeInfoResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ResourceLocalizationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:hasIsExclusive()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:hasIsExclusive()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeLabelProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasRackName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasRackName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNumContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNumContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasHealthReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasHealthReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasLastHealthReportTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasLastHealthReportTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasContainersUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasContainersUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasDecommissioningTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasDecommissioningTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeUpdateType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeReportProto$Builder:hasNodeUpdateType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetApplicationReportRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasIncludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasIncludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasExcludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasExcludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasRolledLogsIncludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasRolledLogsIncludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasRolledLogsExcludePattern()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasRolledLogsExcludePattern()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasLogAggregationPolicyClassName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasLogAggregationPolicyClassName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasLogAggregationPolicyParameters()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationContextProto$Builder:hasLogAggregationPolicyParameters()	1	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$NodeUnpublishVolumeResponse$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeLabelsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeLabelsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeLabelsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$UpdateNodeLabelsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:hasDecommissionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:hasDecommissionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:hasDecommissionTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:hasDecommissionTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshNodesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:hasKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:hasKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:hasValue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:hasValue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$StringFloatMapProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNodesToLabelsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsSubmitted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsSubmitted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsRunning()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsRunning()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsPending()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsPending()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsCompleted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsCompleted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsKilled()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsKilled()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsFailed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumAppsFailed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumActiveUsers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasNumActiveUsers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAvailableMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAvailableMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedMemoryMB()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedMemoryMB()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAvailableVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAvailableVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedVCores()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedVCores()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasAllocatedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasPendingContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStatisticsProto$Builder:hasReservedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:hasResourceProfiles()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:hasResourceProfiles()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAllResourceProfilesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetContainerStatusesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ResourceManagerAdministrationProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$ReplaceLabelsOnNodeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:hasStrictContract()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:hasStrictContract()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:hasContract()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:hasContract()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PreemptionMessageProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasDiagnosticsInfo()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasDiagnosticsInfo()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasMasterContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasMasterContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasYarnApplicationAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptHistoryDataProto$Builder:hasYarnApplicationAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetAttributesToNodesResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PriorityProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetLabelsToNodesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto$Builder:hasApplicationTimeoutType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto$Builder:hasApplicationTimeoutType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto$Builder:hasTimeout()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutMapProto$Builder:hasTimeout()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasIncludeResourceAllocations()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:hasIncludeResourceAllocations()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ReservationListRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ContainerManagementProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainersRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasMasterContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:hasMasterContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ApplicationHistoryServerProtos$ApplicationAttemptStartDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:hasProfile()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:hasProfile()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetResourceProfileRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:hasMaximumCapability()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:hasMaximumCapability()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$GetNewApplicationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasReservationDefinition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasReservationDefinition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasEndTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasEndTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasContainsGangs()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasContainsGangs()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasAcceptanceTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasAcceptanceTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasReservationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:hasReservationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasHost()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasHost()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasRpcPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasRpcPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasClientToAmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasClientToAmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasYarnApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasYarnApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAppResourceUsage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAppResourceUsage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasOriginalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasOriginalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasCurrentApplicationAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasCurrentApplicationAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasProgress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasProgress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasApplicationType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasApplicationType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAmRmToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAmRmToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasLogAggregationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasLogAggregationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasUnmanagedApplication()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasUnmanagedApplication()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAppNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAppNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAmNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasAmNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasLaunchTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasLaunchTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:hasIsKillCompleted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:hasIsKillCompleted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$KillApplicationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$RefreshUserToGroupsMappingsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.util.resource.ResourceUtils:getDefaultUnit(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.yarn.util.TimelineServiceHelper:mapCastToHashMap(java.util.Map)	0	null	0	null
org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser$ConstraintParser:parseScope(java.lang.String)	0	java.lang.String	0	node
org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser$ConstraintParser:parseScope(java.lang.String)	1	java.lang.String	0	rack
org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser$SourceTags:isEmpty()	0	int	0	1
org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser$SourceTags:isEmpty()	1	int	0	0
org.apache.hadoop.yarn.appcatalog.model.Application:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.appcatalog.model.Application:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$5:run()	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.DistributedShellTimelinePlugin:getTimelineEntityGroupId(java.lang.String,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.DistributedShellTimelinePlugin:getTimelineEntityGroupId(java.lang.String,java.util.SortedSet,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:init(java.lang.String[])	1	int	0	1
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:lambda$publishContainerStartFailedEventOnTimelineServiceV2$1(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$4:run()	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$1:run()	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$3:run()	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.Client:init(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.applications.distributedshell.Client:init(java.lang.String[])	1	int	0	1
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$6:run()	0	null	0	null
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$2:run()	0	null	0	null
org.apache.hadoop.applications.mawo.server.common.TaskId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.applications.mawo.server.common.TaskId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.applications.mawo.server.common.TaskStatus:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.applications.mawo.server.common.TaskStatus:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.applications.mawo.server.master.job.JobId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.applications.mawo.server.master.job.JobId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.applications.mawo.server.worker.WorkerId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:init(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:init(java.lang.String[])	1	int	0	1
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl$3:run()	0	null	0	null
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse)	0	int	0	56
org.apache.hadoop.yarn.service.client.ApiServiceClient:processResponse(com.sun.jersey.api.client.ClientResponse)	1	int	0	0
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl$StoppableRemoteIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl$StoppableRemoteIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.service.webapp.ApiServer:lambda$upgradeService$1(org.apache.hadoop.yarn.service.api.records.Service)	0	null	0	null
org.apache.hadoop.yarn.service.webapp.ApiServer$1:run()	0	null	0	null
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto$Builder:hasCompInstances()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto$Builder:hasCompInstances()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto$Builder:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto$Builder:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto$Builder:hasNumberOfContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto$Builder:hasNumberOfContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:hasName()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:hasName()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:hasNumberOfContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:hasNumberOfContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$ComponentCountProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:hasStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:hasStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasAutoFinalize()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasAutoFinalize()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasExpressUpgrade()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:hasExpressUpgrade()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasAutoFinalize()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasAutoFinalize()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasExpressUpgrade()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceRequestProto$Builder:hasExpressUpgrade()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesRequestProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetStatusResponseProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$DecommissionCompInstancesRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$FlexComponentsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto$Builder:hasError()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto$Builder:hasError()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:hasError()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:hasError()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$UpgradeServiceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CompInstancesUpgradeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$StopResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$RestartServiceResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$CancelUpgradeResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:hasCompInstances()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:hasCompInstances()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.ClientAMProtocol$GetCompInstancesResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.provider.ProviderService$1:didLaunchFail()	0	int	0	1
org.apache.hadoop.yarn.service.provider.ProviderUtils:addProviderJar(java.util.Map,java.lang.Class,java.lang.String,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path,java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.yarn.service.provider.ProviderUtils:addProviderJar(java.util.Map,java.lang.Class,java.lang.String,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.fs.Path,java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.yarn.service.provider.ProviderUtils:isStaticFile(org.apache.hadoop.yarn.service.api.records.ConfigFile)	0	int	0	1
org.apache.hadoop.yarn.service.provider.ProviderUtils:isStaticFile(org.apache.hadoop.yarn.service.api.records.ConfigFile)	1	int	0	0
org.apache.hadoop.yarn.service.provider.ProviderService$ResolvedLaunchParams:didLaunchFail()	0	int	0	0
org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders:getAbsoluteSelfRegistrationPath()	0	null	0	null
org.apache.hadoop.yarn.service.ClientAMSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.service.ClientAMSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	yarn.service.am.principal
org.apache.hadoop.yarn.service.ClientAMSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:getProgress()	0	float	0	100.0
org.apache.hadoop.yarn.service.api.records.Artifact:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Artifact:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Artifact:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.PlacementConstraint:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.PlacementConstraint:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.PlacementConstraint:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.ResourceInformation:getUnit()	0	java.lang.String	0	
org.apache.hadoop.yarn.service.api.records.ResourceInformation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.ResourceInformation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.ResourceInformation:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.ConfigFile:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.ConfigFile:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.ConfigFile:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.ServiceStatus:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.ServiceStatus:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.ServiceStatus:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.Container:getLaunchTime()	0	null	0	null
org.apache.hadoop.yarn.service.api.records.Container:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Container:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Container:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.Service:getLaunchTime()	0	null	0	null
org.apache.hadoop.yarn.service.api.records.Service:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Service:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Service:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.Resource:calcMemoryMB()	0	long	0	0
org.apache.hadoop.yarn.service.api.records.Resource:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Resource:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Resource:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.PlacementPolicy:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.PlacementPolicy:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.PlacementPolicy:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.Configuration:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Configuration:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Configuration:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.ServiceState:isUpgrading(org.apache.hadoop.yarn.service.api.records.ServiceState)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.ServiceState:isUpgrading(org.apache.hadoop.yarn.service.api.records.ServiceState)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Error:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Error:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Error:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.KerberosPrincipal:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.KerberosPrincipal:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.KerberosPrincipal:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.ReadinessCheck:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.ReadinessCheck:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.ReadinessCheck:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.api.records.Component:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.api.records.Component:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.api.records.Component:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.service.utils.ServiceRegistryUtils:registryDNSLookupExists(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceRegistryUtils:registryDNSLookupExists(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.utils.ServiceUtils:isUnset(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceUtils:isUnset(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.utils.ServiceUtils:isSet(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceUtils:isSet(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.utils.ServiceUtils:isEmpty(java.util.Collection)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceUtils:isEmpty(java.util.Collection)	1	int	0	0
org.apache.hadoop.yarn.service.utils.ServiceUtils:join(java.util.Collection,java.lang.String,boolean)	0	java.lang.String	0	
org.apache.hadoop.yarn.service.utils.ServiceUtils:isPortAvailable(int)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceUtils:isPortAvailable(int)	1	int	0	0
org.apache.hadoop.yarn.service.utils.CoreFileSystem:isFile(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.yarn.service.utils.ClientRegistryBinder:homePathForUser(java.lang.String)	0	java.lang.String	0	/services/
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:hasComponent(org.apache.hadoop.yarn.service.api.records.Service)	0	int	0	0
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:hasComponent(org.apache.hadoop.yarn.service.api.records.Service)	1	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:isUpgradable(org.apache.hadoop.yarn.service.api.records.Container)	0	int	0	1
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:isUpgradable(org.apache.hadoop.yarn.service.api.records.Container)	1	int	0	0
org.apache.hadoop.yarn.service.utils.Duration:getLimitExceeded()	0	int	0	1
org.apache.hadoop.yarn.service.utils.Duration:getLimitExceeded()	1	int	0	0
org.apache.hadoop.yarn.service.monitor.probe.MonitorUtils:toPlural(int)	0	java.lang.String	0	s
org.apache.hadoop.yarn.service.monitor.probe.MonitorUtils:toPlural(int)	1	java.lang.String	0	
org.apache.hadoop.yarn.service.monitor.probe.LogEntryBuilder:isEmpty()	0	int	0	1
org.apache.hadoop.yarn.service.monitor.probe.LogEntryBuilder:isEmpty()	1	int	0	0
org.apache.hadoop.yarn.service.containerlaunch.JavaCommandLineBuilder:defineIfSet(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.containerlaunch.JavaCommandLineBuilder:defineIfSet(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.ServiceManager:finalizeUpgrade(boolean)	0	int	0	1
org.apache.hadoop.yarn.service.ServiceManager:finalizeUpgrade(boolean)	1	int	0	0
org.apache.hadoop.yarn.service.ServiceManager:lambda$resolveCompsToUpgrade$5(org.apache.hadoop.yarn.service.api.records.Component)	0	int	0	1
org.apache.hadoop.yarn.service.ServiceManager:lambda$resolveCompsToUpgrade$5(org.apache.hadoop.yarn.service.api.records.Component)	1	int	0	0
org.apache.hadoop.yarn.service.ClientAMSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.service.ClientAMSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.service.client.ServiceClient:actionBuild(org.apache.hadoop.yarn.service.api.records.Service)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgradeExpress(java.lang.String,java.io.File)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgradeExpress(org.apache.hadoop.yarn.service.api.records.Service)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:initiateUpgrade(org.apache.hadoop.yarn.service.api.records.Service)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionCancelUpgrade(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionDecommissionInstances(java.lang.String,java.util.List)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionCleanUp(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionCleanUp(java.lang.String,java.lang.String)	1	int	0	-1
org.apache.hadoop.yarn.service.client.ServiceClient:actionUpgrade(org.apache.hadoop.yarn.service.api.records.Service,java.util.List)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionLaunch(java.lang.String,java.lang.String,java.lang.Long,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionFlex(java.lang.String,java.util.Map)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean)	0	int	0	40
org.apache.hadoop.yarn.service.client.ServiceClient:actionStop(java.lang.String,boolean)	1	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:cleanUpRegistryPath(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.client.ServiceClient:cleanUpRegistryPath(java.lang.String,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:deleteZKNode(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.service.client.ServiceClient:deleteZKNode(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionStart(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:getStatusByAppId(org.apache.hadoop.yarn.api.records.ApplicationId)	0	java.lang.String	0	
org.apache.hadoop.yarn.service.client.ServiceClient:actionDependency(java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:actionDependency(java.lang.String,boolean)	1	int	0	-1
org.apache.hadoop.yarn.service.client.ServiceClient:checkPermissions(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.yarn.service.client.ServiceClient:checkPermissions(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.yarn.service.ServiceMaster:lambda$serviceStart$0()	0	null	0	null
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:isLongLived()	0	int	0	1
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:hasCompleted(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:hasCompletedSuccessfully(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:shouldRelaunchInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.api.records.ContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:allowUpgrades()	0	int	0	1
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:shouldTerminate(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.AlwaysRestartPolicy:allowContainerRetriesForInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:hasContainerFailed(org.apache.hadoop.yarn.api.records.ContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:hasContainerFailed(org.apache.hadoop.yarn.api.records.ContainerStatus)	1	int	0	0
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:isFinalState(org.apache.hadoop.yarn.service.api.records.ContainerState)	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:isFinalState(org.apache.hadoop.yarn.service.api.records.ContainerState)	1	int	0	0
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:isLclRetrieverActive()	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:isLclRetrieverActive()	1	int	0	0
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId:compareTo(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId)	0	int	0	-1
org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId:compareTo(org.apache.hadoop.yarn.service.component.instance.ComponentInstanceId)	1	int	0	1
org.apache.hadoop.yarn.service.component.Component$UpgradeStatus:isCompleted()	0	int	0	1
org.apache.hadoop.yarn.service.component.Component$UpgradeStatus:isCompleted()	1	int	0	0
org.apache.hadoop.yarn.service.component.Component$UpgradeStatus:areContainersUpgrading()	0	int	0	1
org.apache.hadoop.yarn.service.component.Component$UpgradeStatus:areContainersUpgrading()	1	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:isLongLived()	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:hasCompleted(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:hasCompleted(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:hasCompletedSuccessfully(org.apache.hadoop.yarn.service.component.Component)	0	int	0	1
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:hasCompletedSuccessfully(org.apache.hadoop.yarn.service.component.Component)	1	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:shouldRelaunchInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.api.records.ContainerStatus)	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:allowUpgrades()	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:shouldTerminate(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:shouldTerminate(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.NeverRestartPolicy:allowContainerRetriesForInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	0	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:isLongLived()	0	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:hasCompleted(org.apache.hadoop.yarn.service.component.Component)	0	int	0	1
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:hasCompleted(org.apache.hadoop.yarn.service.component.Component)	1	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:hasCompletedSuccessfully(org.apache.hadoop.yarn.service.component.Component)	0	int	0	1
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:hasCompletedSuccessfully(org.apache.hadoop.yarn.service.component.Component)	1	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:shouldRelaunchInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.api.records.ContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:shouldRelaunchInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.api.records.ContainerStatus)	1	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:isReadyForDownStream(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:allowUpgrades()	0	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:shouldTerminate(org.apache.hadoop.yarn.service.component.Component)	0	int	0	0
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:shouldTerminate(org.apache.hadoop.yarn.service.component.Component)	1	int	0	1
org.apache.hadoop.yarn.service.component.OnFailureRestartPolicy:allowContainerRetriesForInstance(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	0	int	0	1
org.apache.hadoop.yarn.service.component.Component:doesNeedUpgrade()	0	int	0	1
org.apache.hadoop.yarn.service.component.Component:doesNeedUpgrade()	1	int	0	0
org.apache.hadoop.yarn.service.component.Component:areDependenciesReady()	0	int	0	1
org.apache.hadoop.yarn.service.component.Component:areDependenciesReady()	1	int	0	0
org.apache.hadoop.yarn.service.component.Component:removeFailedInstanceIfExists(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	0	int	0	1
org.apache.hadoop.yarn.service.component.Component:removeFailedInstanceIfExists(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	1	int	0	0
org.apache.hadoop.yarn.service.component.Component:removeSuccessfulInstanceIfExists(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	0	int	0	1
org.apache.hadoop.yarn.service.component.Component:removeSuccessfulInstanceIfExists(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)	1	int	0	0
org.apache.hadoop.yarn.client.SCMAdmin:runCleanerTask()	0	int	0	0
org.apache.hadoop.yarn.client.SCMAdmin:runCleanerTask()	1	int	0	1
org.apache.hadoop.yarn.client.SCMAdmin:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.yarn.client.SCMAdmin:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.api.NMClient:getNodeIdOfStartedContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.client.api.NMClient:getLocalizationStatuses(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)	0	null	0	null
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:isCompletelyDone(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer)	0	int	0	1
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:isCompletelyDone(org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$StatefulContainer)	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:get(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:remove(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:getExecutionTypeMap(org.apache.hadoop.yarn.api.records.Priority,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:getCapabilityMap(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:decResourceRequest(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionTypeRequest,org.apache.hadoop.yarn.api.records.Resource,java.lang.Object)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable$RequestInfoIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable$RequestInfoIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:enforceAsyncAPITimeout()	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:enforceAsyncAPITimeout()	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy:tryCloseProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy:tryCloseProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy:removeProxy(org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData)	0	int	0	0
org.apache.hadoop.yarn.client.cli.SchedConfCLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.SchedConfCLI:run(java.lang.String[])	1	int	0	-1
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI:handleHelpCommand(java.lang.String[],org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$CommandHandler[])	0	int	0	1
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI:handleHelpCommand(java.lang.String[],org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$CommandHandler[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.TopCLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.TopCLI:run(java.lang.String[])	1	int	0	1
org.apache.hadoop.yarn.client.cli.TopCLI:getRMStartTime()	0	long	0	-1
org.apache.hadoop.yarn.client.cli.ClusterCLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:runCommand(java.lang.String[])	0	int	0	-1
org.apache.hadoop.yarn.client.cli.LogsCLI:runCommand(java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:fetchAllLogFiles(java.lang.String[],java.lang.String[])	0	int	0	1
org.apache.hadoop.yarn.client.cli.LogsCLI:fetchAllLogFiles(java.lang.String[],java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:printAMContainerLogs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.util.List,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers,boolean,boolean)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.LogsCLI:printAMContainerLogs(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.util.List,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers,boolean,boolean)	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:showNodeLists(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.LogsCLI:showNodeLists(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers)	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:showApplicationLogInfo(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.LogsCLI:showApplicationLogInfo(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers)	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:fetchContainerLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers,boolean,boolean)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.LogsCLI:getMatchedLogOptions(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers,boolean,boolean)	0	null	0	null
org.apache.hadoop.yarn.client.cli.LogsCLI:isFileMatching(java.lang.String,java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.client.cli.LogsCLI:isFileMatching(java.lang.String,java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:printContainerInfoFromRunningApplication(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,org.apache.hadoop.yarn.logaggregation.LogCLIHelpers)	0	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI:getNodeHttpAddressFromRMWebString(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest)	0	null	0	null
org.apache.hadoop.yarn.client.cli.LogsCLI:getMatchedOptionForRunningApp(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,boolean,boolean)	0	null	0	null
org.apache.hadoop.yarn.client.cli.NodeCLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.LogsCLI$ClientJerseyRetryFilter$1:shouldRetryOn(java.lang.Exception)	0	int	0	1
org.apache.hadoop.yarn.client.cli.LogsCLI$ClientJerseyRetryFilter$1:shouldRetryOn(java.lang.Exception)	1	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$AdminCommandHandler:handleCommand(org.apache.commons.cli.CommandLine)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:run(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:getSingleAppTypeFromCLI(org.apache.commons.cli.CommandLine)	0	java.lang.String	0	yarn-service
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printApplicationAttemptReport(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printApplicationAttemptReport(java.lang.String)	1	int	0	-1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printContainerReport(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printContainerReport(java.lang.String)	1	int	0	-1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printApplicationReport(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:printApplicationReport(java.lang.String)	1	int	0	-1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeStatusCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeListCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeListCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	1	int	0	-1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeMoveToQueueCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeFailCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeUpdatePriorityCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeSignalCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeShellCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeUpdateLifeTimeCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeChangeApplicationQueueCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:executeUpgradeCommand(org.apache.commons.cli.CommandLine,java.lang.String,org.apache.commons.cli.Options)	0	int	0	0
org.apache.hadoop.yarn.client.cli.ApplicationCLI:hasAnyOtherCLIOptions(org.apache.commons.cli.CommandLine,org.apache.commons.cli.Options,java.lang.String[])	0	int	0	1
org.apache.hadoop.yarn.client.cli.ApplicationCLI:hasAnyOtherCLIOptions(org.apache.commons.cli.CommandLine,org.apache.commons.cli.Options,java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.QueueCLI:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.yarn.client.cli.QueueCLI:run(java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshQueues()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshNodes(boolean)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshNodes(int,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshNodesResources()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshUserToGroupsMappings()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshSuperUserGroupsConfiguration()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshAdminAcls()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshServiceAcls()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:refreshClusterMaxPriority()	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:updateNodeResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,int)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:invalidResourceValue(int,int)	0	int	0	1
org.apache.hadoop.yarn.client.cli.RMAdminCLI:invalidResourceValue(int,int)	1	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:getGroups(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:handleAddToClusterNodeLabels(java.lang.String[],java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:handleRemoveFromClusterNodeLabels(java.lang.String[],java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:replaceLabelsOnNodes(java.util.Map,boolean,boolean)	0	int	0	0
org.apache.hadoop.yarn.client.cli.RMAdminCLI:run(java.lang.String[])	0	int	0	-1
org.apache.hadoop.yarn.client.cli.RMAdminCLI:handleRefreshNodes(java.lang.String[],java.lang.String,boolean)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.RMAdminCLI:handleUpdateNodeResource(java.lang.String[],java.lang.String,boolean)	0	int	0	-1
org.apache.hadoop.yarn.client.cli.RMAdminCLI:validateTrackingMode(java.lang.String)	0	java.lang.String	0	client
org.apache.hadoop.yarn.client.cli.RMAdminCLI:validateTrackingMode(java.lang.String)	1	java.lang.String	0	server
org.apache.hadoop.yarn.client.cli.RMAdminCLI:getUsageString()	0	java.lang.String	0	Usage: rmadmin
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$CommandHandler:getHelp(java.lang.String,java.lang.StringBuilder,boolean)	0	int	0	1
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$CommandHandler:getHelp(java.lang.String,java.lang.StringBuilder,boolean)	1	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$ClientCommandHandler:handleCommand(org.apache.commons.cli.CommandLine)	0	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$ClientCommandHandler:printNodesByAttributes(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$ClientCommandHandler:printAttributesByNode(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$ClientCommandHandler:printClusterAttributes()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto$Builder:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto$Builder:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto$Builder:hasClientName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto$Builder:hasClientName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasOwner()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasOwner()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasRenewer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasRenewer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasRealUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasRealUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasIssueDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasIssueDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasMaxDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasMaxDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasSequenceNumber()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasSequenceNumber()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasOwner()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasOwner()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasRenewer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasRenewer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasRealUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasRealUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasIssueDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasIssueDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasMaxDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasMaxDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasSequenceNumber()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasSequenceNumber()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasAppSubmitter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasAppSubmitter()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto$Builder:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto$Builder:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$AMRMTokenIdentifierProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasNmHostAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasNmHostAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasAppSubmitter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasAppSubmitter()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasExpiryTimeStamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasExpiryTimeStamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasContainerType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasContainerType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto$Builder:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasAppSubmitter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasAppSubmitter()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$NMTokenIdentifierProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:hasRegistryUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:hasRegistryUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto$Builder:hasRegistryUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto$Builder:hasRegistryUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$DockerCredentialTokenIdentifierProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:hasAppAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:hasAppAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:hasClientName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:hasClientName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ClientToAMTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasNmHostAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasNmHostAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasAppSubmitter()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasAppSubmitter()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasExpiryTimeStamp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasExpiryTimeStamp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasMasterKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasMasterKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasContainerType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasContainerType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$ContainerTokenIdentifierProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:compareTo(org.apache.hadoop.yarn.nodelabels.RMNodeLabel)	0	int	0	-1
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:compareTo(org.apache.hadoop.yarn.nodelabels.RMNodeLabel)	1	int	0	1
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:isNodeLabelExplicit(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:isNodeLabelExplicit(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:normalizeLabel(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:getNMInNodeSet(org.apache.hadoop.yarn.api.records.NodeId,java.util.Map,boolean)	0	null	0	null
org.apache.hadoop.yarn.nodelabels.NodeLabelUtil:isNodeAttributesEquals(java.util.Set,java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.NodeLabelUtil:isNodeAttributesEquals(java.util.Set,java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.NodeLabelUtil:lambda$isNodeAttributeIncludes$2(org.apache.hadoop.yarn.api.records.NodeAttribute,org.apache.hadoop.yarn.api.records.NodeAttribute)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.NodeLabelUtil:lambda$isNodeAttributeIncludes$2(org.apache.hadoop.yarn.api.records.NodeAttribute,org.apache.hadoop.yarn.api.records.NodeAttribute)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.RMNodeAttribute:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.RMNodeAttribute:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.nodelabels.store.op.RemoveNodeToAttributeLogOp:getOpCode()	0	int	0	1
org.apache.hadoop.yarn.nodelabels.store.op.AddClusterLabelOp:getOpCode()	0	int	0	0
org.apache.hadoop.yarn.nodelabels.store.op.RemoveClusterLabelOp:getOpCode()	0	int	0	2
org.apache.hadoop.yarn.nodelabels.store.op.AddNodeToAttributeLogOp:getOpCode()	0	int	0	0
org.apache.hadoop.yarn.nodelabels.store.op.NodeLabelMirrorOp:getOpCode()	0	int	0	-1
org.apache.hadoop.yarn.nodelabels.store.op.NodeToLabelOp:getOpCode()	0	int	0	1
org.apache.hadoop.yarn.nodelabels.store.op.ReplaceNodeToAttributeLogOp:getOpCode()	0	int	0	2
org.apache.hadoop.yarn.nodelabels.store.op.NodeAttributeMirrorOp:getOpCode()	0	int	0	-1
org.apache.hadoop.yarn.nodelabels.StringAttributeValue:compareForOperation(org.apache.hadoop.yarn.nodelabels.AttributeValue,org.apache.hadoop.yarn.nodelabels.AttributeExpressionOperation)	0	int	0	1
org.apache.hadoop.yarn.nodelabels.StringAttributeValue:compareForOperation(org.apache.hadoop.yarn.nodelabels.AttributeValue,org.apache.hadoop.yarn.nodelabels.AttributeExpressionOperation)	1	int	0	0
org.apache.hadoop.yarn.metrics.DisableEventTypeMetrics:get(java.lang.Enum)	0	long	0	0
org.apache.hadoop.yarn.state.Graph$Edge:sameAs(org.apache.hadoop.yarn.state.Graph$Edge)	0	int	0	1
org.apache.hadoop.yarn.state.Graph$Edge:sameAs(org.apache.hadoop.yarn.state.Graph$Edge)	1	int	0	0
org.apache.hadoop.yarn.event.AsyncDispatcher:isEventThreadWaiting()	0	int	0	1
org.apache.hadoop.yarn.event.AsyncDispatcher:isEventThreadWaiting()	1	int	0	0
org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider:getConfigurationInputStream(org.apache.hadoop.conf.Configuration,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshServiceAclsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeToAttributesPBImpl:getNode()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeToAttributesPBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeToAttributesPBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodesToAttributesMappingRequestPBImpl:getOperation()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodesToAttributesMappingRequestPBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodesToAttributesMappingRequestPBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RunSharedCacheCleanerTaskResponsePBImpl:getAccepted()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshSuperUserGroupsConfigurationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshSuperUserGroupsConfigurationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.AddToClusterNodeLabelsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshQueuesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesResourcesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoveFromClusterNodeLabelsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.CheckForDecommissioningNodesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoveFromClusterNodeLabelsRequestPBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoveFromClusterNodeLabelsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshQueuesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.AddToClusterNodeLabelsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.AddToClusterNodeLabelsRequestPBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesResourcesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.ReplaceLabelsOnNodeRequestPBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.ReplaceLabelsOnNodeRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshAdminAclsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshUserToGroupsMappingsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshClusterMaxPriorityResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshClusterMaxPriorityRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.ReplaceLabelsOnNodeResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshAdminAclsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshServiceAclsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshUserToGroupsMappingsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.CheckForDecommissioningNodesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	0
org.apache.hadoop.yarn.api.resource.PlacementConstraintTransformations$AbstractTransformer:visit(org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression)	0	null	0	null
org.apache.hadoop.yarn.api.resource.PlacementConstraintTransformations$AbstractTransformer:visit(org.apache.hadoop.yarn.api.resource.PlacementConstraint$TimedPlacementConstraint)	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToAttributesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAllResourceTypeInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FailApplicationAttemptResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:getReservationId()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:getStartTime()	0	long	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:getEndTime()	0	long	0	9223372036854775807
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:getIncludeResourceAllocations()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetLabelsToNodesResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetLabelsToNodesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetLabelsToNodesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.NodePublishVolumeResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeAttributesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeAttributesResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToAttributesResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToAttributesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ValidateVolumeCapabilitiesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl:getAMCommand()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationUpdateResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewReservationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.IncreaseContainersResourceResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationDeleteRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeLabelsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeLabelsResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.UpdateApplicationPriorityRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToLabelsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetLocalizationStatusesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationSubmissionRequestPBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationSubmissionRequestPBImpl:getReservationId()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationSubmissionRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ResourceLocalizationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FailApplicationAttemptRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationDeleteResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.UpdateApplicationTimeoutsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ContainerUpdateRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeAttributesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.IncreaseContainersResourceRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAllResourceTypeInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.NodeUnpublishVolumeRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ValidateVolumeCapabilitiesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl:getIncludeApplications()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl:getIncludeChildQueues()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl:getRecursive()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewReservationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl:getFinalApplicationStatus()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RestartContainerResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SignalContainerRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodeLabelsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAttributesToNodesResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAttributesToNodesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.UpdateApplicationPriorityResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.UpdateApplicationTimeoutsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ResourceLocalizationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationUpdateRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReInitializeContainerResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.NodeUnpublishVolumeResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetLocalizationStatusesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetPluginInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.NodePublishVolumeRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ContainerUpdateResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationSubmissionResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetPluginInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToLabelsResponsePBImpl:hashCode()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNodesToLabelsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReInitializeContainerRequestPBImpl:getAutoCommit()	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReInitializeContainerRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAttributesToNodesRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RollbackResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CommitResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.LocalizationStatusPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.CollectorInfoPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl:getKind()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl:getService()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ResourceOptionPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:getStartTime()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:getEndTime()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl:getFile()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl:getScheme()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl:getUserInfo()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl:getHost()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:getContainerSubState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:getCapability()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:getIPs()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:getExposedPorts()	0	java.lang.String	0	
org.apache.hadoop.yarn.api.records.impl.pb.UpdatedContainerPBImpl:getUpdateType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getApplicationResourceUsageReport()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getTrackingUrl()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getOriginalTrackingUrl()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getYarnApplicationState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getHost()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getUser()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getFinalApplicationStatus()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getApplicationType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getLogAggregationStatus()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getAppNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl:getAmNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeInfoPBImpl:getAttributeKey()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeInfoPBImpl:getAttributeType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeInfoPBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeInfoPBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getIncludePattern()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getExcludePattern()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getRolledLogsIncludePattern()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getRolledLogsExcludePattern()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getLogAggregationPolicyClassName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LogAggregationContextPBImpl:getLogAggregationPolicyParameters()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerErrorPBImpl:getCurrentContainerVersion()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:getNumContainers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:getNodeState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerRetryContextPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerRetryContextPBImpl:getMaxRetries()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerRetryContextPBImpl:getRetryInterval()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerRetryContextPBImpl:getFailuresValidityInterval()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.SchedulingRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.SchedulingRequestPBImpl:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:getResourceName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:getNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:getNodeHttpAddress()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:getHost()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:getTrackingUrl()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:getOriginalTrackingUrl()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:getYarnApplicationAttemptState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeLabelPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeLabelPBImpl:getName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl:getCapacity()	0	float	0	-1.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl:getCurrentCapacity()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl:getMaximumCapacity()	0	float	0	-1.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl:getQueueState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:getConcurrency()	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:getDuration()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:getApplicationName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:getApplicationType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:getNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:getAMContainerResourceRequest()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ExecutionTypeRequestPBImpl:getExecutionType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsSubmitted()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsRunning()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsPending()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsCompleted()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsKilled()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumAppsFailed()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getNumActiveUsers()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getAvailableMemoryMB()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getAllocatedMemoryMB()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getPendingMemoryMB()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getReservedMemoryMB()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getAvailableVCores()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getAllocatedVCores()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getPendingVCores()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getReservedVCores()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getPendingContainers()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getAllocatedContainers()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.QueueStatisticsPBImpl:getReservedContainers()	0	long	0	-1
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeKeyPBImpl:getAttributeName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeKeyPBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeKeyPBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeKeyPBImpl:compare(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributeKeyPBImpl:compare(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:getDiagnosticsInfo()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:getContainerState()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:getLogUrl()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:getNodeHttpAddress()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl:getType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl:getVisibility()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl:getPattern()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl:getShouldBeUploadedToSharedCache()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumDecommissioningNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumDecommissionedNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumActiveNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumLostNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumUnhealthyNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumRebootedNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl:getNumShutdownNodeManagers()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.RejectedSchedulingRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.RejectedSchedulingRequestPBImpl:getReason()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ReservationDefinitionPBImpl:getArrival()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationDefinitionPBImpl:getDeadline()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationDefinitionPBImpl:getReservationName()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ReservationDefinitionPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationDefinitionPBImpl:getRecurrenceExpression()	0	java.lang.String	0	0
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:getContainerVersion()	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:getExecutionType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:getContainerUpdateType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:getCapacity()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:getAbsoluteCapacity()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:getMaxCapacity()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:getAbsoluteMaxCapacity()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:getMaxAMPercentage()	0	float	0	0.0
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:getMemorySeconds()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:getVcoreSeconds()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:getPreemptedMemorySeconds()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:getPreemptedVcoreSeconds()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestsPBImpl:getInterpreter()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestsPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeToAttributeValuePBImpl:getHostname()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeToAttributeValuePBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeToAttributeValuePBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeToAttributeValuePBImpl:compare(java.lang.Object,java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeToAttributeValuePBImpl:compare(java.lang.Object,java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributePBImpl:getAttributeKey()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributePBImpl:getAttributeValue()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributePBImpl:getAttributeType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributePBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.api.records.impl.pb.NodeAttributePBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationAllocationStatePBImpl:getAcceptanceTime()	0	long	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ReservationAllocationStatePBImpl:getUser()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ReservationAllocationStatePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationTimeoutPBImpl:getTimeoutType()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationTimeoutPBImpl:getExpiryTime()	0	null	0	null
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationTimeoutPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.api.records.impl.pb.ProtoBase:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.webapp.WebApps$Builder:hasCSRFEnabled(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.webapp.WebApps$Builder:hasCSRFEnabled(java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:isInline(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:needsEscaping(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:needsEscaping(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:isInline(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:needsEscaping(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:needsEscaping(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.webapp.WebApp:port()	0	int	0	-1
org.apache.hadoop.yarn.webapp.WebApp:getPrefix(java.lang.String)	0	java.lang.String	0	/
org.apache.hadoop.yarn.webapp.example.MyApp:anyAPI()	0	java.lang.String	0	anything, really!
org.apache.hadoop.yarn.webapp.Router:methodAllowed(org.apache.hadoop.yarn.webapp.WebApp$HTTP,org.apache.hadoop.yarn.webapp.Router$Dest)	0	int	0	1
org.apache.hadoop.yarn.webapp.Router:methodAllowed(org.apache.hadoop.yarn.webapp.WebApp$HTTP,org.apache.hadoop.yarn.webapp.Router$Dest)	1	int	0	0
org.apache.hadoop.yarn.webapp.Router:prefixMatches(org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.webapp.Router:prefixMatches(org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.webapp.Router:isGoodMatch(org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.webapp.Router:isGoodMatch(org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.webapp.Router:defaultPrefix(java.lang.String,java.lang.String)	0	java.lang.String	0	/
org.apache.hadoop.yarn.webapp.Router:resolveAction(org.apache.hadoop.yarn.webapp.WebApp$HTTP,org.apache.hadoop.yarn.webapp.Router$Dest,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:getApplicationLogURL(org.apache.hadoop.yarn.api.records.ApplicationId)	0	null	0	null
org.apache.hadoop.yarn.webapp.view.ErrorPage:errorDetails()	0	java.lang.String	0	No exception was thrown.
org.apache.hadoop.yarn.webapp.DefaultWrapperServlet$1:getServletPath()	0	java.lang.String	0	
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getRunningLogURL(java.lang.String,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getAggregatedLogURL(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getHttpSchemePrefix(org.apache.hadoop.conf.Configuration)	0	java.lang.String	0	https://
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getHttpSchemePrefix(org.apache.hadoop.conf.Configuration)	1	java.lang.String	0	http://
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getSupportedLogContentType(java.lang.String)	0	java.lang.String	0	text/plain
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getSupportedLogContentType(java.lang.String)	1	java.lang.String	0	application/octet-stream
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getDefaultLogContentType()	0	java.lang.String	0	text/plain
org.apache.hadoop.yarn.webapp.Dispatcher:setCookieParams(org.apache.hadoop.yarn.webapp.Controller$RequestContext,javax.servlet.http.HttpServletRequest)	0	int	0	0
org.apache.hadoop.yarn.logaggregation.LogAggregationUtils$1:hasNext()	0	int	0	1
org.apache.hadoop.yarn.logaggregation.LogAggregationUtils$1:hasNext()	1	int	0	0
org.apache.hadoop.yarn.logaggregation.ContainerLogFileInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.ContainerLogFileInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue:lambda$getFileCandidates$0(java.io.File)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogValue:lambda$getFileCandidates$0(java.io.File)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	0	int	0	-1
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	1	int	0	1
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$2:compare(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileStatus)	2	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$4:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$5:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getRollOverLogMaxSize(org.apache.hadoop.conf.Configuration)	0	long	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:isRollover(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:isRollover(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$3:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$2:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController$1:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:belongsToAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:lambda$cleanOldLogs$0(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:lambda$cleanOldLogs$0(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory:validateAggregatedFileControllerName(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$3:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$1:run()	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationHtmlBlock:verifyAndParseParameters(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)	0	null	0	null
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationHtmlBlock:checkAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.util.Map,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationHtmlBlock:checkAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.util.Map,java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogKey:hashCode()	0	int	0	0
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask:isApplicationTerminated(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.ApplicationClientProtocol)	0	int	0	1
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogType(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,boolean)	0	int	0	-1
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogType(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,boolean)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogTypeWithoutNodeId(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest)	0	int	0	-1
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAContainerLogsForLogTypeWithoutNodeId(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAllContainersLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest)	0	int	0	-1
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:dumpAllContainersLogs(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream)	0	int	0	-1
org.apache.hadoop.yarn.logaggregation.LogCLIHelpers:printAContainerLogMetadata(org.apache.hadoop.yarn.logaggregation.ContainerLogsRequest,java.io.PrintStream,java.io.PrintStream)	1	int	0	0
org.apache.hadoop.yarn.logaggregation.LogAggregationWebUtils:verifyAndGetContainerId(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.logaggregation.LogAggregationWebUtils:verifyAndGetNodeId(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:next(org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogKey)	0	null	0	null
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:readContainerLogsForALogType(java.io.DataInputStream,java.io.PrintStream,long,java.util.List,long)	0	int	0	0
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:readContainerLogsForALogType(java.io.DataInputStream,java.io.PrintStream,long,java.util.List,long)	1	int	0	-1
org.apache.hadoop.yarn.logaggregation.LogAggregationUtils:getBucketSuffix()	0	java.lang.String	0	bucket-
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogRetentionContext:isDisabled()	0	int	0	1
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogRetentionContext:isDisabled()	1	int	0	0
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogRetentionContext:shouldRetainLog()	0	int	0	1
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogRetentionContext:shouldRetainLog()	1	int	0	0
org.apache.hadoop.yarn.security.AMRMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.AMRMTokenSelector:checkService(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)	0	int	0	0
org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.SchedulerSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.SchedulerSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.SchedulerSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.AMRMTokenIdentifier:getApplicationAttemptId()	0	null	0	null
org.apache.hadoop.yarn.security.AMRMTokenIdentifier:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.security.DockerCredentialTokenIdentifier:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.security.PrivilegedEntity:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.security.PrivilegedEntity:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.security.NMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getContainerID()	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getResource()	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getPriority()	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getLogAggregationContext()	0	null	0	null
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getVersion()	0	int	0	0
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getNodeLabelExpression()	0	java.lang.String	0	
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.ContainerManagerSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.NMTokenIdentifier:getApplicationAttemptId()	0	null	0	null
org.apache.hadoop.yarn.security.NMTokenIdentifier:getNodeId()	0	null	0	null
org.apache.hadoop.yarn.security.NMTokenIdentifier:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.security.admin.AdminSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.admin.AdminSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	yarn.resourcemanager.principal
org.apache.hadoop.yarn.security.admin.AdminSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.yarn.security.admin.AdminSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.admin.AdminSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector:checkService(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)	0	int	0	0
org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo$2:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier$Renewer:getRmClient(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	yarn.resourcemanager.principal
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier:getApplicationAttemptID()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier:getUser()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)	0	null	0	null
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer:isManaged(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientRMSecurityInfo$2:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	yarn.timeline-service.principal
org.apache.hadoop.yarn.security.client.ClientTimelineSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isInvalidDivisor(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isInvalidDivisor(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isAnyMajorResourceZeroOrNegative(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isAnyMajorResourceZeroOrNegative(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isAnyMajorResourceAboveZero(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:isAnyMajorResourceAboveZero(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.ResourceCalculator:divideAndCeil(int,int)	0	int	0	0
org.apache.hadoop.yarn.util.resource.ResourceCalculator:divideAndCeil(int,float)	0	int	0	0
org.apache.hadoop.yarn.util.resource.ResourceCalculator:divideAndCeil(long,long)	0	long	0	0
org.apache.hadoop.yarn.util.resource.ResourceCalculator:divideAndCeil(long,float)	0	long	0	0
org.apache.hadoop.yarn.util.resource.ResourceCalculator:divideSafelyAsFloat(long,long)	0	float	0	0.0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:compare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,boolean)	0	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:calculateSharesForTwoMandatoryResources(org.apache.hadoop.yarn.api.records.ResourceInformation[],org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,double[],double[])	0	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:calculateSharesForTwoMandatoryResources(org.apache.hadoop.yarn.api.records.ResourceInformation[],org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,double[],double[])	1	int	0	-1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:calculateSharesForTwoMandatoryResources(org.apache.hadoop.yarn.api.records.ResourceInformation[],org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,double[],double[])	2	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:calculateShare(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.ResourceInformation)	0	double	0	Infinity
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:computeAvailableContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	long	0	2147483647
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isInvalidDivisor(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isInvalidDivisor(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isAnyMajorResourceZeroOrNegative(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isAnyMajorResourceZeroOrNegative(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isAnyMajorResourceAboveZero(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:isAnyMajorResourceAboveZero(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:lambda$getInsufficientResourceNames$0(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int)	0	int	0	1
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:lambda$getInsufficientResourceNames$0(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,int)	1	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:lessThan(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.Resources:lessThan(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:lessThanOrEqual(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.Resources:lessThanOrEqual(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:greaterThan(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.Resources:greaterThan(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:greaterThanOrEqual(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.util.resource.Resources:greaterThanOrEqual(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.util.resource.Resources:fitsIn(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender:requiresLayout()	0	int	0	0
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:isAvailable()	0	int	0	0
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:isAvailable()	1	int	0	1
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:checkPidPgrpidForMatch(java.lang.String,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:getVirtualMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:getRssMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:getCumulativeCpuTime()	0	long	0	-1
org.apache.hadoop.yarn.util.FSDownload$2:run()	0	null	0	null
org.apache.hadoop.yarn.util.ConverterUtils:toString(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.util.LRUCacheHashMap:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.util.LRUCacheHashMap:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree:getVirtualMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree:getRssMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree:getCumulativeCpuTime()	0	long	0	-1
org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree:getCpuUsagePercent()	0	float	0	-1.0
org.apache.hadoop.yarn.util.FSDownload:isPublic(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache)	0	int	0	0
org.apache.hadoop.yarn.util.FSDownload:isPublic(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache)	1	int	0	1
org.apache.hadoop.yarn.util.FSDownload:checkPublicPermsForAll(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)	0	int	0	0
org.apache.hadoop.yarn.util.FSDownload:checkPublicPermsForAll(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)	1	int	0	1
org.apache.hadoop.yarn.util.FSDownload:ancestorsHaveExecutePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache)	0	int	0	0
org.apache.hadoop.yarn.util.FSDownload:ancestorsHaveExecutePermissions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache)	1	int	0	1
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:isAvailable()	0	int	0	0
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:isAvailable()	1	int	0	1
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:checkPidPgrpidForMatch()	0	int	0	1
org.apache.hadoop.yarn.util.Apps:shouldCountTowardsNodeBlacklisting(int)	0	int	0	1
org.apache.hadoop.yarn.util.Apps:shouldCountTowardsNodeBlacklisting(int)	1	int	0	0
org.apache.hadoop.yarn.util.Apps:isApplicationFinalState(org.apache.hadoop.yarn.api.records.YarnApplicationState)	0	int	0	1
org.apache.hadoop.yarn.util.Apps:isApplicationFinalState(org.apache.hadoop.yarn.api.records.YarnApplicationState)	1	int	0	0
org.apache.hadoop.yarn.util.FSDownload$3:run()	0	null	0	null
org.apache.hadoop.yarn.util.timeline.TimelineUtils:shortenFlowName(java.lang.String,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.util.timeline.TimelineEntityV2Converter:getAverageValue(java.util.Collection)	0	long	0	0
org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender$PurgeElement:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender$PurgeElement:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.util.Times:elapsed(long,long,boolean)	0	long	0	-1
org.apache.hadoop.yarn.util.Times:format(long)	0	java.lang.String	0	N/A
org.apache.hadoop.yarn.util.AuxiliaryServiceHelper:getServiceDataFromEnv(java.lang.String,java.util.Map)	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl$EntitiesHolder$1:call()	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter$1:shouldRetryOn(java.lang.Exception)	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter$1:shouldRetryOn(java.lang.Exception)	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$3:run()	0	null	0	null
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache$1:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$AttemptDirCache$1:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFD:writerClosed()	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFD:writerClosed()	1	int	0	0
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineClientRetryOpForOperateDelegationToken:shouldRetryOn(java.lang.Exception)	0	int	0	1
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineClientRetryOpForOperateDelegationToken:shouldRetryOn(java.lang.Exception)	1	int	0	0
org.apache.hadoop.yarn.client.AMRMClientUtils:parseExpectedResponseIdFromException(java.lang.String)	0	int	0	-1
org.apache.hadoop.yarn.client.AMRMClientUtils:isMatchingSchedulingRequests(org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.api.records.SchedulingRequest)	0	int	0	1
org.apache.hadoop.yarn.client.AMRMClientUtils:isMatchingSchedulingRequests(org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.api.records.SchedulingRequest)	1	int	0	0
org.apache.hadoop.yarn.client.RMHAServiceTarget:getFencer()	0	null	0	null
csi.v0.Csi$ControllerGetCapabilitiesResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$NodeStageVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeStageVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$GetPluginCapabilitiesRequest:isInitialized()	0	int	0	1
csi.v0.Csi$GetPluginCapabilitiesRequest:isInitialized()	1	int	0	0
csi.v0.Csi$GetPluginCapabilitiesRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetPluginCapabilitiesRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeServiceCapability$RPC:isInitialized()	0	int	0	1
csi.v0.Csi$NodeServiceCapability$RPC:isInitialized()	1	int	0	0
csi.v0.Csi$NodeServiceCapability$RPC:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeServiceCapability$RPC:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$CapacityRange:isInitialized()	0	int	0	1
csi.v0.Csi$CapacityRange:isInitialized()	1	int	0	0
csi.v0.Csi$CapacityRange:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$CapacityRange:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginInfoResponse:isInitialized()	0	int	0	1
csi.v0.Csi$GetPluginInfoResponse:isInitialized()	1	int	0	0
csi.v0.Csi$GetPluginInfoResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetPluginInfoResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ListSnapshotsRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ListSnapshotsRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListSnapshotsRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry$Builder:hasVolume()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry$Builder:hasVolume()	1	int	0	0
csi.v0.Csi$NodeGetInfoRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetInfoRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetInfoRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetInfoRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ValidateVolumeCapabilitiesRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ValidateVolumeCapabilitiesRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerUnpublishVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerUnpublishVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerUnpublishVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerUnpublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerUnpublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeGetIdRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetIdRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetIdRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetIdRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeGetInfoResponse:hasAccessibleTopology()	0	int	0	1
csi.v0.Csi$NodeGetInfoResponse:hasAccessibleTopology()	1	int	0	0
csi.v0.Csi$NodeGetInfoResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetInfoResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetInfoResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetInfoResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeGetInfoRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteSnapshotRequest:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteSnapshotRequest:isInitialized()	1	int	0	0
csi.v0.Csi$DeleteSnapshotRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$DeleteSnapshotRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$Snapshot$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$Snapshot$Builder:hasStatus()	0	int	0	1
csi.v0.Csi$Snapshot$Builder:hasStatus()	1	int	0	0
csi.v0.Csi$CreateSnapshotResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse$Builder:hasSnapshot()	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse$Builder:hasSnapshot()	1	int	0	0
csi.v0.Csi$VolumeContentSource:hasSnapshot()	0	int	0	1
csi.v0.Csi$VolumeContentSource:hasSnapshot()	1	int	0	0
csi.v0.Csi$VolumeContentSource:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeContentSource:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeContentSource:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeContentSource:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ValidateVolumeCapabilitiesResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ValidateVolumeCapabilitiesResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ValidateVolumeCapabilitiesResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeUnstageVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeResponse:hasVolume()	0	int	0	1
csi.v0.Csi$CreateVolumeResponse:hasVolume()	1	int	0	0
csi.v0.Csi$CreateVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$CreateVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$CreateVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeContentSource$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeContentSource$Builder:hasSnapshot()	0	int	0	1
csi.v0.Csi$VolumeContentSource$Builder:hasSnapshot()	1	int	0	0
csi.v0.Csi$ListVolumesResponse$Entry:hasVolume()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry:hasVolume()	1	int	0	0
csi.v0.Csi$ListVolumesResponse$Entry:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry:isInitialized()	1	int	0	0
csi.v0.Csi$ListVolumesResponse$Entry:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Entry:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$SnapshotStatus$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry:hasSnapshot()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry:hasSnapshot()	1	int	0	0
csi.v0.Csi$ListSnapshotsResponse$Entry:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry:isInitialized()	1	int	0	0
csi.v0.Csi$ListSnapshotsResponse$Entry:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ListSnapshotsResponse$Entry$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry$Builder:hasSnapshot()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Entry$Builder:hasSnapshot()	1	int	0	0
csi.v0.Csi$VolumeCapability$BlockVolume:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability$BlockVolume:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeCapability$BlockVolume:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeCapability$BlockVolume:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerPublishVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest$Builder:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest$Builder:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$NodeStageVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeStageVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeStageVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeStageVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ListVolumesRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ProbeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ProbeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ProbeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ProbeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeStageVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotRequest:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotRequest:isInitialized()	1	int	0	0
csi.v0.Csi$CreateSnapshotRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$CreateSnapshotRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginInfoRequest:isInitialized()	0	int	0	1
csi.v0.Csi$GetPluginInfoRequest:isInitialized()	1	int	0	0
csi.v0.Csi$GetPluginInfoRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetPluginInfoRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerPublishVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeServiceCapability:hasRpc()	0	int	0	1
csi.v0.Csi$NodeServiceCapability:hasRpc()	1	int	0	0
csi.v0.Csi$NodeServiceCapability:isInitialized()	0	int	0	1
csi.v0.Csi$NodeServiceCapability:isInitialized()	1	int	0	0
csi.v0.Csi$NodeServiceCapability:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeServiceCapability:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeContentSource$SnapshotSource:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeContentSource$SnapshotSource:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeContentSource$SnapshotSource:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeContentSource$SnapshotSource:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeCapability$MountVolume$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityRequest$Builder:hasAccessibleTopology()	0	int	0	1
csi.v0.Csi$GetCapacityRequest$Builder:hasAccessibleTopology()	1	int	0	0
csi.v0.Csi$1:assignDescriptors(com.google.protobuf.Descriptors$FileDescriptor)	0	null	0	null
csi.v0.Csi$ProbeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerPublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeStageVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest$Builder:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$NodeStageVolumeRequest$Builder:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$CapacityRange$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteSnapshotResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetIdResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetIdResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetIdResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetIdResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerGetCapabilitiesRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$Snapshot:hasStatus()	0	int	0	1
csi.v0.Csi$Snapshot:hasStatus()	1	int	0	0
csi.v0.Csi$Snapshot:isInitialized()	0	int	0	1
csi.v0.Csi$Snapshot:isInitialized()	1	int	0	0
csi.v0.Csi$Snapshot:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$Snapshot:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$Topology:isInitialized()	0	int	0	1
csi.v0.Csi$Topology:isInitialized()	1	int	0	0
csi.v0.Csi$Topology:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$Topology:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$DeleteVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$DeleteVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$DeleteVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginInfoRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability:hasRpc()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability:hasRpc()	1	int	0	0
csi.v0.Csi$ControllerServiceCapability:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerServiceCapability:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerServiceCapability:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$CreateVolumeRequest:hasCapacityRange()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest:hasCapacityRange()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest:hasVolumeContentSource()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest:hasVolumeContentSource()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest:hasAccessibilityRequirements()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest:hasAccessibilityRequirements()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$CreateVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeGetCapabilitiesResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetCapabilitiesResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeCapability:hasBlock()	0	int	0	1
csi.v0.Csi$VolumeCapability:hasBlock()	1	int	0	0
csi.v0.Csi$VolumeCapability:hasMount()	0	int	0	1
csi.v0.Csi$VolumeCapability:hasMount()	1	int	0	0
csi.v0.Csi$VolumeCapability:hasAccessMode()	0	int	0	1
csi.v0.Csi$VolumeCapability:hasAccessMode()	1	int	0	0
csi.v0.Csi$VolumeCapability:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeCapability:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeCapability:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeGetIdRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest$Builder:hasCapacityRange()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest$Builder:hasCapacityRange()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest$Builder:hasVolumeContentSource()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest$Builder:hasVolumeContentSource()	1	int	0	0
csi.v0.Csi$CreateVolumeRequest$Builder:hasAccessibilityRequirements()	0	int	0	1
csi.v0.Csi$CreateVolumeRequest$Builder:hasAccessibilityRequirements()	1	int	0	0
csi.v0.Csi$GetPluginCapabilitiesResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse:hasSnapshot()	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse:hasSnapshot()	1	int	0	0
csi.v0.Csi$CreateSnapshotResponse:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse:isInitialized()	1	int	0	0
csi.v0.Csi$CreateSnapshotResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$CreateSnapshotResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ProbeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ProbeResponse$Builder:hasReady()	0	int	0	1
csi.v0.Csi$ProbeResponse$Builder:hasReady()	1	int	0	0
csi.v0.Csi$PluginCapability$Service$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$PluginCapability$Service:isInitialized()	0	int	0	1
csi.v0.Csi$PluginCapability$Service:isInitialized()	1	int	0	0
csi.v0.Csi$PluginCapability$Service:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$PluginCapability$Service:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$Topology$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodePublishVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$TopologyRequirement:isInitialized()	0	int	0	1
csi.v0.Csi$TopologyRequirement:isInitialized()	1	int	0	0
csi.v0.Csi$TopologyRequirement:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$TopologyRequirement:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetCapacityResponse:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityResponse:isInitialized()	1	int	0	0
csi.v0.Csi$GetCapacityResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetCapacityResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerGetCapabilitiesResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerGetCapabilitiesResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerGetCapabilitiesResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerGetCapabilitiesResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeCapability$AccessMode:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability$AccessMode:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeCapability$AccessMode:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeCapability$AccessMode:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeUnpublishVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnpublishVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeUnpublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeUnpublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ListVolumesRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ListVolumesRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListVolumesRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$DeleteVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$DeleteVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$DeleteVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$Volume$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$Volume$Builder:hasContentSource()	0	int	0	1
csi.v0.Csi$Volume$Builder:hasContentSource()	1	int	0	0
csi.v0.Csi$NodeServiceCapability$RPC$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnpublishVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnpublishVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeUnpublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeUnpublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeUnpublishVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeServiceCapability$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeServiceCapability$Builder:hasRpc()	0	int	0	1
csi.v0.Csi$NodeServiceCapability$Builder:hasRpc()	1	int	0	0
csi.v0.Csi$NodeUnstageVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$RPC$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetIdResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$ControllerPublishVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerPublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerPublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$DeleteVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$TopologyRequirement$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ListSnapshotsResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListSnapshotsResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginCapabilitiesResponse:isInitialized()	0	int	0	1
csi.v0.Csi$GetPluginCapabilitiesResponse:isInitialized()	1	int	0	0
csi.v0.Csi$GetPluginCapabilitiesResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetPluginCapabilitiesResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$PluginCapability$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$PluginCapability$Builder:hasService()	0	int	0	1
csi.v0.Csi$PluginCapability$Builder:hasService()	1	int	0	0
csi.v0.Csi$NodeGetInfoResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetInfoResponse$Builder:hasAccessibleTopology()	0	int	0	1
csi.v0.Csi$NodeGetInfoResponse$Builder:hasAccessibleTopology()	1	int	0	0
csi.v0.Csi$VolumeCapability$BlockVolume$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeGetCapabilitiesRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodePublishVolumeRequest:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$NodePublishVolumeRequest:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$NodePublishVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodePublishVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodePublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodePublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeCapability$AccessMode$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$SnapshotStatus:isInitialized()	0	int	0	1
csi.v0.Csi$SnapshotStatus:isInitialized()	1	int	0	0
csi.v0.Csi$SnapshotStatus:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$SnapshotStatus:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$DeleteSnapshotResponse:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteSnapshotResponse:isInitialized()	1	int	0	0
csi.v0.Csi$DeleteSnapshotResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$DeleteSnapshotResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginCapabilitiesRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeGetCapabilitiesResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ListVolumesResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ListVolumesResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ListVolumesResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$GetPluginInfoResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$RPC:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$RPC:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerServiceCapability$RPC:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$RPC:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerUnpublishVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerUnpublishVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerUnpublishVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerUnpublishVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$CreateVolumeResponse$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateVolumeResponse$Builder:hasVolume()	0	int	0	1
csi.v0.Csi$CreateVolumeResponse$Builder:hasVolume()	1	int	0	0
csi.v0.Csi$Volume:hasContentSource()	0	int	0	1
csi.v0.Csi$Volume:hasContentSource()	1	int	0	0
csi.v0.Csi$Volume:isInitialized()	0	int	0	1
csi.v0.Csi$Volume:isInitialized()	1	int	0	0
csi.v0.Csi$Volume:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$Volume:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodePublishVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodePublishVolumeRequest$Builder:hasVolumeCapability()	0	int	0	1
csi.v0.Csi$NodePublishVolumeRequest$Builder:hasVolumeCapability()	1	int	0	0
csi.v0.Csi$NodeGetCapabilitiesRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodePublishVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodePublishVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodePublishVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodePublishVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeUnstageVolumeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnstageVolumeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$NodeUnstageVolumeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeUnstageVolumeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ProbeResponse:hasReady()	0	int	0	1
csi.v0.Csi$ProbeResponse:hasReady()	1	int	0	0
csi.v0.Csi$ProbeResponse:isInitialized()	0	int	0	1
csi.v0.Csi$ProbeResponse:isInitialized()	1	int	0	0
csi.v0.Csi$ProbeResponse:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ProbeResponse:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$VolumeCapability$MountVolume:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability$MountVolume:isInitialized()	1	int	0	0
csi.v0.Csi$VolumeCapability$MountVolume:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$VolumeCapability$MountVolume:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$DeleteVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerGetCapabilitiesRequest:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerGetCapabilitiesRequest:isInitialized()	1	int	0	0
csi.v0.Csi$ControllerGetCapabilitiesRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$ControllerGetCapabilitiesRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$NodeUnpublishVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$DeleteSnapshotRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityRequest:hasAccessibleTopology()	0	int	0	1
csi.v0.Csi$GetCapacityRequest:hasAccessibleTopology()	1	int	0	0
csi.v0.Csi$GetCapacityRequest:isInitialized()	0	int	0	1
csi.v0.Csi$GetCapacityRequest:isInitialized()	1	int	0	0
csi.v0.Csi$GetCapacityRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$GetCapacityRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerUnpublishVolumeRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$CreateSnapshotRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$PluginCapability:hasService()	0	int	0	1
csi.v0.Csi$PluginCapability:hasService()	1	int	0	0
csi.v0.Csi$PluginCapability:isInitialized()	0	int	0	1
csi.v0.Csi$PluginCapability:isInitialized()	1	int	0	0
csi.v0.Csi$PluginCapability:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$PluginCapability:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ListSnapshotsRequest$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnstageVolumeRequest:isInitialized()	0	int	0	1
csi.v0.Csi$NodeUnstageVolumeRequest:isInitialized()	1	int	0	0
csi.v0.Csi$NodeUnstageVolumeRequest:equals(java.lang.Object)	0	int	0	1
csi.v0.Csi$NodeUnstageVolumeRequest:equals(java.lang.Object)	1	int	0	0
csi.v0.Csi$ControllerServiceCapability$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$Builder:hasRpc()	0	int	0	1
csi.v0.Csi$ControllerServiceCapability$Builder:hasRpc()	1	int	0	0
csi.v0.Csi$VolumeContentSource$SnapshotSource$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability$Builder:isInitialized()	0	int	0	1
csi.v0.Csi$VolumeCapability$Builder:hasBlock()	0	int	0	1
csi.v0.Csi$VolumeCapability$Builder:hasBlock()	1	int	0	0
csi.v0.Csi$VolumeCapability$Builder:hasMount()	0	int	0	1
csi.v0.Csi$VolumeCapability$Builder:hasMount()	1	int	0	0
csi.v0.Csi$VolumeCapability$Builder:hasAccessMode()	0	int	0	1
csi.v0.Csi$VolumeCapability$Builder:hasAccessMode()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:hasTokenIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:hasTokenIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:hasRenewDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:hasRenewDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto$Builder:hasTokenIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto$Builder:hasTokenIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto$Builder:hasRenewDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$TimelineDelegationTokenIdentifierDataProto$Builder:hasRenewDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerTimelineServerRecoveryProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:cancelDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.CancelDelegationTokenRequest)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:getDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.GetDelegationTokenRequest)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:renewDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.RenewDelegationTokenRequest)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:getApplication(org.apache.hadoop.yarn.api.records.ApplicationId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:getAMContainer(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl:getApplicationName()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl:getApplicationType()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl:getUser()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl:getQueue()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl:getTrackingURL()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl:getDiagnosticsInfo()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl:getFinalApplicationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl:getYarnApplicationAttemptState()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl:getDiagnosticsInfo()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl:getContainerState()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl:getDiagnosticsInfo()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl:getFinalApplicationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl:getYarnApplicationState()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl:getHost()	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore:getAMContainer(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore:getContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileReader:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileReader:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:getAMContainer(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.EntityIdentifier:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityIdentifier:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timeline.RollingLevelDB$RollingPeriod$5:dateFormat()	0	java.lang.String	0	yyyy-MM-dd-HH-mm
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getStartTime(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:checkStartTimeInDb(org.apache.hadoop.yarn.server.timeline.EntityIdentifier,java.lang.Long)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:getTimelineDomain(org.iq80.leveldb.DBIterator,java.lang.String,byte[])	0	null	0	null
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getStartTime(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:checkStartTimeInDb(org.apache.hadoop.yarn.server.timeline.EntityIdentifier,java.lang.Long)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:getTimelineDomain(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String,byte[])	0	null	0	null
org.apache.hadoop.yarn.server.timeline.RollingLevelDB$RollingPeriod$3:dateFormat()	0	java.lang.String	0	yyyy-MM-dd-HH
org.apache.hadoop.yarn.server.timeline.RollingLevelDB$RollingPeriod$4:dateFormat()	0	java.lang.String	0	yyyy-MM-dd-HH
org.apache.hadoop.yarn.server.timeline.GenericObjectMapper:read(byte[],int)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.RollingLevelDB$RollingPeriod$1:dateFormat()	0	java.lang.String	0	yyyy-MM-dd
org.apache.hadoop.yarn.server.timeline.RollingLevelDB$RollingPeriod$2:dateFormat()	0	java.lang.String	0	yyyy-MM-dd-HH
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore$KeyValueBasedTimelineStoreUtils:matchFilter(java.util.Map,org.apache.hadoop.yarn.server.timeline.NameValuePair)	0	int	0	0
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore$KeyValueBasedTimelineStoreUtils:matchFilter(java.util.Map,org.apache.hadoop.yarn.server.timeline.NameValuePair)	1	int	0	1
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore$KeyValueBasedTimelineStoreUtils:matchPrimaryFilter(java.util.Map,org.apache.hadoop.yarn.server.timeline.NameValuePair)	0	int	0	0
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:getDBForStartTime(long)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parseArrayStr(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parsePairStr(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parsePairsStr(java.lang.String,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parseFieldsStr(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parseLongStr(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:parseStr(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilterInitializer:getPrefix()	0	java.lang.String	0	yarn.timeline-service.http-cross-origin.
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:loadDomainFromTimelineStore(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)	0	int	0	1
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)	1	int	0	0
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)	0	int	0	1
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)	1	int	0	0
org.apache.hadoop.yarn.server.timeline.util.LeveldbUtils:prefixMatches(byte[],int,byte[])	0	int	0	0
org.apache.hadoop.yarn.server.timeline.util.LeveldbUtils:prefixMatches(byte[],int,byte[])	1	int	0	1
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getDomain(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:getDomains(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAclImpl:check(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity)	0	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:hasPolicyConfiguration()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:hasPolicyConfiguration()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:hasPolicyConfiguration()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:hasPolicyConfiguration()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto$Builder:hasHomeSubCluster()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto$Builder:hasHomeSubCluster()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasType()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasType()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasParams()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto$Builder:hasParams()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto$Builder:hasHomeSubCluster()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto$Builder:hasHomeSubCluster()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasLastHeartBeat()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasLastHeartBeat()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:hasFilterInactiveSubclusters()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:hasFilterInactiveSubclusters()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasAMRMServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasAMRMServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasClientRMServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasClientRMServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasRMAdminServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasRMAdminServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasRMWebServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasRMWebServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasLastHeartBeat()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasLastHeartBeat()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasLastStartTime()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasLastStartTime()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:hasHomeSubCluster()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:hasHomeSubCluster()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto$Builder:hasSubClusterInfo()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto$Builder:hasSubClusterInfo()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:hasHomeSubCluster()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:hasHomeSubCluster()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$ApplicationHomeSubClusterProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterIdProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPoliciesConfigurationsResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto$Builder:hasSubClusterInfo()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto$Builder:hasSubClusterInfo()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto$Builder:hasFilterInactiveSubclusters()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClustersInfoRequestProto$Builder:hasFilterInactiveSubclusters()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto$Builder:hasPolicyConfiguration()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SetSubClusterPolicyConfigurationRequestProto$Builder:hasPolicyConfiguration()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto$Builder:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationRequestProto$Builder:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto$Builder:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto$Builder:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto$Builder:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$UpdateApplicationHomeSubClusterRequestProto$Builder:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasAMRMServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasAMRMServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasClientRMServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasClientRMServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasRMAdminServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasRMAdminServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasRMWebServiceAddress()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasRMWebServiceAddress()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasLastHeartBeat()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasLastHeartBeat()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasLastStartTime()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasLastStartTime()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterInfoProto$Builder:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto$Builder:hasPolicyConfiguration()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterPolicyConfigurationResponseProto$Builder:hasPolicyConfiguration()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasLastHeartBeat()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasLastHeartBeat()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasCapability()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:hasCapability()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterHeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto$Builder:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto$Builder:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:hasSubClusterInfo()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:hasSubClusterInfo()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto$Builder:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoRequestProto$Builder:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto$Builder:hasSubClusterId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto$Builder:hasSubClusterId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto$Builder:hasState()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterDeregisterRequestProto$Builder:hasState()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterRegisterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationsHomeSubClusterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$AddApplicationHomeSubClusterRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:hasAppSubclusterMap()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:hasAppSubclusterMap()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetApplicationHomeSubClusterResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$DeleteApplicationHomeSubClusterResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:hasSubClusterInfo()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:hasSubClusterInfo()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$GetSubClusterInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasQueue()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasQueue()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasType()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasType()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasParams()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:hasParams()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterPolicyConfigurationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto$Builder:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto$Builder:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto$Builder:hasBytes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto$Builder:hasBytes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasHttpPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasHttpPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNmVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNmVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasPhysicalResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasPhysicalResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeAttributes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeAttributes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:hasNodeStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasRackName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasRackName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasNodePartition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto$Builder:hasNodePartition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasNodeAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasNodeAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasDiagnosticsMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasDiagnosticsMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasRmVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasRmVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasAreNodeLabelsAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasAreNodeLabelsAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasAreNodeAttributesAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:hasAreNodeAttributesAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.SCMUploaderProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto$Builder:hasAccepted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto$Builder:hasAccepted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasIsNodeHealthy()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasIsNodeHealthy()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasHealthReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasHealthReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasLastHealthReportTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:hasLastHealthReportTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:hasUploadable()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:hasUploadable()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto$Builder:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto$Builder:hasUploadable()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadResponseProto$Builder:hasUploadable()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppCollectorAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppCollectorAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppCollectorToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:hasAppCollectorToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNodeAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNodeAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNextHeartBeatInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasNextHeartBeatInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasDiagnosticsMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasDiagnosticsMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasAreNodeLabelsAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasAreNodeLabelsAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasContainerQueuingLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasContainerQueuingLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasAreNodeAttributesAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasAreNodeAttributesAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasTokenSequenceNo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto$Builder:hasTokenSequenceNo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasHttpPort()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasHttpPort()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNmVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNmVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasPhysicalResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasPhysicalResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeAttributes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeAttributes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerRequestProto$Builder:hasNodeStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasLastKnownContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasLastKnownContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasLastKnownNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasLastKnownNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeAttributes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasNodeAttributes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasTokenSequenceNo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:hasTokenSequenceNo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasLogAggregationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasLogAggregationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasRunningOpportContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasRunningOpportContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportMemoryUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportMemoryUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportCoresUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportCoresUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasQueuedOpportContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasQueuedOpportContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasWaitQueueLength()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasWaitQueueLength()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasEstimatedQueueWaitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasEstimatedQueueWaitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportQueueCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:hasOpportQueueCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:hasMajorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:hasMajorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:hasMinorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:hasMinorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:hasCredentialsForApp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:hasCredentialsForApp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasNodeAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasNodeAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasDiagnosticsMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasDiagnosticsMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasRmVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasRmVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasAreNodeLabelsAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasAreNodeLabelsAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasAreNodeAttributesAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterNodeManagerResponseProto$Builder:hasAreNodeAttributesAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasRegisterResponse()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasRegisterResponse()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasMaxContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasMaxContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasMinContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasMinContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasIncrContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasIncrContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasContainerTokenExpiryInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasContainerTokenExpiryInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasContainerIdStart()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto$Builder:hasContainerIdStart()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:hasKeyId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:hasKeyId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:hasBytes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:hasBytes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$MasterKeyProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto$Builder:hasMajorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto$Builder:hasMajorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto$Builder:hasMinorVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$VersionProto$Builder:hasMinorVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:hasAccepted()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:hasAccepted()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNodeAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNodeAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNextHeartBeatInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasNextHeartBeatInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasDiagnosticsMessage()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasDiagnosticsMessage()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasAreNodeLabelsAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasAreNodeLabelsAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasContainerQueuingLimit()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasContainerQueuingLimit()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasAreNodeAttributesAcceptedByRM()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasAreNodeAttributesAcceptedByRM()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasTokenSequenceNo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:hasTokenSequenceNo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeHealthStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeHealthStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasContainersUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasContainersUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasNodeUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasOpportunisticContainersStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto$Builder:hasOpportunisticContainersStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$UnRegisterNodeManagerResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.CollectorNodemanagerProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.ResourceTracker$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeAttributesProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasRunningOpportContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasRunningOpportContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportMemoryUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportMemoryUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportCoresUsed()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportCoresUsed()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasQueuedOpportContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasQueuedOpportContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasWaitQueueLength()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasWaitQueueLength()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasEstimatedQueueWaitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasEstimatedQueueWaitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportQueueCapacity()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$OpportunisticContainersStatusProto$Builder:hasOpportQueueCapacity()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeLabelsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasLastKnownContainerTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasLastKnownContainerTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasLastKnownNmTokenMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasLastKnownNmTokenMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeLabels()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeLabels()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeAttributes()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasNodeAttributes()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasTokenSequenceNo()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeHeartbeatRequestProto$Builder:hasTokenSequenceNo()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppCollectorAddr()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppCollectorAddr()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasRmIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasRmIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppCollectorToken()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$AppCollectorDataProto$Builder:hasAppCollectorToken()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasHttpAddress()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasHttpAddress()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasRackName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasRackName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasNodePartition()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:hasNodePartition()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RemoteNodeProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto$Builder:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderCanUploadRequestProto$Builder:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasPriority()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasPriority()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasCreationTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasCreationTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasNodeLabelExpression()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasNodeLabelExpression()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasExecutionType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasExecutionType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasAllocationRequestId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:hasAllocationRequestId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NMContainerStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto$Builder:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto$Builder:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto$Builder:hasCredentialsForApp()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SystemCredentialsForAppsProto$Builder:hasCredentialsForApp()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasUserId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasUserId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowRunId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto$Builder:hasFlowRunId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasApplicationId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasApplicationId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasLogAggregationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasLogAggregationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$LogAggregationReportProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto$Builder:hasAllocateResponse()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto$Builder:hasAllocateResponse()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:hasFilename()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:hasFilename()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:hasMaxQueueLength()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:hasMaxQueueLength()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:hasMaxQueueWaitTimeInMs()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:hasMaxQueueWaitTimeInMs()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasResponseId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasResponseId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeHealthStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeHealthStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasContainersUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasContainersUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeUtilization()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasNodeUtilization()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasOpportunisticContainersStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:hasOpportunisticContainersStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ReportNewCollectorInfoRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto$Builder:hasAppId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextRequestProto$Builder:hasAppId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeLabelsProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeLabelsProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeLabelsProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$NodeLabelsProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:hasAllocateResponse()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:hasAllocateResponse()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasIsNodeHealthy()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasIsNodeHealthy()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasHealthReport()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasHealthReport()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasLastHealthReportTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeHealthStatusProto$Builder:hasLastHealthReportTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto$Builder:hasAllocateRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto$Builder:hasAllocateRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasRegisterResponse()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasRegisterResponse()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasMaxContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasMaxContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasMinContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasMinContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasIncrContainerResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasIncrContainerResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasContainerTokenExpiryInterval()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasContainerTokenExpiryInterval()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasContainerIdStart()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:hasContainerIdStart()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$RegisterDistributedSchedulingAMResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.DistributedSchedulingAMProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto$Builder:hasResourceKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto$Builder:hasResourceKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto$Builder:hasFilename()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$SCMUploaderNotifyRequestProto$Builder:hasFilename()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:hasAllocateRequest()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:hasAllocateRequest()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$DistributedSchedulingAllocateRequestProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasUserId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasUserId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowRunId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:hasFlowRunId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$GetTimelineCollectorContextResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto$Builder:hasMaxQueueLength()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto$Builder:hasMaxQueueLength()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto$Builder:hasMaxQueueWaitTimeInMs()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos$ContainerQueuingLimitProto$Builder:hasMaxQueueWaitTimeInMs()	1	int	0	0
org.apache.hadoop.yarn.server.metrics.AMRMClientRelayerMetrics$RequestType:toString()	0	java.lang.String	0	G
org.apache.hadoop.yarn.server.metrics.AMRMClientRelayerMetrics$RequestType:toString()	1	java.lang.String	0	O
org.apache.hadoop.yarn.server.metrics.AMRMClientRelayerMetrics$RequestType:toString()	2	java.lang.String	0	P
org.apache.hadoop.yarn.server.metrics.AMRMClientRelayerMetrics$RequestType:toString()	3	java.lang.String	0	D
org.apache.hadoop.yarn.server.RMNMSecurityInfoClass$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.server.RMNMSecurityInfoClass$1:serverPrincipal()	0	java.lang.String	0	yarn.resourcemanager.principal
org.apache.hadoop.yarn.server.RMNMSecurityInfoClass$1:clientPrincipal()	0	java.lang.String	0	yarn.nodemanager.principal
org.apache.hadoop.yarn.server.volume.csi.VolumeId:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.volume.csi.VolumeMetaData:isProvisionedVolume()	0	int	0	1
org.apache.hadoop.yarn.server.volume.csi.VolumeMetaData:isProvisionedVolume()	1	int	0	0
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:getSubClustersForId(long)	0	null	0	null
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:getTotNumLocalizedContainers(long)	0	long	0	0
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:getNumLocalizedContainers(long,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId)	0	long	0	0
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper:isActiveAndEnabled(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId)	0	int	0	0
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy:getLocalityBasedWeighting(long,org.apache.hadoop.yarn.server.federation.store.records.SubClusterId,org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper)	0	float	0	0.0
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy:getPolicyConfigWeighting(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId,org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy$AllocationBookkeeper)	0	float	0	0.0
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:getWeightedRandom(java.util.ArrayList)	0	int	0	-1
org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo:toString()	0	java.lang.String	0	Error serializing to string.
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:isCachingEnabled()	0	int	0	1
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:isCachingEnabled()	1	int	0	0
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade$CacheRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade$CacheRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isUnusable()	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isUnusable()	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isActive()	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isActive()	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isFinal()	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:isFinal()	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.ApplicationHomeSubCluster:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.ApplicationHomeSubCluster:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterIdInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterIdInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterPolicyConfiguration:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterPolicyConfiguration:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.SubClusterId:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.federation.store.records.SubClusterId:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterHeartbeatRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterHeartbeatRequestPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SetSubClusterPolicyConfigurationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterHeartbeatResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.AddApplicationHomeSubClusterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.AddApplicationHomeSubClusterRequestPBImpl:getApplicationHomeSubCluster()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterPolicyConfigurationRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.ApplicationHomeSubClusterPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.ApplicationHomeSubClusterPBImpl:getApplicationId()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetApplicationsHomeSubClusterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.AddApplicationHomeSubClusterResponsePBImpl:getHomeSubCluster()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.AddApplicationHomeSubClusterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.UpdateApplicationHomeSubClusterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.UpdateApplicationHomeSubClusterRequestPBImpl:getApplicationHomeSubCluster()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterInfoRequestPBImpl:getSubClusterId()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterPoliciesConfigurationsResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.DeleteApplicationHomeSubClusterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.DeleteApplicationHomeSubClusterRequestPBImpl:getApplicationId()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetApplicationHomeSubClusterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterRegisterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.DeleteApplicationHomeSubClusterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClustersInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterPolicyConfigurationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterPolicyConfigurationPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClustersInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterInfoPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterInfoPBImpl:getLastStartTime()	0	long	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterDeregisterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetApplicationHomeSubClusterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetApplicationHomeSubClusterResponsePBImpl:getApplicationHomeSubCluster()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetSubClusterPoliciesConfigurationsRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterDeregisterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterDeregisterRequestPBImpl:getSubClusterId()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterDeregisterRequestPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SubClusterRegisterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.GetApplicationsHomeSubClusterRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.SetSubClusterPolicyConfigurationResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.records.impl.pb.UpdateApplicationHomeSubClusterResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest)	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest)	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:getCurrentVersion()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:loadVersion()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getSubCluster(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterInfoRequest)	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getPolicyConfiguration(org.apache.hadoop.yarn.server.federation.store.records.GetSubClusterPolicyConfigurationRequest)	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:getCurrentVersion()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:loadVersion()	0	null	0	null
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:get(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey:compareTo(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	-1
org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey:compareTo(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	1	int	0	0
org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey:compareTo(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	2	int	0	1
org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.scheduler.ResourceRequestSetKey:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.scheduler.ResourceRequestSetKey:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.records.Version:isCompatibleTo(org.apache.hadoop.yarn.server.records.Version)	0	int	0	1
org.apache.hadoop.yarn.server.records.Version:isCompatibleTo(org.apache.hadoop.yarn.server.records.Version)	1	int	0	0
org.apache.hadoop.yarn.server.records.Version:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.records.Version:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus:getVersion()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl:getDiagnosticsMessage()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl:getRMVersion()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl:getNodeAction()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:getContainerState()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:getNodeLabelExpression()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.SCMUploaderCanUploadResponsePBImpl:getUploadable()	0	int	0	1
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.GetTimelineCollectorContextRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.LogAggregationReportPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.LogAggregationReportPBImpl:getLogAggregationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.LogAggregationReportPBImpl:getDiagnosticMessage()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.SCMUploaderNotifyResponsePBImpl:getAccepted()	0	int	0	1
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.GetTimelineCollectorContextResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.GetTimelineCollectorContextResponsePBImpl:getUserId()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.GetTimelineCollectorContextResponsePBImpl:getFlowName()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.GetTimelineCollectorContextResponsePBImpl:getFlowVersion()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.ReportNewCollectorInfoResponsePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.ReportNewCollectorInfoRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoteNodePBImpl:getHttpAddress()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoteNodePBImpl:getRackName()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoteNodePBImpl:getNodePartition()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RemoteNodePBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl:getHttpPort()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl:getNMVersion()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterDistributedSchedulingAMResponsePBImpl:getContainerTokenExpiryInterval()	0	int	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterDistributedSchedulingAMResponsePBImpl:getContainerIdStart()	0	long	0	0
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl:getNodeAction()	0	null	0	null
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl:getDiagnosticsMessage()	0	null	0	null
org.apache.hadoop.yarn.server.api.records.AppCollectorData:happensBefore(org.apache.hadoop.yarn.server.api.records.AppCollectorData,org.apache.hadoop.yarn.server.api.records.AppCollectorData)	0	int	0	0
org.apache.hadoop.yarn.server.api.records.AppCollectorData:happensBefore(org.apache.hadoop.yarn.server.api.records.AppCollectorData,org.apache.hadoop.yarn.server.api.records.AppCollectorData)	1	int	0	1
org.apache.hadoop.yarn.server.api.records.AppCollectorData:isStamped()	0	int	0	1
org.apache.hadoop.yarn.server.api.records.AppCollectorData:isStamped()	1	int	0	0
org.apache.hadoop.yarn.server.api.records.impl.pb.AppCollectorDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.records.impl.pb.AppCollectorDataPBImpl:getRMIdentifier()	0	long	0	-1
org.apache.hadoop.yarn.server.api.records.impl.pb.AppCollectorDataPBImpl:getVersion()	0	long	0	-1
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeHealthStatusPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeHealthStatusPBImpl:getHealthReport()	0	null	0	null
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl:getContainersUtilization()	0	null	0	null
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl:getNodeUtilization()	0	null	0	null
org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl:getOpportunisticContainersStatus()	0	null	0	null
org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.utils.BuilderUtils:parseTokensConf(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext)	0	null	0	null
org.apache.hadoop.yarn.server.RMNMSecurityInfoClass:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.RMNMSecurityInfoClass:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.webapp.AppBlock:clairfyAppFinalStatus(org.apache.hadoop.yarn.api.records.FinalApplicationStatus)	0	java.lang.String	0	Application has not completed yet.
org.apache.hadoop.yarn.server.webapp.AppBlock:getLogAggregationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.webapp.LogWebService:getNodeHttpAddress(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.webapp.LogWebService:getApp(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.webapp.AppAttemptBlock:hasAMContainer(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Collection)	0	int	0	1
org.apache.hadoop.yarn.server.webapp.AppAttemptBlock:hasAMContainer(org.apache.hadoop.yarn.api.records.ContainerId,java.util.Collection)	1	int	0	0
org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo:hasCustomResources()	0	int	0	1
org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo:hasCustomResources()	1	int	0	0
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:getNoRedirectWarning()	0	java.lang.String	0	We do not have NodeManager web address, so we can not re-direct the request to related NodeManager for local container logs.
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:parseLongParam(java.lang.String)	0	long	0	9223372036854775807
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:isRunningState(org.apache.hadoop.yarn.api.records.YarnApplicationState)	0	int	0	1
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:isRunningState(org.apache.hadoop.yarn.api.records.YarnApplicationState)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasLocalPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasLocalPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:hasSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasSubdir()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasSubdir()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasDeletionTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasDeletionTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasTaskType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasTaskType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasDockerContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto$Builder:hasDockerContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto$Builder:hasDeletionTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto$Builder:hasDeletionTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:hasDeletionTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:hasDeletionTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LogDeleterProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:hasAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:hasAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto$Builder:hasAction()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerHeartbeatResponseProto$Builder:hasAction()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasCredentials()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasCredentials()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasAppLogAggregationInitedTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasAppLogAggregationInitedTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasFlowContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:hasFlowContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasLocalPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasLocalPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasLocalSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasLocalSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasException()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:hasException()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:hasLocalizerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:hasLocalizerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.LocalizationProtocol$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:hasDestinationDirectory()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:hasDestinationDirectory()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasCredentials()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasCredentials()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasLogAggregationContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasLogAggregationContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasAppLogAggregationInitedTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasAppLogAggregationInitedTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasFlowContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$ContainerManagerApplicationProto$Builder:hasFlowContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasLocalPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasLocalPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasLocalSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasLocalSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasException()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalResourceStatusProto$Builder:hasException()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowRunId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:hasFlowRunId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto$Builder:hasLocalizerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerStatusProto$Builder:hasLocalizerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowName()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowName()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowVersion()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowVersion()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowRunId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$FlowContextProto$Builder:hasFlowRunId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto$Builder:hasDestinationDirectory()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceLocalizationSpecProto$Builder:hasDestinationDirectory()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasResource()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasResource()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasLocalPath()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasLocalPath()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasSize()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$LocalizedResourceProto$Builder:hasSize()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasSubdir()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasSubdir()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasDeletionTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasDeletionTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasTaskType()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasTaskType()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasDockerContainerId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:hasDockerContainerId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerRecoveryProtos$DeletionServiceDeleteTaskProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:createNonExistentDirs(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.permission.FsPermission)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:createNonExistentDirs(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.permission.FsPermission)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:isDiskUsageOverPercentageLimit(java.io.File,float)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:isDiskUsageOverPercentageLimit(java.io.File,float)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:isDiskFreeSpaceUnderLimit(java.io.File,long)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:isDiskFreeSpaceUnderLimit(java.io.File,long)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeAttributesHandler:getNodeAttributesForHeartbeat()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeAttributesHandler:getNodeAttributesForRegistration()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeAttributesHandler:verifyRMRegistrationResponseForNodeAttributes(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.nodelabels.AbstractNodeDescriptorsProvider:convertToNodeLabelSet(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:delete(org.apache.hadoop.fs.Path,boolean)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeManager:getName()	0	java.lang.String	0	NodeManager
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:checkIfAppExistsInStateStore(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:checkIfAppExistsInStateStore(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AbstractRequestInterceptor:getNMStateStore()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:getSubClusterForNode(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:warnIfNotExists(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:warnIfNotExists(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:isNullOrEmpty(java.util.Collection)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:isNullOrEmpty(java.util.Collection)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:isNullOrEmpty(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:isNullOrEmpty(java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContextImpl:setAMRMToken(org.apache.hadoop.security.token.Token)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContextImpl:setAMRMToken(org.apache.hadoop.security.token.Token)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:getDisksHealthReport(boolean)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:areDisksHealthy()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:areDisksHealthy()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:isInGoodDirs(java.util.List,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:isInGoodDirs(java.util.List,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DeletionService:isTerminated()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DeletionService:isTerminated()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:handleLaunchForLaunchType(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext,org.apache.hadoop.yarn.api.ApplicationConstants$ContainerLaunchType)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:reapContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReapContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService:canRecover()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextRecoveredContainer(org.apache.hadoop.yarn.server.utils.LeveldbIterator)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextRecoveredApplication(org.apache.hadoop.yarn.server.utils.LeveldbIterator)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextRecoveredPrivateLocalizationEntry(org.apache.hadoop.yarn.server.utils.LeveldbIterator)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextCompletedResource(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextStartedResource(org.apache.hadoop.yarn.server.utils.LeveldbIterator,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getResourceTrackerKeyPrefix(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)	0	java.lang.String	0	Localization/public/
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getNextRecoveredDeletionService(org.apache.hadoop.yarn.server.utils.LeveldbIterator)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:getMasterKey(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$BaseRecoveryIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$BaseRecoveryIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService:canRecover()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService:isNewlyCreated()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:isEnabled()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:isEnabled()	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountVolumeSpec:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountVolumeSpec:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountVolumeSpec:compareTo(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.VolumeSpec:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.VolumeSpec:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.VolumeSpec:compareTo(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountDeviceSpec:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountDeviceSpec:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.MountDeviceSpec:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl:getStatus()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl:getLocalizerId()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl:getLocalizerAction()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$CachedNodeDescriptorHandler:isResyncIntervalElapsed()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$CachedNodeDescriptorHandler:isResyncIntervalElapsed()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner$NodeHealthMonitorExecutor:hasErrors(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner$NodeHealthMonitorExecutor:hasErrors(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:newInstance(java.lang.String,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:shouldRun(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:shouldRun(java.lang.String,java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.health.ExceptionReporter:isHealthy()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.health.ExceptionReporter:isHealthy()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.health.ExceptionReporter:getHealthReport()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:isValueUpdated(java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeAttributesHandler:isValueUpdated(java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:parseLongParam(java.lang.String)	0	long	0	9223372036854775807
org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.PerGpuDeviceInformation$StrToMemAdapter:marshal(java.lang.Long)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.PerGpuMemoryUsage:getTotalMemoryMiB()	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.PerGpuDeviceInformation$StrToFloatBeforeSpaceAdapter:marshal(java.lang.Float)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:checkInsecureSetup()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeLabelsHandler:getNodeLabelsForHeartbeat()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeLabelsHandler:getNodeLabelsForRegistration()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMCentralizedNodeLabelsHandler:verifyRMRegistrationResponseForNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:reapContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReapContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:containerIsAlive(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:containerIsAlive(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:isValueUpdated(java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$NMDistributedNodeLabelsHandler:isValueUpdated(java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:isValidStartContainerRequest(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:isValidStartContainerRequest(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isNMUnderSupervisionWithRecoveryEnabled()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isNMUnderSupervisionWithRecoveryEnabled()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isTokenKeepAliveEnabled(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isTokenKeepAliveEnabled(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isApplicationStopped(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:isApplicationStopped(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:handleShutdownOrResyncCommand(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:handleShutdownOrResyncCommand(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler:getResourcesOption(org.apache.hadoop.yarn.api.records.ContainerId)	0	java.lang.String	0	cgroups=none
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:getMtabFileName()	0	java.lang.String	0	/proc/mounts
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:getFlowName()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:getFlowVersion()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:getFlowRunId()	0	long	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:validateContainerState()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:validateContainerState()	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:getContainerPid()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getCleanupDockerVolumesCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:requestedDevice(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:requestedDevice(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:getRuntimeSpec(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.ShellWrapper:existFile(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.ShellWrapper:existFile(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:internalAssignDevices(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:getRequestedDeviceCount(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:getDeviceType(org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.Device)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.AssignedDevice:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.AssignedDevice:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.AssignedDevice:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:initPlugin(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:diagnose(int)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:diagnose(int)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:getFpgaType()	0	java.lang.String	0	IntelOpenCL
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:retrieveIPfilePath(java.lang.String,java.lang.String,java.util.Map)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:configureIP(java.lang.String,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDevice:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaResourcePlugin:getDockerCommandPluginInstance()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaResourcePlugin:getNMResourceInfo()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.AssignedGpuDevice:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.AssignedGpuDevice:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.AssignedGpuDevice:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV2CommandPlugin:requestsGpu(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV2CommandPlugin:requestsGpu(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV2CommandPlugin:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV2CommandPlugin:getCleanupDockerVolumesCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:getGpuIndexFromDeviceName(java.lang.String)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:requestsGpu(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:requestsGpu(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:initializeWhenGpuRequested(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:initializeWhenGpuRequested(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:getCreateDockerVolumeCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:getCleanupDockerVolumesCommand(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDevice:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDevice:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDevice:compareTo(java.lang.Object)	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:onDevicesAllocated(java.util.Set,org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.YarnRuntimeType)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:getScriptFromEnvSetting(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:getDevType(java.nio.file.Path,java.lang.String)	0	int	0	99
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:getDevType(java.nio.file.Path,java.lang.String)	1	int	0	98
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:isPluginDuplicate(java.util.Map,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:isPluginDuplicate(java.util.Map,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:isConfiguredResourceName(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:isConfiguredResourceName(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:hasSufficientResources(org.apache.hadoop.yarn.api.records.ResourceUtilization)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:hasSufficientResources(org.apache.hadoop.yarn.api.records.ResourceUtilization)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(long,long,int)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:hasResourcesAvailable(long,long,int)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord:getLaunchTime()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceConfiguration:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceConfiguration:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceConfiguration:toIndentedString(java.lang.Object)	0	java.lang.String	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext$Attribute:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext$Attribute:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:isResourceCalculatorAvailable()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:isResourceCalculatorAvailable()	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet:addResources(java.util.Map)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet:resourceLocalized(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest,org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:verifyAccess()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:verifyAccess()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:getEstimatedSize(org.apache.hadoop.yarn.api.records.LocalResource)	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:checkLocalDir(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:getLocalizedResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:checkLocalResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:checkLocalResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:remove(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.DeletionService)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:remove(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.DeletionService)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:getPathForLocalization(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.server.nodemanager.DeletionService)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest:compareTo(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest:getSize()	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheCleaner$LRUComparator:compare(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheCleaner$LRUComparator:compare(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource,org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource)	1	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner:getSystemCredentialsSentFromRM(org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager$Directory:getDirectoryNumber(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:checkManifestPermissions(org.apache.hadoop.fs.FileStatus)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:checkManifestPermissions(org.apache.hadoop.fs.FileStatus)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:checkManifestOwnerAndPermissions(org.apache.hadoop.fs.FileStatus)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:maybeReadManifestFile()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:getClassName(org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecord)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:validateAuxServiceName(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LimitSizeContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LimitSizeContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.NoneContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:isAggregationEnabled()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:isAggregationEnabled()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.FailedOrKilledContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.FailedOrKilledContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AMOnlyLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AMOnlyLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.FailedContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.FailedContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.SampleContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.SampleContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AMOrFailedContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AMOrFailedContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy:shouldDoLogAggregation(org.apache.hadoop.yarn.server.api.ContainerLogContext)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:checkIfAlreadyBootstrapped(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:checkIfAlreadyBootstrapped(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:isResourcesAvailable(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:isResourcesAvailable(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:assignAvailableMemory(long,org.apache.hadoop.yarn.api.records.ContainerId)	0	long	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:assignAvailableCpus(int,org.apache.hadoop.yarn.api.records.ContainerId)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:ensureMountPathIsDefined()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:isMountPathDefined()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:isMountPathDefined()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:mountDisabledButMountPathDefined()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:mountDisabledButMountPathDefined()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:mountEnabledAndMountPathDefined()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMountConfig:mountEnabledAndMountPathDefined()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:isAvailable()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:isAvailable()	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:watchAndLogOOMState(long)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:bootstrap(org.apache.hadoop.yarn.util.ResourceCalculatorPlugin,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:cpuLimitsExist(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:cpuLimitsExist(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:getRequestedGpus(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:internalAssignGpus(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:bootstrap(org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:teardown()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler$ContainerCandidate:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler$ContainerCandidate:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler$ContainerCandidate:isOpportunistic(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler$ContainerCandidate:isOpportunistic(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkTagMappingJsonManager$NetworkTagMapping:containsUser(java.lang.String,java.util.List)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkTagMappingJsonManager$NetworkTagMapping:containsUser(java.lang.String,java.util.List)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkTagMappingJsonManager$NetworkTagMapping:containsGroup(java.lang.String,java.util.List)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkTagMappingJsonManager$NetworkTagMapping:containsGroup(java.lang.String,java.util.List)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:getCumulativeCpuTime()	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:getRssMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:getVirtualMemorySize(int)	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:checkPidPgrpidForMatch()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:isAvailable()	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:isAvailable()	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:getMemorySize(java.io.File)	0	long	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:getCgroupsRelativeRoot()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:squashCGroupOperations(java.util.List)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime:isRuntimeRequested(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime:isRuntimeRequested(java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin$LRUCache:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin$LRUCache:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:getLocalImageToHashReader()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:getHdfsImageToHashReader()	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:readImageToHashFile(java.io.BufferedReader)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:isRuntimeRequested(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:isPluggableRuntime(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:isPluggableRuntime(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:isRuntimeAllowed(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:isRuntimeAllowed(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:isOCICompliantContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:isOCICompliantContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowHostPidNamespace(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowHostPidNamespace(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	1	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:isRuntimeRequested(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:isRuntimeRequested(java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:lambda$getGroupPolicyFiles$1(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:lambda$getGroupPolicyFiles$1(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isStoppable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isStoppable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isRemovable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isRemovable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isStartable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:isStartable(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand:containsEnv()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand:containsEnv()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getCgroupPath(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:extractImageEnv(java.io.File)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:extractImageEntrypoint(java.io.File)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:isRuncContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:isRuncContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:lambda$addUserMounts$0(int)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:lambda$addUserMounts$0(int)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:isDockerContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:isDockerContainerRequested(org.apache.hadoop.conf.Configuration,java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)	0	null	0	null
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:lambda$launchContainer$0(int)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:lambda$launchContainer$0(int)	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.SlidingWindowRetryPolicy$RetryContext:getRemainingRetries()	0	int	0	-1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ReInitializationContext:canRollback()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ReInitializationContext:canRollback()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isRetryContextSet()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isRetryContextSet()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:shouldRetry(int)	0	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:hasDefaultExitCode()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:hasDefaultExitCode()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isRunning()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isRunning()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:canRollback()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:canRollback()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isContainerInFinalStates()	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:isContainerInFinalStates()	1	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.SlidingWindowRetryPolicy:shouldRetry(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.SlidingWindowRetryPolicy$RetryContext,int)	0	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.SlidingWindowRetryPolicy:shouldRetry(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.SlidingWindowRetryPolicy$RetryContext,int)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:hasEpoch()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:hasEpoch()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto$Builder:hasEpoch()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$EpochProto$Builder:hasEpoch()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto$Builder:hasCurrentMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto$Builder:hasCurrentMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto$Builder:hasNextMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto$Builder:hasNextMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasApplicationSubmissionContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasApplicationSubmissionContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasCallerContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasCallerContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasLaunchTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasLaunchTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasRealUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:hasRealUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:hasTokenIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:hasTokenIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:hasRenewDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:hasRenewDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasSubmitTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasSubmitTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasApplicationSubmissionContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasApplicationSubmissionContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasApplicationState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasApplicationState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasCallerContext()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasCallerContext()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasLaunchTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasLaunchTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasRealUser()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationStateDataProto$Builder:hasRealUser()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$1:assignDescriptors(org.apache.hadoop.thirdparty.protobuf.Descriptors$FileDescriptor)	0	null	0	null
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasMasterContainer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasMasterContainer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAppAttemptTokens()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAppAttemptTokens()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAppAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAppAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAmContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasAmContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasPreemptedMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasPreemptedMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasPreemptedVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasPreemptedVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasTotalAllocatedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:hasTotalAllocatedContainers()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder:hasTokenIdentifier()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder:hasTokenIdentifier()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder:hasRenewDate()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder:hasRenewDate()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:hasCurrentMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:hasCurrentMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:hasNextMasterKey()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:hasNextMasterKey()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:isInitialized()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:isInitialized()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$AMRMTokenSecretManagerStateProto:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:isInitialized()	0	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:isInitialized()	1	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAttemptId()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAttemptId()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasMasterContainer()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasMasterContainer()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAppAttemptTokens()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAppAttemptTokens()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAppAttemptState()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAppAttemptState()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinalTrackingUrl()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinalTrackingUrl()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasDiagnostics()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasDiagnostics()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasStartTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasStartTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinalApplicationStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinalApplicationStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAmContainerExitStatus()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasAmContainerExitStatus()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinishTime()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasFinishTime()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasPreemptedMemorySeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasPreemptedMemorySeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasPreemptedVcoreSeconds()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasPreemptedVcoreSeconds()	1	int	0	0
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasTotalAllocatedContainers()	0	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$ApplicationAttemptStateDataProto$Builder:hasTotalAllocatedContainers()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:validateForAttributeTypeMismatch(boolean,org.apache.hadoop.yarn.api.records.NodeAttribute,java.util.Map)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:validateForAttributeTypeMismatch(boolean,org.apache.hadoop.yarn.api.records.NodeAttribute,java.util.Map)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:normalizeAttributeValue(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeLabelsUtils:convertToStringSet(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:getActiveNMCountPerLabel(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:isNodeUsableByQueue(java.util.Set,org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager$Queue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:isNodeUsableByQueue(java.util.Set,org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager$Queue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:validateIncreaseDecreaseRequest(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.api.records.UpdateContainerRequest,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:validateIncreaseDecreaseRequest(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.api.records.UpdateContainerRequest,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$TimelinePublishEvent:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$TimelinePublishEvent:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.AllocationExpirationInfo:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.AllocationExpirationInfo:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.AllocationExpirationInfo:compareTo(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.AllocationExpirationInfo)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:completed()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:completed()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:getExposedPorts()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:getNodeLabelExpression()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:compareTo(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.AdminService:isRMActive()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.AdminService:isRMActive()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.AdminService:isSchedulerMutable()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.AdminService:isSchedulerMutable()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.AdminService:validateForInvalidNode(java.lang.String,boolean)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:isValidNode(java.lang.String,java.util.Set,java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:isValidNode(java.lang.String,java.util.Set,java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:isUntrackedNode(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:isUntrackedNode(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$UnknownNode:getNetworkLocation()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$UnknownNode:getParent()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$UnknownNode:getLevel()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.QueuePlacementRuleUtils:isStaticQueueMapping(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.QueuePlacementRuleUtils:isStaticQueueMapping(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:configuredQueue(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:configuredQueue(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:getCreateFlag(org.w3c.dom.Element)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:getCreateFlag(org.w3c.dom.Element)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext:hasParentQueue()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext:hasParentQueue()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementContextWithParent(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:getPlacementContextNoParent(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:ifQueueDoesNotExist(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:ifQueueDoesNotExist(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:isStaticQueueMapping(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:isStaticQueueMapping(org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.FairQueuePlacementUtils:isValidQueueName(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.FairQueuePlacementUtils:isValidQueueName(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.RejectPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.RejectPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.SpecifiedPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.SpecifiedPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:ifQueueDoesNotExist(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:ifQueueDoesNotExist(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:getPlacementForApp(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping:hasParentQueue()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping:hasParentQueue()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.provisioner.VolumeProvisioningResults:isSuccess()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.provisioner.VolumeProvisioningResults:isSuccess()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.RMServiceContext:getHAZookeeperConnectionState()	0	java.lang.String	0	Could not find leader elector. Verify both HA and automatic failover are enabled.
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:checkReadyToBeDecommissioned(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:checkReadyToBeDecommissioned(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:getTimeoutInSec(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:getTimeoutInSec(org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher$DecommissioningNodeContext)	1	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.IsResourceManagerActiveServlet:isActive()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.IsResourceManagerActiveServlet:isActive()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils:isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.slf4j.Logger)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils:isPlaceBlacklisted(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.slf4j.Logger)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest:anythingAllocatedOrReserved()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest:anythingAllocatedOrReserved()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:rmContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:anyContainerInFinalState(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:commonCheckContainerAllocation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:internalUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:internalUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:internalAddResourceRequests(boolean,java.util.List)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:lambda$getRejectedRequest$2(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:lambda$getRejectedRequest$2(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:lambda$getAllSchedulingRequests$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:lambda$getAllSchedulingRequests$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isOvercommitTimedOut()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isOvercommitTimedOut()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isOvercommitTimeOutSet()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isOvercommitTimeOutSet()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isValidContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:isValidContainer(org.apache.hadoop.yarn.api.records.ContainerId)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:getPartition()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:updatePendingAsk(java.util.Collection,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:internalUpdatePendingAsk(org.apache.hadoop.yarn.api.records.SchedulingRequest,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:checkCardinalityAndPending(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,java.util.Optional)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:canDelayTo(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:getUniqueLocationAsks()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:isSorterThreadRunning()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:isSorterThreadRunning()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SimpleCandidateNodeSet:getVersion()	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:hasRequestLabelChanged(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.ResourceRequest)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:hasRequestLabelChanged(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.api.records.ResourceRequest)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:getSchedulingRequest()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdateContext:checkAndAddToOutstandingDecreases(org.apache.hadoop.yarn.api.records.UpdateContainerRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdateContext:checkAndAddToOutstandingDecreases(org.apache.hadoop.yarn.api.records.UpdateContainerRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdateContext:checkAndAddToOutstandingIncreases(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.UpdateContainerRequest)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdateContext:checkAndAddToOutstandingIncreases(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.UpdateContainerRequest)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:updateSchedulingRequests(java.util.List)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:commonReserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:commonReserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:isWaitingForAMContainer()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:isWaitingForAMContainer()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:addMissedNonPartitionedRequestSchedulingOpportunity(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	2147483647
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:compareInputOrderTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:getPartition()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics:getUserMetrics(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementChange:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementChange:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities:getCapacity(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities:getAbsoluteCapacity(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy:isMutationAllowed(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy:isMutationAllowed(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy:queueHasAChild(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy:queueHasAChild(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getFinalConfigPath(org.apache.hadoop.fs.Path)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getConfigFileInputStream()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getLatestConfigPath()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getConfirmedConfHistory(long)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getLogs()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:getConfStoreVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:getConfirmedConfHistory(long)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:createNewZkPath(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:createNewZkPath(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore$1:compare(byte[],byte[])	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore$1:compare(byte[],byte[])	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore$1:compare(byte[],byte[])	2	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore$1:name()	0	java.lang.String	0	keyComparator
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.InMemoryConfigurationStore:getConfirmedConfHistory(long)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.InMemoryConfigurationStore:getLogs()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.InMemoryConfigurationStore:getConfStoreVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.InMemoryConfigurationStore:getCurrentVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore$1:accept(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore$1:accept(org.apache.hadoop.fs.Path)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore:getConfirmedConfHistory(long)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:getUserWeightFromQueue(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getNonLabeledQueueCapacity(java.lang.String)	0	float	0	100.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getNonLabeledQueueCapacity(java.lang.String)	1	float	0	0.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getNonLabeledQueueMaximumCapacity(java.lang.String)	0	float	0	100.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getConfiguredState(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getAccessibleNodeLabels(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:internalGetLabeledQueueCapacity(java.lang.String,java.lang.String,java.lang.String,float)	0	float	0	100.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getDefaultNodeLabelExpression(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getUnits(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getMultiNodesSortingAlgorithmPolicy(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager$WorkflowPriorityMapping:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager$WorkflowPriorityMapping:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:shouldSkipNodeSchedule(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler,boolean)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:shouldSkipNodeSchedule(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler,boolean)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:canAllocateMore(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment,int,int)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:canAllocateMore(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment,int,int)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainerOnSingleNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersOnMultiNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:allocateContainersToNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getAppsInQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:isSystemAppsLimitReached()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:isSystemAppsLimitReached()	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getSchedulerContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getAsyncSchedulingPendingBacklogs()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getMaximumApplicationLifetime(java.lang.String)	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:getNumAsyncSchedulerThreads()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:placementConstraintEnabled()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore:getByFullName(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore:isAmbiguous(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore:get(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore:lambda$getShortNameQueues$0(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueStore:lambda$getShortNameQueues$0(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	2	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:exceedUserMaxParallelApps(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:exceedUserMaxParallelApps(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:exceedQueueMaxParallelApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:exceedQueueMaxParallelApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:getUserMaxParallelApps(java.lang.String)	0	int	0	2147483647
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$1:get()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueResourcesForSorting,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueResourcesForSorting)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueResourcesForSorting,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueResourcesForSorting)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueComparator:compareQueueAccessToPartition(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueComparator:compareQueueAccessToPartition(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,java.lang.String)	1	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy$PriorityQueueComparator:compareQueueAccessToPartition(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,java.lang.String)	2	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy:getConfigName()	0	java.lang.String	0	priority-utilization
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.policy.PriorityUtilizationQueueOrderingPolicy:getConfigName()	1	java.lang.String	0	utilization
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getUserMetrics(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLGroup:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLGroup:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:getIntraQueuePreemptionDisabled()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:getIntraQueuePreemptionDisabled()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:isQueueHierarchyPreemptionDisabled(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:isIntraQueueHierarchyPreemptionDisabled(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:hasChildQueues()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:hasChildQueues()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:accessibleToPartition(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:accessibleToPartition(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:getDefaultApplicationPriority()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:getWorkflowMappingFromString(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:getMappedPriority(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:getWorkflowPriorityMappingStr(java.util.List)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:canAssign(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:canAssign(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:shouldAllocOrReserveNewContainer(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:shouldAllocOrReserveNewContainer(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:getResourceDiagnostics(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:getPolicyName()	0	java.lang.String	0	QueueManagementDynamicEditPolicy
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueState:containsLeafQueue(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueState:containsPartition(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueState:containsPartition(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueState:addLeafQueueStateIfNotExists(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueStatePerPartition)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueState:addLeafQueueStateIfNotExists(java.lang.String,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$LeafQueueStatePerPartition)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$PendingApplicationComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$PendingApplicationComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy$PendingApplicationComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp)	2	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:getMaxLeavesToBeActivated(float,float,int)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:hasPendingApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:hasPendingApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:getQueueOrderingPolicyConfigName()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:getParentName()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:canAssign(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:hasChildQueues()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:getAbstractUsersManager()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:lambda$canAssign$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)	0	java.lang.String	0	Queue skipped because node has been reserved
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:lambda$canAssign$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)	1	java.lang.String	0	Queue skipped because node resource is insufficient
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:getChildQueues()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer$MultiListStartTimeIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.ConfigurableResource:getPercentages()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:getStartTime()	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:assignContainerPreCheck(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:isActive()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:isActive()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:getAccessibleNodeLabels()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:getDefaultNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:getDefaultApplicationPriority()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:fitsInMaxShare(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:fitsInMaxShare(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:verifyAndSetPolicyFromConf(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:verifyAndSetPolicyFromConf(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:exceedUserMaxApps(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:exceedUserMaxApps(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:exceedQueueMaxRunningApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:exceedQueueMaxRunningApps(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visitRack(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visitRack(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visitNode(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visitNode(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker$TrackerPerPriorityResource:visit(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isEmpty()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:canRunAppAM(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarvedForMinShare()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarvedForMinShare()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarvedForFairShare()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarvedForFairShare()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarved()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarved()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.DominantResourceFairnessPolicy:getName()	0	java.lang.String	0	DRF
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.DominantResourceFairnessPolicy:checkIfUsageOverFairShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.DominantResourceFairnessPolicy:checkIfUsageOverFairShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FifoPolicy:getName()	0	java.lang.String	0	FIFO
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FifoPolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:getName()	0	java.lang.String	0	fair
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:checkIfUsageOverFairShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:checkIfUsageOverFairShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares:getFairShareIfFixed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable,boolean,java.lang.String)	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares:getFairShareIfFixed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable,boolean,java.lang.String)	1	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares:safeAdd(long,long)	0	long	0	9223372036854775807
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:compareTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime)	2	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator$IndexAndTime:equals(java.lang.Object)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:isDrfUsed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:isDrfUsedOnQueueLevel(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSQueueConverter:isNotUnboundedResource(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSQueueConverter:isNotUnboundedResource(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler:parseAndConvert(java.lang.String[])	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:canContainerBePreempted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:canContainerBePreempted(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserve(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserve(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reservationExceedsThreshold(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reservationExceedsThreshold(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isReservable(org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isReservable(org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isOverAMShareLimit()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isOverAMShareLimit()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isValidReservation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isValidReservation(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:assignReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:assignReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isUsageBelowShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isUsageBelowShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isStarved()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:isStarved()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:shouldCheckForStarvation()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:shouldCheckForStarvation()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer$MultiListStartTimeIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread$PreemptableContainers:addContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread$PreemptableContainers:addContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:identifyContainersToPreemptOnNode(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode,int)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerUtilities:isWhitespace(char)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerUtilities:isWhitespace(char)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerUtilities:trimQueueName(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:getLeafQueue(java.lang.String,boolean,org.apache.hadoop.yarn.api.records.ApplicationId,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:removeLeafQueue(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:removeLeafQueue(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:getParentQueue(java.lang.String,boolean,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:createNewQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue,java.util.List)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:removeQueueIfEmpty(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:removeQueueIfEmpty(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:isQueueNameValid(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:isQueueNameValid(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:shouldContinueAssigning(int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:shouldContinueAssigning(int,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:shouldAttemptPreemption()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:shouldAttemptPreemption()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:getAppsInQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.QueueProperties$Builder:isAclDefinedForAccessType(java.lang.String,org.apache.hadoop.yarn.security.AccessType)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.QueueProperties$Builder:isAclDefinedForAccessType(java.lang.String,org.apache.hadoop.yarn.security.AccessType)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:isSchedulingPolicy(org.w3c.dom.Element)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:isSchedulingPolicy(org.w3c.dom.Element)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:isQueue(org.w3c.dom.Element)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:isQueue(org.w3c.dom.Element)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:getDefaultFairSharePreemptionTimeout()	0	long	0	9223372036854775807
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:getDefaultFairSharePreemptionThreshold()	0	float	0	0.5
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:getQueueMaxAMShareDefault()	0	float	0	0.5
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getMinSharePreemptionTimeout(java.lang.String)	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getFairSharePreemptionTimeout(java.lang.String)	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getFairSharePreemptionThreshold(java.lang.String)	0	float	0	-1.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:isPreemptable(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:isPreemptable(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getQueueWeight(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:isEmpty()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:getAbstractUsersManager()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:getInfo()	0	java.lang.String	0	FifoOrderingPolicyForPendingApps
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:getConfigName()	0	java.lang.String	0	fifo-for-pending-apps
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:getInfo()	0	java.lang.String	0	FifoOrderingPolicy
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:getConfigName()	0	java.lang.String	0	fifo
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.PriorityComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity,org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.PriorityComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity,org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	1	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.PriorityComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity,org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	2	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.AbstractComparatorOrderingPolicy:removeSchedulableEntity(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyWithExclusivePartitions:getInfo()	0	java.lang.String	0	FifoOrderingPolicyWithExclusivePartitions
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyWithExclusivePartitions:getConfigName()	0	java.lang.String	0	fifo-with-partitions
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.CompoundComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity,org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy:getConfigName()	0	java.lang.String	0	fair
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:getSummarizedAppAllocation(java.util.List,double)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:shouldRecordThisApp(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:shouldRecordThisApp(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:shouldRecordThisNode(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:shouldRecordThisNode(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:getResourceDiagnostics(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:getDiagnostics(java.util.Optional)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:lambda$getAppActivitiesInfo$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:lambda$getAppActivitiesInfo$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesLogger:getRecordingNodeId(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesLogger:getPriority(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation:getNodeId()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation:getContainerId()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation:lambda$filterAllocationAttempts$0(java.util.Set,java.util.Set,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AppAllocation:lambda$filterAllocationAttempts$0(java.util.Set,java.util.Set,org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.NodeAllocation:getContainerId()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:getRequestActivityNodeInfos(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWSConsts$ActivitiesGroupBy)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:lambda$getRequestActivityNodeInfos$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:lambda$getRequestActivityNodeInfos$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:lambda$getRequestActivityNodeInfos$1(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:lambda$getRequestActivityNodeInfos$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesUtils:lambda$getRequestActivityNodeInfos$0(org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode:isAppType()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode:isAppType()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode:isRequestType()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode:isRequestType()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivityNode:getShortDiagnostic()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkResource(org.apache.hadoop.yarn.api.records.ResourceInformation,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkQueueLabelExpression(java.util.Set,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.RMContext)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:checkQueueLabelExpression(java.util.Set,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.RMContext)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:hasPendingResourceRequest(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:hasPendingResourceRequest(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:createOpportunisticRmContainer(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.api.records.Container,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler$1:getQueueName()	0	java.lang.String	0	default
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler$1:getAccessibleNodeLabels()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler$1:getDefaultNodeLabelExpression()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler$1:getDefaultApplicationPriority()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignNodeLocalContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:assignRackLocalContainers(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getRMContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:canDelete(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:canDelete(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getUserMetrics(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:lambda$handleRejectedRequests$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.SchedulingRequestWithPlacementAttempt)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:lambda$handleRejectedRequests$4(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.SchedulingRequestWithPlacementAttempt)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:lambda$handleRejectedRequests$2(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.SchedulingRequestWithPlacementAttempt)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:lambda$handleRejectedRequests$2(org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.api.SchedulingRequestWithPlacementAttempt)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager:allocationTagExistsOnNode(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager:allocationTagExistsOnNode(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateConstraint(java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateConstraint(java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateSourceTags(java.util.Set)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:validateSourceTags(java.util.Set)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfySingleConstraintExpression(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfySingleConstraintExpression(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyNodeConstraintExpression(org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyNodeConstraintExpression(org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetExpression,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:getNodeConstraintEvaluatedResult(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.NodeAttributeOpCode,org.apache.hadoop.yarn.api.records.NodeAttribute)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfySingleConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfySingleConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$SingleConstraint,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyAndConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$And,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyAndConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$And,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyOrConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$Or,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyOrConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint$Or,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:canSatisfyConstraints(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.resource.PlacementConstraint,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager,java.util.Optional)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TargetApplications:lambda$getOtherApplicationIds$1(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TargetApplications:lambda$getOtherApplicationIds$1(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager$TypeToCountedTags:getCardinality(java.lang.Object,java.lang.String)	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager$TypeToCountedTags:getCardinality(java.lang.Object,java.util.Set,java.util.function.LongBinaryOperator)	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:getConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:getGlobalConstraint(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:lambda$getMultilevelConstraint$2(org.apache.hadoop.yarn.api.resource.PlacementConstraint)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:lambda$getMultilevelConstraint$2(org.apache.hadoop.yarn.api.resource.PlacementConstraint)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:attemptPlacementOnNode(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,boolean)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:attemptPlacementOnNode(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,boolean)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:lambda$doPlacement$3(org.apache.hadoop.yarn.api.records.SchedulingRequest)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:lambda$doPlacement$3(org.apache.hadoop.yarn.api.records.SchedulingRequest)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.CircularIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.CircularIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.iterators.SerialIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.iterators.SerialIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.iterators.PopularTagsIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.iterators.PopularTagsIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:validateConstraint(java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:selectLocalNode(java.lang.String,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:selectAnyNode(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$ClusterNode:isQueueFull()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$ClusterNode:isQueueFull()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.QueueLimitCalculator$Stats:getMetric(org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$ClusterNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$LoadComparator:compareAndIncrement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$ClusterNode,int)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$LoadComparator:compareAndIncrement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$ClusterNode,int)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getRMContainer(org.apache.hadoop.yarn.api.records.ContainerId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:placementConstraintEnabled()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:getMaximumApplicationLifetime(java.lang.String)	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$4:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:isFencedState()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:isFencedState()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:checkAndRemovePartialRecord(org.apache.hadoop.fs.Path)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:checkAndRemovePartialRecord(org.apache.hadoop.fs.Path)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$5:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:loadVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:getCurrentVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData:getFirstAttemptId()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.Epoch:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.Epoch:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:getFinalTrackingUrl()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:getFinalApplicationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:getSubmitTime()	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:getUser()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:getRealUser()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:getState()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:getDiagnostics()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:equals(java.lang.Object)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:getAndIncrementEpoch()	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:loadVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:getCurrentVersion()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$3:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore$13:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:isAppStateFinal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition:isAppStateFinal(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:getProxyCACertNodeKey()	0	java.lang.String	0	ProxyCARoot/caCert
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:getProxyCAPrivateKeyNodeKey()	0	java.lang.String	0	ProxyCARoot/caPrivateKey
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:isClosed()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:isClosed()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMAppState(org.apache.hadoop.yarn.api.records.ApplicationId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:loadRMAppAttemptState(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:isWhitelistedUser(java.lang.String,org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:isWhitelistedUser(java.lang.String,org.apache.hadoop.conf.Configuration)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:isParentZnodeSafe(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:isParentZnodeSafe(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:getProgress()	0	float	0	0.0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppFinalStateStored()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppFinalStateStored()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppInFinalState(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppInFinalState(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppInCompletedStates()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:isAppInCompletedStates()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:getLogAggregationStatusTimeout(org.apache.hadoop.conf.Configuration)	0	long	0	600000
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isLogAggregationFinished()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isLogAggregationFinished()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isLogAggregationFinishedForNM(org.apache.hadoop.yarn.server.api.protocolrecords.LogAggregationReport)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isLogAggregationFinishedForNM(org.apache.hadoop.yarn.server.api.protocolrecords.LogAggregationReport)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isThereFailureMessageForNM(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppLogAggregation:isThereFailureMessageForNM(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppToMonitor:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppToMonitor:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:shouldCountTowardsMaxAttemptRetry()	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:getUnexpectedAMRegisteredDiagnostics()	0	java.lang.String	0	Unmanaged AM must register after AM attempt reaches LAUNCHED state.
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:sanitizeTrackingUrl(java.lang.String)	0	java.lang.String	0	N/A
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:getAvailableSchedulingMonitor()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector$TAFairOrderingComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempAppPerPartition,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempAppPerPartition)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector$TAFairOrderingComparator:compare(org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempAppPerPartition,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempAppPerPartition)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector$1:compare(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector$1:compare(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:skipContainerBasedOnIntraQueuePolicy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:skipContainerBasedOnIntraQueuePolicy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:canPreemptEnoughResourceForAsked(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean,java.util.List)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:canPreemptEnoughResourceForAsked(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,boolean,java.util.List)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:preChecksForMovingReservedContainerToNode(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:preChecksForMovingReservedContainerToNode(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:isQueueSatisfied(java.lang.String,java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempUserPerPartition:isUserLimitReached(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempUserPerPartition:isUserLimitReached(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:getPreemptableResource(java.lang.String,java.lang.String,java.util.Map)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:tryToPreemptFromQueue(java.lang.String,java.lang.String,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,boolean)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:tryToPreemptFromQueue(java.lang.String,java.lang.String,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,boolean)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:getPreemptionCandidatesOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionUtils:isContainerAlreadySelected(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,java.util.Map)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionUtils:tryPreemptContainerAndDeductResToObtain(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionContext,java.util.Map,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionUtils:tryPreemptContainerAndDeductResToObtain(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionContext,java.util.Map,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource,java.util.Map,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,boolean)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionUtils:preemptMapContains(java.util.Map,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:getPolicyName()	0	java.lang.String	0	ProportionalCapacityPreemptionPolicy
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:lambda$cleanupStaledPreemptionCandidates$0(long,java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:lambda$cleanupStaledPreemptionCandidates$0(long,java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$2:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$3:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$12:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:shouldRedirect(org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:shouldRedirect(org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:dumpSchedulerLogs(java.lang.String,javax.servlet.http.HttpServletRequest)	0	java.lang.String	0	Capacity scheduler logs are being created.
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getFlatSet(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage:appendPercent(org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo,float)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:isApplicationInFinalState(org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:isApplicationInFinalState(org.apache.hadoop.yarn.api.records.YarnApplicationAttemptState)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.PartitionQueueCapacitiesInfo:getConfiguredMaxResource()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeLabelInfo:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeLabelInfo:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo:isTrackingUrlReady()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo:isTrackingUrlReady()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo:getAppFairShare(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)	0	long	0	-1
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo:getAllocationTagsSummary()	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo:getChildQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo:isLeafQueue()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo:isLeafQueue()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices$13:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:getLogAggregationStatus()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:checkAccess(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:checkAccess(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationAccessType,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:accessToTargetQueueAllowed(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:accessToTargetQueueAllowed(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:isAllowedDelegationTokenOp()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:setAttemptLastResponseId(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,int)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:setAttemptLastResponseId(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,int)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.Priority)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.Priority)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager:getDefaultPriority(java.lang.String,org.apache.hadoop.security.UserGroupInformation)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:skipTokenRenewal(org.apache.hadoop.security.token.Token)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:skipTokenRenewal(org.apache.hadoop.security.token.Token)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.ReservationsACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ReservationACL,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.ReservationsACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.ReservationACL,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,java.lang.String,java.util.List,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:shouldIgnoreException(java.lang.Exception)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:shouldIgnoreException(java.lang.Exception)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew:isTimerCancelled()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew:isTimerCancelled()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenToRenew:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenCancelThread$1:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:getCurator()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:createReservationSystem()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:getWebAppsPath(java.lang.String)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:areActiveServicesRunning()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:areActiveServicesRunning()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:isOpportunisticSchedulingEnabled(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:isOpportunisticSchedulingEnabled(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1:run()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:getPlanQueue(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)	2	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryReservationAllocation:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getReservationWindow(java.lang.String)	0	long	0	86400000
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getAverageCapacity(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getInstantaneousMaxCapacity(java.lang.String)	0	float	0	1.0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getReservationAdmissionPolicy(java.lang.String)	0	java.lang.String	0	org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getReservationAgent(java.lang.String)	0	java.lang.String	0	org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getShowReservationAsQueues(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getReplanner(java.lang.String)	0	java.lang.String	0	org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.SimpleCapacityReplanner
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getMoveOnExpiry(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getEnforcementWindow(java.lang.String)	0	long	0	3600000
org.apache.hadoop.yarn.server.resourcemanager.reservation.NoOverCommitPolicy:getValidWindow()	0	long	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:createPlanFollower()	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation:addInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation:removeInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation:combineValue(org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation$RLEOperator,org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,java.util.Map$Entry,java.util.Map$Entry)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:isOverlap(long)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:isOverlap(long)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval)	0	int	0	-1
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:compareTo(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval)	2	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorGreedyRLE:computeStageAllocation(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.api.records.ReservationRequest,long,long,long,java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorGreedyRLE:exitCondition(long,long,long,long)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorGreedyRLE:exitCondition(long,long,long,long)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:validateOrderNoGap(org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,java.util.Map,boolean)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:validateOrderNoGap(org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,java.util.Map,boolean)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:isNonPreemptiveAllocation(java.util.Map)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:isNonPreemptiveAllocation(java.util.Map)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorLowCostAligned$DurationInterval:canAllocate()	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorLowCostAligned$DurationInterval:canAllocate()	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorLowCostAligned:computeStageAllocation(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.api.records.ReservationRequest,long,long,long,java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.TryManyReservationAgents:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.TryManyReservationAgents:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.TryManyReservationAgents:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.TryManyReservationAgents:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)	1	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:addInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:removeInterval(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationInterval,org.apache.hadoop.yarn.api.records.Resource)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:removeReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)	0	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:removeReservation(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)	1	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:getReservationById(org.apache.hadoop.yarn.api.records.ReservationId)	0	null	0	null
org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl:getAppProxyUrl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationId)	0	java.lang.String	0	N/A
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:isNodeInDecommissioning(org.apache.hadoop.yarn.api.records.NodeId)	0	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:isNodeInDecommissioning(org.apache.hadoop.yarn.api.records.NodeId)	1	int	0	0
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getApp(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getApps(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set,java.util.Set,java.lang.String,java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:clone(javax.servlet.http.HttpServletRequest)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:getAppState(javax.servlet.http.HttpServletRequest,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST$1:call()	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:genericForward(java.lang.String,javax.servlet.http.HttpServletRequest,java.lang.Class,org.apache.hadoop.yarn.server.router.webapp.HTTPMethods,java.lang.String,java.lang.Object,java.util.Map,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:clientResponseToResponse(com.sun.jersey.api.client.ClientResponse)	0	null	0	null
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:getMediaTypeFromHttpServletRequest(javax.servlet.http.HttpServletRequest,java.lang.Class)	0	java.lang.String	0	application/xml
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:getMediaTypeFromHttpServletRequest(javax.servlet.http.HttpServletRequest,java.lang.Class)	1	java.lang.String	0	application/json
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:removeResourceFromCacheFileSystem(org.apache.hadoop.fs.Path)	0	int	0	0
org.apache.hadoop.yarn.server.sharedcachemanager.store.SharedCacheResourceReference:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.sharedcachemanager.store.SharedCacheResourceReference:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:addResourceReference(java.lang.String,org.apache.hadoop.yarn.server.sharedcachemanager.store.SharedCacheResourceReference)	0	null	0	null
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:removeResource(java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:removeResource(java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:getAccessTime(java.lang.String)	0	long	0	-1
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:isResourceEvictable(java.lang.String,org.apache.hadoop.fs.FileStatus)	0	int	0	0
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:isResourceEvictable(java.lang.String,org.apache.hadoop.fs.FileStatus)	1	int	0	1
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:writeGlobalCleanerPidFile()	0	int	0	0
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:writeGlobalCleanerPidFile()	1	int	0	1
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:isApplicationActive(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	0
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter$1:hasNext()	0	int	0	0
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter$1:hasNext()	1	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$1:removeEldestEntry(java.util.Map$Entry)	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$1:removeEldestEntry(java.util.Map$Entry)	1	int	0	0
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:get(org.apache.hadoop.yarn.server.timeline.EntityIdentifier)	0	null	0	null
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter:getEntityForKey(byte[])	0	null	0	null
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:isValidClusterTimeStampDir(org.apache.hadoop.fs.FileStatus)	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:isValidClusterTimeStampDir(org.apache.hadoop.fs.FileStatus)	1	int	0	0
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:shouldCleanAppLogDir(org.apache.hadoop.fs.Path,long,org.apache.hadoop.fs.FileSystem,long)	0	int	0	0
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:shouldCleanAppLogDir(org.apache.hadoop.fs.Path,long,org.apache.hadoop.fs.FileSystem,long)	1	int	0	1
org.apache.hadoop.yarn.server.timeline.LogInfo:matchesGroupId(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.timeline.LogInfo:matchesGroupId(java.lang.String)	1	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityCacheItem:needRefresh()	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityCacheItem:needRefresh()	1	int	0	0
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:isDone()	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs:isDone()	1	int	0	0
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$StoppableRemoteIterator:hasNext()	0	int	0	1
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$StoppableRemoteIterator:hasNext()	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.flowactivity.FlowActivitySubDoc:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.flowactivity.FlowActivitySubDoc:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineEntityDocument:getCreatedTime()	0	long	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineMetricSubDoc:getSingleDataTimestamp()	0	long	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineMetricSubDoc:isValid()	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineMetricSubDoc:isValid()	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineMetricSubDoc:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineMetricSubDoc:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineEventSubDoc:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.entity.TimelineEventSubDoc:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:fetchEntityCreationTime(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	long	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:aggregate(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineAggregationTrack)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isNullOrEmpty(java.lang.String[])	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isNullOrEmpty(java.lang.String[])	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isTimeInRange(long,long,long)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isTimeInRange(long,long,long)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isFilterNotMatching(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineEntityFilters,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:isFilterNotMatching(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineEntityFilters,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:hasDataToBeRetrieve(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.Set)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreUtils:hasDataToBeRetrieve(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.Set)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:getAppIdFromAttributes(org.apache.hadoop.yarn.server.timelineservice.storage.flow.Attribute[])	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:readResult(org.apache.hadoop.hbase.client.Result,byte[],byte[],org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:aggregate(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineAggregationTrack)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:constructFilterListBasedOnFields(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:parseEntity(org.apache.hadoop.hbase.client.Result)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:constructFilterListBasedOnFields(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:parseEntity(org.apache.hadoop.hbase.client.Result)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:getNextRowKey(byte[],java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:parseEntityForType(org.apache.hadoop.hbase.client.Result)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialEventCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialEventCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialRelatesToCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialRelatesToCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialIsRelatedToCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialIsRelatedToCols(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,java.util.EnumSet)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialColsFromInfoFamily()	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:fetchPartialColsFromInfoFamily()	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:constructFilterListBasedOnFields(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:parseEntity(org.apache.hadoop.hbase.client.Result)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:constructFilterListBasedOnFilters()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:constructFilterListBasedOnFields(java.util.Set)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:hasField(java.util.EnumSet,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader$Field)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:hasField(java.util.EnumSet,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader$Field)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:supplementCellTimeStamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:supplementCellTimestamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:supplementCellTimeStamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:supplementCellTimestamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityColumnPrefix:supplementCellTimeStamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:supplementCellTimestamp()	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnPrefix:supplementCellTimeStamp()	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:supplementCellTimeStamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:supplementCellTimestamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.AppIdKeyConverter:getKeySize()	0	int	0	12
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineSchemaUtils:getAppOpLength(org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineSchemaUtils:getAppOpLength(org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineSchemaUtils:getAttributesLength(org.apache.hadoop.yarn.server.timelineservice.storage.flow.Attribute[])	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineSchemaUtils:isIntegralValue(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineSchemaUtils:isIntegralValue(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:decode(byte[],org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator[])	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:decode(java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator[])	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:joinEncoded(java.lang.String[])	0	java.lang.String	0	
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:joinEncoded(java.lang.Iterable)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.timelineservice.storage.common.LongConverter:decodeValue(byte[])	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimestampGenerator:getAppIdSuffix(java.lang.String)	0	long	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:supplementCellTimestamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:supplementCellTimestamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:supplementCellTimeStamp()	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:emitCells(java.util.List,java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,long)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:emitCells(java.util.List,java.util.SortedSet,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter,long)	1	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:hasMore()	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:renewTokenForAppCollector(org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector)	0	long	0	-1
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:addApplicationIfAbsent(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:addApplicationIfAbsent(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:remove(org.apache.hadoop.yarn.api.records.ApplicationId)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:remove(org.apache.hadoop.yarn.api.records.ApplicationId)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineFromIdConverter$1:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$2:encodeUID(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$2:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$5:encodeUID(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$5:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServicesUtils:parseFieldsStr(java.lang.String,java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServicesUtils:parseLongStr(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServicesUtils:parseIntStr(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServicesUtils:getUserName(org.apache.hadoop.security.UserGroupInformation)	0	java.lang.String	0	
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineParserForEqualityExpr:parse()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:getTimelineEntityType(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:checkAccess(org.apache.hadoop.security.UserGroupInformation)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:checkAccess(org.apache.hadoop.security.UserGroupInformation)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$3:encodeUID(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$3:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineFromIdConverter$2:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineParserForCompareExpr:parse()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$1:encodeUID(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$1:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderUtils:split(java.lang.String,char,char)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderUtils:escapeString(java.lang.String,char,char)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderUtils:escapeString(java.lang.String,char,char)	1	java.lang.String	0	
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderUtils:joinAndEscapeStrings(java.lang.String[],char,char)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineCompareFilter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineCompareFilter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValuesFilter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValuesFilter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineExistsFilter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineExistsFilter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelinePrefixFilter:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelinePrefixFilter:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineFromIdConverter$3:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineParserForDataToRetrieve:parse()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:isDisplayEntityPerUserFilterEnabled(org.apache.hadoop.conf.Configuration)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:isDisplayEntityPerUserFilterEnabled(org.apache.hadoop.conf.Configuration)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:validateAuthUserWithEntityUser(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager,org.apache.hadoop.security.UserGroupInformation,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:validateAuthUserWithEntityUser(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager,org.apache.hadoop.security.UserGroupInformation,java.lang.String)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:checkAccess(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager,org.apache.hadoop.security.UserGroupInformation,java.lang.String)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$4:encodeUID(org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderContext)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineUIDConverter$4:decodeUID(java.lang.String)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:write(org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorContext,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:aggregate(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineAggregationTrack)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$3:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$3:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$1:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$1:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$5:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$5:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchKeyValuesFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValuesFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchKeyValuesFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValuesFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	1	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchKeyValueFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValueFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchKeyValueFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineKeyValueFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	1	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchExistsFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineExistsFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchExistsFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineExistsFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	1	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:compareValues(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineCompareOp,long,long)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:compareValues(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineCompareOp,long,long)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchCompareFilter(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineCompareFilter,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchFilters(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:matchFilters(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterList,org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType)	1	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:isIntegralValue(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineStorageUtils:isIntegralValue(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$2:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$2:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$4:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$4:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$6:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.common.TimelineEntityFiltersType$6:isValidFilter(org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilter$TimelineFilterType)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:isTimeInRange(java.lang.Long,java.lang.Long,java.lang.Long)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:isTimeInRange(java.lang.Long,java.lang.Long,java.lang.Long)	1	int	0	0
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl$1:run()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl$2:run()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator:createTimelineSchema(java.lang.String[],org.apache.hadoop.conf.Configuration)	0	int	0	0
org.apache.hadoop.yarn.server.timelineservice.security.CollectorNodemanagerSecurityInfo$1:annotationType()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.security.CollectorNodemanagerSecurityInfo$1:serverPrincipal()	0	java.lang.String	0	yarn.nodemanager.principal
org.apache.hadoop.yarn.server.timelineservice.security.CollectorNodemanagerSecurityInfo$1:clientPrincipal()	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.security.CollectorNodemanagerSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.security.CollectorNodemanagerSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)	0	null	0	null
org.apache.hadoop.yarn.server.timelineservice.TimelineContext:equals(java.lang.Object)	0	int	0	1
org.apache.hadoop.yarn.server.timelineservice.TimelineContext:equals(java.lang.Object)	1	int	0	0
org.apache.hadoop.yarn.server.webproxy.ProxyCA$3:verify(java.lang.String,javax.net.ssl.SSLSession)	0	int	0	1
org.apache.hadoop.yarn.server.webproxy.ProxyCA$3:verify(java.lang.String,javax.net.ssl.SSLSession)	1	int	0	0
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpServletRequestWrapper:isUserInRole(java.lang.String)	0	int	0	0
org.apache.hadoop.yarn.server.webproxy.ProxyCA$2:chooseClientAlias(java.lang.String[],java.security.Principal[],java.net.Socket)	0	java.lang.String	0	client
org.apache.hadoop.yarn.server.webproxy.ProxyCA$2:getServerAliases(java.lang.String,java.security.Principal[])	0	null	0	null
org.apache.hadoop.yarn.server.webproxy.ProxyCA$2:chooseServerAlias(java.lang.String,java.security.Principal[],java.net.Socket)	0	null	0	null
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:checkHttpsStrictAndNotProvided(javax.servlet.http.HttpServletResponse,java.net.URI,org.apache.hadoop.yarn.conf.YarnConfiguration)	0	int	0	1
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:checkHttpsStrictAndNotProvided(javax.servlet.http.HttpServletResponse,java.net.URI,org.apache.hadoop.yarn.conf.YarnConfiguration)	1	int	0	0
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:isSecurityEnabled()	0	int	0	0
org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils:appendQuery(java.lang.StringBuilder,java.lang.String,boolean)	0	int	0	0
org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils:getSchemeFromUrl(java.lang.String)	0	java.lang.String	0	
