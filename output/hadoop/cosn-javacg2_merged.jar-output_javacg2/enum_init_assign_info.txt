org.apache.hadoop.ipc.Server$AuthProtocol:<init>(java.lang.String,int,int)	NONE	0	3	int	0	0
org.apache.hadoop.ipc.Server$AuthProtocol:<init>(java.lang.String,int,int)	SASL	1	3	int	0	-33
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	NEGOTIATE	1	3	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	INITIATE	2	3	int	0	2
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	CHALLENGE	3	3	int	0	3
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	RESPONSE	4	3	int	0	4
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState:<init>(java.lang.String,int,int)	WRAP	5	3	int	0	5
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto:<init>(java.lang.String,int,int)	RPC_FINAL_PACKET	0	3	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto:<init>(java.lang.String,int,int)	RPC_CONTINUATION_PACKET	1	3	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto:<init>(java.lang.String,int,int)	RPC_CLOSE_CONNECTION	2	3	int	0	2
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto:<init>(java.lang.String,int,int)	ERROR	1	3	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto:<init>(java.lang.String,int,int)	FATAL	2	3	int	0	2
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto:<init>(java.lang.String,int,int)	RPC_BUILTIN	0	3	int	0	0
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto:<init>(java.lang.String,int,int)	RPC_WRITABLE	1	3	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto:<init>(java.lang.String,int,int)	RPC_PROTOCOL_BUFFER	2	3	int	0	2
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_APPLICATION	0	3	int	0	1
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_NO_SUCH_METHOD	1	3	int	0	2
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_NO_SUCH_PROTOCOL	2	3	int	0	3
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_RPC_SERVER	3	3	int	0	4
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_SERIALIZING_RESPONSE	4	3	int	0	5
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	ERROR_RPC_VERSION_MISMATCH	5	3	int	0	6
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_UNKNOWN	6	3	int	0	10
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_UNSUPPORTED_SERIALIZATION	7	3	int	0	11
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_INVALID_RPC_HEADER	8	3	int	0	12
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_DESERIALIZING_REQUEST	9	3	int	0	13
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_VERSION_MISMATCH	10	3	int	0	14
org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto:<init>(java.lang.String,int,int)	FATAL_UNAUTHORIZED	11	3	int	0	15
org.apache.hadoop.ipc.RPC$RpcKind:<init>(java.lang.String,int,short)	RPC_BUILTIN	0	3	int	0	1
org.apache.hadoop.ipc.RPC$RpcKind:<init>(java.lang.String,int,short)	RPC_WRITABLE	1	3	int	0	2
org.apache.hadoop.ipc.RPC$RpcKind:<init>(java.lang.String,int,short)	RPC_PROTOCOL_BUFFER	2	3	int	0	3
org.apache.hadoop.http.HttpServer2$XFrameOption:<init>(java.lang.String,int,java.lang.String)	DENY	0	3	java.lang.String	0	DENY
org.apache.hadoop.http.HttpServer2$XFrameOption:<init>(java.lang.String,int,java.lang.String)	SAMEORIGIN	1	3	java.lang.String	0	SAMEORIGIN
org.apache.hadoop.http.HttpServer2$XFrameOption:<init>(java.lang.String,int,java.lang.String)	ALLOWFROM	2	3	java.lang.String	0	ALLOW-FROM
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	JvmMetrics	0	3	java.lang.String	0	JVM related metrics etc.
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemNonHeapUsedM	1	3	java.lang.String	0	Non-heap memory used in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemNonHeapCommittedM	2	3	java.lang.String	0	Non-heap memory committed in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemNonHeapMaxM	3	3	java.lang.String	0	Non-heap memory max in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemHeapUsedM	4	3	java.lang.String	0	Heap memory used in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemHeapCommittedM	5	3	java.lang.String	0	Heap memory committed in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemHeapMaxM	6	3	java.lang.String	0	Heap memory max in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	MemMaxM	7	3	java.lang.String	0	Max memory size in MB
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcCount	8	3	java.lang.String	0	Total GC count
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcTimeMillis	9	3	java.lang.String	0	Total GC time in milliseconds
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsNew	10	3	java.lang.String	0	Number of new threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsRunnable	11	3	java.lang.String	0	Number of runnable threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsBlocked	12	3	java.lang.String	0	Number of blocked threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsWaiting	13	3	java.lang.String	0	Number of waiting threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsTimedWaiting	14	3	java.lang.String	0	Number of timed waiting threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	ThreadsTerminated	15	3	java.lang.String	0	Number of terminated threads
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	LogFatal	16	3	java.lang.String	0	Total number of fatal log events
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	LogError	17	3	java.lang.String	0	Total number of error log events
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	LogWarn	18	3	java.lang.String	0	Total number of warning log events
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	LogInfo	19	3	java.lang.String	0	Total number of info log events
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcNumWarnThresholdExceeded	20	3	java.lang.String	0	Number of times that the GC warn threshold is exceeded
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcNumInfoThresholdExceeded	21	3	java.lang.String	0	Number of times that the GC info threshold is exceeded
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcTotalExtraSleepTime	22	3	java.lang.String	0	Total GC extra sleep time in milliseconds
org.apache.hadoop.metrics2.source.JvmMetricsInfo:<init>(java.lang.String,int,java.lang.String)	GcTimePercentage	23	3	java.lang.String	0	Percentage of time the JVM was paused in GC
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	NumActiveSources	0	3	java.lang.String	0	Number of active metrics sources
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	NumAllSources	1	3	java.lang.String	0	Number of all registered metrics sources
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	NumActiveSinks	2	3	java.lang.String	0	Number of active metrics sinks
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	NumAllSinks	3	3	java.lang.String	0	Number of all registered metrics sinks
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	Context	4	3	java.lang.String	0	Metrics context
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	Hostname	5	3	java.lang.String	0	Local hostname
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	SessionId	6	3	java.lang.String	0	Session ID
org.apache.hadoop.metrics2.impl.MsInfo:<init>(java.lang.String,int,java.lang.String)	ProcessName	7	3	java.lang.String	0	Process name
org.apache.hadoop.fs.FSProtos$FileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_ACL	0	3	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_CRYPT	1	3	int	0	2
org.apache.hadoop.fs.FSProtos$FileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_EC	2	3	int	0	4
org.apache.hadoop.fs.FSProtos$FileStatusProto$Flags:<init>(java.lang.String,int,int)	SNAPSHOT_ENABLED	3	3	int	0	8
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	CREATE	0	3	int	0	1
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	OVERWRITE	1	3	int	0	2
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	APPEND	2	3	int	0	4
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	SYNC_BLOCK	3	3	int	0	8
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	LAZY_PERSIST	4	3	int	0	16
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	NEW_BLOCK	5	3	int	0	32
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	NO_LOCAL_WRITE	6	3	int	0	64
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	SHOULD_REPLICATE	7	3	int	0	128
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	IGNORE_CLIENT_LOCALITY	8	3	int	0	256
org.apache.hadoop.fs.CreateFlag:<init>(java.lang.String,int,short)	NO_LOCAL_RACK	9	3	int	0	288
org.apache.hadoop.fs.Options$Rename:<init>(java.lang.String,int,byte)	NONE	0	3	int	0	0
org.apache.hadoop.fs.Options$Rename:<init>(java.lang.String,int,byte)	OVERWRITE	1	3	int	0	1
org.apache.hadoop.fs.Options$Rename:<init>(java.lang.String,int,byte)	TO_TRASH	2	3	int	0	2
org.apache.hadoop.fs.StorageType:<init>(java.lang.String,int,boolean)	RAM_DISK	0	3	int	0	1
org.apache.hadoop.fs.StorageType:<init>(java.lang.String,int,boolean)	SSD	1	3	int	0	0
org.apache.hadoop.fs.StorageType:<init>(java.lang.String,int,boolean)	DISK	2	3	int	0	0
org.apache.hadoop.fs.StorageType:<init>(java.lang.String,int,boolean)	ARCHIVE	3	3	int	0	0
org.apache.hadoop.fs.StorageType:<init>(java.lang.String,int,boolean)	PROVIDED	4	3	int	0	0
org.apache.hadoop.fs.XAttrSetFlag:<init>(java.lang.String,int,short)	CREATE	0	3	int	0	1
org.apache.hadoop.fs.XAttrSetFlag:<init>(java.lang.String,int,short)	REPLACE	1	3	int	0	2
org.apache.hadoop.fs.StreamCapabilities$StreamCapability:<init>(java.lang.String,int,java.lang.String)	HFLUSH	0	3	java.lang.String	0	hflush
org.apache.hadoop.fs.StreamCapabilities$StreamCapability:<init>(java.lang.String,int,java.lang.String)	HSYNC	1	3	java.lang.String	0	hsync
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	NONE	0	3	java.lang.String	0	---
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	EXECUTE	1	3	java.lang.String	0	--x
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	WRITE	2	3	java.lang.String	0	-w-
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	WRITE_EXECUTE	3	3	java.lang.String	0	-wx
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	READ	4	3	java.lang.String	0	r--
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	READ_EXECUTE	5	3	java.lang.String	0	r-x
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	READ_WRITE	6	3	java.lang.String	0	rw-
org.apache.hadoop.fs.permission.FsAction:<init>(java.lang.String,int,java.lang.String)	ALL	7	3	java.lang.String	0	rwx
org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:<init>(java.lang.String,int,java.lang.String)	REPLACE_RESOLVED_DST_PATH	0	3	java.lang.String	0	replaceresolveddstpath
org.apache.hadoop.fs.FSProtos$FileStatusProto$FileType:<init>(java.lang.String,int,int)	FT_DIR	0	3	int	0	1
org.apache.hadoop.fs.FSProtos$FileStatusProto$FileType:<init>(java.lang.String,int,int)	FT_FILE	1	3	int	0	2
org.apache.hadoop.fs.FSProtos$FileStatusProto$FileType:<init>(java.lang.String,int,int)	FT_SYMLINK	2	3	int	0	3
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	UNKNOWN	0	3	java.lang.String	0	??
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	UNKNOWN	0	4	java.lang.String	0	unknown
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	UNKNOWN	0	5	int	0	0
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CANCEL_PREFETCHES	1	3	java.lang.String	0	CP
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CANCEL_PREFETCHES	1	4	java.lang.String	0	cancelPrefetches
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CANCEL_PREFETCHES	1	5	int	0	0
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CLOSE	2	3	java.lang.String	0	CX
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CLOSE	2	4	java.lang.String	0	close
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CLOSE	2	5	int	0	0
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CACHE_PUT	3	3	java.lang.String	0	C+
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CACHE_PUT	3	4	java.lang.String	0	putC
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	CACHE_PUT	3	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_CACHED	4	3	java.lang.String	0	GC
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_CACHED	4	4	java.lang.String	0	getCached
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_CACHED	4	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_PREFETCHED	5	3	java.lang.String	0	GP
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_PREFETCHED	5	4	java.lang.String	0	getPrefetched
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_PREFETCHED	5	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_READ	6	3	java.lang.String	0	GR
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_READ	6	4	java.lang.String	0	getRead
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	GET_READ	6	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	PREFETCH	7	3	java.lang.String	0	PF
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	PREFETCH	7	4	java.lang.String	0	prefetch
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	PREFETCH	7	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	RELEASE	8	3	java.lang.String	0	RL
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	RELEASE	8	4	java.lang.String	0	release
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	RELEASE	8	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_CACHING	9	3	java.lang.String	0	RC
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_CACHING	9	4	java.lang.String	0	requestCaching
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_CACHING	9	5	int	0	1
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_PREFETCH	10	3	java.lang.String	0	RP
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_PREFETCH	10	4	java.lang.String	0	requestPrefetch
org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:<init>(java.lang.String,int,java.lang.String,java.lang.String,boolean)	REQUEST_PREFETCH	10	5	int	0	1
org.apache.hadoop.crypto.CryptoProtocolVersion:<init>(java.lang.String,int,java.lang.String,int)	UNKNOWN	0	3	java.lang.String	0	Unknown
org.apache.hadoop.crypto.CryptoProtocolVersion:<init>(java.lang.String,int,java.lang.String,int)	UNKNOWN	0	4	int	0	1
org.apache.hadoop.crypto.CryptoProtocolVersion:<init>(java.lang.String,int,java.lang.String,int)	ENCRYPTION_ZONES	1	3	java.lang.String	0	Encryption zones
org.apache.hadoop.crypto.CryptoProtocolVersion:<init>(java.lang.String,int,java.lang.String,int)	ENCRYPTION_ZONES	1	4	int	0	2
org.apache.hadoop.crypto.CipherSuite:<init>(java.lang.String,int,java.lang.String,int)	UNKNOWN	0	3	java.lang.String	0	Unknown
org.apache.hadoop.crypto.CipherSuite:<init>(java.lang.String,int,java.lang.String,int)	UNKNOWN	0	4	int	0	0
org.apache.hadoop.crypto.CipherSuite:<init>(java.lang.String,int,java.lang.String,int)	AES_CTR_NOPADDING	1	3	java.lang.String	0	AES/CTR/NoPadding
org.apache.hadoop.crypto.CipherSuite:<init>(java.lang.String,int,java.lang.String,int)	AES_CTR_NOPADDING	1	4	int	0	16
org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:<init>(java.lang.String,int,int)	ACCESS_READ	0	3	int	0	1
org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:<init>(java.lang.String,int,int)	ACCESS_WRITE	1	3	int	0	2
org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:<init>(java.lang.String,int,int)	ACCESS_EXECUTE	2	3	int	0	32
org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:<init>(java.lang.String,int,int)	UNSUPPORTED	0	3	int	0	-1
org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:<init>(java.lang.String,int,int)	PMDK_LIB_NOT_FOUND	1	3	int	0	1
org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:<init>(java.lang.String,int,int)	SUPPORTED	2	3	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:<init>(java.lang.String,int,int)	FILTERED	0	3	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:<init>(java.lang.String,int,int)	HUFFMAN_ONLY	1	3	int	0	2
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:<init>(java.lang.String,int,int)	RLE	2	3	int	0	3
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:<init>(java.lang.String,int,int)	FIXED	3	3	int	0	4
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:<init>(java.lang.String,int,int)	DEFAULT_STRATEGY	4	3	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:<init>(java.lang.String,int,int)	NO_HEADER	0	3	int	0	-15
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:<init>(java.lang.String,int,int)	DEFAULT_HEADER	1	3	int	0	15
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:<init>(java.lang.String,int,int)	GZIP_FORMAT	2	3	int	0	31
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:<init>(java.lang.String,int,int)	NO_HEADER	0	3	int	0	-15
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:<init>(java.lang.String,int,int)	DEFAULT_HEADER	1	3	int	0	15
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:<init>(java.lang.String,int,int)	GZIP_FORMAT	2	3	int	0	31
org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:<init>(java.lang.String,int,int)	AUTODETECT_GZIP_ZLIB	3	3	int	0	47
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	NO_COMPRESSION	0	3	int	0	0
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	BEST_SPEED	1	3	int	0	1
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	TWO	2	3	int	0	2
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	THREE	3	3	int	0	3
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	FOUR	4	3	int	0	4
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	FIVE	5	3	int	0	5
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	SIX	6	3	int	0	6
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	SEVEN	7	3	int	0	7
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	EIGHT	8	3	int	0	8
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	BEST_COMPRESSION	9	3	int	0	9
org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:<init>(java.lang.String,int,int)	DEFAULT_COMPRESSION	10	3	int	0	-1
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	NOTINITED	0	3	int	0	0
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	NOTINITED	0	4	java.lang.String	0	NOTINITED
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	INITED	1	3	int	0	1
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	INITED	1	4	java.lang.String	0	INITED
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	STARTED	2	3	int	0	2
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	STARTED	2	4	java.lang.String	0	STARTED
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	STOPPED	3	3	int	0	3
org.apache.hadoop.service.Service$STATE:<init>(java.lang.String,int,int,java.lang.String)	STOPPED	3	4	java.lang.String	0	STOPPED
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	GETDELEGATIONTOKEN	0	3	java.lang.String	0	GET
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	GETDELEGATIONTOKEN	0	4	int	0	1
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	RENEWDELEGATIONTOKEN	1	3	java.lang.String	0	PUT
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	RENEWDELEGATIONTOKEN	1	4	int	0	1
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	CANCELDELEGATIONTOKEN	2	3	java.lang.String	0	PUT
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	CANCELDELEGATIONTOKEN	2	4	int	0	0
org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:<init>(java.lang.String,int,java.lang.String)	AUTHENTICATION	0	3	java.lang.String	0	auth
org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:<init>(java.lang.String,int,java.lang.String)	INTEGRITY	1	3	java.lang.String	0	auth-int
org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:<init>(java.lang.String,int,java.lang.String)	PRIVACY	2	3	java.lang.String	0	auth-conf
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod,java.lang.String)	SIMPLE	0	3		0	
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod,java.lang.String)	SIMPLE	0	4	java.lang.String	0	hadoop-simple
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod,java.lang.String)	KERBEROS	1	3		0	
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod,java.lang.String)	KERBEROS	1	4	java.lang.String	0	hadoop-kerberos
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod)	TOKEN	2	3		0	
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod)	CERTIFICATE	3	3	null	0	null
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod)	KERBEROS_SSL	4	3	null	0	null
org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:<init>(java.lang.String,int,org.apache.hadoop.security.SaslRpcServer$AuthMethod)	PROXY	5	3	null	0	null
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	SIMPLE	0	3	int	0	80
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	SIMPLE	0	4	java.lang.String	0	
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	KERBEROS	1	3	int	0	81
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	KERBEROS	1	4	java.lang.String	0	GSSAPI
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	DIGEST	2	3	int	0	82
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	DIGEST	2	4	java.lang.String	0	DIGEST-MD5
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	TOKEN	3	3	int	0	82
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	TOKEN	3	4	java.lang.String	0	DIGEST-MD5
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	PLAIN	4	3	int	0	83
org.apache.hadoop.security.SaslRpcServer$AuthMethod:<init>(java.lang.String,int,byte,java.lang.String)	PLAIN	4	4	java.lang.String	0	PLAIN
org.apache.hadoop.security.Credentials$SerializedFormat:<init>(java.lang.String,int,byte)	WRITABLE	0	3	int	0	0
org.apache.hadoop.security.Credentials$SerializedFormat:<init>(java.lang.String,int,byte)	PROTOBUF	1	3	int	0	1
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	NULL	0	3	int	0	0
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	NULL	0	4	int	0	0
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	CRC32	1	3	int	0	1
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	CRC32	1	4	int	0	4
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	CRC32C	2	3	int	0	2
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	CRC32C	2	4	int	0	4
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	DEFAULT	3	3	int	0	3
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	DEFAULT	3	4	int	0	0
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	MIXED	4	3	int	0	4
org.apache.hadoop.util.DataChecksum$Type:<init>(java.lang.String,int,int,int)	MIXED	4	4	int	0	0
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	KILO	0	3	int	0	10
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	MEGA	1	3		0	
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	GIGA	2	3		0	
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	TERA	3	3		0	
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	PETA	4	3		0	
org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:<init>(java.lang.String,int,int)	EXA	5	3		0	
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto:<init>(java.lang.String,int,int)	INITIALIZING	0	3	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto:<init>(java.lang.String,int,int)	ACTIVE	1	3	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto:<init>(java.lang.String,int,int)	STANDBY	2	3	int	0	2
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto:<init>(java.lang.String,int,int)	OBSERVER	3	3	int	0	3
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HARequestSource:<init>(java.lang.String,int,int)	REQUEST_BY_USER	0	3	int	0	0
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HARequestSource:<init>(java.lang.String,int,int)	REQUEST_BY_USER_FORCED	1	3	int	0	1
org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HARequestSource:<init>(java.lang.String,int,int)	REQUEST_BY_ZKFC	2	3	int	0	2
org.apache.hadoop.ha.HAServiceProtocol$HAServiceState:<init>(java.lang.String,int,java.lang.String)	INITIALIZING	0	3	java.lang.String	0	initializing
org.apache.hadoop.ha.HAServiceProtocol$HAServiceState:<init>(java.lang.String,int,java.lang.String)	ACTIVE	1	3	java.lang.String	0	active
org.apache.hadoop.ha.HAServiceProtocol$HAServiceState:<init>(java.lang.String,int,java.lang.String)	STANDBY	2	3	java.lang.String	0	standby
org.apache.hadoop.ha.HAServiceProtocol$HAServiceState:<init>(java.lang.String,int,java.lang.String)	OBSERVER	3	3	java.lang.String	0	observer
org.apache.hadoop.ha.HAServiceProtocol$HAServiceState:<init>(java.lang.String,int,java.lang.String)	STOPPING	4	3	java.lang.String	0	stopping
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	CREATE	8	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	MKDIR	9	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	SYMLINK	10	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	MKNOD	11	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	REMOVE	12	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	RMDIR	13	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	RENAME	14	3	int	0	0
org.apache.hadoop.nfs.nfs3.Nfs3Constant$NFSPROC3:<init>(java.lang.String,int,boolean)	LINK	15	3	int	0	0
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSREG	0	3	int	0	1
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSDIR	1	3	int	0	2
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSBLK	2	3	int	0	3
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSCHR	3	3	int	0	4
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSLNK	4	3	int	0	5
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSSOCK	5	3	int	0	6
org.apache.hadoop.nfs.NfsFileType:<init>(java.lang.String,int,int)	NFSFIFO	6	3	int	0	7
org.apache.hadoop.oncrpc.security.RpcAuthInfo$AuthFlavor:<init>(java.lang.String,int,int)	AUTH_NONE	0	3	int	0	0
org.apache.hadoop.oncrpc.security.RpcAuthInfo$AuthFlavor:<init>(java.lang.String,int,int)	AUTH_SYS	1	3	int	0	1
org.apache.hadoop.oncrpc.security.RpcAuthInfo$AuthFlavor:<init>(java.lang.String,int,int)	AUTH_SHORT	2	3	int	0	2
org.apache.hadoop.oncrpc.security.RpcAuthInfo$AuthFlavor:<init>(java.lang.String,int,int)	AUTH_DH	3	3	int	0	3
org.apache.hadoop.oncrpc.security.RpcAuthInfo$AuthFlavor:<init>(java.lang.String,int,int)	RPCSEC_GSS	4	3	int	0	6
org.apache.hadoop.fs.CacheFlag:<init>(java.lang.String,int,short)	FORCE	0	3	int	0	1
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	OPEN	0	3	int	0	1
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	OPEN	0	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILESTATUS	1	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILESTATUS	1	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTSTATUS	2	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTSTATUS	2	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETCONTENTSUMMARY	3	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETCONTENTSUMMARY	3	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETQUOTAUSAGE	4	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETQUOTAUSAGE	4	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILECHECKSUM	5	3	int	0	1
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILECHECKSUM	5	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETHOMEDIRECTORY	6	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETHOMEDIRECTORY	6	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	GETDELEGATIONTOKEN	7	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	GETDELEGATIONTOKEN	7	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	GETDELEGATIONTOKEN	7	5	int	0	1
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GET_BLOCK_LOCATIONS	8	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GET_BLOCK_LOCATIONS	8	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILEBLOCKLOCATIONS	9	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETFILEBLOCKLOCATIONS	9	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETACLSTATUS	10	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETACLSTATUS	10	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETXATTRS	11	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETXATTRS	11	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETTRASHROOT	12	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETTRASHROOT	12	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTXATTRS	13	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTXATTRS	13	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETALLSTORAGEPOLICY	14	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETALLSTORAGEPOLICY	14	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSTORAGEPOLICY	15	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSTORAGEPOLICY	15	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETECPOLICY	16	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETECPOLICY	16	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	17	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	17	4	int	0	501
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	CHECKACCESS	18	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	CHECKACCESS	18	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTSTATUS_BATCH	19	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	LISTSTATUS_BATCH	19	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSERVERDEFAULTS	20	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSERVERDEFAULTS	20	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSNAPSHOTDIFF	21	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSNAPSHOTDIFF	21	4	int	0	200
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSNAPSHOTTABLEDIRECTORYLIST	22	3	int	0	0
org.apache.hadoop.hdfs.web.resources.GetOpParam$Op:<init>(java.lang.String,int,boolean,int)	GETSNAPSHOTTABLEDIRECTORYLIST	22	4	int	0	200
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:<init>(java.lang.String,int,int)	DELETE	0	3	int	0	200
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:<init>(java.lang.String,int,int)	DELETESNAPSHOT	1	3	int	0	200
org.apache.hadoop.hdfs.web.resources.DeleteOpParam$Op:<init>(java.lang.String,int,int)	NULL	2	3	int	0	501
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	APPEND	0	3	int	0	1
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	APPEND	0	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	CONCAT	1	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	CONCAT	1	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	TRUNCATE	2	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	TRUNCATE	2	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	UNSETECPOLICY	3	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	UNSETECPOLICY	3	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	UNSETSTORAGEPOLICY	4	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	UNSETSTORAGEPOLICY	4	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	5	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PostOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	5	4	int	0	501
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATE	0	3	int	0	1
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATE	0	4	int	0	201
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	MKDIRS	1	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	MKDIRS	1	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATESYMLINK	2	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATESYMLINK	2	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	RENAME	3	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	RENAME	3	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETREPLICATION	4	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETREPLICATION	4	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETOWNER	5	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETOWNER	5	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETPERMISSION	6	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETPERMISSION	6	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETTIMES	7	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETTIMES	7	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	RENEWDELEGATIONTOKEN	8	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	RENEWDELEGATIONTOKEN	8	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	RENEWDELEGATIONTOKEN	8	5	int	0	1
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	CANCELDELEGATIONTOKEN	9	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	CANCELDELEGATIONTOKEN	9	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int,boolean)	CANCELDELEGATIONTOKEN	9	5	int	0	1
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	MODIFYACLENTRIES	10	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	MODIFYACLENTRIES	10	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEACLENTRIES	11	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEACLENTRIES	11	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEDEFAULTACL	12	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEDEFAULTACL	12	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEACL	13	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEACL	13	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SATISFYSTORAGEPOLICY	14	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SATISFYSTORAGEPOLICY	14	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETACL	15	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETACL	15	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETXATTR	16	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETXATTR	16	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEXATTR	17	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	REMOVEXATTR	17	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	ENABLEECPOLICY	18	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	ENABLEECPOLICY	18	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	DISABLEECPOLICY	19	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	DISABLEECPOLICY	19	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETECPOLICY	20	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETECPOLICY	20	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	ALLOWSNAPSHOT	21	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	ALLOWSNAPSHOT	21	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	DISALLOWSNAPSHOT	22	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	DISALLOWSNAPSHOT	22	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATESNAPSHOT	23	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	CREATESNAPSHOT	23	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	RENAMESNAPSHOT	24	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	RENAMESNAPSHOT	24	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETSTORAGEPOLICY	25	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETSTORAGEPOLICY	25	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETQUOTA	26	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETQUOTA	26	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETQUOTABYSTORAGETYPE	27	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	SETQUOTABYSTORAGETYPE	27	4	int	0	200
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	28	3	int	0	0
org.apache.hadoop.hdfs.web.resources.PutOpParam$Op:<init>(java.lang.String,int,boolean,int)	NULL	28	4	int	0	501
org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkStatus$Result:<init>(java.lang.String,int,int)	NO_PLAN	0	3	int	0	0
org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkStatus$Result:<init>(java.lang.String,int,int)	PLAN_UNDER_PROGRESS	1	3	int	0	1
org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkStatus$Result:<init>(java.lang.String,int,int)	PLAN_DONE	2	3	int	0	2
org.apache.hadoop.hdfs.server.datanode.DiskBalancerWorkStatus$Result:<init>(java.lang.String,int,int)	PLAN_CANCELLED	3	3	int	0	3
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports$DiskOp:<init>(java.lang.String,int,java.lang.String)	METADATA	0	3	java.lang.String	0	MetadataOp
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports$DiskOp:<init>(java.lang.String,int,java.lang.String)	READ	1	3	java.lang.String	0	ReadIO
org.apache.hadoop.hdfs.server.protocol.SlowDiskReports$DiskOp:<init>(java.lang.String,int,java.lang.String)	WRITE	2	3	java.lang.String	0	WriteIO
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	ADD_CACHE_DIRECTIVE	0	3	java.lang.String	0	op_add_cache_directive
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	ADD_CACHE_POOL	1	3	java.lang.String	0	op_add_cache_pool
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	ADD_EC_POLICY	2	3	java.lang.String	0	op_add_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	ALLOW_SNAPSHOT	3	3	java.lang.String	0	op_allow_snapshot
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	APPEND	4	3	java.lang.String	0	op_append
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CONCAT	5	3	java.lang.String	0	op_concat
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	COPY_FROM_LOCAL_FILE	6	3	java.lang.String	0	op_copy_from_local_file
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CREATE	7	3	java.lang.String	0	op_create
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CREATE_ENCRYPTION_ZONE	8	3	java.lang.String	0	op_create_encryption_zone
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CREATE_NON_RECURSIVE	9	3	java.lang.String	0	op_create_non_recursive
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CREATE_SNAPSHOT	10	3	java.lang.String	0	op_create_snapshot
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	CREATE_SYM_LINK	11	3	java.lang.String	0	op_create_symlink
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	DELETE	12	3	java.lang.String	0	op_delete
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	DELETE_SNAPSHOT	13	3	java.lang.String	0	op_delete_snapshot
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	DISABLE_EC_POLICY	14	3	java.lang.String	0	op_disable_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	DISALLOW_SNAPSHOT	15	3	java.lang.String	0	op_disallow_snapshot
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	ENABLE_EC_POLICY	16	3	java.lang.String	0	op_enable_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	EXISTS	17	3	java.lang.String	0	op_exists
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_BYTES_WITH_FUTURE_GS	18	3	java.lang.String	0	op_get_bytes_with_future_generation_stamps
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_CONTENT_SUMMARY	19	3	java.lang.String	0	op_get_content_summary
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_EC_CODECS	20	3	java.lang.String	0	op_get_ec_codecs
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_EC_POLICY	21	3	java.lang.String	0	op_get_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_EC_POLICIES	22	3	java.lang.String	0	op_get_ec_policies
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_ENCRYPTION_ZONE	23	3	java.lang.String	0	op_get_encryption_zone
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_FILE_BLOCK_LOCATIONS	24	3	java.lang.String	0	op_get_file_block_locations
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_FILE_CHECKSUM	25	3	java.lang.String	0	op_get_file_checksum
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_FILE_LINK_STATUS	26	3	java.lang.String	0	op_get_file_link_status
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_FILE_STATUS	27	3	java.lang.String	0	op_get_file_status
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_LINK_TARGET	28	3	java.lang.String	0	op_get_link_target
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_QUOTA_USAGE	29	3	java.lang.String	0	op_get_quota_usage
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_STATUS	30	3	java.lang.String	0	op_get_status
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_STORAGE_POLICIES	31	3	java.lang.String	0	op_get_storage_policies
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_STORAGE_POLICY	32	3	java.lang.String	0	op_get_storage_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_TRASH_ROOT	33	3	java.lang.String	0	op_get_trash_root
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_XATTR	34	3	java.lang.String	0	op_get_xattr
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	LIST_CACHE_DIRECTIVE	35	3	java.lang.String	0	op_list_cache_directive
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	LIST_CACHE_POOL	36	3	java.lang.String	0	op_list_cache_pool
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	LIST_ENCRYPTION_ZONE	37	3	java.lang.String	0	op_list_encryption_zone
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	LIST_LOCATED_STATUS	38	3	java.lang.String	0	op_list_located_status
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	LIST_STATUS	39	3	java.lang.String	0	op_list_status
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	MODIFY_CACHE_POOL	40	3	java.lang.String	0	op_modify_cache_pool
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	MODIFY_CACHE_DIRECTIVE	41	3	java.lang.String	0	op_modify_cache_directive
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	MKDIRS	42	3	java.lang.String	0	op_mkdirs
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	MODIFY_ACL_ENTRIES	43	3	java.lang.String	0	op_modify_acl_entries
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	OPEN	44	3	java.lang.String	0	op_open
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	PRIMITIVE_CREATE	45	3	java.lang.String	0	op_primitive_create
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	PRIMITIVE_MKDIR	46	3	java.lang.String	0	op_primitive_mkdir
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_ACL	47	3	java.lang.String	0	op_remove_acl
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_ACL_ENTRIES	48	3	java.lang.String	0	op_remove_acl_entries
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_CACHE_DIRECTIVE	49	3	java.lang.String	0	op_remove_cache_directive
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_CACHE_POOL	50	3	java.lang.String	0	op_remove_cache_pool
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_DEFAULT_ACL	51	3	java.lang.String	0	op_remove_default_acl
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_EC_POLICY	52	3	java.lang.String	0	op_remove_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	REMOVE_XATTR	53	3	java.lang.String	0	op_remove_xattr
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	RENAME	54	3	java.lang.String	0	op_rename
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	RENAME_SNAPSHOT	55	3	java.lang.String	0	op_rename_snapshot
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	RESOLVE_LINK	56	3	java.lang.String	0	op_resolve_link
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SATISFY_STORAGE_POLICY	57	3	java.lang.String	0	op_satisfy_storagepolicy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_ACL	58	3	java.lang.String	0	op_set_acl
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_EC_POLICY	59	3	java.lang.String	0	op_set_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_OWNER	60	3	java.lang.String	0	op_set_owner
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_PERMISSION	61	3	java.lang.String	0	op_set_permission
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_QUOTA_BYTSTORAGEYPE	62	3	java.lang.String	0	op_set_quota_bystoragetype
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_QUOTA_USAGE	63	3	java.lang.String	0	op_set_quota_usage
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_REPLICATION	64	3	java.lang.String	0	op_set_replication
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_STORAGE_POLICY	65	3	java.lang.String	0	op_set_storagePolicy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_TIMES	66	3	java.lang.String	0	op_set_times
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	SET_XATTR	67	3	java.lang.String	0	op_set_xattr
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_SNAPSHOT_DIFF	68	3	java.lang.String	0	op_get_snapshot_diff
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	GET_SNAPSHOTTABLE_DIRECTORY_LIST	69	3	java.lang.String	0	op_get_snapshottable_directory_list
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	TRUNCATE	70	3	java.lang.String	0	op_truncate
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	UNSET_EC_POLICY	71	3	java.lang.String	0	op_unset_ec_policy
org.apache.hadoop.hdfs.DFSOpsCountStatistics$OpType:<init>(java.lang.String,int,java.lang.String)	UNSET_STORAGE_POLICY	72	3	java.lang.String	0	op_unset_storage_policy
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	UNUSABLE	0	3	int	0	0
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	UNUSABLE	0	4	int	0	0
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	SHORT_CIRCUIT_DISABLED	1	3	int	0	1
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	SHORT_CIRCUIT_DISABLED	1	4	int	0	0
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	VALID	2	3	int	0	1
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory$PathState:<init>(java.lang.String,int,boolean,boolean)	VALID	2	4	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CacheFlagProto:<init>(java.lang.String,int,int)	FORCE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryTypeProto:<init>(java.lang.String,int,int)	USER	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryTypeProto:<init>(java.lang.String,int,int)	GROUP	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryTypeProto:<init>(java.lang.String,int,int)	MASK	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryTypeProto:<init>(java.lang.String,int,int)	OTHER	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$XAttrNamespaceProto:<init>(java.lang.String,int,int)	USER	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$XAttrNamespaceProto:<init>(java.lang.String,int,int)	TRUSTED	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$XAttrNamespaceProto:<init>(java.lang.String,int,int)	SECURITY	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$XAttrNamespaceProto:<init>(java.lang.String,int,int)	SYSTEM	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrProto$XAttrNamespaceProto:<init>(java.lang.String,int,int)	RAW	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	NONE	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	EXECUTE	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	WRITE	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	WRITE_EXECUTE	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	READ	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	READ_EXECUTE	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	READ_WRITE	6	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$FsActionProto:<init>(java.lang.String,int,int)	PERM_ALL	7	3	int	0	7
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeProto:<init>(java.lang.String,int,int)	DISK	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeProto:<init>(java.lang.String,int,int)	SSD	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeProto:<init>(java.lang.String,int,int)	ARCHIVE	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeProto:<init>(java.lang.String,int,int)	RAM_DISK	3	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$StorageTypeProto:<init>(java.lang.String,int,int)	PROVIDED	4	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptActionProto:<init>(java.lang.String,int,int)	CANCEL_REENCRYPT	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptActionProto:<init>(java.lang.String,int,int)	START_REENCRYPT	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesTypeProto:<init>(java.lang.String,int,int)	ALL_OPEN_FILES	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$OpenFilesTypeProto:<init>(java.lang.String,int,int)	BLOCKING_DECOMMISSION	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumTypeProto:<init>(java.lang.String,int,int)	MD5CRC	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockChecksumTypeProto:<init>(java.lang.String,int,int)	COMPOSITE_CRC	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockFlagProto:<init>(java.lang.String,int,int)	NO_LOCAL_WRITE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockFlagProto:<init>(java.lang.String,int,int)	IGNORE_CLIENT_LOCALITY	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyState:<init>(java.lang.String,int,int)	DISABLED	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyState:<init>(java.lang.String,int,int)	ENABLED	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ErasureCodingPolicyState:<init>(java.lang.String,int,int)	REMOVED	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$DataTransferEncryptorStatus:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$DataTransferEncryptorStatus:<init>(java.lang.String,int,int)	ERROR_UNKNOWN_KEY	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$DataTransferEncryptorMessageProto$DataTransferEncryptorStatus:<init>(java.lang.String,int,int)	ERROR	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$INodeType:<init>(java.lang.String,int,int)	I_TYPE_FILE	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$INodeType:<init>(java.lang.String,int,int)	I_TYPE_DIRECTORY	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$INodeType:<init>(java.lang.String,int,int)	I_TYPE_SYMLINK	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherSuiteProto:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CipherSuiteProto:<init>(java.lang.String,int,int)	AES_CTR_NOPADDING	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_ACL	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_CRYPT	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Flags:<init>(java.lang.String,int,int)	HAS_EC	2	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$Flags:<init>(java.lang.String,int,int)	SNAPSHOT_ENABLED	3	3	int	0	8
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTypeProto:<init>(java.lang.String,int,int)	CONTIGUOUS	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$BlockTypeProto:<init>(java.lang.String,int,int)	STRIPED	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryScopeProto:<init>(java.lang.String,int,int)	ACCESS	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.AclProtos$AclEntryProto$AclEntryScopeProto:<init>(java.lang.String,int,int)	DEFAULT	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_SETUP_APPEND	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_SETUP_APPEND_RECOVERY	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	DATA_STREAMING	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_SETUP_STREAMING_RECOVERY	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_CLOSE	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_CLOSE_RECOVERY	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	PIPELINE_SETUP_CREATE	6	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	TRANSFER_RBW	7	3	int	0	7
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$OpWriteBlockProto$BlockConstructionStage:<init>(java.lang.String,int,int)	TRANSFER_FINALIZED	8	3	int	0	8
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeActionProto:<init>(java.lang.String,int,int)	QUERY	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeActionProto:<init>(java.lang.String,int,int)	START	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$RollingUpgradeActionProto:<init>(java.lang.String,int,int)	FINALIZE	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SafeModeActionProto:<init>(java.lang.String,int,int)	SAFEMODE_LEAVE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SafeModeActionProto:<init>(java.lang.String,int,int)	SAFEMODE_ENTER	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SafeModeActionProto:<init>(java.lang.String,int,int)	SAFEMODE_GET	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$SafeModeActionProto:<init>(java.lang.String,int,int)	SAFEMODE_FORCE_EXIT	3	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AccessModeProto:<init>(java.lang.String,int,int)	READ	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AccessModeProto:<init>(java.lang.String,int,int)	WRITE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AccessModeProto:<init>(java.lang.String,int,int)	COPY	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$AccessModeProto:<init>(java.lang.String,int,int)	REPLACE	3	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrSetFlagProto:<init>(java.lang.String,int,int)	XATTR_CREATE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.XAttrProtos$XAttrSetFlagProto:<init>(java.lang.String,int,int)	XATTR_REPLACE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitFdResponse:<init>(java.lang.String,int,int)	DO_NOT_USE_RECEIPT_VERIFICATION	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$ShortCircuitFdResponse:<init>(java.lang.String,int,int)	USE_RECEIPT_VERIFICATION	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_CHECKSUM	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_INVALID	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_EXISTS	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_ACCESS_TOKEN	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	CHECKSUM_OK	6	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_UNSUPPORTED	7	3	int	0	7
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	OOB_RESTART	8	3	int	0	8
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	OOB_RESERVED1	9	3	int	0	9
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	OOB_RESERVED2	10	3	int	0	10
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	OOB_RESERVED3	11	3	int	0	11
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	IN_PROGRESS	12	3	int	0	12
org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$Status:<init>(java.lang.String,int,int)	ERROR_BLOCK_PINNED	13	3	int	0	13
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$FileType:<init>(java.lang.String,int,int)	IS_DIR	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$FileType:<init>(java.lang.String,int,int)	IS_FILE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$HdfsFileStatusProto$FileType:<init>(java.lang.String,int,int)	IS_SYMLINK	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	ALL	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	LIVE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	DEAD	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	DECOMMISSIONING	3	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	ENTERING_MAINTENANCE	4	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$DatanodeReportTypeProto:<init>(java.lang.String,int,int)	IN_MAINTENANCE	5	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_TIMES	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_REPLICATION	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_OWNER	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_PERMS	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_ACLS	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$MetadataUpdateType:<init>(java.lang.String,int,int)	META_TYPE_XATTRS	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptionStateProto:<init>(java.lang.String,int,int)	SUBMITTED	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptionStateProto:<init>(java.lang.String,int,int)	PROCESSING	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.EncryptionZonesProtos$ReencryptionStateProto:<init>(java.lang.String,int,int)	COMPLETED	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_CREATE	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_CLOSE	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_APPEND	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_RENAME	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_METADATA	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_UNLINK	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.InotifyProtos$EventType:<init>(java.lang.String,int,int)	EVENT_TRUNCATE	6	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ChecksumTypeProto:<init>(java.lang.String,int,int)	CHECKSUM_NULL	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ChecksumTypeProto:<init>(java.lang.String,int,int)	CHECKSUM_CRC32	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$ChecksumTypeProto:<init>(java.lang.String,int,int)	CHECKSUM_CRC32C	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	CREATE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	OVERWRITE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	APPEND	2	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	LAZY_PERSIST	3	3	int	0	16
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	NEW_BLOCK	4	3	int	0	32
org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$CreateFlagProto:<init>(java.lang.String,int,int)	SHOULD_REPLICATE	5	3	int	0	128
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$StorageState:<init>(java.lang.String,int,int)	NORMAL	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeStorageProto$StorageState:<init>(java.lang.String,int,int)	READ_ONLY_SHARED	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CryptoProtocolVersionProto:<init>(java.lang.String,int,int)	UNKNOWN_PROTOCOL_VERSION	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$CryptoProtocolVersionProto:<init>(java.lang.String,int,int)	ENCRYPTION_ZONES	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$AdminState:<init>(java.lang.String,int,int)	NORMAL	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$AdminState:<init>(java.lang.String,int,int)	DECOMMISSION_INPROGRESS	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$AdminState:<init>(java.lang.String,int,int)	DECOMMISSIONED	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$AdminState:<init>(java.lang.String,int,int)	ENTERING_MAINTENANCE	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.HdfsProtos$DatanodeInfoProto$AdminState:<init>(java.lang.String,int,int)	IN_MAINTENANCE	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffType:<init>(java.lang.String,int,java.lang.String)	CREATE	0	3	java.lang.String	0	+
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffType:<init>(java.lang.String,int,java.lang.String)	MODIFY	1	3	java.lang.String	0	M
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffType:<init>(java.lang.String,int,java.lang.String)	DELETE	2	3	java.lang.String	0	-
org.apache.hadoop.hdfs.protocol.SnapshotDiffReport$DiffType:<init>(java.lang.String,int,java.lang.String)	RENAME	3	3	java.lang.String	0	R
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	STATUS	0	3	null	0	null
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	STATUS	0	4	int	0	4
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	RESERVED	1	3		0	
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	RESERVED	1	4	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	ECN_BITS	2	3		0	
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$StatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	ECN_BITS	2	4	int	0	2
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$ECN:<init>(java.lang.String,int,int)	DISABLED	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$ECN:<init>(java.lang.String,int,int)	SUPPORTED	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$ECN:<init>(java.lang.String,int,int)	SUPPORTED2	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck$ECN:<init>(java.lang.String,int,int)	CONGESTED	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	WRITE_BLOCK	0	3	int	0	80
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	READ_BLOCK	1	3	int	0	81
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	READ_METADATA	2	3	int	0	82
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	REPLACE_BLOCK	3	3	int	0	83
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	COPY_BLOCK	4	3	int	0	84
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	BLOCK_CHECKSUM	5	3	int	0	85
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	TRANSFER_BLOCK	6	3	int	0	86
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	REQUEST_SHORT_CIRCUIT_FDS	7	3	int	0	87
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	RELEASE_SHORT_CIRCUIT_FDS	8	3	int	0	88
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	REQUEST_SHORT_CIRCUIT_SHM	9	3	int	0	89
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	BLOCK_GROUP_CHECKSUM	10	3	int	0	90
org.apache.hadoop.hdfs.protocol.datatransfer.Op:<init>(java.lang.String,int,byte)	CUSTOM	11	3	int	0	127
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy:<init>(java.lang.String,int,org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Condition)	DISABLE	0	3		0	
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy:<init>(java.lang.String,int,org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Condition)	NEVER	1	3		0	
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy:<init>(java.lang.String,int,org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Condition)	DEFAULT	2	3		0	
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy:<init>(java.lang.String,int,org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Condition)	ALWAYS	3	3		0	
org.apache.hadoop.hdfs.protocol.OpenFilesIterator$OpenFilesType:<init>(java.lang.String,int,short)	ALL_OPEN_FILES	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.OpenFilesIterator$OpenFilesType:<init>(java.lang.String,int,short)	BLOCKING_DECOMMISSION	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.DatanodeInfo$AdminStates:<init>(java.lang.String,int,java.lang.String)	NORMAL	0	3	java.lang.String	0	In Service
org.apache.hadoop.hdfs.protocol.DatanodeInfo$AdminStates:<init>(java.lang.String,int,java.lang.String)	DECOMMISSION_INPROGRESS	1	3	java.lang.String	0	Decommission In Progress
org.apache.hadoop.hdfs.protocol.DatanodeInfo$AdminStates:<init>(java.lang.String,int,java.lang.String)	DECOMMISSIONED	2	3	java.lang.String	0	Decommissioned
org.apache.hadoop.hdfs.protocol.DatanodeInfo$AdminStates:<init>(java.lang.String,int,java.lang.String)	ENTERING_MAINTENANCE	3	3	java.lang.String	0	Entering Maintenance
org.apache.hadoop.hdfs.protocol.DatanodeInfo$AdminStates:<init>(java.lang.String,int,java.lang.String)	IN_MAINTENANCE	4	3	java.lang.String	0	In Maintenance
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState:<init>(java.lang.String,int,int)	DISABLED	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState:<init>(java.lang.String,int,int)	ENABLED	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyState:<init>(java.lang.String,int,int)	REMOVED	2	3	int	0	3
org.apache.hadoop.hdfs.client.CreateEncryptionZoneFlag:<init>(java.lang.String,int,short)	NO_TRASH	0	3	int	0	0
org.apache.hadoop.hdfs.client.CreateEncryptionZoneFlag:<init>(java.lang.String,int,short)	PROVISION_TRASH	1	3	int	0	1
org.apache.hadoop.hdfs.AddBlockFlag:<init>(java.lang.String,int,short)	NO_LOCAL_WRITE	0	3	int	0	1
org.apache.hadoop.hdfs.AddBlockFlag:<init>(java.lang.String,int,short)	IGNORE_CLIENT_LOCALITY	1	3	int	0	2
org.apache.hadoop.hdfs.AddBlockFlag:<init>(java.lang.String,int,short)	NO_LOCAL_RACK	2	3	int	0	3
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	OPEN	0	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETFILESTATUS	1	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	LISTSTATUS	2	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETHOMEDIRECTORY	3	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETCONTENTSUMMARY	4	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETQUOTAUSAGE	5	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETFILECHECKSUM	6	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETFILEBLOCKLOCATIONS	7	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	INSTRUMENTATION	8	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETACLSTATUS	9	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETTRASHROOT	10	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	APPEND	11	3	java.lang.String	0	POST
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	CONCAT	12	3	java.lang.String	0	POST
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	TRUNCATE	13	3	java.lang.String	0	POST
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	CREATE	14	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	MKDIRS	15	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	RENAME	16	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETOWNER	17	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETPERMISSION	18	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETREPLICATION	19	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETTIMES	20	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	MODIFYACLENTRIES	21	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	REMOVEACLENTRIES	22	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	REMOVEDEFAULTACL	23	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	REMOVEACL	24	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETACL	25	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	DELETE	26	3	java.lang.String	0	DELETE
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETXATTR	27	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETXATTRS	28	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	REMOVEXATTR	29	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	LISTXATTRS	30	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	LISTSTATUS_BATCH	31	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETALLSTORAGEPOLICY	32	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETSTORAGEPOLICY	33	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETSTORAGEPOLICY	34	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	UNSETSTORAGEPOLICY	35	3	java.lang.String	0	POST
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	ALLOWSNAPSHOT	36	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	DISALLOWSNAPSHOT	37	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	CREATESNAPSHOT	38	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	DELETESNAPSHOT	39	3	java.lang.String	0	DELETE
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	RENAMESNAPSHOT	40	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETSNAPSHOTDIFF	41	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETSNAPSHOTTABLEDIRECTORYLIST	42	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETSERVERDEFAULTS	43	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	CHECKACCESS	44	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SETECPOLICY	45	3	java.lang.String	0	PUT
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	GETECPOLICY	46	3	java.lang.String	0	GET
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	UNSETECPOLICY	47	3	java.lang.String	0	POST
org.apache.hadoop.fs.http.client.HttpFSFileSystem$Operation:<init>(java.lang.String,int,java.lang.String)	SATISFYSTORAGEPOLICY	48	3	java.lang.String	0	PUT
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	UNDEF	0	3	int	0	0
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	UNDEF	0	4	int	0	0
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	BOOTING	1	3	int	0	0
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	BOOTING	1	4	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	HALTED	2	3	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	HALTED	2	4	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	ADMIN	3	3	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	ADMIN	3	4	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	NORMAL	4	3	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	NORMAL	4	4	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	SHUTTING_DOWN	5	3	int	0	0
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	SHUTTING_DOWN	5	4	int	0	1
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	SHUTDOWN	6	3	int	0	0
org.apache.hadoop.lib.server.Server$Status:<init>(java.lang.String,int,boolean,boolean)	SHUTDOWN	6	4	int	0	0
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S01	0	3	java.lang.String	0	Dir [{0}] does not exist
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S02	1	3	java.lang.String	0	[{0}] is not a directory
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S03	2	3	java.lang.String	0	Could not load file from classpath [{0}], {1}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S04	3	3	java.lang.String	0	Service [{0}] does not implement declared interface [{1}]
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S05	4	3	java.lang.String	0	[{0}] is not a file
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S06	5	3	java.lang.String	0	Could not load file [{0}], {1}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S07	6	3	java.lang.String	0	Could not instanciate service class [{0}], {1}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S08	7	3	java.lang.String	0	Could not load service classes, {0}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S09	8	3	java.lang.String	0	Could not set service [{0}] programmatically -server shutting down-, {1}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S10	9	3	java.lang.String	0	Service [{0}] requires service [{1}]
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S11	10	3	java.lang.String	0	Service [{0}] exception during status change to [{1}] -server shutting down-, {2}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S12	11	3	java.lang.String	0	Could not start service [{0}], {1}
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S13	12	3	java.lang.String	0	Missing system property [{0}]
org.apache.hadoop.lib.server.ServerException$ERROR:<init>(java.lang.String,int,java.lang.String)	S14	13	3	java.lang.String	0	Could not initialize server, {0}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H01	0	3	java.lang.String	0	Service property [{0}] not defined
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H02	1	3	java.lang.String	0	Kerberos initialization failed, {0}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H03	2	3	java.lang.String	0	FileSystemExecutor error, {0}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H04	3	3	java.lang.String	0	Invalid configuration, it has not be created by the FileSystemAccessService
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H05	4	3	java.lang.String	0	[{0}] validation failed, {1}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H06	5	3	java.lang.String	0	Property [{0}] not defined in configuration object
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H07	6	3	java.lang.String	0	[{0}] not healthy, {1}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H08	7	3	java.lang.String	0	{0}
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H09	8	3	java.lang.String	0	Invalid FileSystemAccess security mode [{0}]
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H10	9	3	java.lang.String	0	Hadoop config directory not found [{0}]
org.apache.hadoop.lib.service.FileSystemAccessException$ERROR:<init>(java.lang.String,int,java.lang.String)	H11	10	3	java.lang.String	0	Could not load Hadoop config files, {0}
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$DestOrder:<init>(java.lang.String,int,int)	HASH	0	3	int	0	0
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$DestOrder:<init>(java.lang.String,int,int)	LOCAL	1	3	int	0	1
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$DestOrder:<init>(java.lang.String,int,int)	RANDOM	2	3	int	0	2
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$DestOrder:<init>(java.lang.String,int,int)	HASH_ALL	3	3	int	0	3
org.apache.hadoop.hdfs.federation.protocol.proto.HdfsServerFederationProtos$MountTableRecordProto$DestOrder:<init>(java.lang.String,int,int)	SPACE	4	3	int	0	4
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NAMENODE	0	3	java.lang.String	0	-namenodes
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NAMENODE	0	4	java.lang.String	0	gets list of namenodes in the cluster.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SECONDARY	1	3	java.lang.String	0	-secondaryNameNodes
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SECONDARY	1	4	java.lang.String	0	gets list of secondary namenodes in the cluster.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BACKUP	2	3	java.lang.String	0	-backupNodes
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BACKUP	2	4	java.lang.String	0	gets list of backup nodes in the cluster.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	JOURNALNODE	3	3	java.lang.String	0	-journalNodes
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	JOURNALNODE	3	4	java.lang.String	0	gets list of journal nodes in the cluster.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	INCLUDE_FILE	4	3	java.lang.String	0	-includeFile
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	INCLUDE_FILE	4	4	java.lang.String	0	gets the include file path that defines the datanodes that can join the cluster.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	EXCLUDE_FILE	5	3	java.lang.String	0	-excludeFile
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	EXCLUDE_FILE	5	4	java.lang.String	0	gets the exclude file path that defines the datanodes that need to decommissioned.
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NNRPCADDRESSES	6	3	java.lang.String	0	-nnRpcAddresses
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NNRPCADDRESSES	6	4	java.lang.String	0	gets the namenode rpc addresses
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CONFKEY	7	3	java.lang.String	0	-confKey [key]
org.apache.hadoop.hdfs.tools.GetConf$Command:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CONFKEY	7	4	java.lang.String	0	gets a specific key from the configuration
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruption$PBImageCorruptionType:<init>(java.lang.String,int,java.lang.String)	CORRUPT_NODE	0	3	java.lang.String	0	CorruptNode
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruption$PBImageCorruptionType:<init>(java.lang.String,int,java.lang.String)	MISSING_CHILD	1	3	java.lang.String	0	MissingChild
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole:<init>(java.lang.String,int,java.lang.String)	NAMENODE	0	3	java.lang.String	0	NameNode
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole:<init>(java.lang.String,int,java.lang.String)	BACKUP	1	3	java.lang.String	0	Backup Node
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$NamenodeRole:<init>(java.lang.String,int,java.lang.String)	CHECKPOINT	2	3	java.lang.String	0	Checkpoint Node
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	FORMAT	0	3	java.lang.String	0	-format
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	CLUSTERID	1	3	java.lang.String	0	-clusterid
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	GENCLUSTERID	2	3	java.lang.String	0	-genclusterid
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	REGULAR	3	3	java.lang.String	0	-regular
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	BACKUP	4	3	java.lang.String	0	-backup
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	CHECKPOINT	5	3	java.lang.String	0	-checkpoint
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	UPGRADE	6	3	java.lang.String	0	-upgrade
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	ROLLBACK	7	3	java.lang.String	0	-rollback
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	ROLLINGUPGRADE	8	3	java.lang.String	0	-rollingUpgrade
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	IMPORT	9	3	java.lang.String	0	-importCheckpoint
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	BOOTSTRAPSTANDBY	10	3	java.lang.String	0	-bootstrapStandby
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	INITIALIZESHAREDEDITS	11	3	java.lang.String	0	-initializeSharedEdits
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	RECOVER	12	3	java.lang.String	0	-recover
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	FORCE	13	3	java.lang.String	0	-force
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	NONINTERACTIVE	14	3	java.lang.String	0	-nonInteractive
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	SKIPSHAREDEDITSCHECK	15	3	java.lang.String	0	-skipSharedEditsCheck
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	RENAMERESERVED	16	3	java.lang.String	0	-renameReserved
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	METADATAVERSION	17	3	java.lang.String	0	-metadataVersion
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	UPGRADEONLY	18	3	java.lang.String	0	-upgradeOnly
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	HOTSWAP	19	3	java.lang.String	0	-hotswap
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption:<init>(java.lang.String,int,java.lang.String)	OBSERVER	20	3	java.lang.String	0	-observer
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState:<init>(java.lang.String,int,int)	FINALIZED	0	3	int	0	0
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState:<init>(java.lang.String,int,int)	RBW	1	3	int	0	1
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState:<init>(java.lang.String,int,int)	RWR	2	3	int	0	2
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState:<init>(java.lang.String,int,int)	RUR	3	3	int	0	3
org.apache.hadoop.hdfs.server.common.HdfsServerConstants$ReplicaState:<init>(java.lang.String,int,int)	TEMPORARY	4	3	int	0	4
org.apache.hadoop.hdfs.server.common.sps.BlockMovementStatus:<init>(java.lang.String,int,int)	DN_BLK_STORAGE_MOVEMENT_SUCCESS	0	3	int	0	0
org.apache.hadoop.hdfs.server.common.sps.BlockMovementStatus:<init>(java.lang.String,int,int)	DN_BLK_STORAGE_MOVEMENT_FAILURE	1	3	int	0	-1
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NOT_IN_SERVICE	0	3	java.lang.String	0	the node is not in service
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NODE_STALE	1	3	java.lang.String	0	the node is stale
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NODE_TOO_BUSY	2	3	java.lang.String	0	the node is too busy
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	TOO_MANY_NODES_ON_RACK	3	3	java.lang.String	0	the rack has too many chosen nodes
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NOT_ENOUGH_STORAGE_SPACE	4	3	java.lang.String	0	not enough storage space to place the block
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NO_REQUIRED_STORAGE_TYPE	5	3	java.lang.String	0	required storage types are unavailable
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault$NodeNotChosenReason:<init>(java.lang.String,int,java.lang.String)	NODE_SLOW	6	3	java.lang.String	0	the node is too slow
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature[])	FIRST_LAYOUT	0	3	int	0	-55
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature[])	FIRST_LAYOUT	0	4	int	0	-53
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature[])	FIRST_LAYOUT	0	5	java.lang.String	0	First datanode layout
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature[])	FIRST_LAYOUT	0	6	int	0	0
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature[])	FIRST_LAYOUT	0	7		0	
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	BLOCKID_BASED_LAYOUT	1	3	int	0	-56
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	BLOCKID_BASED_LAYOUT	1	4	java.lang.String	0	The block ID of a finalized block uniquely determines its position in the directory structure
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	BLOCKID_BASED_LAYOUT_32_by_32	2	3	int	0	-57
org.apache.hadoop.hdfs.server.datanode.DataNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	BLOCKID_BASED_LAYOUT_32_by_32	2	4	java.lang.String	0	Identical to the block id based layout (-56) except it uses a smaller directory structure (32x32)
org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult:<init>(java.lang.String,int,int)	HEALTHY	0	3	int	0	1
org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult:<init>(java.lang.String,int,int)	DEGRADED	1	3	int	0	2
org.apache.hadoop.hdfs.server.datanode.checker.VolumeCheckResult:<init>(java.lang.String,int,int)	FAILED	2	3	int	0	3
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	PERMISSION	0	3	null	0	null
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	PERMISSION	0	4	int	0	3
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	TYPE	1	3		0	
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	TYPE	1	4	int	0	2
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	SCOPE	2	3		0	
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	SCOPE	2	4	int	0	1
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NAME	3	3		0	
org.apache.hadoop.hdfs.server.namenode.AclEntryStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NAME	3	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	MODE	0	3	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	MODE	0	4	int	0	16
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	GROUP	1	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	GROUP	1	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	USER	2	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	USER	2	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	MODE	0	3	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	MODE	0	4	int	0	16
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	GROUP	1	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	GROUP	1	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	USER	2	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeWithAdditionalFields$PermissionStatusFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	USER	2	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Type:<init>(java.lang.String,int,int)	FILEDIFF	0	3	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$SnapshotDiffSection$DiffEntry$Type:<init>(java.lang.String,int,int)	DIRECTORYDIFF	1	3	int	0	2
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	SUCCESS	0	3	int	0	200
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	SUCCESS	0	4	int	0	0
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	AUTHENTICATION_FAILURE	1	3	int	0	403
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	AUTHENTICATION_FAILURE	1	4	int	0	1
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	NOT_ACTIVE_NAMENODE_FAILURE	2	3	int	0	417
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	NOT_ACTIVE_NAMENODE_FAILURE	2	4	int	0	0
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	OLD_TRANSACTION_ID_FAILURE	3	3	int	0	409
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	OLD_TRANSACTION_ID_FAILURE	3	4	int	0	0
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	UNEXPECTED_FAILURE	4	3	int	0	-1
org.apache.hadoop.hdfs.server.namenode.TransferFsImage$TransferResult:<init>(java.lang.String,int,int,boolean)	UNEXPECTED_FAILURE	4	4	int	0	1
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	RESERVED	0	3	null	0	null
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	RESERVED	0	4	int	0	5
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NS_EXT	1	3		0	
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NS_EXT	1	4	int	0	1
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NAME	2	3		0	
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NAME	2	4	int	0	24
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NS	3	3		0	
org.apache.hadoop.hdfs.server.namenode.XAttrFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int)	NS	3	4	int	0	2
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD	0	3	int	0	0
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD	0	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME_OLD	1	3	int	0	1
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME_OLD	1	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DELETE	2	3	int	0	2
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DELETE	2	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MKDIR	3	3	int	0	3
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MKDIR	3	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_REPLICATION	4	3	int	0	4
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_REPLICATION	4	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte)	OP_DATANODE_ADD	5	3	int	0	5
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte)	OP_DATANODE_REMOVE	6	3	int	0	6
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_PERMISSIONS	7	3	int	0	7
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_PERMISSIONS	7	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_OWNER	8	3	int	0	8
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_OWNER	8	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CLOSE	9	3	int	0	9
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CLOSE	9	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_GENSTAMP_V1	10	3	int	0	10
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_GENSTAMP_V1	10	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_NS_QUOTA	11	3	int	0	11
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_NS_QUOTA	11	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CLEAR_NS_QUOTA	12	3	int	0	12
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CLEAR_NS_QUOTA	12	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_TIMES	13	3	int	0	13
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_TIMES	13	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_QUOTA	14	3	int	0	14
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_QUOTA	14	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME	15	3	int	0	15
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME	15	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CONCAT_DELETE	16	3	int	0	16
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CONCAT_DELETE	16	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SYMLINK	17	3	int	0	17
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SYMLINK	17	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_GET_DELEGATION_TOKEN	18	3	int	0	18
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_GET_DELEGATION_TOKEN	18	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENEW_DELEGATION_TOKEN	19	3	int	0	19
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENEW_DELEGATION_TOKEN	19	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CANCEL_DELEGATION_TOKEN	20	3	int	0	20
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CANCEL_DELEGATION_TOKEN	20	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_UPDATE_MASTER_KEY	21	3	int	0	21
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_UPDATE_MASTER_KEY	21	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REASSIGN_LEASE	22	3	int	0	22
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REASSIGN_LEASE	22	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_END_LOG_SEGMENT	23	3	int	0	23
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_END_LOG_SEGMENT	23	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_START_LOG_SEGMENT	24	3	int	0	24
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_START_LOG_SEGMENT	24	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_UPDATE_BLOCKS	25	3	int	0	25
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_UPDATE_BLOCKS	25	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CREATE_SNAPSHOT	26	3	int	0	26
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_CREATE_SNAPSHOT	26	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DELETE_SNAPSHOT	27	3	int	0	27
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DELETE_SNAPSHOT	27	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME_SNAPSHOT	28	3	int	0	28
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_RENAME_SNAPSHOT	28	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ALLOW_SNAPSHOT	29	3	int	0	29
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ALLOW_SNAPSHOT	29	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DISALLOW_SNAPSHOT	30	3	int	0	30
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DISALLOW_SNAPSHOT	30	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_GENSTAMP_V2	31	3	int	0	31
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_GENSTAMP_V2	31	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ALLOCATE_BLOCK_ID	32	3	int	0	32
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ALLOCATE_BLOCK_ID	32	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_BLOCK	33	3	int	0	33
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_BLOCK	33	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_CACHE_DIRECTIVE	34	3	int	0	34
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_CACHE_DIRECTIVE	34	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_CACHE_DIRECTIVE	35	3	int	0	35
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_CACHE_DIRECTIVE	35	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_CACHE_POOL	36	3	int	0	36
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_CACHE_POOL	36	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MODIFY_CACHE_POOL	37	3	int	0	37
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MODIFY_CACHE_POOL	37	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_CACHE_POOL	38	3	int	0	38
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_CACHE_POOL	38	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MODIFY_CACHE_DIRECTIVE	39	3	int	0	39
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_MODIFY_CACHE_DIRECTIVE	39	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_ACL	40	3	int	0	40
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_ACL	40	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ROLLING_UPGRADE_START	41	3	int	0	41
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ROLLING_UPGRADE_START	41	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ROLLING_UPGRADE_FINALIZE	42	3	int	0	42
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ROLLING_UPGRADE_FINALIZE	42	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_XATTR	43	3	int	0	43
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_XATTR	43	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_XATTR	44	3	int	0	44
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_XATTR	44	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_STORAGE_POLICY	45	3	int	0	45
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_STORAGE_POLICY	45	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_TRUNCATE	46	3	int	0	46
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_TRUNCATE	46	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_APPEND	47	3	int	0	47
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_APPEND	47	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_QUOTA_BY_STORAGETYPE	48	3	int	0	48
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_SET_QUOTA_BY_STORAGETYPE	48	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_ERASURE_CODING_POLICY	49	3	int	0	49
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ADD_ERASURE_CODING_POLICY	49	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ENABLE_ERASURE_CODING_POLICY	50	3	int	0	50
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_ENABLE_ERASURE_CODING_POLICY	50	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DISABLE_ERASURE_CODING_POLICY	51	3	int	0	51
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_DISABLE_ERASURE_CODING_POLICY	51	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_ERASURE_CODING_POLICY	52	3	int	0	52
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte,java.lang.Class)	OP_REMOVE_ERASURE_CODING_POLICY	52	4		0	
org.apache.hadoop.hdfs.server.namenode.FSEditLogOpCodes:<init>(java.lang.String,int,byte)	OP_INVALID	53	3	int	0	-1
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	PREFERRED_BLOCK_SIZE	0	3	null	0	null
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	PREFERRED_BLOCK_SIZE	0	4	int	0	48
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	PREFERRED_BLOCK_SIZE	0	5	long	0	1
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	BLOCK_LAYOUT_AND_REDUNDANCY	1	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	BLOCK_LAYOUT_AND_REDUNDANCY	1	4	int	0	12
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	BLOCK_LAYOUT_AND_REDUNDANCY	1	5	long	0	0
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	STORAGE_POLICY_ID	2	3		0	
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	STORAGE_POLICY_ID	2	4	int	0	4
org.apache.hadoop.hdfs.server.namenode.INodeFile$HeaderFormat:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat,int,long)	STORAGE_POLICY_ID	2	5	long	0	0
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	LOADING_FSIMAGE	0	3	java.lang.String	0	LoadingFsImage
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	LOADING_FSIMAGE	0	4	java.lang.String	0	Loading fsimage
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	LOADING_EDITS	1	3	java.lang.String	0	LoadingEdits
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	LOADING_EDITS	1	4	java.lang.String	0	Loading edits
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAVING_CHECKPOINT	2	3	java.lang.String	0	SavingCheckpoint
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAVING_CHECKPOINT	2	4	java.lang.String	0	Saving checkpoint
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAFEMODE	3	3	java.lang.String	0	SafeMode
org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAFEMODE	3	4	java.lang.String	0	Safe mode
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	AWAITING_REPORTED_BLOCKS	0	3	java.lang.String	0	AwaitingReportedBlocks
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	AWAITING_REPORTED_BLOCKS	0	4	java.lang.String	0	awaiting reported blocks
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DELEGATION_KEYS	1	3	java.lang.String	0	DelegationKeys
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DELEGATION_KEYS	1	4	java.lang.String	0	delegation keys
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DELEGATION_TOKENS	2	3	java.lang.String	0	DelegationTokens
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DELEGATION_TOKENS	2	4	java.lang.String	0	delegation tokens
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	INODES	3	3	java.lang.String	0	Inodes
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	INODES	3	4	java.lang.String	0	inodes
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CACHE_POOLS	4	3	java.lang.String	0	CachePools
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CACHE_POOLS	4	4	java.lang.String	0	cache pools
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CACHE_ENTRIES	5	3	java.lang.String	0	CacheEntries
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CACHE_ENTRIES	5	4	java.lang.String	0	cache entries
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	ERASURE_CODING_POLICIES	6	3	java.lang.String	0	ErasureCodingPolicies
org.apache.hadoop.hdfs.server.namenode.startupprogress.StepType:<init>(java.lang.String,int,java.lang.String,java.lang.String)	ERASURE_CODING_POLICIES	6	4	java.lang.String	0	erasure coding policies
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	3	int	0	-55
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	4	int	0	-53
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	5	int	0	-55
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	6	java.lang.String	0	Support rolling upgrade
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	7	int	0	0
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature[])	ROLLING_UPGRADE	0	8		0	
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EDITLOG_LENGTH	1	3	int	0	-56
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EDITLOG_LENGTH	1	4	int	0	-56
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EDITLOG_LENGTH	1	5	java.lang.String	0	Add length field to every edit log op
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS	2	3	int	0	-57
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS	2	4	int	0	-57
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS	2	5	java.lang.String	0	Extended attributes
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	CREATE_OVERWRITE	3	3	int	0	-58
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	CREATE_OVERWRITE	3	4	int	0	-58
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	CREATE_OVERWRITE	3	5	java.lang.String	0	Use single editlog record for creating file with overwrite
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS_NAMESPACE_EXT	4	3	int	0	-59
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS_NAMESPACE_EXT	4	4	int	0	-59
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	XATTRS_NAMESPACE_EXT	4	5	java.lang.String	0	Increase number of xattr namespaces
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	BLOCK_STORAGE_POLICY	5	3	int	0	-60
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	BLOCK_STORAGE_POLICY	5	4	int	0	-60
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	BLOCK_STORAGE_POLICY	5	5	java.lang.String	0	Block Storage policy
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	TRUNCATE	6	3	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	TRUNCATE	6	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	TRUNCATE	6	5	java.lang.String	0	Truncate
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	APPEND_NEW_BLOCK	7	3	int	0	-62
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	APPEND_NEW_BLOCK	7	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	APPEND_NEW_BLOCK	7	5	java.lang.String	0	Support appending to new block
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	QUOTA_BY_STORAGE_TYPE	8	3	int	0	-63
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	QUOTA_BY_STORAGE_TYPE	8	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	QUOTA_BY_STORAGE_TYPE	8	5	java.lang.String	0	Support quota for specific storage types
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	ERASURE_CODING	9	3	int	0	-64
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	ERASURE_CODING	9	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	ERASURE_CODING	9	5	java.lang.String	0	Support erasure coding
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EXPANDED_STRING_TABLE	10	3	int	0	-65
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EXPANDED_STRING_TABLE	10	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	EXPANDED_STRING_TABLE	10	5	java.lang.String	0	Support expanded string table in fsimage
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	SNAPSHOT_MODIFICATION_TIME	11	3	int	0	-66
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	SNAPSHOT_MODIFICATION_TIME	11	4	int	0	-61
org.apache.hadoop.hdfs.server.namenode.NameNodeLayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String)	SNAPSHOT_MODIFICATION_TIME	11	5	java.lang.String	0	Support modification time for snapshot
org.apache.hadoop.hdfs.server.namenode.SerialNumberManager:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat$Enum[])	GLOBAL	0	3		0	
org.apache.hadoop.hdfs.server.namenode.SerialNumberManager:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat$Enum[])	USER	1	3		0	
org.apache.hadoop.hdfs.server.namenode.SerialNumberManager:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat$Enum[])	GROUP	2	3		0	
org.apache.hadoop.hdfs.server.namenode.SerialNumberManager:<init>(java.lang.String,int,org.apache.hadoop.hdfs.util.LongBitFormat$Enum[])	XATTR	3	3		0	
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	IMAGE	0	3	java.lang.String	0	fsimage
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	TIME	1	3	java.lang.String	0	fstime
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	SEEN_TXID	2	3	java.lang.String	0	seen_txid
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	EDITS	3	3	java.lang.String	0	edits
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	IMAGE_NEW	4	3	java.lang.String	0	fsimage.ckpt
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	IMAGE_ROLLBACK	5	3	java.lang.String	0	fsimage_rollback
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	EDITS_NEW	6	3	java.lang.String	0	edits.new
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	EDITS_INPROGRESS	7	3	java.lang.String	0	edits_inprogress
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	EDITS_TMP	8	3	java.lang.String	0	edits_tmp
org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile:<init>(java.lang.String,int,java.lang.String)	IMAGE_LEGACY_OIV	9	3	java.lang.String	0	fsimage_legacy_oiv
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	NS_INFO	0	3	java.lang.String	0	NS_INFO
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	STRING_TABLE	1	3	java.lang.String	0	STRING_TABLE
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	EXTENDED_ACL	2	3	java.lang.String	0	EXTENDED_ACL
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	ERASURE_CODING	3	3	java.lang.String	0	ERASURE_CODING
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE	4	3	java.lang.String	0	INODE
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE_SUB	5	3	java.lang.String	0	INODE_SUB
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE_REFERENCE	6	3	java.lang.String	0	INODE_REFERENCE
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE_REFERENCE_SUB	7	3	java.lang.String	0	INODE_REFERENCE_SUB
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	SNAPSHOT	8	3	java.lang.String	0	SNAPSHOT
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE_DIR	9	3	java.lang.String	0	INODE_DIR
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	INODE_DIR_SUB	10	3	java.lang.String	0	INODE_DIR_SUB
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	FILES_UNDERCONSTRUCTION	11	3	java.lang.String	0	FILES_UNDERCONSTRUCTION
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	SNAPSHOT_DIFF	12	3	java.lang.String	0	SNAPSHOT_DIFF
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	SNAPSHOT_DIFF_SUB	13	3	java.lang.String	0	SNAPSHOT_DIFF_SUB
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	SECRET_MANAGER	14	3	java.lang.String	0	SECRET_MANAGER
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$SectionName:<init>(java.lang.String,int,java.lang.String)	CACHE_MANAGER	15	3	java.lang.String	0	CACHE_MANAGER
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Type:<init>(java.lang.String,int,int)	FILE	0	3	int	0	1
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Type:<init>(java.lang.String,int,int)	DIRECTORY	1	3	int	0	2
org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeSection$INode$Type:<init>(java.lang.String,int,int)	SYMLINK	2	3	int	0	3
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	IN_PROGRESS	1	3	int	0	1
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	ALREADY_RUNNING	2	3	int	0	-1
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	NO_MOVE_BLOCK	3	3	int	0	-2
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	NO_MOVE_PROGRESS	4	3	int	0	-3
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	IO_EXCEPTION	5	3	int	0	-4
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	ILLEGAL_ARGUMENTS	6	3	int	0	-5
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	INTERRUPTED	7	3	int	0	-6
org.apache.hadoop.hdfs.server.balancer.ExitStatus:<init>(java.lang.String,int,int)	UNFINALIZED_UPGRADE	8	3	int	0	-7
org.apache.hadoop.hdfs.server.protocol.NamespaceInfo$Capability:<init>(java.lang.String,int,boolean)	UNKNOWN	0	3	int	0	0
org.apache.hadoop.hdfs.server.protocol.NamespaceInfo$Capability:<init>(java.lang.String,int,boolean)	STORAGE_BLOCK_REPORT_BUFFERS	1	3	int	0	1
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo$BlockStatus:<init>(java.lang.String,int,int)	RECEIVING_BLOCK	0	3	int	0	1
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo$BlockStatus:<init>(java.lang.String,int,int)	RECEIVED_BLOCK	1	3	int	0	2
org.apache.hadoop.hdfs.server.protocol.ReceivedDeletedBlockInfo$BlockStatus:<init>(java.lang.String,int,int)	DELETED_BLOCK	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Type:<init>(java.lang.String,int,int)	NamenodeCommand	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeCommandProto$Type:<init>(java.lang.String,int,int)	CheckPointCommand	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$NamenodeRoleProto:<init>(java.lang.String,int,int)	NAMENODE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$NamenodeRoleProto:<init>(java.lang.String,int,int)	BACKUP	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NamenodeRegistrationProto$NamenodeRoleProto:<init>(java.lang.String,int,int)	CHECKPOINT	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	BalancerBandwidthCommand	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	BlockCommand	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	BlockRecoveryCommand	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	FinalizeCommand	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	KeyUpdateCommand	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	RegisterCommand	5	3	int	0	5
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	UnusedUpgradeCommand	6	3	int	0	6
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	NullDatanodeCommand	7	3	int	0	7
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	BlockIdCommand	8	3	int	0	8
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeCommandProto$Type:<init>(java.lang.String,int,int)	BlockECReconstructionCommand	9	3	int	0	9
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto:<init>(java.lang.String,int,int)	FINALIZED	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto:<init>(java.lang.String,int,int)	RBW	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto:<init>(java.lang.String,int,int)	RWR	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto:<init>(java.lang.String,int,int)	RUR	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ReplicaStateProto:<init>(java.lang.String,int,int)	TEMPORARY	4	3	int	0	4
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Action:<init>(java.lang.String,int,int)	CACHE	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockIdCommandProto$Action:<init>(java.lang.String,int,int)	UNCACHE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State:<init>(java.lang.String,int,int)	ACTIVE	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State:<init>(java.lang.String,int,int)	STANDBY	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$State:<init>(java.lang.String,int,int)	OBSERVER	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Action:<init>(java.lang.String,int,int)	TRANSFER	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Action:<init>(java.lang.String,int,int)	INVALIDATE	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$BlockCommandProto$Action:<init>(java.lang.String,int,int)	SHUTDOWN	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode:<init>(java.lang.String,int,int)	NOTIFY	0	3	int	0	0
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode:<init>(java.lang.String,int,int)	DISK_ERROR	1	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode:<init>(java.lang.String,int,int)	INVALID_BLOCK	2	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ErrorReportRequestProto$ErrorCode:<init>(java.lang.String,int,int)	FATAL_DISK_ERROR	3	3	int	0	3
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$BlockStatus:<init>(java.lang.String,int,int)	RECEIVING	0	3	int	0	1
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$BlockStatus:<init>(java.lang.String,int,int)	RECEIVED	1	3	int	0	2
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$ReceivedDeletedBlockInfoProto$BlockStatus:<init>(java.lang.String,int,int)	DELETED	2	3	int	0	3
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	NAMESPACE_QUOTA	0	3	int	0	-16
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	NAMESPACE_QUOTA	0	4	java.lang.String	0	Support for namespace quotas
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FILE_ACCESS_TIME	1	3	int	0	-17
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FILE_ACCESS_TIME	1	4	java.lang.String	0	Support for access time on files
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	DISKSPACE_QUOTA	2	3	int	0	-18
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	DISKSPACE_QUOTA	2	4	java.lang.String	0	Support for disk space quotas
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	STICKY_BIT	3	3	int	0	-19
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	STICKY_BIT	3	4	java.lang.String	0	Support for sticky bits
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	APPEND_RBW_DIR	4	3	int	0	-20
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	APPEND_RBW_DIR	4	4	java.lang.String	0	Datanode has "rbw" subdirectory for append
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ATOMIC_RENAME	5	3	int	0	-21
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ATOMIC_RENAME	5	4	java.lang.String	0	Support for atomic rename
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	CONCAT	6	3	int	0	-22
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	CONCAT	6	4	java.lang.String	0	Support for concat operation
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SYMLINKS	7	3	int	0	-23
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SYMLINKS	7	4	java.lang.String	0	Support for symbolic links
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	DELEGATION_TOKEN	8	3	int	0	-24
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	DELEGATION_TOKEN	8	4	java.lang.String	0	Support for delegation tokens for security
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_COMPRESSION	9	3	int	0	-25
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_COMPRESSION	9	4	java.lang.String	0	Support for fsimage compression
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_CHECKSUM	10	3	int	0	-26
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_CHECKSUM	10	4	java.lang.String	0	Support checksum for fsimage
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	REMOVE_REL13_DISK_LAYOUT_SUPPORT	11	3	int	0	-27
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	REMOVE_REL13_DISK_LAYOUT_SUPPORT	11	4	java.lang.String	0	Remove support for 0.13 disk layout
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITS_CHECKSUM	12	3	int	0	-28
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITS_CHECKSUM	12	4	java.lang.String	0	Support checksum for editlog
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	UNUSED	13	3	int	0	-29
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	UNUSED	13	4	java.lang.String	0	Skipped version
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_NAME_OPTIMIZATION	14	3	int	0	-30
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FSIMAGE_NAME_OPTIMIZATION	14	4	java.lang.String	0	Store only last part of path in fsimage
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_203	15	3	int	0	-31
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_203	15	4	int	0	-19
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_203	15	5	java.lang.String	0	Reserved for release 0.20.203
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_203	15	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_203	15	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_204	16	3	int	0	-32
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_204	16	4	int	0	-31
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_204	16	5	java.lang.String	0	Reserved for release 0.20.204
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_204	16	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL20_204	16	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL22	17	3	int	0	-33
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL22	17	4	int	0	-27
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL22	17	5	java.lang.String	0	Reserved for release 0.22
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL22	17	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL22	17	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL23	18	3	int	0	-34
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL23	18	4	int	0	-30
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL23	18	5	java.lang.String	0	Reserved for release 0.23
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL23	18	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL23	18	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FEDERATION	19	3	int	0	-35
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	FEDERATION	19	4	java.lang.String	0	Support for namenode federation
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	LEASE_REASSIGNMENT	20	3	int	0	-36
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	LEASE_REASSIGNMENT	20	4	java.lang.String	0	Support for persisting lease holder reassignment
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	STORED_TXIDS	21	3	int	0	-37
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	STORED_TXIDS	21	4	java.lang.String	0	Transaction IDs are stored in edits log and image files
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	TXID_BASED_LAYOUT	22	3	int	0	-38
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	TXID_BASED_LAYOUT	22	4	java.lang.String	0	File names in NN Storage are based on transaction IDs
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_OP_OPTIMIZATION	23	3	int	0	-39
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_OP_OPTIMIZATION	23	4	java.lang.String	0	Use LongWritable and ShortWritable directly instead of ArrayWritable of UTF8
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	OPTIMIZE_PERSIST_BLOCKS	24	3	int	0	-40
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	OPTIMIZE_PERSIST_BLOCKS	24	4	java.lang.String	0	Serialize block lists with delta-encoded variable length ints, add OP_UPDATE_BLOCKS
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_2_0	25	3	int	0	-41
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_2_0	25	4	int	0	-32
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_2_0	25	5	java.lang.String	0	Reserved for release 1.2.0
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_2_0	25	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_2_0	25	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	ADD_INODE_ID	26	3	int	0	-42
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	ADD_INODE_ID	26	4	int	0	-40
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	ADD_INODE_ID	26	5	java.lang.String	0	Assign a unique inode id for each inode
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	ADD_INODE_ID	26	6	int	0	0
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	ADD_INODE_ID	26	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SNAPSHOT	27	3	int	0	-43
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SNAPSHOT	27	4	java.lang.String	0	Support for snapshot feature
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_3_0	28	3	int	0	-44
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_3_0	28	4	int	0	-41
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_3_0	28	5	java.lang.String	0	Reserved for release 1.3.0
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_3_0	28	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL1_3_0	28	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	OPTIMIZE_SNAPSHOT_INODES	29	3	int	0	-45
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	OPTIMIZE_SNAPSHOT_INODES	29	4	int	0	-43
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	OPTIMIZE_SNAPSHOT_INODES	29	5	java.lang.String	0	Reduce snapshot inode memory footprint
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	OPTIMIZE_SNAPSHOT_INODES	29	6	int	0	0
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	OPTIMIZE_SNAPSHOT_INODES	29	7		0	
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SEQUENTIAL_BLOCK_ID	30	3	int	0	-46
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	SEQUENTIAL_BLOCK_ID	30	4	java.lang.String	0	Allocate block IDs sequentially and store block IDs in the edits log and image files
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_SUPPORT_RETRYCACHE	31	3	int	0	-47
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_SUPPORT_RETRYCACHE	31	4	java.lang.String	0	Record ClientId and CallId in editlog to enable rebuilding retry cache in case of HA failover
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_ADD_BLOCK	32	3	int	0	-48
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EDITLOG_ADD_BLOCK	32	4	java.lang.String	0	Add new editlog that only records allocation of the new block instead of the entire block list
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ADD_DATANODE_AND_STORAGE_UUIDS	33	3	int	0	-49
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ADD_DATANODE_AND_STORAGE_UUIDS	33	4	java.lang.String	0	Replace StorageID with DatanodeUuid. Use distinct StorageUuid per storage directory.
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ADD_LAYOUT_FLAGS	34	3	int	0	-50
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	ADD_LAYOUT_FLAGS	34	4	java.lang.String	0	Add support for layout flags.
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	CACHING	35	3	int	0	-51
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	CACHING	35	4	java.lang.String	0	Support for cache pools and path-based caching
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	PROTOBUF_FORMAT	36	3	int	0	-52
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	PROTOBUF_FORMAT	36	4	java.lang.String	0	Use protobuf to serialize FSImage
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EXTENDED_ACL	37	3	int	0	-53
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,java.lang.String)	EXTENDED_ACL	37	4	java.lang.String	0	Extended ACL
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL2_4_0	38	3	int	0	-54
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL2_4_0	38	4	int	0	-51
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL2_4_0	38	5	java.lang.String	0	Reserved for release 2.4.0
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL2_4_0	38	6	int	0	1
org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature:<init>(java.lang.String,int,int,int,java.lang.String,boolean,org.apache.hadoop.hdfs.protocol.LayoutVersion$Feature[])	RESERVED_REL2_4_0	38	7		0	
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_NEW	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_SCHEDULED	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_RUNNING	2	3	int	0	3
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_SUCCEEDED	3	3	int	0	4
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_FAILED	4	3	int	0	5
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskStateProto:<init>(java.lang.String,int,int)	TS_KILLED	5	3	int	0	6
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_NEW	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_INITED	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_RUNNING	2	3	int	0	3
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_SUCCEEDED	3	3	int	0	4
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_FAILED	4	3	int	0	5
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_KILLED	5	3	int	0	6
org.apache.hadoop.mapreduce.v2.proto.MRProtos$JobStateProto:<init>(java.lang.String,int,int)	J_ERROR	6	3	int	0	7
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskTypeProto:<init>(java.lang.String,int,int)	MAP	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskTypeProto:<init>(java.lang.String,int,int)	REDUCE	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventStatusProto:<init>(java.lang.String,int,int)	TACE_FAILED	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventStatusProto:<init>(java.lang.String,int,int)	TACE_KILLED	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventStatusProto:<init>(java.lang.String,int,int)	TACE_SUCCEEDED	2	3	int	0	3
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventStatusProto:<init>(java.lang.String,int,int)	TACE_OBSOLETE	3	3	int	0	4
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptCompletionEventStatusProto:<init>(java.lang.String,int,int)	TACE_TIPFAILED	4	3	int	0	5
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_STARTING	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_MAP	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_SHUFFLE	2	3	int	0	3
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_SORT	3	3	int	0	4
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_REDUCE	4	3	int	0	5
org.apache.hadoop.mapreduce.v2.proto.MRProtos$PhaseProto:<init>(java.lang.String,int,int)	P_CLEANUP	5	3	int	0	6
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_NEW	0	3	int	0	1
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_STARTING	1	3	int	0	2
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_RUNNING	2	3	int	0	3
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_COMMIT_PENDING	3	3	int	0	4
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_SUCCEEDED	4	3	int	0	5
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_FAILED	5	3	int	0	6
org.apache.hadoop.mapreduce.v2.proto.MRProtos$TaskAttemptStateProto:<init>(java.lang.String,int,int)	TA_KILLED	6	3	int	0	7
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskState[])	RUNNING	0	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskState[])	PENDING	1	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskState[])	COMPLETED	2	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskAttemptStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptState[])	NEW	0	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskAttemptStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptState[])	RUNNING	1	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskAttemptStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptState[])	SUCCESSFUL	2	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskAttemptStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptState[])	FAILED	3	3		0	
org.apache.hadoop.mapreduce.v2.util.MRApps$TaskAttemptStateUI:<init>(java.lang.String,int,org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptState[])	KILLED	4	3		0	
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	START	0	3	int	0	0
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	SET_JOB_CONF	1	3	int	0	1
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	SET_INPUT_TYPES	2	3	int	0	2
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	RUN_MAP	3	3	int	0	3
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	MAP_ITEM	4	3	int	0	4
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	RUN_REDUCE	5	3	int	0	5
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	REDUCE_KEY	6	3	int	0	6
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	REDUCE_VALUE	7	3	int	0	7
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	CLOSE	8	3	int	0	8
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	ABORT	9	3	int	0	9
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	AUTHENTICATION_REQ	10	3	int	0	10
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	OUTPUT	11	3	int	0	50
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	PARTITIONED_OUTPUT	12	3	int	0	51
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	STATUS	13	3	int	0	52
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	PROGRESS	14	3	int	0	53
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	DONE	15	3	int	0	54
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	REGISTER_COUNTER	16	3	int	0	55
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	INCREMENT_COUNTER	17	3	int	0	56
org.apache.hadoop.mapred.pipes.BinaryProtocol$MessageType:<init>(java.lang.String,int,int)	AUTHENTICATION_RESP	18	3	int	0	57
org.apache.hadoop.mapred.TaskLog$LogName:<init>(java.lang.String,int,java.lang.String)	STDOUT	0	3	java.lang.String	0	stdout
org.apache.hadoop.mapred.TaskLog$LogName:<init>(java.lang.String,int,java.lang.String)	STDERR	1	3	java.lang.String	0	stderr
org.apache.hadoop.mapred.TaskLog$LogName:<init>(java.lang.String,int,java.lang.String)	SYSLOG	2	3	java.lang.String	0	syslog
org.apache.hadoop.mapred.TaskLog$LogName:<init>(java.lang.String,int,java.lang.String)	PROFILE	3	3	java.lang.String	0	profile.out
org.apache.hadoop.mapred.TaskLog$LogName:<init>(java.lang.String,int,java.lang.String)	DEBUGOUT	4	3	java.lang.String	0	debugout
org.apache.hadoop.mapred.QueueACL:<init>(java.lang.String,int,java.lang.String)	SUBMIT_JOB	0	3	java.lang.String	0	acl-submit-job
org.apache.hadoop.mapred.QueueACL:<init>(java.lang.String,int,java.lang.String)	ADMINISTER_JOBS	1	3	java.lang.String	0	acl-administer-jobs
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_JOB_COUNTERS	0	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_JOB_COUNTERS	0	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_JOB_DETAILS	1	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_JOB_DETAILS	1	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_TASK_LOGS	2	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	VIEW_TASK_LOGS	2	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	KILL_JOB	3	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	KILL_JOB	3	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	FAIL_TASK	4	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	FAIL_TASK	4	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	KILL_TASK	5	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	KILL_TASK	5	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	SET_JOB_PRIORITY	6	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	SET_JOB_PRIORITY	6	4		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	SUBMIT_JOB	7	3		0	
org.apache.hadoop.mapred.Operation:<init>(java.lang.String,int,org.apache.hadoop.mapred.QueueACL,org.apache.hadoop.mapreduce.JobACL)	SUBMIT_JOB	7	4	null	0	null
org.apache.hadoop.mapreduce.JobStatus$State:<init>(java.lang.String,int,int)	RUNNING	0	3	int	0	1
org.apache.hadoop.mapreduce.JobStatus$State:<init>(java.lang.String,int,int)	SUCCEEDED	1	3	int	0	2
org.apache.hadoop.mapreduce.JobStatus$State:<init>(java.lang.String,int,int)	FAILED	2	3	int	0	3
org.apache.hadoop.mapreduce.JobStatus$State:<init>(java.lang.String,int,int)	PREP	3	3	int	0	4
org.apache.hadoop.mapreduce.JobStatus$State:<init>(java.lang.String,int,int)	KILLED	4	3	int	0	5
org.apache.hadoop.mapreduce.QueueState:<init>(java.lang.String,int,java.lang.String)	STOPPED	0	3	java.lang.String	0	stopped
org.apache.hadoop.mapreduce.QueueState:<init>(java.lang.String,int,java.lang.String)	RUNNING	1	3	java.lang.String	0	running
org.apache.hadoop.mapreduce.QueueState:<init>(java.lang.String,int,java.lang.String)	UNDEFINED	2	3	java.lang.String	0	undefined
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	DISABLED	0	3	java.lang.String	0	Disabled
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	DISABLED	0	4	int	0	0
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	NOTHING_TO_CLEAN_UP	1	3	java.lang.String	0	Nothing to clean up
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	NOTHING_TO_CLEAN_UP	1	4	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	PARALLEL_DELETE	2	3	java.lang.String	0	Parallel Delete of Task Attempt Directories
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	PARALLEL_DELETE	2	4	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	DELETED	3	3	java.lang.String	0	Delete of job directory
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	DELETED	3	4	int	0	1
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	FAILURE	4	3	java.lang.String	0	Delete failed
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage$Outcome:<init>(java.lang.String,int,java.lang.String,boolean)	FAILURE	4	4	int	0	0
org.apache.hadoop.mapreduce.JobACL:<init>(java.lang.String,int,java.lang.String)	VIEW_JOB	0	3	java.lang.String	0	mapreduce.job.acl-view-job
org.apache.hadoop.mapreduce.JobACL:<init>(java.lang.String,int,java.lang.String)	MODIFY_JOB	1	3	java.lang.String	0	mapreduce.job.acl-modify-job
org.apache.hadoop.mapred.nativetask.serde.SerializationFramework:<init>(java.lang.String,int,int)	WRITABLE_SERIALIZATION	0	3	int	0	0
org.apache.hadoop.mapred.nativetask.serde.SerializationFramework:<init>(java.lang.String,int,int)	NATIVE_SERIALIZATION	1	3	int	0	1
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NUM_ROWS	0	3	java.lang.String	0	mapreduce.terasort.num-rows
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NUM_ROWS	0	4	java.lang.String	0	Number of rows to generate during teragen.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NUM_PARTITIONS	1	3	java.lang.String	0	mapreduce.terasort.num.partitions
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	NUM_PARTITIONS	1	4	java.lang.String	0	Number of partitions used for sampling.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAMPLE_SIZE	2	3	java.lang.String	0	mapreduce.terasort.partitions.sample
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SAMPLE_SIZE	2	4	java.lang.String	0	Sample size for each partition.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FINAL_SYNC_ATTRIBUTE	3	3	java.lang.String	0	mapreduce.terasort.final.sync
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FINAL_SYNC_ATTRIBUTE	3	4	java.lang.String	0	Perform a disk-persisting hsync at end of every file-write.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	USE_TERA_SCHEDULER	4	3	java.lang.String	0	mapreduce.terasort.use.terascheduler
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	USE_TERA_SCHEDULER	4	4	java.lang.String	0	Use TeraScheduler for computing input split distribution.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	USE_SIMPLE_PARTITIONER	5	3	java.lang.String	0	mapreduce.terasort.simplepartitioner
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	USE_SIMPLE_PARTITIONER	5	4	java.lang.String	0	Use SimplePartitioner instead of TotalOrderPartitioner.
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	OUTPUT_REPLICATION	6	3	java.lang.String	0	mapreduce.terasort.output.replication
org.apache.hadoop.examples.terasort.TeraSortConfigKeys:<init>(java.lang.String,int,java.lang.String,java.lang.String)	OUTPUT_REPLICATION	6	4	java.lang.String	0	Replication factor to use for output data files.
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_1	0	3	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_1	0	4	long	0	1
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_1	0	5	int	0	8
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_1	0	6	int	0	-1
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_3	1	3	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_3	1	4	long	0	3
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_3	1	5	int	0	8
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P8_3	1	6	int	0	-6
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P8_5	2	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P8_7	3	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_21	4	3	int	0	1
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_21	4	4	long	0	1
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_21	4	5	int	0	20
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_21	4	6	int	0	2
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_3	5	3	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_3	5	4	long	0	3
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_3	5	5	int	0	20
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_3	5	6	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_5	6	3	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_5	6	4	long	0	5
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_5	6	5	int	0	20
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_5	6	6	int	0	-4
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_7	7	3	int	0	0
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_7	7	4	long	0	7
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_7	7	5	int	0	20
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_7	7	6	int	0	-4
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_9	8	3	int	0	1
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_9	8	4	long	0	9
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_9	8	5	int	0	20
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,boolean,long,int,int)	P20_9	8	6	int	0	-6
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P20_11	9	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P20_13	10	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P20_15	11	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P20_17	12	3		0	
org.apache.hadoop.examples.pi.math.Bellard$Parameter:<init>(java.lang.String,int,org.apache.hadoop.examples.pi.math.Bellard$Parameter)	P20_19	13	3		0	
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	NONE	0	3	java.lang.String	0	
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	NONE	0	4	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	NONE	0	5	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_S3	1	3	java.lang.String	0	AES256
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_S3	1	4	int	0	1
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_S3	1	5	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_KMS	2	3	java.lang.String	0	SSE-KMS
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_KMS	2	4	int	0	1
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_KMS	2	5	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_C	3	3	java.lang.String	0	SSE-C
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_C	3	4	int	0	1
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	SSE_C	3	5	int	0	1
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_KMS	4	3	java.lang.String	0	CSE-KMS
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_KMS	4	4	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_KMS	4	5	int	0	1
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_CUSTOM	5	3	java.lang.String	0	CSE-CUSTOM
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_CUSTOM	5	4	int	0	0
org.apache.hadoop.fs.s3a.S3AEncryptionMethods:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	CSE_CUSTOM	5	5	int	0	1
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Normal	0	3	java.lang.String	0	default
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Normal	0	4	int	0	0
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Normal	0	5	int	0	1
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Random	1	3	java.lang.String	0	random
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Random	1	4	int	0	1
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Random	1	5	int	0	0
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Sequential	2	3	java.lang.String	0	sequential
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Sequential	2	4	int	0	0
org.apache.hadoop.fs.s3a.S3AInputPolicy:<init>(java.lang.String,int,java.lang.String,boolean,boolean)	Sequential	2	5	int	0	0
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_EXECUTOR_ACQUIRED	0	3	java.lang.String	0	action_executor_acquired
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_EXECUTOR_ACQUIRED	0	4	java.lang.String	0	Executor acquired.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_EXECUTOR_ACQUIRED	0	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_HEAD_REQUEST	1	3	java.lang.String	0	action_http_head_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_HEAD_REQUEST	1	4	java.lang.String	0	HEAD request.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_HEAD_REQUEST	1	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_FILE_OPENED	2	3	java.lang.String	0	action_file_opened
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_FILE_OPENED	2	4	java.lang.String	0	File opened.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_FILE_OPENED	2	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_GET_REQUEST	3	3	java.lang.String	0	action_http_get_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_GET_REQUEST	3	4	java.lang.String	0	GET request.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	ACTION_HTTP_GET_REQUEST	3	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_CREATED	4	3	java.lang.String	0	directories_created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_CREATED	4	4	java.lang.String	0	Total number of directories created through the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_CREATED	4	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_DELETED	5	3	java.lang.String	0	directories_deleted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_DELETED	5	4	java.lang.String	0	Total number of directories deleted through the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DIRECTORIES_DELETED	5	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED	6	3	java.lang.String	0	files_copied
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED	6	4	java.lang.String	0	Total number of files copied within the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED	6	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED_BYTES	7	3	java.lang.String	0	files_copied_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED_BYTES	7	4	java.lang.String	0	Total number of bytes copied within the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_COPIED_BYTES	7	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_CREATED	8	3	java.lang.String	0	files_created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_CREATED	8	4	java.lang.String	0	Total number of files created through the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_CREATED	8	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETED	9	3	java.lang.String	0	files_deleted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETED	9	4	java.lang.String	0	Total number of files deleted from the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETED	9	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETE_REJECTED	10	3	java.lang.String	0	files_delete_rejected
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETE_REJECTED	10	4	java.lang.String	0	Total number of files whose delete request was rejected
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FILES_DELETE_REJECTED	10	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_CREATED	11	3	java.lang.String	0	fake_directories_created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_CREATED	11	4	java.lang.String	0	Total number of fake directory entries created in the object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_CREATED	11	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_DELETED	12	3	java.lang.String	0	fake_directories_deleted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_DELETED	12	4	java.lang.String	0	Total number of fake directory deletes submitted to object store.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	FAKE_DIRECTORIES_DELETED	12	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	IGNORED_ERRORS	13	3	java.lang.String	0	ignored_errors
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	IGNORED_ERRORS	13	4	java.lang.String	0	Errors caught and ignored
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	IGNORED_ERRORS	13	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ABORT	14	3	java.lang.String	0	op_abort
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ABORT	14	4	java.lang.String	0	Calls of abort()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ABORT	14	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ACCESS	15	3	java.lang.String	0	op_access
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ACCESS	15	4	java.lang.String	0	Calls of access()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_ACCESS	15	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_COPY_FROM_LOCAL_FILE	16	3	java.lang.String	0	op_copy_from_local_file
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_COPY_FROM_LOCAL_FILE	16	4	java.lang.String	0	Calls of copyFromLocalFile()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_COPY_FROM_LOCAL_FILE	16	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE	17	3	java.lang.String	0	op_create
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE	17	4	java.lang.String	0	Calls of create()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE	17	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_FILE	18	3	java.lang.String	0	op_createfile
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_FILE	18	4	java.lang.String	0	Calls of createFile()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_FILE	18	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_NON_RECURSIVE	19	3	java.lang.String	0	op_create_non_recursive
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_NON_RECURSIVE	19	4	java.lang.String	0	Calls of createNonRecursive()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_CREATE_NON_RECURSIVE	19	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_DELETE	20	3	java.lang.String	0	op_delete
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_DELETE	20	4	java.lang.String	0	Calls of delete()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_DELETE	20	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_EXISTS	21	3	java.lang.String	0	op_exists
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_EXISTS	21	4	java.lang.String	0	Calls of exists()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_EXISTS	21	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_CONTENT_SUMMARY	22	3	java.lang.String	0	op_get_content_summary
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_CONTENT_SUMMARY	22	4	java.lang.String	0	Calls of getContentSummary()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_CONTENT_SUMMARY	22	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_DELEGATION_TOKEN	23	3	java.lang.String	0	op_get_delegation_token
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_DELEGATION_TOKEN	23	4	java.lang.String	0	Calls of getDelegationToken()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_DELEGATION_TOKEN	23	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_CHECKSUM	24	3	java.lang.String	0	op_get_file_checksum
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_CHECKSUM	24	4	java.lang.String	0	Calls of getFileChecksum()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_CHECKSUM	24	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_STATUS	25	3	java.lang.String	0	op_get_file_status
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_STATUS	25	4	java.lang.String	0	Calls of getFileStatus()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GET_FILE_STATUS	25	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GLOB_STATUS	26	3	java.lang.String	0	op_glob_status
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GLOB_STATUS	26	4	java.lang.String	0	Calls of globStatus()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_GLOB_STATUS	26	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_DIRECTORY	27	3	java.lang.String	0	op_is_directory
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_DIRECTORY	27	4	java.lang.String	0	Calls of isDirectory()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_DIRECTORY	27	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_FILE	28	3	java.lang.String	0	op_is_file
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_FILE	28	4	java.lang.String	0	Calls of isFile()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_IS_FILE	28	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HFLUSH	29	3	java.lang.String	0	op_hflush
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HFLUSH	29	4	java.lang.String	0	Calls of hflush()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HFLUSH	29	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HSYNC	30	3	java.lang.String	0	op_hsync
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HSYNC	30	4	java.lang.String	0	Calls of hsync()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_HSYNC	30	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_FILES	31	3	java.lang.String	0	op_list_files
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_FILES	31	4	java.lang.String	0	Calls of listFiles()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_FILES	31	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_LOCATED_STATUS	32	3	java.lang.String	0	op_list_located_status
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_LOCATED_STATUS	32	4	java.lang.String	0	Calls of listLocatedStatus()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_LOCATED_STATUS	32	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_STATUS	33	3	java.lang.String	0	op_list_status
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_STATUS	33	4	java.lang.String	0	Calls of listStatus()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_LIST_STATUS	33	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_MKDIRS	34	3	java.lang.String	0	op_mkdirs
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_MKDIRS	34	4	java.lang.String	0	Calls of mkdirs()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_MKDIRS	34	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPEN	35	3	java.lang.String	0	op_open
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPEN	35	4	java.lang.String	0	Calls of open()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPEN	35	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPENFILE	36	3	java.lang.String	0	op_openfile
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPENFILE	36	4	java.lang.String	0	Calls of openFile()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OPENFILE	36	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_RENAME	37	3	java.lang.String	0	op_rename
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_RENAME	37	4	java.lang.String	0	Calls of rename()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_RENAME	37	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_MAP	38	3	java.lang.String	0	op_xattr_get_map
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_MAP	38	4	java.lang.String	0	Calls of getXAttrs(Path path)
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_MAP	38	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED	39	3	java.lang.String	0	op_xattr_get_named
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED	39	4	java.lang.String	0	Calls of getXAttr(Path, String)
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED	39	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED_MAP	40	3	java.lang.String	0	op_xattr_get_named_map
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED_MAP	40	4	java.lang.String	0	Calls of xattr()
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_XATTR_GET_NAMED_MAP	40	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OP_XATTR_LIST	41	3	java.lang.String	0	op_xattr_list
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OP_XATTR_LIST	41	4	java.lang.String	0	Calls of getXAttrs(Path path, List<String> names)
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	INVOCATION_OP_XATTR_LIST	41	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_COPY_REQUESTS	42	3	java.lang.String	0	object_copy_requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_COPY_REQUESTS	42	4	java.lang.String	0	Object copy requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_COPY_REQUESTS	42	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_REQUEST	43	3	java.lang.String	0	object_delete_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_REQUEST	43	4	java.lang.String	0	Object delete requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_REQUEST	43	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_BULK_DELETE_REQUEST	44	3	java.lang.String	0	object_bulk_delete_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_BULK_DELETE_REQUEST	44	4	java.lang.String	0	Object bulk delete requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_BULK_DELETE_REQUEST	44	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_OBJECTS	45	3	java.lang.String	0	object_delete_objects
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_OBJECTS	45	4	java.lang.String	0	Objects deleted in delete requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_DELETE_OBJECTS	45	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_LIST_REQUEST	46	3	java.lang.String	0	object_list_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_LIST_REQUEST	46	4	java.lang.String	0	Count of object listings made
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_LIST_REQUEST	46	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_CONTINUE_LIST_REQUESTS	47	3	java.lang.String	0	object_continue_list_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_CONTINUE_LIST_REQUESTS	47	4	java.lang.String	0	Count of continued object listings made
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_CONTINUE_LIST_REQUESTS	47	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_METADATA_REQUESTS	48	3	java.lang.String	0	object_metadata_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_METADATA_REQUESTS	48	4	java.lang.String	0	Count of requests for object metadata
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_METADATA_REQUESTS	48	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_INITIATED	49	3	java.lang.String	0	object_multipart_initiated
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_INITIATED	49	4	java.lang.String	0	Object multipart upload initiated
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_INITIATED	49	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_ABORTED	50	3	java.lang.String	0	object_multipart_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_ABORTED	50	4	java.lang.String	0	Object multipart upload aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_MULTIPART_UPLOAD_ABORTED	50	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS	51	3	java.lang.String	0	object_put_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS	51	4	java.lang.String	0	Object put/multipart upload count
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS	51	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_COMPLETED	52	3	java.lang.String	0	object_put_request_completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_COMPLETED	52	4	java.lang.String	0	Object put/multipart upload completed count
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_COMPLETED	52	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_ACTIVE	53	3	java.lang.String	0	object_put_request_active
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_ACTIVE	53	4	java.lang.String	0	Current number of active put requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_REQUESTS_ACTIVE	53	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES	54	3	java.lang.String	0	object_put_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES	54	4	java.lang.String	0	number of bytes uploaded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES	54	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES_PENDING	55	3	java.lang.String	0	object_put_bytes_pending
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES_PENDING	55	4	java.lang.String	0	number of bytes queued for upload/being actively uploaded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_PUT_BYTES_PENDING	55	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_SELECT_REQUESTS	56	3	java.lang.String	0	object_select_requests
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_SELECT_REQUESTS	56	4	java.lang.String	0	Count of S3 Select requests issued
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	OBJECT_SELECT_REQUESTS	56	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ABORTED	57	3	java.lang.String	0	stream_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ABORTED	57	4	java.lang.String	0	Count of times the TCP stream was aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ABORTED	57	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES	58	3	java.lang.String	0	stream_read_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES	58	4	java.lang.String	0	Bytes read from an input stream in read() calls
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES	58	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_DISCARDED_ABORT	59	3	java.lang.String	0	stream_read_bytes_discarded_in_abort
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_DISCARDED_ABORT	59	4	java.lang.String	0	Count of bytes discarded by aborting an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_DISCARDED_ABORT	59	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_READ_CLOSE	60	3	java.lang.String	0	stream_read_bytes_discarded_in_close
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_READ_CLOSE	60	4	java.lang.String	0	Count of bytes read and discarded when closing an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BYTES_READ_CLOSE	60	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSED	61	3	java.lang.String	0	stream_read_closed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSED	61	4	java.lang.String	0	Count of times the TCP stream was closed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSED	61	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSE_OPERATIONS	62	3	java.lang.String	0	stream_read_close_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSE_OPERATIONS	62	4	java.lang.String	0	Total count of times an attempt to close an input stream was made
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_CLOSE_OPERATIONS	62	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_EXCEPTIONS	63	3	java.lang.String	0	stream_read_exceptions
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_EXCEPTIONS	63	4	java.lang.String	0	Count of exceptions raised during input stream reads
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_EXCEPTIONS	63	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_FULLY_OPERATIONS	64	3	java.lang.String	0	stream_read_fully_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_FULLY_OPERATIONS	64	4	java.lang.String	0	Count of readFully() operations in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_FULLY_OPERATIONS	64	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPENED	65	3	java.lang.String	0	stream_read_opened
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPENED	65	4	java.lang.String	0	Total count of times an input stream to object store data was opened
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPENED	65	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS	66	3	java.lang.String	0	stream_read_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS	66	4	java.lang.String	0	Count of read() operations in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS	66	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_OPERATIONS	67	3	java.lang.String	0	stream_read_vectored_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_OPERATIONS	67	4	java.lang.String	0	Count of readVectored() operations in an input stream.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_OPERATIONS	67	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_READ_BYTES_DISCARDED	68	3	java.lang.String	0	stream_read_vectored_read_bytes_discarded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_READ_BYTES_DISCARDED	68	4	java.lang.String	0	Count of bytes discarded during readVectored() operation. in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_READ_BYTES_DISCARDED	68	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_INCOMING_RANGES	69	3	java.lang.String	0	stream_read_vectored_incoming_ranges
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_INCOMING_RANGES	69	4	java.lang.String	0	Count of incoming file ranges during readVectored() operation.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_INCOMING_RANGES	69	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_COMBINED_RANGES	70	3	java.lang.String	0	stream_read_vectored_combined_ranges
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_COMBINED_RANGES	70	4	java.lang.String	0	Count of combined file ranges during readVectored() operation.
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VECTORED_COMBINED_RANGES	70	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_ABORTED	71	3	java.lang.String	0	stream_read_remote_stream_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_ABORTED	71	4	java.lang.String	0	Duration of aborting a remote stream during stream IO
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_ABORTED	71	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_CLOSED	72	3	java.lang.String	0	stream_read_remote_stream_drain
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_CLOSED	72	4	java.lang.String	0	Duration of closing a remote stream during stream IO
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_REMOTE_STREAM_CLOSED	72	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS_INCOMPLETE	73	3	java.lang.String	0	stream_read_operations_incomplete
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS_INCOMPLETE	73	4	java.lang.String	0	Count of incomplete read() operations in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_OPERATIONS_INCOMPLETE	73	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VERSION_MISMATCHES	74	3	java.lang.String	0	stream_read_version_mismatches
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VERSION_MISMATCHES	74	4	java.lang.String	0	Count of version mismatches encountered while reading an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_VERSION_MISMATCHES	74	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BACKWARD_OPERATIONS	75	3	java.lang.String	0	stream_read_seek_backward_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BACKWARD_OPERATIONS	75	4	java.lang.String	0	Count of executed seek operations which went backwards in a stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BACKWARD_OPERATIONS	75	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_BACKWARDS	76	3	java.lang.String	0	stream_read_bytes_backwards_on_seek
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_BACKWARDS	76	4	java.lang.String	0	Count of bytes moved backwards during seek operations in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_BACKWARDS	76	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_DISCARDED	77	3	java.lang.String	0	stream_read_seek_bytes_discarded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_DISCARDED	77	4	java.lang.String	0	Count of bytes read and discarded during seek() in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_DISCARDED	77	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_SKIPPED	78	3	java.lang.String	0	stream_read_seek_bytes_skipped
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_SKIPPED	78	4	java.lang.String	0	Count of bytes skipped during forward seek operations an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_BYTES_SKIPPED	78	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_FORWARD_OPERATIONS	79	3	java.lang.String	0	stream_read_seek_forward_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_FORWARD_OPERATIONS	79	4	java.lang.String	0	Count of executed seek operations which went forward in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_FORWARD_OPERATIONS	79	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_OPERATIONS	80	3	java.lang.String	0	stream_read_seek_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_OPERATIONS	80	4	java.lang.String	0	Count of seek operations in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_OPERATIONS	80	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_POLICY_CHANGED	81	3	java.lang.String	0	stream_read_seek_policy_changed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_POLICY_CHANGED	81	4	java.lang.String	0	Count of times the seek policy was dynamically changed in an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_SEEK_POLICY_CHANGED	81	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_TOTAL_BYTES	82	3	java.lang.String	0	stream_read_total_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_TOTAL_BYTES	82	4	java.lang.String	0	Total count of bytes read from an input stream
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_TOTAL_BYTES	82	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_UNBUFFERED	83	3	java.lang.String	0	stream_read_unbuffered
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_UNBUFFERED	83	4	java.lang.String	0	Total count of input stream unbuffering operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_UNBUFFERED	83	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BLOCKS_IN_FILE_CACHE	84	3	java.lang.String	0	stream_read_blocks_in_cache
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BLOCKS_IN_FILE_CACHE	84	4	java.lang.String	0	Gauge of blocks in disk cache
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_BLOCKS_IN_FILE_CACHE	84	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_PREFETCH_OPERATIONS	85	3	java.lang.String	0	stream_read_active_prefetch_operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_PREFETCH_OPERATIONS	85	4	java.lang.String	0	Gauge of active prefetches
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_PREFETCH_OPERATIONS	85	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_MEMORY_IN_USE	86	3	java.lang.String	0	stream_read_active_memory_in_use
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_MEMORY_IN_USE	86	4	java.lang.String	0	Gauge of active memory in use
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_READ_ACTIVE_MEMORY_IN_USE	86	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS	87	3	java.lang.String	0	stream_write_exceptions
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS	87	4	java.lang.String	0	Count of stream write failures reported
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS	87	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS_COMPLETING_UPLOADS	88	3	java.lang.String	0	stream_write_exceptions_completing_upload
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS_COMPLETING_UPLOADS	88	4	java.lang.String	0	Count of failures when finalizing a multipart upload
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_EXCEPTIONS_COMPLETING_UPLOADS	88	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS	89	3	java.lang.String	0	stream_write_block_uploads
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS	89	4	java.lang.String	0	Count of block/partition uploads completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS	89	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ACTIVE	90	3	java.lang.String	0	stream_write_block_uploads_active
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ACTIVE	90	4	java.lang.String	0	Count of block/partition uploads active
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ACTIVE	90	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_COMMITTED	91	3	java.lang.String	0	stream_write_block_uploads_committed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_COMMITTED	91	4	java.lang.String	0	Count of number of block uploads committed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_COMMITTED	91	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ABORTED	92	3	java.lang.String	0	stream_write_block_uploads_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ABORTED	92	4	java.lang.String	0	Count of number of block uploads aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_ABORTED	92	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_PENDING	93	3	java.lang.String	0	stream_write_block_uploads_pending
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_PENDING	93	4	java.lang.String	0	Gauge of block/partitions uploads queued to be written
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_PENDING	93	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_BYTES_PENDING	94	3	java.lang.String	0	stream_write_block_uploads_data_pending
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_BYTES_PENDING	94	4	java.lang.String	0	Gauge of data queued to be written
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BLOCK_UPLOADS_BYTES_PENDING	94	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_TIME	95	3	java.lang.String	0	stream_write_total_time
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_TIME	95	4	java.lang.String	0	Count of total time taken for uploads to complete
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_TIME	95	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_DATA	96	3	java.lang.String	0	stream_write_total_data
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_DATA	96	4	java.lang.String	0	Count of total data uploaded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_TOTAL_DATA	96	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BYTES	97	3	java.lang.String	0	stream_write_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BYTES	97	4	java.lang.String	0	Count of bytes written to output stream (including all not yet uploaded)
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_BYTES	97	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_QUEUE_DURATION	98	3	java.lang.String	0	stream_write_queue_duration
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_QUEUE_DURATION	98	4	java.lang.String	0	Total queue duration of all block uploads
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STREAM_WRITE_QUEUE_DURATION	98	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_CREATED	99	3	java.lang.String	0	committer_commits_created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_CREATED	99	4	java.lang.String	0	Count of files to commit created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_CREATED	99	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_COMPLETED	100	3	java.lang.String	0	committer_commits_completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_COMPLETED	100	4	java.lang.String	0	Count of files committed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_COMPLETED	100	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMIT_JOB	101	3	java.lang.String	0	committer_commit_job
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMIT_JOB	101	4	java.lang.String	0	Duration Tracking of time to commit an entire job
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMIT_JOB	101	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_SUCCEEDED	102	3	java.lang.String	0	committer_jobs_completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_SUCCEEDED	102	4	java.lang.String	0	Count of successful jobs
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_SUCCEEDED	102	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_FAILED	103	3	java.lang.String	0	committer_jobs_failed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_FAILED	103	4	java.lang.String	0	Count of failed jobs
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_JOBS_FAILED	103	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_SUCCEEDED	104	3	java.lang.String	0	committer_tasks_completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_SUCCEEDED	104	4	java.lang.String	0	Count of successful tasks
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_SUCCEEDED	104	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_FAILED	105	3	java.lang.String	0	committer_tasks_failed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_FAILED	105	4	java.lang.String	0	Count of failed tasks
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_TASKS_FAILED	105	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_COMMITTED	106	3	java.lang.String	0	committer_bytes_committed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_COMMITTED	106	4	java.lang.String	0	Amount of data committed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_COMMITTED	106	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_UPLOADED	107	3	java.lang.String	0	committer_bytes_uploaded
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_UPLOADED	107	4	java.lang.String	0	Count of bytes uploaded duing commit operations
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_BYTES_UPLOADED	107	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_FAILED	108	3	java.lang.String	0	committer_commits.failures
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_FAILED	108	4	java.lang.String	0	Count of commits failed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_FAILED	108	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_ABORTED	109	3	java.lang.String	0	committer_commits_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_ABORTED	109	4	java.lang.String	0	Count of commits aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_ABORTED	109	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_REVERTED	110	3	java.lang.String	0	committer_commits_reverted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_REVERTED	110	4	java.lang.String	0	Count of commits reverted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_COMMITS_REVERTED	110	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_LOAD_SINGLE_PENDING_FILE	111	3	java.lang.String	0	committer_load_single_pending_file
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_LOAD_SINGLE_PENDING_FILE	111	4	java.lang.String	0	Duration to load a single pending file in task commit
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_LOAD_SINGLE_PENDING_FILE	111	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_FILES_CREATED	112	3	java.lang.String	0	committer_magic_files_created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_FILES_CREATED	112	4	java.lang.String	0	Count of files created under 'magic' paths
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_FILES_CREATED	112	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_MARKER_PUT	113	3	java.lang.String	0	committer_magic_marker_put
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_MARKER_PUT	113	4	java.lang.String	0	Duration Tracking of marker files created under 'magic' paths
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MAGIC_MARKER_PUT	113	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MATERIALIZE_FILE	114	3	java.lang.String	0	committer_materialize_file
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MATERIALIZE_FILE	114	4	java.lang.String	0	Duration Tracking of time to materialize a file in job commit
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_MATERIALIZE_FILE	114	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_STAGE_FILE_UPLOAD	115	3	java.lang.String	0	committer_stage_file_upload
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_STAGE_FILE_UPLOAD	115	4	java.lang.String	0	Duration Tracking of files uploaded from a local staging path
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	COMMITTER_STAGE_FILE_UPLOAD	115	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_EXISTS_PROBE	116	3	java.lang.String	0	store_exists_probe
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_EXISTS_PROBE	116	4	java.lang.String	0	Store Existence Probe
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_EXISTS_PROBE	116	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_REQUEST	117	3	java.lang.String	0	store_io_request
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_REQUEST	117	4	java.lang.String	0	requests made of the remote store
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_REQUEST	117	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_RETRY	118	3	java.lang.String	0	store_io_retry
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_RETRY	118	4	java.lang.String	0	retried requests made of the remote store
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_RETRY	118	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLED	119	3	java.lang.String	0	store_io_throttled
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLED	119	4	java.lang.String	0	Requests throttled and retried
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLED	119	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLE_RATE	120	3	java.lang.String	0	store_io_throttle_rate
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLE_RATE	120	4	java.lang.String	0	Rate of S3 request throttling
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	STORE_IO_THROTTLE_RATE	120	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DELEGATION_TOKENS_ISSUED	121	3	java.lang.String	0	delegation_tokens_issued
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DELEGATION_TOKENS_ISSUED	121	4	java.lang.String	0	Count of delegation tokens issued
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	DELEGATION_TOKENS_ISSUED	121	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_INSTANTIATED	122	3	java.lang.String	0	multipart_instantiated
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_INSTANTIATED	122	4	java.lang.String	0	Multipart Uploader Instantiated
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_INSTANTIATED	122	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT	123	3	java.lang.String	0	multipart_upload_part_put
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT	123	4	java.lang.String	0	Multipart Part Put Operation
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT	123	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT_BYTES	124	3	java.lang.String	0	multipart_upload_part_put_bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT_BYTES	124	4	java.lang.String	0	Multipart Part Put Bytes
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_PART_PUT_BYTES	124	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORTED	125	3	java.lang.String	0	multipart_upload_aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORTED	125	4	java.lang.String	0	Multipart Upload Aborted
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORTED	125	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED	126	3	java.lang.String	0	multipart_upload_abort_under_path_invoked
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED	126	4	java.lang.String	0	Multipart Upload Abort Unner Path Invoked
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_ABORT_UNDER_PATH_INVOKED	126	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_COMPLETED	127	3	java.lang.String	0	multipart_upload_completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_COMPLETED	127	4	java.lang.String	0	Multipart Upload Completed
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_COMPLETED	127	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_LIST	128	3	java.lang.String	0	multipart_upload_list
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_LIST	128	4	java.lang.String	0	Multipart Upload List
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_LIST	128	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_STARTED	129	3	java.lang.String	0	multipart_upload_started
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_STARTED	129	4	java.lang.String	0	Multipart Upload Started
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	MULTIPART_UPLOAD_STARTED	129	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_ACCESS_CHECK_FAILURE	130	3	java.lang.String	0	audit_access_check_failure
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_ACCESS_CHECK_FAILURE	130	4	java.lang.String	0	Audit access check was rejected
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_ACCESS_CHECK_FAILURE	130	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_SPAN_CREATION	131	3	java.lang.String	0	audit_span_creation
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_SPAN_CREATION	131	4	java.lang.String	0	Audit Span Created
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_SPAN_CREATION	131	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_FAILURE	132	3	java.lang.String	0	audit_failure
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_FAILURE	132	4	java.lang.String	0	Audit failure/rejection
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_FAILURE	132	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_REQUEST_EXECUTION	133	3	java.lang.String	0	audit_request_execution
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_REQUEST_EXECUTION	133	4	java.lang.String	0	AWS request made
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	AUDIT_REQUEST_EXECUTION	133	5		0	
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	CLIENT_SIDE_ENCRYPTION_ENABLED	134	3	java.lang.String	0	client_side_encryption_enabled
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	CLIENT_SIDE_ENCRYPTION_ENABLED	134	4	java.lang.String	0	gauge to indicate if client side encryption is enabled
org.apache.hadoop.fs.s3a.Statistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,org.apache.hadoop.fs.s3a.statistics.StatisticTypeEnum)	CLIENT_SIDE_ENCRYPTION_ENABLED	134	5		0	
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$JobUUIDSource:<init>(java.lang.String,int,java.lang.String)	SparkWriteUUID	0	3	java.lang.String	0	spark.sql.sources.writeJobUUID
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$JobUUIDSource:<init>(java.lang.String,int,java.lang.String)	CommitterUUIDProperty	1	3	java.lang.String	0	fs.s3a.committer.uuid
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$JobUUIDSource:<init>(java.lang.String,int,java.lang.String)	JobID	2	3	java.lang.String	0	JobID
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$JobUUIDSource:<init>(java.lang.String,int,java.lang.String)	GeneratedLocally	3	3	java.lang.String	0	Generated Locally
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired:<init>(java.lang.String,int,java.lang.String)	Empty	0	3	java.lang.String	0	None
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired:<init>(java.lang.String,int,java.lang.String)	AnyIncludingEmpty	1	3	java.lang.String	0	Full, Session or None
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired:<init>(java.lang.String,int,java.lang.String)	AnyNonEmpty	2	3	java.lang.String	0	Full or Session
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired:<init>(java.lang.String,int,java.lang.String)	SessionOnly	3	3	java.lang.String	0	Session
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials$CredentialTypeRequired:<init>(java.lang.String,int,java.lang.String)	FullOnly	4	3	java.lang.String	0	Full
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode:<init>(java.lang.String,int,java.lang.String)	Client	0	3	java.lang.String	0	client
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode:<init>(java.lang.String,int,java.lang.String)	Server	1	3	java.lang.String	0	server
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode:<init>(java.lang.String,int,java.lang.String)	Warn	2	3	java.lang.String	0	warn
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Mode:<init>(java.lang.String,int,java.lang.String)	None	3	3	java.lang.String	0	none
org.apache.hadoop.fs.s3a.impl.DirectoryPolicy$MarkerPolicy:<init>(java.lang.String,int,java.lang.String)	Delete	0	3	java.lang.String	0	delete
org.apache.hadoop.fs.s3a.impl.DirectoryPolicy$MarkerPolicy:<init>(java.lang.String,int,java.lang.String)	Keep	1	3	java.lang.String	0	keep
org.apache.hadoop.fs.s3a.impl.DirectoryPolicy$MarkerPolicy:<init>(java.lang.String,int,java.lang.String)	Authoritative	2	3	java.lang.String	0	authoritative
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source:<init>(java.lang.String,int,java.lang.String)	ETag	0	3	java.lang.String	0	etag
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source:<init>(java.lang.String,int,java.lang.String)	VersionId	1	3	java.lang.String	0	versionid
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$Source:<init>(java.lang.String,int,java.lang.String)	None	2	3	java.lang.String	0	none
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_CREATE	0	3	java.lang.String	0	op_create
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_CREATE	0	4	java.lang.String	0	Calls of create().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_OPEN	1	3	java.lang.String	0	op_open
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_OPEN	1	4	java.lang.String	0	Calls of open().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_GET_FILE_STATUS	2	3	java.lang.String	0	op_get_file_status
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_GET_FILE_STATUS	2	4	java.lang.String	0	Calls of getFileStatus().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_APPEND	3	3	java.lang.String	0	op_append
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_APPEND	3	4	java.lang.String	0	Calls of append().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_CREATE_NON_RECURSIVE	4	3	java.lang.String	0	op_create_non_recursive
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_CREATE_NON_RECURSIVE	4	4	java.lang.String	0	Calls of createNonRecursive().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_DELETE	5	3	java.lang.String	0	op_delete
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_DELETE	5	4	java.lang.String	0	Calls of delete().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_EXIST	6	3	java.lang.String	0	op_exists
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_EXIST	6	4	java.lang.String	0	Calls of exist().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_GET_DELEGATION_TOKEN	7	3	java.lang.String	0	op_get_delegation_token
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_GET_DELEGATION_TOKEN	7	4	java.lang.String	0	Calls of getDelegationToken().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_LIST_STATUS	8	3	java.lang.String	0	op_list_status
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_LIST_STATUS	8	4	java.lang.String	0	Calls of listStatus().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_MKDIRS	9	3	java.lang.String	0	op_mkdirs
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_MKDIRS	9	4	java.lang.String	0	Calls of mkdirs().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_RENAME	10	3	java.lang.String	0	op_rename
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CALL_RENAME	10	4	java.lang.String	0	Calls of rename().
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DIRECTORIES_CREATED	11	3	java.lang.String	0	directories_created
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DIRECTORIES_CREATED	11	4	java.lang.String	0	Total number of directories created through the object store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DIRECTORIES_DELETED	12	3	java.lang.String	0	directories_deleted
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	DIRECTORIES_DELETED	12	4	java.lang.String	0	Total number of directories deleted through the object store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FILES_CREATED	13	3	java.lang.String	0	files_created
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FILES_CREATED	13	4	java.lang.String	0	Total number of files created through the object store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FILES_DELETED	14	3	java.lang.String	0	files_deleted
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	FILES_DELETED	14	4	java.lang.String	0	Total number of files deleted from the object store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	ERROR_IGNORED	15	3	java.lang.String	0	error_ignored
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	ERROR_IGNORED	15	4	java.lang.String	0	Errors caught and ignored.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CONNECTIONS_MADE	16	3	java.lang.String	0	connections_made
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	CONNECTIONS_MADE	16	4	java.lang.String	0	Total number of times a connection was made with the data store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SEND_REQUESTS	17	3	java.lang.String	0	send_requests
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SEND_REQUESTS	17	4	java.lang.String	0	Total number of times http requests were sent to the data store.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	GET_RESPONSES	18	3	java.lang.String	0	get_responses
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	GET_RESPONSES	18	4	java.lang.String	0	Total number of times a response was received.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BYTES_SENT	19	3	java.lang.String	0	bytes_sent
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BYTES_SENT	19	4	java.lang.String	0	Total bytes uploaded.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BYTES_RECEIVED	20	3	java.lang.String	0	bytes_received
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	BYTES_RECEIVED	20	4	java.lang.String	0	Total bytes received.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	READ_THROTTLES	21	3	java.lang.String	0	read_throttles
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	READ_THROTTLES	21	4	java.lang.String	0	Total number of times a read operation is throttled.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	WRITE_THROTTLES	22	3	java.lang.String	0	write_throttles
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	WRITE_THROTTLES	22	4	java.lang.String	0	Total number of times a write operation is throttled.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SERVER_UNAVAILABLE	23	3	java.lang.String	0	server_unavailable
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SERVER_UNAVAILABLE	23	4	java.lang.String	0	Total number of times HTTP 503 status code is received in response.
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_HEAD_REQUEST	24	3	java.lang.String	0	action_http_head_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_HEAD_REQUEST	24	4	java.lang.String	0	Time taken to complete a HEAD request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_HEAD_REQUEST	24	5	java.lang.String	0	HEAD
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_GET_REQUEST	25	3	java.lang.String	0	action_http_get_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_GET_REQUEST	25	4	java.lang.String	0	Time taken to complete a GET request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_GET_REQUEST	25	5	java.lang.String	0	GET
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_DELETE_REQUEST	26	3	java.lang.String	0	action_http_delete_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_DELETE_REQUEST	26	4	java.lang.String	0	Time taken to complete a DELETE request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_DELETE_REQUEST	26	5	java.lang.String	0	DELETE
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PUT_REQUEST	27	3	java.lang.String	0	action_http_put_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PUT_REQUEST	27	4	java.lang.String	0	Time taken to complete a PUT request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PUT_REQUEST	27	5	java.lang.String	0	PUT
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PATCH_REQUEST	28	3	java.lang.String	0	action_http_patch_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PATCH_REQUEST	28	4	java.lang.String	0	Time taken to complete a PATCH request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_PATCH_REQUEST	28	5	java.lang.String	0	PATCH
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_POST_REQUEST	29	3	java.lang.String	0	action_http_post_request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_POST_REQUEST	29	4	java.lang.String	0	Time taken to complete a POST request
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)	HTTP_POST_REQUEST	29	5	java.lang.String	0	POST
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	RENAME_RECOVERY	30	3	java.lang.String	0	rename_recovery
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	RENAME_RECOVERY	30	4	java.lang.String	0	Number of times Rename recoveries happened
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	METADATA_INCOMPLETE_RENAME_FAILURES	31	3	java.lang.String	0	metadata_incomplete_rename_failures
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	METADATA_INCOMPLETE_RENAME_FAILURES	31	4	java.lang.String	0	Number of times rename operation failed due to metadata being incomplete
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	RENAME_PATH_ATTEMPTS	32	3	java.lang.String	0	rename_path_attempts
org.apache.hadoop.fs.azurebfs.AbfsStatistic:<init>(java.lang.String,int,java.lang.String,java.lang.String)	RENAME_PATH_ATTEMPTS	32	4	java.lang.String	0	Number of times we attempt to rename a path internally
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_ALREADY_EXISTS	0	3	java.lang.String	0	FilesystemAlreadyExists
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_ALREADY_EXISTS	0	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_ALREADY_EXISTS	0	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_ALREADY_EXISTS	1	3	java.lang.String	0	PathAlreadyExists
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_ALREADY_EXISTS	1	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_ALREADY_EXISTS	1	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INTERNAL_OPERATION_ABORT	2	3	java.lang.String	0	InternalOperationAbortError
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INTERNAL_OPERATION_ABORT	2	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INTERNAL_OPERATION_ABORT	2	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_CONFLICT	3	3	java.lang.String	0	PathConflict
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_CONFLICT	3	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_CONFLICT	3	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_NOT_FOUND	4	3	java.lang.String	0	FilesystemNotFound
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_NOT_FOUND	4	4	int	0	404
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	FILE_SYSTEM_NOT_FOUND	4	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_NOT_FOUND	5	3	java.lang.String	0	PathNotFound
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_NOT_FOUND	5	4	int	0	404
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PATH_NOT_FOUND	5	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PRE_CONDITION_FAILED	6	3	java.lang.String	0	PreconditionFailed
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PRE_CONDITION_FAILED	6	4	int	0	412
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	PRE_CONDITION_FAILED	6	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	SOURCE_PATH_NOT_FOUND	7	3	java.lang.String	0	SourcePathNotFound
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	SOURCE_PATH_NOT_FOUND	7	4	int	0	404
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	SOURCE_PATH_NOT_FOUND	7	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE	8	3	java.lang.String	0	InvalidSourceOrDestinationResourceType
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE	8	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_SOURCE_OR_DESTINATION_RESOURCE_TYPE	8	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	RENAME_DESTINATION_PARENT_PATH_NOT_FOUND	9	3	java.lang.String	0	RenameDestinationParentPathNotFound
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	RENAME_DESTINATION_PARENT_PATH_NOT_FOUND	9	4	int	0	404
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	RENAME_DESTINATION_PARENT_PATH_NOT_FOUND	9	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_RENAME_SOURCE_PATH	10	3	java.lang.String	0	InvalidRenameSourcePath
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_RENAME_SOURCE_PATH	10	4	int	0	409
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_RENAME_SOURCE_PATH	10	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INGRESS_OVER_ACCOUNT_LIMIT	11	3	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INGRESS_OVER_ACCOUNT_LIMIT	11	4	int	0	503
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INGRESS_OVER_ACCOUNT_LIMIT	11	5	java.lang.String	0	Ingress is over the account limit.
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	EGRESS_OVER_ACCOUNT_LIMIT	12	3	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	EGRESS_OVER_ACCOUNT_LIMIT	12	4	int	0	503
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	EGRESS_OVER_ACCOUNT_LIMIT	12	5	java.lang.String	0	Egress is over the account limit.
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_QUERY_PARAMETER_VALUE	13	3	java.lang.String	0	InvalidQueryParameterValue
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_QUERY_PARAMETER_VALUE	13	4	int	0	400
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	INVALID_QUERY_PARAMETER_VALUE	13	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	AUTHORIZATION_PERMISSION_MISS_MATCH	14	3	java.lang.String	0	AuthorizationPermissionMismatch
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	AUTHORIZATION_PERMISSION_MISS_MATCH	14	4	int	0	403
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	AUTHORIZATION_PERMISSION_MISS_MATCH	14	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	ACCOUNT_REQUIRES_HTTPS	15	3	java.lang.String	0	AccountRequiresHttps
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	ACCOUNT_REQUIRES_HTTPS	15	4	int	0	400
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	ACCOUNT_REQUIRES_HTTPS	15	5	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	UNKNOWN	16	3	null	0	null
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	UNKNOWN	16	4	int	0	-1
org.apache.hadoop.fs.azurebfs.contracts.services.AzureServiceErrorCode:<init>(java.lang.String,int,java.lang.String,int,java.lang.String)	UNKNOWN	16	5	null	0	null
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	ACCESS	0	3	java.lang.String	0	AS
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	APPEND	1	3	java.lang.String	0	AP
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	BREAK_LEASE	2	3	java.lang.String	0	BL
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	CREATE	3	3	java.lang.String	0	CR
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	CREATE_FILESYSTEM	4	3	java.lang.String	0	CF
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	CREATE_NON_RECURSIVE	5	3	java.lang.String	0	CN
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	DELETE	6	3	java.lang.String	0	DL
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	GET_ACL_STATUS	7	3	java.lang.String	0	GA
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	GET_ATTR	8	3	java.lang.String	0	GR
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	GET_FILESTATUS	9	3	java.lang.String	0	GF
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	LISTSTATUS	10	3	java.lang.String	0	LS
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	MKDIR	11	3	java.lang.String	0	MK
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	MODIFY_ACL	12	3	java.lang.String	0	MA
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	OPEN	13	3	java.lang.String	0	OP
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	HAS_PATH_CAPABILITY	14	3	java.lang.String	0	PC
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	SET_PERMISSION	15	3	java.lang.String	0	SP
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	READ	16	3	java.lang.String	0	RE
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	RELEASE_LEASE	17	3	java.lang.String	0	RL
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	REMOVE_ACL	18	3	java.lang.String	0	RA
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	REMOVE_ACL_ENTRIES	19	3	java.lang.String	0	RT
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	REMOVE_DEFAULT_ACL	20	3	java.lang.String	0	RD
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	RENAME	21	3	java.lang.String	0	RN
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	SET_ATTR	22	3	java.lang.String	0	SR
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	SET_OWNER	23	3	java.lang.String	0	SO
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	SET_ACL	24	3	java.lang.String	0	SA
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	TEST_OP	25	3	java.lang.String	0	TS
org.apache.hadoop.fs.azurebfs.constants.FSOperationType:<init>(java.lang.String,int,java.lang.String)	WRITE	26	3	java.lang.String	0	WR
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	IGNORE_FAILURES	0	3	java.lang.String	0	distcp.ignore.failures
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	IGNORE_FAILURES	0	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	PRESERVE_STATUS	1	3	java.lang.String	0	distcp.preserve.status
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	PRESERVE_STATUS	1	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SYNC_FOLDERS	2	3	java.lang.String	0	distcp.sync.folders
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SYNC_FOLDERS	2	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	DELETE_MISSING	3	3	java.lang.String	0	distcp.delete.missing.source
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	DELETE_MISSING	3	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	TRACK_MISSING	4	3	java.lang.String	0	distcp.track.missing.source
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	TRACK_MISSING	4	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	NUM_LISTSTATUS_THREADS	5	3	java.lang.String	0	distcp.liststatus.threads
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	NUM_LISTSTATUS_THREADS	5	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	MAX_MAPS	6	3	java.lang.String	0	distcp.max.maps
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	MAX_MAPS	6	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SOURCE_FILE_LISTING	7	3	java.lang.String	0	distcp.source.listing
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SOURCE_FILE_LISTING	7	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	ATOMIC_COMMIT	8	3	java.lang.String	0	distcp.atomic.copy
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	ATOMIC_COMMIT	8	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	WORK_PATH	9	3	java.lang.String	0	distcp.work.path
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	WORK_PATH	9	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	LOG_PATH	10	3	java.lang.String	0	distcp.log.path
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	LOG_PATH	10	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	VERBOSE_LOG	11	3	java.lang.String	0	distcp.verbose.log
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	VERBOSE_LOG	11	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	COPY_STRATEGY	12	3	java.lang.String	0	distcp.copy.strategy
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	COPY_STRATEGY	12	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SKIP_CRC	13	3	java.lang.String	0	distcp.skip.crc
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SKIP_CRC	13	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	OVERWRITE	14	3	java.lang.String	0	distcp.copy.overwrite
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	OVERWRITE	14	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	APPEND	15	3	java.lang.String	0	distcp.copy.append
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	APPEND	15	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	DIFF	16	3	java.lang.String	0	distcp.copy.diff
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	DIFF	16	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	DIFF	16	5	int	0	2
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	RDIFF	17	3	java.lang.String	0	distcp.copy.rdiff
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	RDIFF	17	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option,int)	RDIFF	17	5	int	0	2
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BLOCKING	18	3	java.lang.String	0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BLOCKING	18	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	FILE_LIMIT	19	3	java.lang.String	0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	FILE_LIMIT	19	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SIZE_LIMIT	20	3	java.lang.String	0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	SIZE_LIMIT	20	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BLOCKS_PER_CHUNK	21	3	java.lang.String	0	distcp.blocks.per.chunk
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BLOCKS_PER_CHUNK	21	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	COPY_BUFFER_SIZE	22	3	java.lang.String	0	distcp.copy.buffer.size
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	COPY_BUFFER_SIZE	22	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BANDWIDTH	23	3	java.lang.String	0	distcp.map.bandwidth.mb
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	BANDWIDTH	23	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	FILTERS	24	3	java.lang.String	0	distcp.filters.file
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	FILTERS	24	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	DIRECT_WRITE	25	3	java.lang.String	0	distcp.direct.write
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	DIRECT_WRITE	25	4		0	
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	USE_ITERATOR	26	3	java.lang.String	0	distcp.use.iterator
org.apache.hadoop.tools.DistCpOptionSwitch:<init>(java.lang.String,int,java.lang.String,org.apache.commons.cli.Option)	USE_ITERATOR	26	4		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	APPEND	0	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	CREATE	1	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	GETFILEINFO	2	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	CONTENTSUMMARY	3	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	MKDIRS	4	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	RENAME	5	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	LISTSTATUS	6	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	DELETE	7	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	OPEN	8	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	SETPERMISSION	9	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	SETOWNER	10	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	SETTIMES	11	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	SETREPLICATION	12	3		0	
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$ReplayCommand:<init>(java.lang.String,int,org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper$CommandType)	CONCAT	13	3		0	
org.apache.hadoop.tools.DistCh$Option:<init>(java.lang.String,int,java.lang.String,java.lang.String)	IGNORE_FAILURES	0	3	java.lang.String	0	-i
org.apache.hadoop.tools.DistCh$Option:<init>(java.lang.String,int,java.lang.String,java.lang.String)	IGNORE_FAILURES	0	4	java.lang.String	0	distch.ignore.failures
org.apache.hadoop.tools.rumen.JobConfPropertyNames:<init>(java.lang.String,int,java.lang.String[])	QUEUE_NAMES	0	3		0	
org.apache.hadoop.tools.rumen.JobConfPropertyNames:<init>(java.lang.String,int,java.lang.String[])	JOB_NAMES	1	3		0	
org.apache.hadoop.tools.rumen.JobConfPropertyNames:<init>(java.lang.String,int,java.lang.String[])	TASK_JAVA_OPTS_S	2	3		0	
org.apache.hadoop.tools.rumen.JobConfPropertyNames:<init>(java.lang.String,int,java.lang.String[])	MAP_JAVA_OPTS_S	3	3		0	
org.apache.hadoop.tools.rumen.JobConfPropertyNames:<init>(java.lang.String,int,java.lang.String[])	REDUCE_JAVA_OPTS_S	4	3		0	
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric:<init>(java.lang.String,int,java.lang.String)	DEMAND	0	3	java.lang.String	0	demand
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric:<init>(java.lang.String,int,java.lang.String)	USAGE	1	3	java.lang.String	0	usage
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric:<init>(java.lang.String,int,java.lang.String)	MINSHARE	2	3	java.lang.String	0	minshare
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric:<init>(java.lang.String,int,java.lang.String)	MAXSHARE	3	3	java.lang.String	0	maxshare
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics$Metric:<init>(java.lang.String,int,java.lang.String)	FAIRSHARE	4	3	java.lang.String	0	fairshare
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$QueueMetric:<init>(java.lang.String,int,java.lang.String)	PENDING_MEMORY	0	3	java.lang.String	0	pending.memory
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$QueueMetric:<init>(java.lang.String,int,java.lang.String)	PENDING_VCORES	1	3	java.lang.String	0	pending.cores
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$QueueMetric:<init>(java.lang.String,int,java.lang.String)	ALLOCATED_MEMORY	2	3	java.lang.String	0	allocated.memory
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics$QueueMetric:<init>(java.lang.String,int,java.lang.String)	ALLOCATED_VCORES	3	3	java.lang.String	0	allocated.cores
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	BYTES	0	3	int	0	0
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	BYTE	1	3	int	0	1
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	BOOL	2	3	int	0	2
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	INT	3	3	int	0	3
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	LONG	4	3	int	0	4
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	FLOAT	5	3	int	0	5
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	DOUBLE	6	3	int	0	6
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	STRING	7	3	int	0	7
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	VECTOR	8	3	int	0	8
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	LIST	9	3	int	0	9
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	MAP	10	3	int	0	10
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	WRITABLE	11	3	int	0	50
org.apache.hadoop.typedbytes.Type:<init>(java.lang.String,int,int)	MARKER	12	3	int	0	255
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetConstraint$TargetOperator:<init>(java.lang.String,int,java.lang.String)	IN	0	3	java.lang.String	0	in
org.apache.hadoop.yarn.api.resource.PlacementConstraint$TargetConstraint$TargetOperator:<init>(java.lang.String,int,java.lang.String)	NOT_IN	1	3	java.lang.String	0	notin
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	USER	0	3	java.lang.String	0	USER
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LOGNAME	1	3	java.lang.String	0	LOGNAME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	HOME	2	3	java.lang.String	0	HOME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	PWD	3	3	java.lang.String	0	PWD
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	PATH	4	3	java.lang.String	0	PATH
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	SHELL	5	3	java.lang.String	0	SHELL
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	JAVA_HOME	6	3	java.lang.String	0	JAVA_HOME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	CLASSPATH	7	3	java.lang.String	0	CLASSPATH
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	APP_CLASSPATH	8	3	java.lang.String	0	APP_CLASSPATH
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LD_LIBRARY_PATH	9	3	java.lang.String	0	LD_LIBRARY_PATH
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	HADOOP_CONF_DIR	10	3	java.lang.String	0	HADOOP_CONF_DIR
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	HADOOP_COMMON_HOME	11	3	java.lang.String	0	HADOOP_COMMON_HOME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	HADOOP_HDFS_HOME	12	3	java.lang.String	0	HADOOP_HDFS_HOME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	MALLOC_ARENA_MAX	13	3	java.lang.String	0	MALLOC_ARENA_MAX
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	HADOOP_YARN_HOME	14	3	java.lang.String	0	HADOOP_YARN_HOME
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	CLASSPATH_PREPEND_DISTCACHE	15	3	java.lang.String	0	CLASSPATH_PREPEND_DISTCACHE
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LOCALIZATION_COUNTERS	16	3	java.lang.String	0	LOCALIZATION_COUNTERS
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	CONTAINER_ID	17	3	java.lang.String	0	CONTAINER_ID
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	NM_HOST	18	3	java.lang.String	0	NM_HOST
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	NM_HTTP_PORT	19	3	java.lang.String	0	NM_HTTP_PORT
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	NM_PORT	20	3	java.lang.String	0	NM_PORT
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LOCAL_DIRS	21	3	java.lang.String	0	LOCAL_DIRS
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LOCAL_USER_DIRS	22	3	java.lang.String	0	LOCAL_USER_DIRS
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	LOG_DIRS	23	3	java.lang.String	0	LOG_DIRS
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE	24	3	java.lang.String	0	YARN_CONTAINER_RUNTIME_DOCKER_RUN_OVERRIDE_DISABLE
org.apache.hadoop.yarn.api.ApplicationConstants$Environment:<init>(java.lang.String,int,java.lang.String)	YARN_CONTAINER_RUNTIME_YARN_SYSFS_ENABLE	25	3	java.lang.String	0	YARN_CONTAINER_RUNTIME_YARN_SYSFS_ENABLE
org.apache.hadoop.yarn.api.records.AllocationTagNamespaceType:<init>(java.lang.String,int,java.lang.String)	SELF	0	3	java.lang.String	0	self
org.apache.hadoop.yarn.api.records.AllocationTagNamespaceType:<init>(java.lang.String,int,java.lang.String)	NOT_SELF	1	3	java.lang.String	0	not-self
org.apache.hadoop.yarn.api.records.AllocationTagNamespaceType:<init>(java.lang.String,int,java.lang.String)	APP_ID	2	3	java.lang.String	0	app-id
org.apache.hadoop.yarn.api.records.AllocationTagNamespaceType:<init>(java.lang.String,int,java.lang.String)	APP_TAG	3	3	java.lang.String	0	app-tag
org.apache.hadoop.yarn.api.records.AllocationTagNamespaceType:<init>(java.lang.String,int,java.lang.String)	ALL	4	3	java.lang.String	0	all
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$CompositeType:<init>(java.lang.String,int,int)	AND	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$CompositeType:<init>(java.lang.String,int,int)	OR	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$CompositePlacementConstraintProto$CompositeType:<init>(java.lang.String,int,int)	DELAYED_OR	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_RUNNING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_UNHEALTHY	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_DECOMMISSIONED	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_LOST	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_REBOOTED	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_DECOMMISSIONING	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnProtos$NodeStateProto:<init>(java.lang.String,int,int)	NS_SHUTDOWN	7	3	int	0	8
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AttributeMappingOperationTypeProto:<init>(java.lang.String,int,int)	REPLACE	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AttributeMappingOperationTypeProto:<init>(java.lang.String,int,int)	ADD	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$AttributeMappingOperationTypeProto:<init>(java.lang.String,int,int)	REMOVE	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$ContainerExitStatusProto:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerExitStatusProto:<init>(java.lang.String,int,int)	INVALID	1	3	int	0	-1000
org.apache.hadoop.yarn.proto.YarnProtos$ContainerExitStatusProto:<init>(java.lang.String,int,int)	ABORTED	2	3	int	0	-100
org.apache.hadoop.yarn.proto.YarnProtos$ContainerExitStatusProto:<init>(java.lang.String,int,int)	DISKS_FAILED	3	3	int	0	-101
org.apache.hadoop.yarn.proto.YarnServiceProtos$ApplicationsRequestScopeProto:<init>(java.lang.String,int,int)	ALL	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ApplicationsRequestScopeProto:<init>(java.lang.String,int,int)	VIEWABLE	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ApplicationsRequestScopeProto:<init>(java.lang.String,int,int)	OWN	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$AMCommandProto:<init>(java.lang.String,int,int)	AM_RESYNC	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$AMCommandProto:<init>(java.lang.String,int,int)	AM_SHUTDOWN	1	3	int	0	2
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	SINGLE_NODE_WRITER	1	3	int	0	1
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	SINGLE_NODE_READER_ONLY	2	3	int	0	2
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	MULTI_NODE_READER_ONLY	3	3	int	0	3
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	MULTI_NODE_SINGLE_WRITER	4	3	int	0	4
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$AccessMode:<init>(java.lang.String,int,int)	MULTI_NODE_MULTI_WRITER	5	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryPolicyProto:<init>(java.lang.String,int,int)	NEVER_RETRY	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryPolicyProto:<init>(java.lang.String,int,int)	RETRY_ON_ALL_ERRORS	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerRetryPolicyProto:<init>(java.lang.String,int,int)	RETRY_ON_SPECIFIC_ERROR_CODES	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceTypeProto:<init>(java.lang.String,int,int)	ARCHIVE	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceTypeProto:<init>(java.lang.String,int,int)	FILE	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceTypeProto:<init>(java.lang.String,int,int)	PATTERN	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateTypeProto:<init>(java.lang.String,int,int)	INCREASE_RESOURCE	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateTypeProto:<init>(java.lang.String,int,int)	DECREASE_RESOURCE	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateTypeProto:<init>(java.lang.String,int,int)	PROMOTE_EXECUTION_TYPE	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServiceProtos$ContainerUpdateTypeProto:<init>(java.lang.String,int,int)	DEMOTE_EXECUTION_TYPE	3	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServiceProtos$SchedulerResourceTypes:<init>(java.lang.String,int,int)	MEMORY	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnServiceProtos$SchedulerResourceTypes:<init>(java.lang.String,int,int)	CPU	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerSubStateProto:<init>(java.lang.String,int,int)	CSS_SCHEDULED	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerSubStateProto:<init>(java.lang.String,int,int)	CSS_RUNNING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$ContainerSubStateProto:<init>(java.lang.String,int,int)	CSS_PAUSED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$ContainerSubStateProto:<init>(java.lang.String,int,int)	CSS_COMPLETING	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$ContainerSubStateProto:<init>(java.lang.String,int,int)	CSS_DONE	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_SUBMITTED	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_SCHEDULED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_ALLOCATED_SAVING	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_ALLOCATED	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_LAUNCHED	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_FAILED	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_RUNNING	7	3	int	0	8
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_FINISHING	8	3	int	0	9
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_FINISHED	9	3	int	0	10
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationAttemptStateProto:<init>(java.lang.String,int,int)	APP_ATTEMPT_KILLED	10	3	int	0	11
org.apache.hadoop.yarn.proto.YarnProtos$FinalApplicationStatusProto:<init>(java.lang.String,int,int)	APP_UNDEFINED	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$FinalApplicationStatusProto:<init>(java.lang.String,int,int)	APP_SUCCEEDED	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$FinalApplicationStatusProto:<init>(java.lang.String,int,int)	APP_FAILED	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$FinalApplicationStatusProto:<init>(java.lang.String,int,int)	APP_KILLED	3	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$FinalApplicationStatusProto:<init>(java.lang.String,int,int)	APP_ENDED	4	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$RejectionReasonProto:<init>(java.lang.String,int,int)	RRP_COULD_NOT_PLACE_ON_NODE	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$RejectionReasonProto:<init>(java.lang.String,int,int)	RRP_COULD_NOT_SCHEDULE_ON_NODE	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeTypeProto:<init>(java.lang.String,int,int)	STRING	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceVisibilityProto:<init>(java.lang.String,int,int)	PUBLIC	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceVisibilityProto:<init>(java.lang.String,int,int)	PRIVATE	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$LocalResourceVisibilityProto:<init>(java.lang.String,int,int)	APPLICATION	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStateProto:<init>(java.lang.String,int,int)	L_PENDING	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStateProto:<init>(java.lang.String,int,int)	L_COMPLETED	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServiceProtos$LocalizationStateProto:<init>(java.lang.String,int,int)	L_FAILED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$NodeUpdateTypeProto:<init>(java.lang.String,int,int)	NODE_USABLE	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$NodeUpdateTypeProto:<init>(java.lang.String,int,int)	NODE_UNUSABLE	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeUpdateTypeProto:<init>(java.lang.String,int,int)	NODE_DECOMMISSIONING	2	3	int	0	2
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$VolumeType:<init>(java.lang.String,int,int)	BLOCK	0	3	int	0	0
org.apache.hadoop.yarn.proto.CsiAdaptorProtos$VolumeCapability$VolumeType:<init>(java.lang.String,int,int)	FILE_SYSTEM	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ResourceTypesProto:<init>(java.lang.String,int,int)	COUNTABLE	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationTimeoutTypeProto:<init>(java.lang.String,int,int)	APP_TIMEOUT_LIFETIME	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_DISABLED	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_NOT_START	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_RUNNING	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_SUCCEEDED	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_FAILED	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_TIME_OUT	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnProtos$LogAggregationStatusProto:<init>(java.lang.String,int,int)	LOG_RUNNING_WITH_FAILURE	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$DelayUnit:<init>(java.lang.String,int,int)	MILLISECONDS	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$TimedPlacementConstraintProto$DelayUnit:<init>(java.lang.String,int,int)	OPPORTUNITIES	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$TargetType:<init>(java.lang.String,int,int)	NODE_ATTRIBUTE	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$TargetType:<init>(java.lang.String,int,int)	ALLOCATION_TAG	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$PlacementConstraintTargetProto$TargetType:<init>(java.lang.String,int,int)	SELF	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestInterpreterProto:<init>(java.lang.String,int,int)	R_ANY	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestInterpreterProto:<init>(java.lang.String,int,int)	R_ALL	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestInterpreterProto:<init>(java.lang.String,int,int)	R_ORDER	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$ReservationRequestInterpreterProto:<init>(java.lang.String,int,int)	R_ORDER_NO_GAP	3	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAccessTypeProto:<init>(java.lang.String,int,int)	APPACCESS_VIEW_APP	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ApplicationAccessTypeProto:<init>(java.lang.String,int,int)	APPACCESS_MODIFY_APP	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$QueueACLProto:<init>(java.lang.String,int,int)	QACL_SUBMIT_APPLICATIONS	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueACLProto:<init>(java.lang.String,int,int)	QACL_ADMINISTER_QUEUE	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStateProto:<init>(java.lang.String,int,int)	C_NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStateProto:<init>(java.lang.String,int,int)	C_RUNNING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$ContainerStateProto:<init>(java.lang.String,int,int)	C_COMPLETE	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeOpCodeProto:<init>(java.lang.String,int,int)	NO_OP	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeOpCodeProto:<init>(java.lang.String,int,int)	EQ	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$NodeAttributeOpCodeProto:<init>(java.lang.String,int,int)	NE	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$DecommissionTypeProto:<init>(java.lang.String,int,int)	NORMAL	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$DecommissionTypeProto:<init>(java.lang.String,int,int)	GRACEFUL	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerResourceManagerServiceProtos$DecommissionTypeProto:<init>(java.lang.String,int,int)	FORCEFUL	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	NEW_SAVING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	SUBMITTED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	ACCEPTED	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	RUNNING	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	FINISHED	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	FAILED	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnProtos$YarnApplicationStateProto:<init>(java.lang.String,int,int)	KILLED	7	3	int	0	8
org.apache.hadoop.yarn.proto.YarnProtos$ContainerTypeProto:<init>(java.lang.String,int,int)	APPLICATION_MASTER	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ContainerTypeProto:<init>(java.lang.String,int,int)	TASK	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$QueueStateProto:<init>(java.lang.String,int,int)	Q_STOPPED	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$QueueStateProto:<init>(java.lang.String,int,int)	Q_RUNNING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$QueueStateProto:<init>(java.lang.String,int,int)	Q_DRAINING	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$SignalContainerCommandProto:<init>(java.lang.String,int,int)	OUTPUT_THREAD_DUMP	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$SignalContainerCommandProto:<init>(java.lang.String,int,int)	GRACEFUL_SHUTDOWN	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnProtos$SignalContainerCommandProto:<init>(java.lang.String,int,int)	FORCEFUL_SHUTDOWN	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeProto:<init>(java.lang.String,int,int)	GUARANTEED	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnProtos$ExecutionTypeProto:<init>(java.lang.String,int,int)	OPPORTUNISTIC	1	3	int	0	2
org.apache.hadoop.yarn.service.api.records.PlacementScope:<init>(java.lang.String,int,java.lang.String)	NODE	0	3	java.lang.String	0	node
org.apache.hadoop.yarn.service.api.records.PlacementScope:<init>(java.lang.String,int,java.lang.String)	RACK	1	3	java.lang.String	0	rack
org.apache.hadoop.yarn.service.api.records.Artifact$TypeEnum:<init>(java.lang.String,int,java.lang.String)	DOCKER	0	3	java.lang.String	0	DOCKER
org.apache.hadoop.yarn.service.api.records.Artifact$TypeEnum:<init>(java.lang.String,int,java.lang.String)	TARBALL	1	3	java.lang.String	0	TARBALL
org.apache.hadoop.yarn.service.api.records.Artifact$TypeEnum:<init>(java.lang.String,int,java.lang.String)	SERVICE	2	3	java.lang.String	0	SERVICE
org.apache.hadoop.yarn.service.api.records.ReadinessCheck$TypeEnum:<init>(java.lang.String,int,java.lang.String)	DEFAULT	0	3	java.lang.String	0	DEFAULT
org.apache.hadoop.yarn.service.api.records.ReadinessCheck$TypeEnum:<init>(java.lang.String,int,java.lang.String)	HTTP	1	3	java.lang.String	0	HTTP
org.apache.hadoop.yarn.service.api.records.ReadinessCheck$TypeEnum:<init>(java.lang.String,int,java.lang.String)	PORT	2	3	java.lang.String	0	PORT
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	XML	0	3	java.lang.String	0	XML
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	PROPERTIES	1	3	java.lang.String	0	PROPERTIES
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	JSON	2	3	java.lang.String	0	JSON
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	YAML	3	3	java.lang.String	0	YAML
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	TEMPLATE	4	3	java.lang.String	0	TEMPLATE
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	HADOOP_XML	5	3	java.lang.String	0	HADOOP_XML
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	STATIC	6	3	java.lang.String	0	STATIC
org.apache.hadoop.yarn.service.api.records.ConfigFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	ARCHIVE	7	3	java.lang.String	0	ARCHIVE
org.apache.hadoop.yarn.service.api.records.Component$RestartPolicyEnum:<init>(java.lang.String,int,java.lang.String)	ALWAYS	0	3	java.lang.String	0	ALWAYS
org.apache.hadoop.yarn.service.api.records.Component$RestartPolicyEnum:<init>(java.lang.String,int,java.lang.String)	ON_FAILURE	1	3	java.lang.String	0	ON_FAILURE
org.apache.hadoop.yarn.service.api.records.Component$RestartPolicyEnum:<init>(java.lang.String,int,java.lang.String)	NEVER	2	3	java.lang.String	0	NEVER
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	JSON	0	3	java.lang.String	0	json
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	PROPERTIES	1	3	java.lang.String	0	properties
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	XML	2	3	java.lang.String	0	xml
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	HADOOP_XML	3	3	java.lang.String	0	hadoop_xml
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	TEMPLATE	4	3	java.lang.String	0	template
org.apache.hadoop.yarn.service.api.records.ConfigFormat:<init>(java.lang.String,int,java.lang.String)	YAML	5	3	java.lang.String	0	yaml
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	GETDELEGATIONTOKEN	0	3	java.lang.String	0	GET
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	GETDELEGATIONTOKEN	0	4	int	0	1
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	RENEWDELEGATIONTOKEN	1	3	java.lang.String	0	PUT
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	RENEWDELEGATIONTOKEN	1	4	int	0	1
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	CANCELDELEGATIONTOKEN	2	3	java.lang.String	0	PUT
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation:<init>(java.lang.String,int,java.lang.String,boolean)	CANCELDELEGATIONTOKEN	2	4	int	0	1
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	SIZE	0	3	java.lang.String	0	Size
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	RSS	1	3	java.lang.String	0	Rss
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	PSS	2	3	java.lang.String	0	Pss
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	SHARED_CLEAN	3	3	java.lang.String	0	Shared_Clean
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	SHARED_DIRTY	4	3	java.lang.String	0	Shared_Dirty
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	PRIVATE_CLEAN	5	3	java.lang.String	0	Private_Clean
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	PRIVATE_DIRTY	6	3	java.lang.String	0	Private_Dirty
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	REFERENCED	7	3	java.lang.String	0	Referenced
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	ANONYMOUS	8	3	java.lang.String	0	Anonymous
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	ANON_HUGE_PAGES	9	3	java.lang.String	0	AnonHugePages
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	SWAP	10	3	java.lang.String	0	swap
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	KERNEL_PAGE_SIZE	11	3	java.lang.String	0	kernelPageSize
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	MMU_PAGE_SIZE	12	3	java.lang.String	0	mmuPageSize
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree$MemInfo:<init>(java.lang.String,int,java.lang.String)	INVALID	13	3	java.lang.String	0	invalid
csi.v0.Csi$PluginCapability$TypeCase:<init>(java.lang.String,int,int)	SERVICE	0	3	int	0	1
csi.v0.Csi$PluginCapability$TypeCase:<init>(java.lang.String,int,int)	TYPE_NOT_SET	1	3	int	0	0
csi.v0.Csi$PluginCapability$Service$Type:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
csi.v0.Csi$PluginCapability$Service$Type:<init>(java.lang.String,int,int)	CONTROLLER_SERVICE	1	3	int	0	1
csi.v0.Csi$PluginCapability$Service$Type:<init>(java.lang.String,int,int)	ACCESSIBILITY_CONSTRAINTS	2	3	int	0	2
csi.v0.Csi$PluginCapability$Service$Type:<init>(java.lang.String,int,int)	UNRECOGNIZED	3	3	int	0	-1
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	CREATE_DELETE_VOLUME	1	3	int	0	1
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	PUBLISH_UNPUBLISH_VOLUME	2	3	int	0	2
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	LIST_VOLUMES	3	3	int	0	3
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	GET_CAPACITY	4	3	int	0	4
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	CREATE_DELETE_SNAPSHOT	5	3	int	0	5
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	LIST_SNAPSHOTS	6	3	int	0	6
csi.v0.Csi$ControllerServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	UNRECOGNIZED	7	3	int	0	-1
csi.v0.Csi$ControllerServiceCapability$TypeCase:<init>(java.lang.String,int,int)	RPC	0	3	int	0	1
csi.v0.Csi$ControllerServiceCapability$TypeCase:<init>(java.lang.String,int,int)	TYPE_NOT_SET	1	3	int	0	0
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	SINGLE_NODE_WRITER	1	3	int	0	1
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	SINGLE_NODE_READER_ONLY	2	3	int	0	2
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	MULTI_NODE_READER_ONLY	3	3	int	0	3
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	MULTI_NODE_SINGLE_WRITER	4	3	int	0	4
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	MULTI_NODE_MULTI_WRITER	5	3	int	0	5
csi.v0.Csi$VolumeCapability$AccessMode$Mode:<init>(java.lang.String,int,int)	UNRECOGNIZED	6	3	int	0	-1
csi.v0.Csi$VolumeCapability$AccessTypeCase:<init>(java.lang.String,int,int)	BLOCK	0	3	int	0	1
csi.v0.Csi$VolumeCapability$AccessTypeCase:<init>(java.lang.String,int,int)	MOUNT	1	3	int	0	2
csi.v0.Csi$VolumeCapability$AccessTypeCase:<init>(java.lang.String,int,int)	ACCESSTYPE_NOT_SET	2	3	int	0	0
csi.v0.Csi$SnapshotStatus$Type:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
csi.v0.Csi$SnapshotStatus$Type:<init>(java.lang.String,int,int)	READY	1	3	int	0	1
csi.v0.Csi$SnapshotStatus$Type:<init>(java.lang.String,int,int)	UPLOADING	2	3	int	0	2
csi.v0.Csi$SnapshotStatus$Type:<init>(java.lang.String,int,int)	ERROR_UPLOADING	3	3	int	0	3
csi.v0.Csi$SnapshotStatus$Type:<init>(java.lang.String,int,int)	UNRECOGNIZED	4	3	int	0	-1
csi.v0.Csi$NodeServiceCapability$TypeCase:<init>(java.lang.String,int,int)	RPC	0	3	int	0	1
csi.v0.Csi$NodeServiceCapability$TypeCase:<init>(java.lang.String,int,int)	TYPE_NOT_SET	1	3	int	0	0
csi.v0.Csi$VolumeContentSource$TypeCase:<init>(java.lang.String,int,int)	SNAPSHOT	0	3	int	0	1
csi.v0.Csi$VolumeContentSource$TypeCase:<init>(java.lang.String,int,int)	TYPE_NOT_SET	1	3	int	0	0
csi.v0.Csi$NodeServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	UNKNOWN	0	3	int	0	0
csi.v0.Csi$NodeServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	STAGE_UNSTAGE_VOLUME	1	3	int	0	1
csi.v0.Csi$NodeServiceCapability$RPC$Type:<init>(java.lang.String,int,int)	UNRECOGNIZED	2	3	int	0	-1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_NEW	0	3	int	0	1
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_RUNNING	1	3	int	0	2
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_UNHEALTHY	2	3	int	0	3
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_DECOMMISSIONING	3	3	int	0	4
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_LOST	4	3	int	0	5
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_UNREGISTERED	5	3	int	0	6
org.apache.hadoop.yarn.federation.proto.YarnServerFederationProtos$SubClusterStateProto:<init>(java.lang.String,int,int)	SC_DECOMMISSIONED	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeActionProto:<init>(java.lang.String,int,int)	NORMAL	0	3	int	0	0
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeActionProto:<init>(java.lang.String,int,int)	RESYNC	1	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerCommonProtos$NodeActionProto:<init>(java.lang.String,int,int)	SHUTDOWN	2	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceStatusTypeProto:<init>(java.lang.String,int,int)	FETCH_PENDING	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceStatusTypeProto:<init>(java.lang.String,int,int)	FETCH_SUCCESS	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$ResourceStatusTypeProto:<init>(java.lang.String,int,int)	FETCH_FAILURE	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerActionProto:<init>(java.lang.String,int,int)	LIVE	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerNodemanagerServiceProtos$LocalizerActionProto:<init>(java.lang.String,int,int)	DIE	1	3	int	0	2
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	FORCE_KILLED	1	3	int	0	137
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	TERMINATED	2	3	int	0	143
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	LOST	3	3	int	0	154
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	NULL	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	NULL	0	4	java.lang.String	0	NULL
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	QUIT	1	3	int	0	3
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	QUIT	1	4	java.lang.String	0	SIGQUIT
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	KILL	2	3	int	0	9
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	KILL	2	4	java.lang.String	0	SIGKILL
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	TERM	3	3	int	0	15
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor$Signal:<init>(java.lang.String,int,int,java.lang.String)	TERM	3	4	java.lang.String	0	SIGTERM
org.apache.hadoop.yarn.server.nodemanager.NodeManager$NodeManagerStatus:<init>(java.lang.String,int,int)	NO_ERROR	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.NodeManager$NodeManagerStatus:<init>(java.lang.String,int,int)	EXCEPTION	1	3	int	0	1
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.YarnRuntimeType:<init>(java.lang.String,int,java.lang.String)	RUNTIME_DEFAULT	0	3	java.lang.String	0	default
org.apache.hadoop.yarn.server.nodemanager.api.deviceplugin.YarnRuntimeType:<init>(java.lang.String,int,java.lang.String)	RUNTIME_DOCKER	1	3	java.lang.String	0	docker
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	SUCCESS	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_ARGUMENT_NUMBER	1	3	int	0	1
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_COMMAND_PROVIDED	2	3	int	0	3
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_NM_ROOT_DIRS	3	3	int	0	5
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	SETUID_OPER_FAILED	4	3	int	0	6
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	UNABLE_TO_EXECUTE_CONTAINER_SCRIPT	5	3	int	0	7
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	UNABLE_TO_SIGNAL_CONTAINER	6	3	int	0	8
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_CONTAINER_PID	7	3	int	0	9
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	OUT_OF_MEMORY	8	3	int	0	18
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INITIALIZE_USER_FAILED	9	3	int	0	20
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	PATH_TO_DELETE_IS_NULL	10	3	int	0	21
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_CONTAINER_EXEC_PERMISSIONS	11	3	int	0	22
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	INVALID_CONFIG_FILE	12	3	int	0	24
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	SETSID_OPER_FAILED	13	3	int	0	25
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	WRITE_PIDFILE_FAILED	14	3	int	0	26
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	WRITE_CGROUP_FAILED	15	3	int	0	27
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	TRAFFIC_CONTROL_EXECUTION_FAILED	16	3	int	0	28
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	DOCKER_RUN_FAILED	17	3	int	0	29
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	ERROR_OPENING_DOCKER_FILE	18	3	int	0	30
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	ERROR_READING_DOCKER_FILE	19	3	int	0	31
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	FEATURE_DISABLED	20	3	int	0	32
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	COULD_NOT_CREATE_SCRIPT_COPY	21	3	int	0	33
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	COULD_NOT_CREATE_CREDENTIALS_FILE	22	3	int	0	34
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	COULD_NOT_CREATE_WORK_DIRECTORIES	23	3	int	0	35
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	COULD_NOT_CREATE_APP_LOG_DIRECTORIES	24	3	int	0	36
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	COULD_NOT_CREATE_TMP_DIRECTORIES	25	3	int	0	37
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor$ExitCode:<init>(java.lang.String,int,int)	ERROR_CREATE_CONTAINER_DIRECTORIES_ARGUMENTS	26	3	int	0	38
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl$DeviceType:<init>(java.lang.String,int,java.lang.String)	BLOCK	0	3	java.lang.String	0	b
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl$DeviceType:<init>(java.lang.String,int,java.lang.String)	CHAR	1	3	java.lang.String	0	c
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink9	0	3	int	0	10
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink8	1	3	int	0	20
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink7	2	3	int	0	30
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink6	3	3	int	0	40
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink5	4	3	int	0	50
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink4	5	3	int	0	60
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink3	6	3	int	0	70
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink2	7	3	int	0	80
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkNVLink1	8	3	int	0	90
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkSameCPUSocket	9	3	int	0	200
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkCrossCPUSocket	10	3	int	0	300
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkSingleSwitch	11	3	int	0	600
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2$DeviceLinkType:<init>(java.lang.String,int,int)	P2PLinkMultiSwitch	12	3	int	0	1200
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	STATIC	0	3	java.lang.String	0	STATIC
org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceFile$TypeEnum:<init>(java.lang.String,int,java.lang.String)	ARCHIVE	1	3	java.lang.String	0	ARCHIVE
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	CPU	0	3	java.lang.String	0	cpu
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	NET_CLS	1	3	java.lang.String	0	net_cls
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	BLKIO	2	3	java.lang.String	0	blkio
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	MEMORY	3	3	java.lang.String	0	memory
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	CPUACCT	4	3	java.lang.String	0	cpuacct
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	CPUSET	5	3	java.lang.String	0	cpuset
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	FREEZER	6	3	java.lang.String	0	freezer
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandler$CGroupController:<init>(java.lang.String,int,java.lang.String)	DEVICES	7	3	java.lang.String	0	devices
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	CHECK_SETUP	0	3	java.lang.String	0	--checksetup
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	MOUNT_CGROUPS	1	3	java.lang.String	0	--mount-cgroups
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	INITIALIZE_CONTAINER	2	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	LAUNCH_CONTAINER	3	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	SIGNAL_CONTAINER	4	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	EXEC_CONTAINER	5	3	java.lang.String	0	--exec-container
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	DELETE_AS_USER	6	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	LAUNCH_DOCKER_CONTAINER	7	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	TC_MODIFY_STATE	8	3	java.lang.String	0	--tc-modify-state
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	TC_READ_STATE	9	3	java.lang.String	0	--tc-read-state
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	TC_READ_STATS	10	3	java.lang.String	0	--tc-read-stats
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	ADD_PID_TO_CGROUP	11	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	RUN_DOCKER_CMD	12	3	java.lang.String	0	--run-docker
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	GPU	13	3	java.lang.String	0	--module-gpu
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	FPGA	14	3	java.lang.String	0	--module-fpga
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	DEVICE	15	3	java.lang.String	0	--module-devices
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	LIST_AS_USER	16	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	ADD_NUMA_PARAMS	17	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	REMOVE_DOCKER_CONTAINER	18	3	java.lang.String	0	--remove-docker-container
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	INSPECT_DOCKER_CONTAINER	19	3	java.lang.String	0	--inspect-docker-container
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	SYNC_YARN_SYSFS	20	3	java.lang.String	0	
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	RUN_RUNC_CONTAINER	21	3	java.lang.String	0	--run-runc-container
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$OperationType:<init>(java.lang.String,int,java.lang.String)	REAP_RUNC_LAYER_MOUNTS	22	3	java.lang.String	0	--reap-runc-layer-mounts
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	INITIALIZE_CONTAINER	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	LAUNCH_CONTAINER	1	3	int	0	1
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	SIGNAL_CONTAINER	2	3	int	0	2
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	DELETE_AS_USER	3	3	int	0	3
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	LAUNCH_DOCKER_CONTAINER	4	3	int	0	4
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	LIST_AS_USER	5	3	int	0	5
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$RunAsUserCommand:<init>(java.lang.String,int,int)	SYNC_YARN_SYSFS	6	3	int	0	6
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	OK	0	3	int	0	0
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	INVALID_USER_NAME	1	3	int	0	2
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	UNABLE_TO_EXECUTE_CONTAINER_SCRIPT	2	3	int	0	7
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	INVALID_CONTAINER_PID	3	3	int	0	9
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	INVALID_CONTAINER_EXEC_PERMISSIONS	4	3	int	0	22
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	INVALID_CONFIG_FILE	5	3	int	0	24
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation$ResultCode:<init>(java.lang.String,int,int)	WRITE_CGROUP_FAILED	6	3	int	0	27
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	CREATED	0	3	java.lang.String	0	created
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	RUNNING	1	3	java.lang.String	0	running
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	STOPPED	2	3	java.lang.String	0	stopped
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	RESTARTING	3	3	java.lang.String	0	restarting
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	REMOVING	4	3	java.lang.String	0	removing
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	DEAD	5	3	java.lang.String	0	dead
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	EXITED	6	3	java.lang.String	0	exited
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	NONEXISTENT	7	3	java.lang.String	0	nonexistent
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor$DockerContainerStatus:<init>(java.lang.String,int,java.lang.String)	UNKNOWN	8	3	java.lang.String	0	unknown
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$SandboxMode:<init>(java.lang.String,int,java.lang.String)	enforcing	0	3	java.lang.String	0	enforcing
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$SandboxMode:<init>(java.lang.String,int,java.lang.String)	permissive	1	3	java.lang.String	0	permissive
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$SandboxMode:<init>(java.lang.String,int,java.lang.String)	disabled	2	3	java.lang.String	0	disabled
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_SUBMITTED	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_SCHEDULED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_ALLOCATED	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_LAUNCHED	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_FAILED	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_RUNNING	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_FINISHING	7	3	int	0	8
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_FINISHED	8	3	int	0	9
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_KILLED	9	3	int	0	10
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_ALLOCATED_SAVING	10	3	int	0	11
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_LAUNCHED_UNMANAGED_SAVING	11	3	int	0	12
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_RECOVERED	12	3	int	0	13
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppAttemptStateProto:<init>(java.lang.String,int,int)	RMATTEMPT_FINAL_SAVING	13	3	int	0	14
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_NEW	0	3	int	0	1
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_NEW_SAVING	1	3	int	0	2
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_SUBMITTED	2	3	int	0	3
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_ACCEPTED	3	3	int	0	4
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_RUNNING	4	3	int	0	5
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_FINAL_SAVING	5	3	int	0	6
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_FINISHING	6	3	int	0	7
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_FINISHED	7	3	int	0	8
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_FAILED	8	3	int	0	9
org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMAppStateProto:<init>(java.lang.String,int,int)	RMAPP_KILLED	9	3	int	0	10
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping$MappingType:<init>(java.lang.String,int,java.lang.String)	USER	0	3	java.lang.String	0	u
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping$MappingType:<init>(java.lang.String,int,java.lang.String)	GROUP	1	3	java.lang.String	0	g
org.apache.hadoop.yarn.server.resourcemanager.placement.QueueMapping$MappingType:<init>(java.lang.String,int,java.lang.String)	APPLICATION	2	3	java.lang.String	0	a
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	USED	0	3	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	PENDING	1	3	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	AMUSED	2	3	int	0	2
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	RESERVED	3	3	int	0	3
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	CACHED_USED	4	3	int	0	4
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	CACHED_PENDING	5	3	int	0	5
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	AMLIMIT	6	3	int	0	6
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	MIN_RESOURCE	7	3	int	0	7
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	MAX_RESOURCE	8	3	int	0	8
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	EFF_MIN_RESOURCE	9	3	int	0	9
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	EFF_MAX_RESOURCE	10	3	int	0	10
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType:<init>(java.lang.String,int,int)	USERAMLIMIT	11	3	int	0	11
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser$AppPriorityACLKeyType:<init>(java.lang.String,int,int)	USER	0	3	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser$AppPriorityACLKeyType:<init>(java.lang.String,int,int)	GROUP	1	3	int	0	2
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser$AppPriorityACLKeyType:<init>(java.lang.String,int,int)	MAX_PRIORITY	2	3	int	0	3
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser$AppPriorityACLKeyType:<init>(java.lang.String,int,int)	DEFAULT_PRIORITY	3	3	int	0	4
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	USED_CAP	0	3	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	ABS_USED_CAP	1	3	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	MAX_CAP	2	3	int	0	2
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	ABS_MAX_CAP	3	3	int	0	3
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	CAP	4	3	int	0	4
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	ABS_CAP	5	3	int	0	5
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	MAX_AM_PERC	6	3	int	0	6
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	RESERVED_CAP	7	3	int	0	7
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities$CapacityType:<init>(java.lang.String,int,int)	ABS_RESERVED_CAP	8	3	int	0	8
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	YARN_SITE	0	3	java.lang.String	0	yarn-site.xml
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	YARN_SITE	0	4	java.lang.String	0	y
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	YARN_SITE	0	5	java.lang.String	0	yarnsiteconfig
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	YARN_SITE	0	6	java.lang.String	0	Path to a valid yarn-site.xml config file
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	YARN_SITE	0	7	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	FAIR_SCHEDULER	1	3	java.lang.String	0	fair-scheduler.xml
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	FAIR_SCHEDULER	1	4	java.lang.String	0	f
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	FAIR_SCHEDULER	1	5	java.lang.String	0	fsconfig
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	FAIR_SCHEDULER	1	6	java.lang.String	0	Path to a valid fair-scheduler.xml config file
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	FAIR_SCHEDULER	1	7	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERSION_RULES	2	3	java.lang.String	0	conversion rules config file
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERSION_RULES	2	4	java.lang.String	0	r
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERSION_RULES	2	5	java.lang.String	0	rulesconfig
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERSION_RULES	2	6	java.lang.String	0	Optional parameter. If given, should specify a valid path to the conversion rules file (property format).
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERSION_RULES	2	7	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONSOLE_MODE	3	3	java.lang.String	0	console mode
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONSOLE_MODE	3	4	java.lang.String	0	p
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONSOLE_MODE	3	5	java.lang.String	0	print
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONSOLE_MODE	3	6	java.lang.String	0	If defined, the converted configuration will only be emitted to the console.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONSOLE_MODE	3	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CLUSTER_RESOURCE	4	3	java.lang.String	0	cluster resource
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CLUSTER_RESOURCE	4	4	java.lang.String	0	c
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CLUSTER_RESOURCE	4	5	java.lang.String	0	cluster-resource
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CLUSTER_RESOURCE	4	6	java.lang.String	0	Needs to be given if maxResources is defined as percentages for any queue, otherwise this parameter can be omitted.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CLUSTER_RESOURCE	4	7	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	OUTPUT_DIR	5	3	java.lang.String	0	output directory
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	OUTPUT_DIR	5	4	java.lang.String	0	o
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	OUTPUT_DIR	5	5	java.lang.String	0	output-directory
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	OUTPUT_DIR	5	6	java.lang.String	0	Output directory for yarn-site.xml and capacity-scheduler.xml files.Must have write permission for user who is running this script.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	OUTPUT_DIR	5	7	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	DRY_RUN	6	3	java.lang.String	0	dry run
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	DRY_RUN	6	4	java.lang.String	0	d
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	DRY_RUN	6	5	java.lang.String	0	dry-run
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	DRY_RUN	6	6	java.lang.String	0	Performs a dry-run of the conversion.Outputs whether the conversion is possible or not.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	DRY_RUN	6	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	NO_TERMINAL_RULE_CHECK	7	3	java.lang.String	0	no terminal rule check
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	NO_TERMINAL_RULE_CHECK	7	4	java.lang.String	0	t
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	NO_TERMINAL_RULE_CHECK	7	5	java.lang.String	0	no-terminal-rule-check
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	NO_TERMINAL_RULE_CHECK	7	6	java.lang.String	0	Disables checking whether a placement rule is terminal to maintain backward compatibility with configs that were made before YARN-8967.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	NO_TERMINAL_RULE_CHECK	7	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERT_PLACEMENT_RULES	8	3	java.lang.String	0	convert placement rules
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERT_PLACEMENT_RULES	8	4	java.lang.String	0	m
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERT_PLACEMENT_RULES	8	5	java.lang.String	0	convert-placement-rules
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERT_PLACEMENT_RULES	8	6	java.lang.String	0	Convert Fair Scheduler placement rules to Capacity Scheduler mapping rules
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	CONVERT_PLACEMENT_RULES	8	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	SKIP_VERIFICATION	9	3	java.lang.String	0	skip verification
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	SKIP_VERIFICATION	9	4	java.lang.String	0	s
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	SKIP_VERIFICATION	9	5	java.lang.String	0	skip-verification
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	SKIP_VERIFICATION	9	6	java.lang.String	0	Skips the verification of the converted configuration
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	SKIP_VERIFICATION	9	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	ENABLE_ASYNC_SCHEDULER	10	3	java.lang.String	0	enable asynchronous scheduler
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	ENABLE_ASYNC_SCHEDULER	10	4	java.lang.String	0	a
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	ENABLE_ASYNC_SCHEDULER	10	5	java.lang.String	0	enable-async-scheduler
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	ENABLE_ASYNC_SCHEDULER	10	6	java.lang.String	0	Enables the Asynchronous scheduler which decouples the CapacityScheduler scheduling from Node Heartbeats.
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	ENABLE_ASYNC_SCHEDULER	10	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	HELP	11	3	java.lang.String	0	help
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	HELP	11	4	java.lang.String	0	h
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	HELP	11	5	java.lang.String	0	help
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	HELP	11	6	java.lang.String	0	Displays the list of options
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler$CliOption:<init>(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)	HELP	11	7	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt$AMState:<init>(java.lang.String,int,java.lang.String)	UNMANAGED	0	3	java.lang.String	0	User launched the Application Master, since it's unmanaged. 
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt$AMState:<init>(java.lang.String,int,java.lang.String)	INACTIVATED	1	3	java.lang.String	0	Application is added to the scheduler and is not yet activated. 
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt$AMState:<init>(java.lang.String,int,java.lang.String)	ACTIVATED	2	3	java.lang.String	0	Application is Activated, waiting for resources to be assigned for AM. 
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt$AMState:<init>(java.lang.String,int,java.lang.String)	ASSIGNED	3	3	java.lang.String	0	Scheduler has assigned a container for AM, waiting for AM container to be launched
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt$AMState:<init>(java.lang.String,int,java.lang.String)	LAUNCHED	4	3	java.lang.String	0	AM container is launched, waiting for AM container to Register with RM
org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType:<init>(java.lang.String,int,int)	NODE_LOCAL	0	3	int	0	0
org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType:<init>(java.lang.String,int,int)	RACK_LOCAL	1	3	int	0	1
org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType:<init>(java.lang.String,int,int)	OFF_SWITCH	2	3	int	0	2
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor$ContextProp:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.resourcemanager.preprocessor.ContextProcessor)	NL	0	3		0	
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor$ContextProp:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.resourcemanager.preprocessor.ContextProcessor)	Q	1	3		0	
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor$ContextProp:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.resourcemanager.preprocessor.ContextProcessor)	TA	2	3		0	
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields$DeSelectType:<init>(java.lang.String,int,java.lang.String)	RESOURCE_REQUESTS	0	3	java.lang.String	0	resourceRequests
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields$DeSelectType:<init>(java.lang.String,int,java.lang.String)	TIMEOUTS	1	3	java.lang.String	0	timeouts
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields$DeSelectType:<init>(java.lang.String,int,java.lang.String)	APP_NODE_LABEL_EXPRESSION	2	3	java.lang.String	0	appNodeLabelExpression
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields$DeSelectType:<init>(java.lang.String,int,java.lang.String)	AM_NODE_LABEL_EXPRESSION	3	3	java.lang.String	0	amNodeLabelExpression
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields$DeSelectType:<init>(java.lang.String,int,java.lang.String)	RESOURCE_INFO	4	3	java.lang.String	0	resourceInfo
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType:<init>(java.lang.String,int,java.lang.String)	ENTITY	0	3	java.lang.String	0	EntityCollection
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType:<init>(java.lang.String,int,java.lang.String)	APPLICATION	1	3	java.lang.String	0	AppCollection
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType:<init>(java.lang.String,int,java.lang.String)	FLOW_RUN	2	3	java.lang.String	0	FlowRunCollection
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.CollectionType:<init>(java.lang.String,int,java.lang.String)	FLOW_ACTIVITY	3	3	java.lang.String	0	FlowActivityCollection
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_NAME	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_NAME	0	4	java.lang.String	0	flow_name
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_RUN_ID	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_RUN_ID	1	4	java.lang.String	0	flow_run_id
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	USER_ID	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	USER_ID	2	4	java.lang.String	0	user_id
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_ID	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_ID	0	4	java.lang.String	0	flow_id
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_RUN_ID	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_RUN_ID	1	4	java.lang.String	0	flow_run_id
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	USER_ID	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	USER_ID	2	4	java.lang.String	0	user_id
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowColumnFamily:<init>(java.lang.String,int,java.lang.String)	MAPPING	0	3	java.lang.String	0	m
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	4	java.lang.String	0	s
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	4	java.lang.String	0	r
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	4	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	EVENT	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	EVENT	3	4	java.lang.String	0	e
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	CONFIGS	1	3	java.lang.String	0	c
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	METRICS	2	3	java.lang.String	0	m
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	4	java.lang.String	0	id
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	1	4	java.lang.String	0	created_time
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	1	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	2	4	java.lang.String	0	flow_version
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationCompactionDimension:<init>(java.lang.String,int,byte)	APPLICATION_ID	0	3	int	0	101
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	RUN_ID	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	RUN_ID	0	4	java.lang.String	0	r
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	RUN_ID	0	5	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MIN_START_TIME	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MIN_START_TIME	0	4	java.lang.String	0	min_start_time
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MIN_START_TIME	0	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MIN_START_TIME	0	6		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MAX_END_TIME	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MAX_END_TIME	1	4	java.lang.String	0	max_end_time
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MAX_END_TIME	1	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	MAX_END_TIME	1	6		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	FLOW_VERSION	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	FLOW_VERSION	2	4	java.lang.String	0	flow_version
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation)	FLOW_VERSION	2	5	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	0	4	java.lang.String	0	m
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	0	5	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	0	6		0	
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	GLOBAL_MIN	0	3	int	0	71
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	GLOBAL_MAX	1	3	int	0	73
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	SUM	2	3	int	0	79
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	SUM_FINAL	3	3	int	0	83
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	LATEST_MIN	4	3	int	0	89
org.apache.hadoop.yarn.server.timelineservice.storage.flow.AggregationOperation:<init>(java.lang.String,int,byte)	LATEST_MAX	5	3	int	0	97
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	4	java.lang.String	0	s
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	4	java.lang.String	0	r
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	4	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	4	java.lang.String	0	e
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	5	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	CONFIGS	1	3	java.lang.String	0	c
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumnFamily:<init>(java.lang.String,int,java.lang.String)	METRICS	2	3	java.lang.String	0	m
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	4	java.lang.String	0	id
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	TYPE	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	TYPE	1	4	java.lang.String	0	type
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	4	java.lang.String	0	created_time
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	3	4	java.lang.String	0	flow_version
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	QUALIFIERS	0	3	java.lang.String	0	!
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	QUALIFIERS	0	4	java.lang.String	0	%0$
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	VALUES	1	3	java.lang.String	0	=
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	VALUES	1	4	java.lang.String	0	%1$
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SPACE	2	3	java.lang.String	0	 
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	SPACE	2	4	java.lang.String	0	%2$
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	TAB	3	3	java.lang.String	1	CQ==
org.apache.hadoop.yarn.server.timelineservice.storage.common.Separator:<init>(java.lang.String,int,java.lang.String,java.lang.String)	TAB	3	4	java.lang.String	0	%3$
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CREATED_TIME	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CREATED_TIME	0	4	java.lang.String	0	created_time
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	DESCRIPTION	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	DESCRIPTION	1	4	java.lang.String	0	description
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	MODIFICATION_TIME	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	MODIFICATION_TIME	2	4	java.lang.String	0	modification_time
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	OWNER	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	OWNER	3	4	java.lang.String	0	owner
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	READERS	4	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	READERS	4	4	java.lang.String	0	readers
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	WRITERS	5	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	WRITERS	5	4	java.lang.String	0	writers
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	ID	0	4	java.lang.String	0	id
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	TYPE	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	TYPE	1	4	java.lang.String	0	type
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	4	java.lang.String	0	created_time
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	CREATED_TIME	2	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumn:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	FLOW_VERSION	3	4	java.lang.String	0	flow_version
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	IS_RELATED_TO	0	4	java.lang.String	0	s
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	RELATES_TO	1	4	java.lang.String	0	r
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	INFO	2	4	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	4	java.lang.String	0	e
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,boolean)	EVENT	3	5	int	0	1
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String)	CONFIG	4	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	3		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	4	null	0	null
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnPrefix:<init>(java.lang.String,int,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnFamily,java.lang.String,org.apache.hadoop.yarn.server.timelineservice.storage.common.ValueConverter)	METRIC	5	5		0	
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnFamily:<init>(java.lang.String,int,java.lang.String)	INFO	0	3	java.lang.String	0	i
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnFamily:<init>(java.lang.String,int,java.lang.String)	CONFIGS	1	3	java.lang.String	0	c
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityColumnFamily:<init>(java.lang.String,int,java.lang.String)	METRICS	2	3	java.lang.String	0	m
