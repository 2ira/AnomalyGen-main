org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:runAll()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:onFailLazyPersist(java.lang.String,long)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:metaSave(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$20:getUrl()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:synchronizePlan(java.lang.String,boolean)
org.apache.hadoop.registry.conf.RegistryConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:<clinit>()
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:nextKeyValue()
org.apache.hadoop.fs.s3a.S3AFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.TaskAttemptContext:getProfileTaskRange(boolean)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:activeQueue()
org.apache.hadoop.fs.viewfs.NflyFSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSHAAdmin:getTargetIds(java.lang.String)
org.apache.hadoop.mapred.Counters:<clinit>()
org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close()
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:serviceStop()
org.apache.hadoop.fs.azure.Wasb:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:stop()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:stop()
org.apache.hadoop.mapred.join.OuterJoinRecordReader:createValue()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElector:close()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:initSharedJournalsForRead()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:cleanSubtreeRecursively(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,int,int,java.util.Map)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:stop()
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:getCallerUGI()
org.apache.hadoop.hdfs.net.DomainPeerServer:<clinit>()
org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.fs.s3a.prefetch.S3ABlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:<clinit>()
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:setKeyFieldComparatorOptions(org.apache.hadoop.mapreduce.Job,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:deleteMetadata()
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.security.authentication.examples.WhoClient:main(java.lang.String[])
org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getNewReservation(org.apache.hadoop.yarn.api.protocolrecords.GetNewReservationRequest)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.TaskAttemptContext:getJobSetupCleanupNeeded()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:deleteKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:main(java.lang.String[])
org.apache.hadoop.fs.ftp.FTPFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.NativeFileSystemStore:storeFile(java.lang.String,java.io.File,byte[])
org.apache.hadoop.util.IntrusiveCollection:<clinit>()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap)
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:hashCode()
org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider:init()
org.apache.hadoop.mapreduce.v2.app.client.ClientService:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.AllocationTagsManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitterFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:rollEdits()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:rollEditLog()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$HealthBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:start()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:stop()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:getDefaultBlockSize()
org.apache.hadoop.mapred.FileOutputCommitter:isRecoverySupported(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.yarn.webapp.view.DefaultPage:render(java.lang.Class)
org.apache.hadoop.mapred.gridmix.SleepJob:buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:save(java.lang.Object,java.io.File)
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getBPServiceActorInfo()
org.apache.hadoop.applications.mawo.server.common.MawoConfiguration:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.s3a.S3AFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MultiFileInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.fs.azurebfs.security.AbfssDtFetcher:isTokenRequired()
org.apache.hadoop.registry.conf.RegistryConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getContainerLogFile(java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:unset(java.lang.String)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.util.resource.ResourceUtils:resetResourceTypes(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:close()
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getReportsForNode(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.compress.DirectDecompressionCodec:getDefaultExtension()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.azure.SecureStorageInterfaceImpl:<clinit>()
org.apache.hadoop.fs.FsShellPermissions$Chmod:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:getLength(org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:<clinit>()
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumDeadDataNodes()
org.apache.hadoop.fs.shell.Delete$Rmr:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:configureAmazonS3Client(com.amazonaws.services.s3.AmazonS3,java.lang.String,boolean)
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer:write(int)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:contextDestroyed(javax.servlet.ServletContextEvent)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:stop()
org.apache.hadoop.fs.local.LocalFs:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.FsShell$Usage:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.FileDeletionTask:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:write(java.io.DataOutput)
org.apache.hadoop.fs.s3a.S3AFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:move(org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:start()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobConf(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.client.api.AHSClient:getApplications()
org.apache.hadoop.fs.shell.FsUsage$Dus:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.fs.sftp.SFTPFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.Job:addFileToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobPage:render(java.lang.Class)
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.cosn.CosNFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)
org.apache.hadoop.fs.azurebfs.Abfs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.server.federation.router.Router:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest)
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:enterClosedState()
org.apache.hadoop.hdfs.protocol.datatransfer.Sender:<clinit>()
org.apache.hadoop.mapred.lib.MultipleOutputs:close()
org.apache.hadoop.mapred.JobClient:submitJob(java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx:<clinit>()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:serviceStart()
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,long)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:endCurrentLogSegment(boolean)
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:close()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList:saveSelf2Snapshot(int,org.apache.hadoop.hdfs.server.namenode.INode,org.apache.hadoop.hdfs.server.namenode.INodeAttributes)
org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils:<clinit>()
org.apache.hadoop.fs.Hdfs:listCorruptFileBlocks(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.local.LocalFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:checkVersion()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:main(java.lang.String[])
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:finalizeUpgrade()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.hdfs.qjournal.server.Journal:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:getCallerUGI()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:rmdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.http.HttpFileSystem:getServerDefaults()
org.apache.hadoop.conf.ConfigurationWithLogging:getInts(java.lang.String)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:nextKeyValue()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.mapred.TaskAttemptContext:getCombinerClass()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:attempts()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:addSnapshot(int,java.lang.String,org.apache.hadoop.hdfs.server.namenode.LeaseManager,boolean,int,long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:<clinit>()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:addIfService(java.lang.Object)
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)
org.apache.hadoop.hdfs.server.namenode.BackupNode:setRpcServiceServerAddress(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceStop()
org.apache.hadoop.filecache.DistributedCache:setFileTimestamps(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:getNodeList(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:truncateBlock(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.fs.FsShell$Usage:expandArgument(java.lang.String)
org.apache.hadoop.examples.RandomTextWriter:main(java.lang.String[])
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher$1:run()
org.apache.hadoop.yarn.webapp.view.FooterBlock:render()
org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$23:connect(java.net.URL)
org.apache.hadoop.fs.impl.WeakReferenceThreadMap:create(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,int)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.mapred.Counters$GenericGroup:addCounter(java.lang.String,java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.adl.AdlFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:setModificationTime(long,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:unsetStoragePolicy(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApplicationAttemptsReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptsRequest)
org.apache.hadoop.fs.cosn.CosN:getDelegationTokens(java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceStop()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:close()
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:start()
org.apache.hadoop.fs.s3a.select.SelectInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block:subView(java.lang.Class)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapred.JobConf:write(java.io.DataOutput)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:initializeCacheExecutor(java.io.File)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:blockDataExists()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.shell.find.Print$Print0:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.ReduceTask:reportFatalError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.Throwable,java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode:setRpcServerAddress(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:<clinit>()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read()
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceStart()
org.apache.hadoop.mapred.ShuffleHandler$HttpPipelineFactory:getPipeline()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.s3a.S3AFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetOwnerOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileQueueParser:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$18:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.yarn.YarnUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:stop()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:getCallerUGI()
org.apache.hadoop.fs.s3a.Invoker:<clinit>()
org.apache.hadoop.mapreduce.Job:<clinit>()
org.apache.hadoop.fs.adl.AdlFsInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setVcoresPerNode(java.lang.String,int)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:readEvents(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.pipes.Submitter:<clinit>()
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:getNodeList(java.lang.String)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.s3a.S3AFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:expandArgument(java.lang.String)
org.apache.hadoop.streaming.StreamJob:createJob(java.lang.String[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.server.api.ServerRMProxy:getProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:container()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:getDelegationTokenSeqNum()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceSizingPBImpl:toString()
org.apache.hadoop.fs.shell.Count:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.FileOutputCommitter:abortJob(org.apache.hadoop.mapred.JobContext,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:incUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:must(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSDirectory:getINode4Write(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSCopyFileTask:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:start()
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:start()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.Counters$GenericGroup:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:_getAll(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType)
org.apache.hadoop.fs.shell.MoveCommands$Rename:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getVirtualMemorySize(int)
org.apache.hadoop.security.token.DtUtilShell$Remove:validate()
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:serviceStop()
org.apache.hadoop.tools.mapred.lib.DynamicInputChunkContext:<clinit>()
org.apache.hadoop.fs.s3a.S3AUtils:closeAll(org.slf4j.Logger,java.io.Closeable[])
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:createDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueUsersInfoBlock:getCallerUGI()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsXmlLoader:endElement(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:expandArgument(java.lang.String)
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.registry.client.api.RegistryOperationsFactory:createAnonymousInstance(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:recommissionNode(org.apache.hadoop.net.Node)
org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:toString()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getCpuFrequency()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:nodes()
org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getServerDefaults()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:run(java.lang.String[])
org.apache.hadoop.security.UserGroupInformation:<clinit>()
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenIdentifier:getBytes()
org.apache.hadoop.mapred.JobConf:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.metrics2.util.MetricsCache$RecordCache:removeEldestEntry(java.util.Map$Entry)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:renderJSON(java.lang.Object)
org.apache.hadoop.ipc.DecayRpcScheduler:<clinit>()
org.apache.hadoop.mapred.FileOutputCommitter:getJobAttemptPath(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:logs()
org.apache.hadoop.util.AsyncDiskService:shutdownNow()
org.apache.hadoop.fs.cosn.CosNFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:key(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapred.MapTaskStatus:setStartTime(long)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:serviceStart()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getDataPort()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getEffectiveCapacity(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.InternalScanner)
org.apache.hadoop.mapred.TaskAttemptContext:getOutputKeyClass()
org.apache.hadoop.crypto.key.kms.server.KMS:<clinit>()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainerLogFile(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:stop()
org.apache.hadoop.hdfs.ReaderStrategy:readFromBlock(org.apache.hadoop.hdfs.BlockReader)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:unsetStoragePolicy(java.lang.String)
org.apache.hadoop.minikdc.KerberosSecurityTestcase:stopMiniKdc()
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:open(java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalReleaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:internalUpdateLabelsOnNodes(java.util.Map,org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$NodeLabelUpdateOperation)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext)
org.apache.hadoop.mapred.lib.NLineInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceStart()
org.apache.hadoop.fs.http.HttpFileSystem:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:setAvailableResourcesToQueue(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.Hdfs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:readdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:stop()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[])
org.apache.hadoop.fs.local.LocalFs:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:serviceStart()
org.apache.hadoop.hdfs.server.blockmanagement.LowRedundancyBlocks:chooseLowRedundancyBlocks(int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])
org.apache.hadoop.yarn.service.utils.ClientRegistryBinder:lookupExternalRestAPI(java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:read(long,byte[],int,int)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.fs.cosn.CosNOutputStream:write(byte[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceStop()
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:<clinit>()
org.apache.hadoop.util.NativeCodeLoader:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getPreferredBlockSize(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.net.unix.DomainSocketWatcher$1:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getProcessId(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.fs.shell.CopyCommands$Cp:runAll()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.azurebfs.utils.TextFileBasedIdentityHandler:<clinit>()
org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String)
org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:skipUntil(long)
org.apache.hadoop.yarn.client.AHSProxy$1:run()
org.apache.hadoop.fs.shell.Mkdir:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azure.Wasb:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapred.JobQueueClient:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:_inc(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close()
org.apache.hadoop.conf.ConfigurationWithLogging:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.examples.pi.DistSum$ReduceSide$SummingReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.shell.Test:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getRunCommandForWindows(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.router.Router:serviceStop()
org.apache.hadoop.tools.dynamometer.blockgenerator.XMLParserMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:read(byte[],int,int)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logDelete(java.lang.String,long,boolean)
org.apache.hadoop.yarn.nodelabels.AbstractLabel:<init>(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:renderJSON(java.lang.Object)
org.apache.hadoop.fs.shell.XAttrCommands:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.security.YarnAuthorizationProvider:<clinit>()
org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)
org.apache.hadoop.mapred.ClientCache:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:getDataNodeProxy(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState:prepareToExitState(org.apache.hadoop.hdfs.server.namenode.ha.HAContext)
org.apache.hadoop.yarn.util.resource.Resources:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$ParentQueueBlock:renderPartial()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:renderPartial()
org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter:init(org.apache.commons.daemon.DaemonContext)
org.apache.hadoop.mapred.JobConf:getRaw(java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:readFully(long,byte[])
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getStoragePolicy(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.InfoPage:render(java.lang.Class)
org.apache.hadoop.hdfs.server.balancer.NameNodeConnector:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)
org.apache.hadoop.mount.MountInterface:umnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$WintuilsProcessStubExecutor$1:run()
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:execute(org.apache.commons.cli.CommandLine)
org.apache.hadoop.fs.HarFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:serviceStart()
org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.FsShell$Usage:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager:<clinit>()
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:performFailover(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:start()
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:createValue()
org.apache.hadoop.conf.ConfigurationWithLogging:onlyKeyExists(java.lang.String)
org.apache.hadoop.fs.shell.SetReplication:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.conf.YarnConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.yarn.client.cli.TopCLI$KeyboardMonitor:run()
org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger:<init>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getDataEncryptionKey()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:cleanupStagingDirs()
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:hasAccess(org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.conf.YarnConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.shell.Display$Checksum:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshAdminAcls()
org.apache.hadoop.mapred.JobConf:addTags(java.util.Properties)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getRssMemorySize(int)
org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)
org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetReplication(java.lang.String,short)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeContentSummary4Snapshot(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,org.apache.hadoop.hdfs.server.namenode.ContentCounts)
org.apache.hadoop.fs.shell.XAttrCommands:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:renameData(java.net.URI)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:incrementDelegationTokenSeqNum()
org.apache.hadoop.streaming.PipeMapper:setStreamJobDetails(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:updateNodeResource(org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceOptionInfo:toString()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:runAll()
org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:<clinit>()
org.apache.hadoop.mapred.nativetask.NativeBatchProcessor:sendCommandToJava(int,byte[])
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:createSchedContainerChangeRequests(java.util.List,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Count:runAll()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:toString()
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker$CheckedVolume:toString()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:renameData(java.net.URI)
org.apache.hadoop.hdfs.DFSStripedOutputStream:abort()
org.apache.hadoop.io.compress.SplittableCompressionCodec:createInputStream(java.io.InputStream)
org.apache.hadoop.mapred.JobConf:getMaxMapTaskFailuresPercent()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increaseAllocated(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshLoadedJobCache()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)
org.apache.hadoop.yarn.event.EventDispatcher:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:maybeIgnore(boolean,java.lang.String,java.io.IOException)
org.apache.hadoop.fs.http.HttpsFileSystem:close()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:shutdown()
org.apache.hadoop.fs.azurebfs.oauth2.UserPasswordTokenProvider:isTokenAboutToExpire()
org.apache.hadoop.mapred.SequenceFileOutputFormat:getReaders(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream:write(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:incrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.Job:setCacheArchives(java.net.URI[])
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage:render(java.lang.Class)
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:throwExceptionWhenArrayOutOfBound(int)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:run()
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_syncRead_jmhTest:syncRead_AverageTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolume:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:deleteTaskAttemptPathQuietly(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:getStoragePolicyID()
org.apache.hadoop.examples.WordMedian$WordMedianReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:fsyncDirectory()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:update()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseStorageMonitor:start()
org.apache.hadoop.yarn.client.cli.ApplicationCLI:main(java.lang.String[])
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO:createWriter(java.io.File)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.TaskReport:readFields(java.io.DataInput)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.security.SaslInputStream:read()
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:close()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.WritableComparator:get(java.lang.Class)
org.apache.hadoop.fs.azurebfs.Abfs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:render(java.lang.Class)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:<clinit>()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getVirtualMemorySize()
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$YamlOutputter:save(java.io.File)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:incAMUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:updateClusterResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupImage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAppendFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean,boolean)
org.apache.hadoop.yarn.server.resourcemanager.ClusterMonitor:updateNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[])
org.apache.hadoop.fs.s3a.auth.SignerManager:<clinit>()
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.StatefulStripeReader:readParityChunks(int)
org.apache.hadoop.net.DNSToSwitchMappingWithDependency:resolve(java.util.List)
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageReconstructor:<clinit>()
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.yarn.sls.SLSRunner$1:createAndInitActiveServices(boolean)
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getNodeCPUs(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.fs.viewfs.NflyFSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.shell.XAttrCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getUsed()
org.apache.hadoop.yarn.service.utils.CoreFileSystem:<clinit>()
org.apache.hadoop.mapreduce.util.ProcessTree:sigQuitProcess(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:selectInputStreams(long,long)
org.apache.hadoop.service.launcher.AbstractLaunchableService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.Router:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRename(java.lang.String,java.lang.String,long,boolean,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.Hdfs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:<clinit>()
org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:enterState(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState,org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState)
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.s3a.impl.CallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.DFSStripedInputStream:fetchBlockByteRange(org.apache.hadoop.hdfs.protocol.LocatedBlock,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks)
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.NodeManager:serviceStop()
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:read(long,byte[],int,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:stop()
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:notifyAMContainerLaunched(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:getContainersCPUs(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Delete$Rmr:runAll()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetAcl(java.lang.String,java.util.List)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:close()
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:<init>(org.apache.hadoop.mapreduce.v2.app.webapp.App,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.webapp.Controller$RequestContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.fs.s3a.commit.impl.CommitContext$PoolSubmitter:close()
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:makeFinalResult()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.util.ShutdownHookManager$1:run()
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.mapred.join.OuterJoinRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:toString()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.fs.viewfs.NflyFSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getInts(java.lang.String)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:serviceStop()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.FSSchedulerConfigurationStore:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.yarn.conf.YarnConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:close()
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getEffectiveMaxCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3a.S3AFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:<clinit>()
org.apache.hadoop.mapred.SequenceFileInputFilter:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.shell.AclCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.hdfs.server.namenode.ImageServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorJob:createValueAggregatorJobs(java.lang.String[])
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:destroyServices()
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:dump()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:cleanSubtree(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,int,int)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManager:getMaximumProfile()
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:loadCache(java.lang.Class)
org.apache.hadoop.fs.s3a.S3AFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increaseReserved(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.FsUsage$Df:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.aliyun.oss.OSS:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:stop()
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:lock()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:beforeExecution(com.amazonaws.AmazonWebServiceRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor:finishApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterResponse)
org.apache.hadoop.yarn.client.api.impl.DirectTimelineWriter:putDomain(org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbstractJobOrTaskStage:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listCacheDirectives(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowRunEntityReader:readRelationship(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix,boolean)
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitterFactory:getDestinationFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.ReservationInvariantsChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)
org.apache.hadoop.fs.FsShell$Usage:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathConnectionRunner:connect(java.net.URL)
org.apache.hadoop.examples.WordMean$WordMeanMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.datanode.web.HostRestrictingAuthorizationFilterHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MergeSorter:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:stop()
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:serviceStart()
org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Stat:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getPartitionQueueMetrics(java.lang.String)
org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getHomeDirectory()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getHomeDirectory()
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:getPreemptedMemorySeconds()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getBoolean(java.lang.String,boolean)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean)
org.apache.hadoop.yarn.server.router.Router:shutDown()
org.apache.hadoop.fs.azure.Wasbs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RollbackContainerTransition:createReInitContext(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)
org.apache.hadoop.fs.cosn.CosNInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.namenode.ha.IPFailoverProxyProvider:getProxyAddresses(java.net.URI,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:deleteSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.examples.terasort.TeraChecksum$ChecksumMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:computeContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.streaming.PipeCombiner:envPut(java.util.Properties,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.federation.policies.manager.AbstractPolicyManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:serviceStart()
org.apache.hadoop.examples.Sort:main(java.lang.String[])
org.apache.hadoop.yarn.client.cli.RMAdminCLI:getTargetIds(java.lang.String)
org.apache.hadoop.streaming.PipeCombiner:setStreamJobDetails(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:deleteKey(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$2:run()
org.apache.hadoop.fs.s3a.impl.DeleteOperation:apply()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getFloat(java.lang.String,float)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl$1:run()
org.apache.hadoop.tools.mapred.CopyOutputFormat:getWorkingDirectory(org.apache.hadoop.mapreduce.Job)
org.apache.hadoop.yarn.server.timeline.DomainLogInfo:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapred.JobContext:getTaskCleanupNeeded()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:serviceStart()
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialBinding:requestSessionCredentials(com.amazonaws.auth.AWSCredentialsProvider,com.amazonaws.ClientConfiguration,java.lang.String,java.lang.String,int,org.apache.hadoop.fs.s3a.Invoker)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.WebHdfs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DFSClient:mkdirs(java.lang.String)
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:start()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:getPendingAsk(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getTaskReports(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportsRequestProto)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:removeAll(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceStop()
org.apache.hadoop.io.compress.DirectDecompressionCodec:createDecompressor()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ContainerTokensStateIterator:hasNext()
org.apache.hadoop.mapred.lib.FieldSelectionMapReduce:<clinit>()
org.apache.hadoop.mapred.nativetask.util.LocalJobOutputFiles:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:jobCompleted(boolean)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.lib.input.NLineInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.audit.OperationAuditor:start()
org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler:<clinit>()
org.apache.hadoop.fs.azurebfs.Abfss:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.mapred.CopyMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper:<clinit>()
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:throwExceptionWhenArrayOutOfBound(int)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:start()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.RMFailoverProxyProvider:getProxy()
org.apache.hadoop.yarn.webapp.example.HelloWorld$Hello:json()
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:readOp()
org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:serviceStop()
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:getHttpRequest(java.lang.String[],java.lang.String,java.util.List,int,java.lang.String,boolean)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeXAttr(java.lang.String,org.apache.hadoop.fs.XAttr)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:stop()
org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)
org.apache.hadoop.mapred.Counters$GenericGroup:findCounter(java.lang.String,boolean)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager)
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextInitialized(javax.servlet.ServletContextEvent)
org.apache.hadoop.yarn.conf.YarnConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getUserAMResourceLimit()
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:setStatus(org.apache.hadoop.lib.server.Server$Status)
org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)
org.apache.hadoop.fs.s3a.S3AFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:applicationStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationStartData)
org.apache.hadoop.fs.s3a.S3AFileSystem:setAmazonS3Client(com.amazonaws.services.s3.AmazonS3)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:setAMResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:initOutput(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getFSState()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.webapp.Controller:<clinit>()
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:fromJson(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.sftp.SFTPFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.DFSStripedInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.MultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)
org.apache.hadoop.fs.azure.Wasb:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:addIfService(java.lang.Object)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getSpillFile(int)
org.apache.hadoop.yarn.server.resourcemanager.webapp.ErrorBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getQueueMaxResource(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FsckServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getBlockKeys()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:listPendingUploadsToCommit(org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:close()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseLocalOrFavoredStorage(org.apache.hadoop.net.Node,boolean,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapred.JobContext:getArchiveTimestamps()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:normalizeResourceRequests(java.util.List,java.lang.String)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:render()
org.apache.hadoop.mapred.JobContext:getOutputKeyClass()
org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:checkAndGetApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refreshUserToGroupsMappings()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage:render(java.lang.Class)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,boolean)
org.apache.hadoop.fs.WindowsGetSpaceUsed:init()
org.apache.hadoop.yarn.util.RackResolver:resolve(org.apache.hadoop.conf.Configuration,java.util.List)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setPending(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:moveAppFrom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:<init>(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.mapreduce.Job:setMaxMapAttempts(int)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable)
org.apache.hadoop.fs.s3a.auth.MarshalledCredentials:setSecretsInConfiguration(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.OSS:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempUserPerPartition:toString()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,java.util.EnumSet,int,org.apache.hadoop.util.Progressable,java.net.InetSocketAddress[])
org.apache.hadoop.applications.mawo.server.common.TeardownTask:<init>(org.apache.hadoop.applications.mawo.server.common.TaskId,java.util.Map,java.lang.String,long)
org.apache.hadoop.fs.StreamCapabilitiesPolicy:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode:getSlowDisksReport()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getAclStatus(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:start()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetQuotaByStorageTypeOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:decPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveFSImageInAllDirs(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long)
org.apache.hadoop.examples.terasort.TeraInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.adl.AdlFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:serviceStart()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,double)
org.apache.hadoop.fs.LocalFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:close()
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManager:reloadProfiles()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.Job:addFileToSharedCacheAndClasspath(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DiskResourceHandler:teardown()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.WorkflowPriorityMappingsManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)
org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:shuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.InputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:clearPendingContainerCache()
org.apache.hadoop.mapred.JobConf:getSpeculativeExecution()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptLaunchedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.DeadNodeDetector:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3a.S3AInstrumentation$MetricsUpdatingIOStatisticsStore:setCounter(java.lang.String,long)
org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)
org.apache.hadoop.yarn.client.RMProxy:<clinit>()
org.apache.hadoop.fs.adl.Adl:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.shell.find.Find:processRawArguments(java.util.LinkedList)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:close()
org.apache.hadoop.mapred.FileOutputCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:pauseContainer()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkGetACLS(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:isMultiThreadNecessary(java.util.LinkedList)
org.apache.hadoop.jmx.JMXJsonServlet:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:recover(java.util.Map)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getSnapshottableDirectoryList()
org.apache.hadoop.mapreduce.v2.api.impl.pb.client.HSClientProtocolPBClientImpl:close()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:copyBlockdata(java.net.URI)
org.apache.hadoop.hdfs.tools.GetConf$NameNodesCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.mapred.nativetask.handlers.IDataLoader:load()
org.apache.hadoop.mapred.TaskAttemptContext:getProfileEnabled()
org.apache.hadoop.fs.azurebfs.Abfs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.s3a.S3ABlockOutputStream$BlockUploadProgress:progressChanged(com.amazonaws.event.ProgressEvent)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:initiateTaskOperation(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.security.IngressPortBasedResolver:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.Object,java.lang.Object)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)
org.apache.hadoop.fs.shell.FsUsage$Df:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.CounterGroup:addCounter(org.apache.hadoop.mapreduce.Counter)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:cleanupRegistry()
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:close()
org.apache.hadoop.examples.DBCountPageView$PageviewReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.FileOutputCommitter:getCommittedTaskPath(org.apache.hadoop.mapred.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManager:getMinimumProfile()
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setQuota(java.lang.String,long,long,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.HarFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:serviceStart()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsNavBlock:getCallerUGI()
org.apache.hadoop.fs.WebHdfs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logTimes(java.lang.String,long,long)
org.apache.hadoop.fs.ftp.FtpFs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.examples.terasort.TeraValidate$ValidateMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.Counters:incrAllCounters(org.apache.hadoop.mapreduce.counters.AbstractCounters)
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEvents(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:<clinit>()
org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator:hasNext()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.hdfs.DFSStripedInputStream:read()
org.apache.hadoop.mapred.gridmix.StressJobFactory:<clinit>()
org.apache.hadoop.io.BloomMapFile:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceStart()
org.apache.hadoop.hdfs.tools.DFSAdmin:run(java.lang.String[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalUnReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.crypto.key.kms.KMSClientProvider:<clinit>()
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.RMHAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$NotifyContainerSchedulerOfUpdateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:destroy()
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockListCommand:execute()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:serviceStart()
org.apache.hadoop.fs.cosn.CosNFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.BufferedFSInputStream:skip(long)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getVirtualMemorySize()
org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:<init>(org.apache.hadoop.yarn.server.router.Router,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter:getParentId(long)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:stop()
org.apache.hadoop.mapred.lib.db.DBInputFormat$DBRecordReader:getSelectQuery()
org.apache.hadoop.ipc.Client:close()
org.apache.hadoop.hdfs.DFSStripedOutputStream:hsync()
org.apache.hadoop.yarn.sls.SLSRunner$1:createApplicationMasterService()
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:addBlockToBeErasureCoded(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor[],org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[],byte[],byte[],org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy)
org.apache.hadoop.net.SocketOutputStream:write(int)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read(byte[],int,int)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getUsed()
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:serviceStart()
org.apache.hadoop.fs.http.HttpFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:hsync()
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.Router:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ReassignLeaseOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStop()
org.apache.hadoop.contrib.utils.join.DataJoinMapperBase:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.cosn.CosNFileSystem:getHomeDirectory()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.GenericEntityReader:augmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Concat:runAll()
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:run()
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageLoader:<clinit>()
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:<init>(int,org.apache.hadoop.conf.Configuration,int,java.lang.Class)
org.apache.hadoop.fs.viewfs.NflyFSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList)
org.apache.hadoop.io.ArrayFile$Reader:next(org.apache.hadoop.io.Writable)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerTask:<clinit>()
org.apache.hadoop.hdfs.DFSStripedInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.hdfs.tools.GetConf$NNRpcAddressesCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeNewReservation(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.security.DockerCredentialTokenIdentifier:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:serviceStart()
org.apache.hadoop.mapred.MapRunnable:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:buildKeytabInstallationDirPath(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintsUtil:<clinit>()
org.apache.hadoop.contrib.utils.join.JobBase:<clinit>()
org.apache.hadoop.fs.shell.Stat:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.shell.Test:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:notifyStoreOperationFailed(java.lang.Exception)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:registerPathListener(org.apache.hadoop.registry.client.impl.zk.PathListener)
org.apache.hadoop.fs.s3a.S3AFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:killJob(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillJobRequest)
org.apache.hadoop.hdfs.DFSClient:getBlockSize(java.lang.String)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumOfBlocksPendingDeletion()
org.apache.hadoop.hdfs.server.federation.resolver.order.RouterResolver:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV2CommandPlugin:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeFinalizeOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.aliyun.oss.OSS:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:symlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getHomeDirectory()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeErasureCodingPolicy(java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:addTags(java.util.Properties)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:breakHardLinksIfNeeded()
org.apache.hadoop.hdfs.qjournal.server.JournalNode:stopAndJoin(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAcls(java.lang.String,java.util.Map)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getSlowDatanodeReport()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$10:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:disallowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:setPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:setMapInputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration,float)
org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed:<clinit>()
org.apache.hadoop.yarn.service.component.Component$ContainerCompletedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceAllocator:<clinit>()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getCapacityRemaining()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:incPending(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)
org.apache.hadoop.fs.FileSystem:closeAll()
org.apache.hadoop.security.UserGroupInformation$TestingGroups:cacheGroupsAdd(java.util.List)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddErasureCodingPolicyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerCleanup:<clinit>()
org.apache.hadoop.fs.shell.Truncate:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.metrics2.MetricsSystemMXBean:stop()
org.apache.hadoop.mapred.JobClient:jobsToComplete()
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:about()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getPendingResources()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getContentSummary(java.lang.String)
org.apache.hadoop.hdfs.client.impl.BlockReaderLocal:<clinit>()
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readAndDiscard(int)
org.apache.hadoop.fs.ftp.FtpFs:getCanonicalServiceName()
org.apache.hadoop.fs.cosn.CosNFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:serviceStart()
org.apache.hadoop.yarn.sls.SLSRunner$1:createRMAppManager()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:defaultAugmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:stop()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.api.impl.pb.client.SCMAdminProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getRunCommandForWindows(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:seekToNewSource(long)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:updateBlockForPipeline(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.LocalAllocationTagsManager:<clinit>()
org.apache.hadoop.yarn.api.impl.pb.client.ClientSCMProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:close()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setOffSwitchPerHeartbeatLimit(int)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:hflush()
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.io.WritableComparator:<init>()
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromResource(java.lang.String)
org.apache.hadoop.registry.server.services.AddingCompositeService:addIfService(java.lang.Object)
org.apache.hadoop.streaming.StreamInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.fs.Hdfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.registry.server.dns.PrivilegedRegistryDNSStarter:<clinit>()
org.apache.hadoop.tools.rumen.JobBuilder:<clinit>()
org.apache.hadoop.http.WebServlet:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.nativetask.serde.FloatWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:renderPartial()
org.apache.hadoop.fs.s3a.S3A:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:run()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:precommitCheckPendingFiles(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:incPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:setUser(java.lang.String,int)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.mapred.CleanupQueue:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:<init>()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.AclCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getProvidedCapacityTotal()
org.apache.hadoop.yarn.webapp.WebApp:<clinit>()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.ContextFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:submitApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.fs.azure.Wasbs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:close()
org.apache.hadoop.yarn.sls.scheduler.FifoSchedulerMetrics:addAMRuntime(org.apache.hadoop.yarn.api.records.ApplicationId,long,long,long,long)
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:allowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDatanodeStorageInfo:setState(org.apache.hadoop.hdfs.server.protocol.DatanodeStorage$State)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider()
org.apache.hadoop.fs.http.HttpFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$9:connect(java.net.URL)
org.apache.hadoop.mapred.gridmix.Gridmix$Component:add(java.lang.Object)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest)
org.apache.hadoop.mapreduce.v2.app.rm.preemption.CheckpointAMPreemptionPolicy:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeNewReservation(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)
org.apache.hadoop.fs.local.LocalFs:getFsStatus()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:renameSnapshot(java.lang.String,java.lang.String,java.lang.String,long)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:startReconfiguration()
org.apache.hadoop.mapred.ResourceMgrDelegate:stop()
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.tools.SimpleCopyListing:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:hashCode()
org.apache.hadoop.yarn.conf.YarnConfiguration:clear()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Mkdir:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapreduce.jobhistory.EventWriter:<clinit>()
org.apache.hadoop.mapred.MultiFileInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateDNBlockInfosReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:skipUntil(long)
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.fs.azurebfs.Abfss:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setFloat(java.lang.String,float)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getDestinationFS(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.LocalFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathBooleanRunner:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:getSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.GetSafeModeRequest)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:run(java.lang.String[])
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getBlacklistedNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.mapred.lib.MultipleSequenceFileOutputFormat:getInputFileBasedOutputFileName(org.apache.hadoop.mapred.JobConf,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:removeAclFeature(int)
org.apache.hadoop.streaming.PipeCombiner:getContext()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.appcatalog.application.AppCatalogInitializer:contextInitialized(javax.servlet.ServletContextEvent)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:interruptAndJoin()
org.apache.hadoop.fs.s3a.S3AFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:getBytesSentPerContainer()
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logGenerationStamp(long)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:stop()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapred.Counters$GenericGroup:findCounter(java.lang.String)
org.apache.hadoop.yarn.service.ContainerFailureTracker:<clinit>()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:initiateJobOperation(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getJobReport(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetJobReportRequest)
org.apache.hadoop.fs.shell.CopyCommands$Merge:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.client.HdfsAdmin:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetPermissions(java.lang.String,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.JMXGet:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.azure.Wasb:getHomeDirectory()
org.apache.hadoop.yarn.service.component.Component:<clinit>()
org.apache.hadoop.streaming.StreamInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.HttpFSServer:putRoot(java.io.InputStream,javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.fs.s3a.InconsistentAmazonS3Client:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:doPreUpgradeOfSharedLog()
org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext:startStandbyServices()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$4:connect(java.net.URL)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.CSConfigurationProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getPreferredBlockSize(java.lang.String)
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:mark(int)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:write(java.lang.String,java.lang.Object,java.lang.Object)
org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture)
org.apache.hadoop.examples.terasort.TeraChecksum$ChecksumReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.MapTaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:start()
org.apache.hadoop.fs.s3a.audit.OperationAuditor:close()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getEffectiveMaxCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:serviceStart()
org.apache.hadoop.hdfs.DFSStripedInputStream:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readFully(long,byte[])
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:performFailover(java.lang.Object)
org.apache.hadoop.util.ThreadUtil:joinUninterruptibly(java.lang.Thread)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:unbuffer()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuDiscoverer:<clinit>()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:serviceStart()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:recoverFromStore()
org.apache.hadoop.security.ProviderUtils:<clinit>()
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:noteFailure(java.lang.Exception)
org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:refreshMaximumAllocation(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getHomeDirectory()
org.apache.hadoop.util.InstrumentedWriteLock:check(long,long,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getHomeDirectory()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
org.apache.hadoop.mapreduce.v2.app.webapp.CountersPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread:run()
org.apache.hadoop.fs.WebHdfs:getServerDefaults()
org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.fs.viewfs.NflyFSystem:getUsed()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$HealthBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:toString()
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:<clinit>()
org.apache.hadoop.mapred.QueueManager:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:fetchReplicaInfo(java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:info()
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:decAMUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.service.AbstractService:<clinit>()
org.apache.hadoop.fs.shell.TouchCommands$Touch:expandArgument(java.lang.String)
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:close()
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:close()
org.apache.hadoop.hdfs.server.federation.store.RouterStore:getRouterRegistration(org.apache.hadoop.hdfs.server.federation.store.protocol.GetRouterRegistrationRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin:<clinit>()
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:requestCreated(com.amazonaws.AmazonWebServiceRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:start()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.DFSStripedInputStream:read(byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:setConfig(java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FifoPolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore:checkVersion()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:get(int)
org.apache.hadoop.CustomOutputCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapred.TaskAttemptContext:getLocalCacheFiles()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getContentSummary(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptSucceededTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster$RMCallbackHandler:onRequestsRejected(java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setFloat(java.lang.String,float)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:saveChild2Snapshot(org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Concat:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.security.SaslRpcServer:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.fs.http.HttpFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:finalizeUpgrade()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:addApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,boolean,boolean)
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getTrashRoots(boolean)
org.apache.hadoop.fs.shell.Count:run(java.lang.String[])
org.apache.hadoop.fs.cosn.CosNFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getStoragePolicy(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:disableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.fs.HarFileSystem:close()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:start()
org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManagerMXBean:isSecurityEnabled()
org.apache.hadoop.yarn.server.api.AuxiliaryLocalPathHandler:getAllLocalPathsForRead(java.lang.String)
org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:serviceStart()
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.conf.ConfigurationWithLogging:getStringCollection(java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.LocalFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:incReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector:<clinit>()
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:setGroup(java.lang.String,int)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:replaceLabelsOnNode(org.apache.hadoop.yarn.server.api.protocolrecords.ReplaceLabelsOnNodeRequest)
org.apache.hadoop.fs.FsShellPermissions$Chmod:processArguments(java.util.LinkedList)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock:renderPartial()
org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:close()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:storeToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:updatePreemptedForCustomResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.lib.db.DBConfiguration:setInputBoundingQuery(java.lang.String)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getPathForWorkFile(org.apache.hadoop.mapreduce.TaskInputOutputContext,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:incrementCurrentKeyId()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueuePriority(java.lang.String,int)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.fs.shell.Count:displayError(java.lang.Exception)
org.apache.hadoop.mapreduce.counters.AbstractCounters:<clinit>()
org.apache.hadoop.yarn.service.client.ServiceClient:stop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getTrashRoots(boolean)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refreshCallQueue()
org.apache.hadoop.mapred.JobConf:setKeepFailedTaskFiles(boolean)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:moveAppTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.HttpFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:storeToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:beforeAttempt(com.amazonaws.handlers.HandlerBeforeAttemptContext)
org.apache.hadoop.mapreduce.lib.chain.Chain$ReduceRunner:run()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeDefaultAcl(java.lang.String)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:unset(java.lang.String)
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:<clinit>()
org.apache.hadoop.io.compress.SplittableCompressionCodec:createOutputStream(java.io.OutputStream)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getTransactionID()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:addIfService(java.lang.Object)
org.apache.hadoop.fs.shell.CopyCommands$Merge:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:setGroup(java.lang.String,int)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getKeys()
org.apache.hadoop.fs.local.RawLocalFs:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,boolean)
org.apache.hadoop.io.LongWritable$DecreasingComparator:<init>()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:hasCapability(java.lang.String)
org.apache.hadoop.mapred.ReduceTask:getMapFiles(org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:render()
org.apache.hadoop.fs.s3a.select.SelectInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.viewfs.NflyFSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.RegistryAdminService:stop()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.LocalityAppPlacementAllocator:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.PreemptableResourceCalculator:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:reserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Container,boolean)
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1:run()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:signalToContainer(org.apache.hadoop.yarn.api.protocolrecords.SignalContainerRequest)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet:<clinit>()
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:doGetGroups(java.lang.String,int)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowRunEntityReader:readEntities(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.mapred.TaskAttemptContext:getOutputValueClass()
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:addIfService(java.lang.Object)
org.apache.hadoop.mapreduce.v2.app.speculate.SimpleExponentialTaskRuntimeEstimator:contextualize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:decreasePending(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setSafeMode(org.apache.hadoop.fs.SafeModeAction)
org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.azure.WasbAuthorizerInterface:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:relaunchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)
org.apache.hadoop.ipc.RefreshRegistry:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getErasureCodingPolicies()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.KeyValueBasedTimelineStore:<clinit>()
org.apache.hadoop.mapred.FileOutputFormat:getPathForCustomFile(org.apache.hadoop.mapred.JobConf,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getKeyProviderUri()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext)
org.apache.hadoop.yarn.sls.scheduler.FifoSchedulerMetrics:init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Delete$Rmr:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:stop()
org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:toString()
org.apache.hadoop.mapreduce.lib.map.WrappedMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.S3AUtils:mapLocatedFiles(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.fs.s3a.S3AUtils$LocatedFileStatusMap)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:jobCompleted(boolean)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:noteFailure(java.lang.Exception)
org.apache.hadoop.ipc.WritableRpcEngine$Server:queueCall(org.apache.hadoop.ipc.Server$Call)
org.apache.hadoop.mapred.MapTaskStatus:statusUpdate(float,java.lang.String,org.apache.hadoop.mapred.Counters)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:clearPendingContainerCache()
org.apache.hadoop.streaming.DumpTypedBytes:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$1:run()
org.apache.hadoop.crypto.key.kms.server.KMS:invalidateCache(java.lang.String)
org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:write(byte[])
org.apache.hadoop.yarn.webapp.WebApp:route(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.tools.GetConf$SecondaryNameNodesCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.fs.ftp.FTPFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeNewApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ReservedContainerCandidatesSelector:<clinit>()
org.apache.hadoop.hdfs.protocol.OpenFilesIterator:hasNext()
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.LocalFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationHistoryProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.util.InstrumentedWriteLock:lock()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.conf.ReconfigurableBase:<clinit>()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.Stat:displayError(java.lang.Exception)
org.apache.hadoop.conf.ConfigurationWithLogging:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.mapreduce.Job:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$KillAttemptTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.JobConf:getPropertySources(java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:updateStorage(org.apache.hadoop.hdfs.server.protocol.DatanodeStorage)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:removeAll(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitter:<clinit>()
org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension$2:load(java.lang.Object)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage$AllApplicationsBlock:render()
org.apache.hadoop.fs.ftp.FTPFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)
org.apache.hadoop.mapreduce.util.MRJobConfUtil:setLocalDirectoriesConfigForTesting(org.apache.hadoop.conf.Configuration,java.io.File)
org.apache.hadoop.mapred.JobConf:setUseNewMapper(boolean)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStop()
org.apache.hadoop.yarn.service.client.ApiServiceClient:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB:close()
org.apache.hadoop.lib.server.Server:<init>(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:initEditLog(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI:main(java.lang.String[])
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getCompressor()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AllocateBlockIdOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:createProxyIfNeeded(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo)
org.apache.hadoop.yarn.webapp.Dispatcher:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[],java.util.EnumSet)
org.apache.hadoop.yarn.applications.distributedshell.Client:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.mapred.jobcontrol.Job:setJobConf(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsSingleCounterPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeDescriptorsScriptRunner:<clinit>()
org.apache.hadoop.hdfs.protocol.CachePoolIterator:next()
org.apache.hadoop.yarn.service.monitor.probe.PortProbe:ping(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)
org.apache.hadoop.examples.QuasiMonteCarlo$QmcReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:stopThreads()
org.apache.hadoop.streaming.PipeMapper:envline(java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:main(java.lang.String[])
org.apache.hadoop.mapred.ShuffleHandler:serviceStart()
org.apache.hadoop.fs.Hdfs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.oncrpc.RpcProgram:doPortMonitoring(java.net.SocketAddress)
org.apache.hadoop.hdfs.protocol.EncryptionZoneIterator:next()
org.apache.hadoop.mapred.JobConf:getPassword(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getServerDefaults()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:start()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getStoragePolicies()
org.apache.hadoop.fs.s3a.impl.RequestFactoryImpl:<clinit>()
org.apache.hadoop.yarn.service.component.instance.ComponentInstance:<clinit>()
org.apache.hadoop.mapred.FileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandom(java.lang.String,java.util.Collection)
org.apache.hadoop.fs.cosn.CosNFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.shell.Stat:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeReservation(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.client.HdfsAdmin:removeErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:<clinit>()
org.apache.hadoop.registry.client.api.RegistryOperations:clearWriteAccessors()
org.apache.hadoop.registry.conf.RegistryConfiguration:getStrings(java.lang.String)
org.apache.hadoop.fs.s3a.auth.SignerManager:close()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:beforeRequest(com.amazonaws.Request)
org.apache.hadoop.hdfs.DFSStripedOutputStream:flush()
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:displayError(java.lang.Exception)
org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:releaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:nextRawKey()
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:needsPassword()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:close()
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:flush()
org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.fs.HarFs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyTokenSecretManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.azurebfs.services.AbfsLease$1:onFailure(java.lang.Throwable)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getHomeDirectory()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$17:connect(java.net.URL)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getServerDefaults()
org.apache.hadoop.yarn.webapp.example.MyApp:main(java.lang.String[])
org.apache.hadoop.mapreduce.Cluster:getJobHistoryUrl(org.apache.hadoop.mapreduce.JobID)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenIdentifier:getTrackingId()
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics:renderPartial()
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:readObject(java.io.ObjectInputStream)
org.apache.hadoop.mapreduce.checkpoint.CheckpointService:delete(org.apache.hadoop.mapreduce.checkpoint.CheckpointID)
org.apache.hadoop.mapred.JobClient:getJob(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker:<clinit>()
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialProvider:getCredentials()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:breakHardLinksIfNeeded()
org.apache.hadoop.registry.conf.RegistryConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.viewfs.ViewFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getBlockKeys()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.fs.adl.Adl:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:<clinit>()
org.apache.hadoop.hdfs.tools.GetConf$JournalNodeCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.registry.server.services.RegistryAdminService:createEnsembleProvider()
org.apache.hadoop.fs.http.HttpsFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.Adl:getServerDefaults()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:getTargetPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$StartLogSegmentOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.service.provider.ProviderFactory:<clinit>()
org.apache.hadoop.fs.s3a.commit.impl.CommitContext:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.registry.client.types.Endpoint$Marshal:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)
org.apache.hadoop.security.KDiag:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkCreate(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List)
org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:<clinit>()
org.apache.hadoop.examples.AggregateWordCount:main(java.lang.String[])
org.apache.hadoop.mapred.Counters$FSGroupImpl:addCounter(org.apache.hadoop.mapreduce.Counter)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:close()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.yarn.security.NMTokenIdentifier:<clinit>()
org.apache.hadoop.fs.FsShellPermissions$Chmod:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:serviceStart()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getNumVCoresUsed()
org.apache.hadoop.fs.cosn.CosNInputStream:read(byte[],int,int)
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:<clinit>()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.blockmanagement.SlowDiskTracker:<clinit>()
org.apache.hadoop.mapred.lib.ChainReducer:addMapper(org.apache.hadoop.mapred.JobConf,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:allocateResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:start()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoCandidatesSelector:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSClient:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:<clinit>()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:mark(int)
org.apache.hadoop.fs.shell.Count:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.aliyun.oss.OSS:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:stop()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:stop()
org.apache.hadoop.fs.cosn.CosNFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.MetricsSink:init(org.apache.commons.configuration2.SubsetConfiguration)
org.apache.hadoop.hdfs.client.HdfsAdmin:disableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[])
org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryContext:getAllJobs(org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:killAllAppsInQueue(java.lang.String)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:createKey()
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.Mkdir:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore$LevelDBMapAdapter$1:remove()
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:<clinit>()
org.apache.hadoop.fs.s3a.S3A:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.mapred.JobConf:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$22:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.mapred.Counters$FSGroupImpl:findCounter(java.lang.String,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.router.webapp.RouterController:apps()
org.apache.hadoop.hdfs.server.datanode.FSCachingGetSpaceUsed$Builder:getInterval()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.FsShell$Usage:runAll()
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:start()
org.apache.hadoop.yarn.sls.SLSRunner$1:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.LocalFileSystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.webapp.example.MyApp$MyController:renderText(java.lang.String)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getMountTable()
org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])
org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File)
org.apache.hadoop.mapred.FileOutputCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.examples.SecondarySort$MapClass:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:renderPartial()
org.apache.hadoop.hdfs.server.namenode.sps.DatanodeCacheManager:<clinit>()
org.apache.hadoop.hdfs.server.common.Util:<clinit>()
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.fs.s3a.S3AInstrumentation$MetricsUpdatingIOStatisticsStore:incrementCounter(java.lang.String)
org.apache.hadoop.mapred.TaskLog$Reader:<init>(org.apache.hadoop.mapred.TaskAttemptID,org.apache.hadoop.mapred.TaskLog$LogName,long,long,boolean)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkList(java.lang.String)
org.apache.hadoop.mapreduce.Job:setUser(java.lang.String)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:startOperation(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.adl.Adl:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodeLabelsPage$NodeLabelsBlock:render()
org.apache.hadoop.yarn.conf.YarnConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:setupConfigurableCapacities()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:start()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:read(long,byte[],int,int)
org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider:isTokenAboutToExpire()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.jobcontrol.Job:<init>(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.azure.RemoteWasbAuthorizerImpl:<clinit>()
org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()
org.apache.hadoop.yarn.server.timelineservice.storage.common.HBaseTimelineStorageUtils:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.fs.LocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.cosn.CosN:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getSnapshotDiffReport(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.S3A:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:initializeSecretProvider(javax.servlet.FilterConfig)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:warmUpEncryptedKeys(java.lang.String[])
org.apache.hadoop.fs.s3a.S3AFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.api.ServerRMProxy:getRMAddress(org.apache.hadoop.yarn.conf.YarnConfiguration,java.lang.Class)
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:shutDown()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.PeriodicService:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:listManifests()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.ContextFactory:createContext(java.lang.Class[],java.util.Map)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:addLabelsToNode(java.util.Map)
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addSample(java.lang.String,long)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemVolumeManager:<clinit>()
org.apache.hadoop.fs.shell.TouchCommands$Touch:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.webapp.view.HeaderBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner:<clinit>()
org.apache.hadoop.yarn.server.router.clientrm.ClientMethod:<clinit>()
org.apache.hadoop.fs.shell.FsUsage$Dus:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.examples.WordMean$WordMeanReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listOpenFiles(java.util.EnumSet,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalDecrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setInt(java.lang.String,int)
org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getTaskOutput(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.authorize.ImpersonationProvider:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:displayError(java.lang.Exception)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappableBlockLoader:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getFairShare()
org.apache.hadoop.fs.adl.AdlFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:decreaseAllocated(org.apache.hadoop.yarn.api.records.Resource,int)
org.apache.hadoop.io.BloomMapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.tools.mapred.CopyOutputFormat:getCommitDirectory(org.apache.hadoop.mapreduce.Job)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:init(org.apache.hadoop.yarn.ams.ApplicationMasterServiceContext,org.apache.hadoop.yarn.ams.ApplicationMasterServiceProcessor)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getEffectiveCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:getEffectiveMaxCapacity(java.lang.String)
org.apache.hadoop.hdfs.server.sps.ExternalSPSFilePathCollector:<clinit>()
org.apache.hadoop.hdfs.tools.DFSHAAdmin:run(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listOpenFiles(long,java.util.EnumSet,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,boolean)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionReducer:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:serviceStart()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getBlockLocations(java.lang.String,long,long)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver:<init>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceOptionInfo:<init>(org.apache.hadoop.yarn.api.records.ResourceOption)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:<clinit>()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getUnderReplicatedBlocks()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeErasureCodingPolicy(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:reserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getContainer(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.mapred.JobContext:getMaxReduceAttempts()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:<init>(com.google.inject.Injector,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.JobConf:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRunApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.azurebfs.security.AbfsTokenRenewer:<clinit>()
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getDecompressor()
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isCommitJobRepeatable(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.NameNode:<clinit>()
org.apache.hadoop.fs.BatchListingOperations:batchedListStatusIterator(java.util.List)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)
org.apache.hadoop.mapred.nativetask.serde.IKVSerializer:serializeKV(org.apache.hadoop.mapred.nativetask.buffer.DataOutputStream,org.apache.hadoop.mapred.nativetask.util.SizedWritable,org.apache.hadoop.mapred.nativetask.util.SizedWritable)
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:<clinit>()
org.apache.hadoop.security.alias.JavaKeyStoreProvider:stashOriginalFilePermissions()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$DeletionStateIterator:hasNext()
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRename(java.lang.String,java.lang.String,long,boolean)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:ensureAppendEditLogFile()
org.apache.hadoop.io.Stringifier:fromString(java.lang.String)
org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:close()
org.apache.hadoop.util.LightWeightCache:<clinit>()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.examples.WordCount$TokenizerMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.registry.server.dns.RegistryDNSServer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.server.federation.router.RouterPermissionChecker:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.client.cli.LogsCLI:main(java.lang.String[])
org.apache.hadoop.fs.adl.Adl:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.gridmix.JobMonitor$MonitorThread:run()
org.apache.hadoop.mapreduce.v2.hs.HSProxies:<clinit>()
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:hflush()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.tools.DFSHAAdmin:getAllServiceState()
org.apache.hadoop.io.ArrayFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient:<clinit>()
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:setupConfigurableCapacities()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String)
org.apache.hadoop.fs.ftp.FTPFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:start()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.AbstractContainersLauncher:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.mapred.pipes.DownwardProtocol:close()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy:<clinit>()
org.apache.hadoop.fs.local.RawLocalFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceStop()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:disallowSnapshot(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:stop()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage$AllContainersBlock:renderPartial()
org.apache.hadoop.util.AsyncDiskService:shutdown()
org.apache.hadoop.fs.FsShell$Help:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapred.uploader.FrameworkUploader:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$CacheCleanup:run()
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:getReplanner(java.lang.String)
org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat:<clinit>()
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.fs.aliyun.oss.OSS:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:incUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getInputFileForWrite(org.apache.hadoop.mapred.TaskID,long,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:getHomeDirectory()
org.apache.hadoop.mapreduce.checkpoint.CheckpointService:commit(org.apache.hadoop.mapreduce.checkpoint.CheckpointService$CheckpointWriteChannel)
org.apache.hadoop.fs.shell.SetReplication:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:serviceAttemptUpdated(org.apache.hadoop.yarn.service.api.records.Service)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<clinit>()
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:close()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:msync()
org.apache.hadoop.fs.viewfs.NflyFSystem:close()
org.apache.hadoop.fs.Hdfs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.registry.server.dns.RegistryDNS:createPrimaryQuery(org.xbill.DNS.Message)
org.apache.hadoop.fs.azure.Wasbs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:stop()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,org.apache.hadoop.fs.azure.SelfRenewingLease)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceStart()
org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long)
org.apache.hadoop.yarn.sls.SLSRunner$1:createEmbeddedElector()
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:getRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:logDeprecationOnce(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:rename(java.lang.String,java.lang.String)
org.apache.hadoop.fs.FsShell:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.Abfs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:getDomains(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.shell.Head:runAll()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:serviceStop()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getResourceProfile(org.apache.hadoop.yarn.api.protocolrecords.GetResourceProfileRequest)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:read()
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:processDeleteOnExit()
org.apache.hadoop.examples.SecondarySort$Reduce:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumFiles()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.http.HttpsFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:updateContainer(org.apache.hadoop.yarn.api.protocolrecords.ContainerUpdateRequest)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSStripedInputStream:refreshBlockLocations(java.util.Map)
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:render(java.lang.Class)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.NameNode:format(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.GridmixJob$RawBytesOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:getResourceByAttribute(org.apache.hadoop.yarn.api.records.NodeAttribute)
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:addBlockToBeReplicated(org.apache.hadoop.hdfs.protocol.Block,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo[])
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:<clinit>()
org.apache.hadoop.yarn.service.impl.pb.client.ClientAMProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpFileSystem:getTrashRoots(boolean)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPassword(java.lang.String)
org.apache.hadoop.mapred.lib.CombineTextInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<init>(java.lang.String[],long,long)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:incReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.containermanager.volume.csi.ContainerVolumePublisher:<clinit>()
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:flush()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:<init>(org.apache.hadoop.hdfs.client.HdfsDataOutputStream,org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes,java.lang.String,org.apache.hadoop.hdfs.DFSClient,org.apache.hadoop.security.IdMappingServiceProvider)
org.apache.hadoop.fs.ftp.FTPFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,double)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver$NoSplitTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:<clinit>()
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.jobcontrol.JobControl:run()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:set(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Count:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:stop()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:enableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.util.InstrumentedReadLock:unlock()
org.apache.hadoop.fs.shell.Stat:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:<clinit>()
org.apache.hadoop.hdfs.tools.DFSck:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:startContainers(org.apache.hadoop.yarn.api.protocolrecords.StartContainersRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getQueueUserAclInfo(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.hdfs.server.datanode.DataNode:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:recoverResourceRequestsForContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerRequest)
org.apache.hadoop.yarn.client.cli.TopCLI:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:getMultiple(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:putAll(java.util.List,boolean,boolean)
org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower$ReservationAllocationComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:addPersistedPassword(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:handleContainerExitWithFailure(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.fs.Path,java.lang.StringBuilder)
org.apache.hadoop.yarn.client.ServerProxy:createRetriableProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.ipc.YarnRPC,java.net.InetSocketAddress,org.apache.hadoop.io.retry.RetryPolicy)
org.apache.hadoop.mapred.QueueManager:getJobQueueInfoMapping()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.examples.AggregateWordHistogram$AggregateWordHistogramPlugin:configure(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.job.impl.MapTaskImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getMaximumAllocation()
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:start()
org.apache.hadoop.fs.s3a.S3AFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getReservationAgent(java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:setGroup(java.lang.String,int)
org.apache.hadoop.mapred.gridmix.InputStriper:<clinit>()
org.apache.hadoop.fs.shell.Ls$Lsr:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listXAttrs(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[])
org.apache.hadoop.yarn.conf.YarnConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStop()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.examples.terasort.TeraOutputFormat:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:stop()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkMkPath(java.lang.String,org.apache.zookeeper.CreateMode,boolean,java.util.List)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:stop()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:breakHardLinksIfNeeded()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:get()
org.apache.hadoop.hdfs.server.namenode.Checkpointer:run()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:getMultiple(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.hdfs.server.datanode.LocalReplica:<clinit>()
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:stop()
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$YamlOutputter:save(java.io.OutputStream)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueuePreemptionComputePlugin:skipContainerBasedOnIntraQueuePolicy(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.adl.Adl:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.tools.rumen.DeskewedJobTraceReader:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.QueueManager:getJobQueueInfos()
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.azurebfs.oauth2.LocalIdentityTransformer:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:getPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.Abfss:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1:run()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:satisfyStoragePolicy(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:close()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:<clinit>()
org.apache.hadoop.crypto.key.KeyProvider:findProvider(java.util.List,java.lang.String)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getBlockPoolId()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getClusterId()
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:<clinit>()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.nativetask.serde.KVSerializer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:recoverDrainingState()
org.apache.hadoop.fs.aliyun.oss.OSS:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:cleanup(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.conf.YarnConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.TimelineContext:<init>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest)
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileInputStream:read(byte[])
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.shell.Head:run(java.lang.String[])
org.apache.hadoop.ipc.ProcessingDetails:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.OutboundBandwidthResourceHandler:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteObjectReader:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:signalContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerSignalContext)
org.apache.hadoop.mapred.TaskAttemptContext:getTaskCleanupNeeded()
org.apache.hadoop.net.SocketInputStream:read()
org.apache.hadoop.streaming.StreamInputFormat:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.resourceestimator.skylinestore.impl.InMemoryStore:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor:<clinit>()
org.apache.hadoop.hdfs.tools.CryptoAdmin:main(java.lang.String[])
org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getCorruptFilesCount()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getMaximumAllocation()
org.apache.hadoop.security.token.Token:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long)
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:createAndRegisterNewUAM(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String)
org.apache.hadoop.mapreduce.v2.util.LocalResourceBuilder:<clinit>()
org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:hashCode()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SymlinkOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.azure.Wasbs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KMS:handleEncryptedKeyOp(java.lang.String,java.lang.String,java.util.Map)
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:stop()
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:close()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getOutputFile()
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList)
org.apache.hadoop.fs.shell.XAttrCommands:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:metadataExists()
org.apache.hadoop.fs.s3a.S3AFileSystem:shareCredentials(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.placement.FSPlacementRule:<clinit>()
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl:close()
org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setQuota(java.lang.String,long,long,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:isInCurrentState()
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:put(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:stop()
org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getBlockLocations(java.lang.String,long,long)
org.apache.hadoop.yarn.client.RMProxy$1:run()
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineJerseyRetryFilter:handle(com.sun.jersey.api.client.ClientRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.shell.Delete$Rmr:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:serviceStart()
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:setAvailableResourcesToQueue(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:existsInternal(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:incPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:stop()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:<init>()
org.apache.hadoop.yarn.server.router.webapp.RouterWebServiceUtil$1:run()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ContainerCleanedupAfterKillToDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.client.api.TimelineClient:putDomain(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getTotalKillableResource(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodeLabelsPage$NodeLabelsBlock:getCallerUGI()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getCurrentEditLogTxid()
org.apache.hadoop.mapred.JobContext:getReducerClass()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setTimes(java.lang.String,long,long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueuesBlock:render()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:start()
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:loadLastPartialChunkChecksum(java.io.File,java.io.File)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:close()
org.apache.hadoop.io.MD5Hash:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:exists(java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:createSchedulerProxy()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:toString()
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:close()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:disableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.hdfs.server.sps.ExternalSPSBlockMoveTaskHandler:<clinit>()
org.apache.hadoop.fs.s3a.WriteOperations:abortMultipartUpload(com.amazonaws.services.s3.model.MultipartUpload)
org.apache.hadoop.fs.WebHdfs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getAllStoragePolicies()
org.apache.hadoop.examples.pi.DistSum:<clinit>()
org.apache.hadoop.metrics2.sink.StatsDSink:<clinit>()
org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getPhysicalMemorySize()
org.apache.hadoop.mapred.JobConf:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.http.HttpsFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.common.JspHelper:getUGI(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.JobConf:getDouble(java.lang.String,double)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:recoverContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.viewfs.ViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])
org.apache.hadoop.mapreduce.JobResourceUploader:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:close()
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.local.RawLocalFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:pauseContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:readOp()
org.apache.hadoop.fs.shell.CopyCommands$Cp:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:flush()
org.apache.hadoop.fs.Hdfs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:addIfService(java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:fsync(java.lang.String,long,java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.placement.FairQueuePlacementUtils:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl:hashCode()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.util.timeline.TimelineUtils:timelineServiceEnabled(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.SnapshotCommands:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:updateJobTaskAttemptState(org.apache.hadoop.mapreduce.v2.app.webapp.dao.JobTaskAttemptState,javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:must(java.lang.String,double)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:containerFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerFinishData)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getNumberOfMissingBlocks()
org.apache.hadoop.conf.ConfigurationWithLogging:getRaw(java.lang.String)
org.apache.hadoop.mapred.JobContext:getJobSetupCleanupNeeded()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getCapacityTotal()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:moveAllApps(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager:checkAccess(org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,boolean)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.registry.server.dns.RegistryDNS$6:addDSRecord(org.xbill.DNS.Zone,org.xbill.DNS.Name,int,long,java.util.Date,java.util.Date)
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshSuperUserGroupsConfiguration()
org.apache.hadoop.fs.shell.SetReplication:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:stop()
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:serviceStart()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.cosn.CosNFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.Wasb:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.viewfs.NflyFSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:ensureAppendEditLogFile()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getFloat(java.lang.String,float)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:close()
org.apache.hadoop.yarn.server.resourcemanager.placement.SpecifiedPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.fs.BatchListingOperations:batchedListLocatedStatusIterator(java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:incUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:getattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getUnderConstructionFiles(long)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getAvailable()
org.apache.hadoop.fs.azure.Wasb:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:<clinit>()
org.apache.hadoop.hdfs.ClientContext:<clinit>()
org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)
org.apache.hadoop.fs.shell.SnapshotCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.cosn.CosN:getServerDefaults()
org.apache.hadoop.portmap.RpcProgramPortmap:<clinit>()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:close()
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:prefetchOperationStarted()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:render(java.lang.Class)
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.service.LoggingStateChangeListener:<clinit>()
org.apache.hadoop.mapred.join.WrappedRecordReader:<init>(int,org.apache.hadoop.mapred.RecordReader,java.lang.Class)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long)
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesLogger:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock:render(java.lang.Class)
org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1:run()
org.apache.hadoop.hdfs.server.namenode.BackupNode:joinHttpServer()
org.apache.hadoop.registry.conf.RegistryConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:killContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStoreOpDurations:<clinit>()
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:getCallerUGI()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskLog:<clinit>()
org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:readFully(long,byte[])
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:start()
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:stop()
org.apache.hadoop.crypto.random.OsSecureRandom:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.ftp.FTPFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.adl.AdlFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[])
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.registry.server.services.RegistryAdminService:addIfService(java.lang.Object)
org.apache.hadoop.fs.s3a.S3AInstrumentation$InputStreamStatistics:incCounter(java.lang.String)
org.apache.hadoop.mapred.JobClient:getDefaultReduces()
org.apache.hadoop.hdfs.server.datanode.DataNode:instantiateDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:hasCapability(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo:setvCores(int)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:close()
org.apache.hadoop.conf.ConfigurationWithLogging:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setShouldFailAutoQueueCreationWhenGuaranteedCapacityExceeded(java.lang.String,boolean)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:serviceStart()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getCounters(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetCountersRequest)
org.apache.hadoop.yarn.service.component.Component$DecommissionInstanceTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:reserve(org.apache.hadoop.hdfs.ExtendedBlockId,long)
org.apache.hadoop.yarn.service.webapp.ApiServer:createService(javax.servlet.http.HttpServletRequest,org.apache.hadoop.yarn.service.api.records.Service)
org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.LocalFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapred.lib.InputSampler$Sampler:getSample(org.apache.hadoop.mapreduce.InputFormat,org.apache.hadoop.mapreduce.Job)
org.apache.hadoop.hdfs.server.blockmanagement.BlockReconstructionWork:<clinit>()
org.apache.hadoop.mapred.tools.GetGroups:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getResourceLimitForAllUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:handleContainerExitCode(int,org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getReportsForAllDataNodes()
org.apache.hadoop.mapred.ShuffleHandler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.MultipartUploaderBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logCreateSnapshot(java.lang.String,java.lang.String,boolean,long)
org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceStop()
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:hashCode()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl:hashCode()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getTaskAttemptCompletionEvents(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptCompletionEventsRequestProto)
org.apache.hadoop.fs.azurebfs.services.ListingSupport:listStatus(org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:containerIncreasedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.examples.WordCount:main(java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:noteFailure(java.lang.Exception)
org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:removeEntry(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:size()
org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)
org.apache.hadoop.fs.http.HttpFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session)
org.apache.hadoop.yarn.conf.YarnConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:getSplitRatio(int,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode:startMetricsLogger(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy:getAssignmentIterator(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.IteratorSelector)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:pullNewlyPromotedContainers()
org.apache.hadoop.mapred.RunningJob:killTask(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setPermission(java.lang.String,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:middleStep()
org.apache.hadoop.yarn.server.nodemanager.webapp.NavBlock:render()
org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getLocalResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.yarn.server.scheduler.ResourceRequestSet:setNumContainers(int)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:stop()
org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:batchedListStatusIterator(java.util.List)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.api.NMClient:increaseContainerResource(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.mapred.JobClient:getStagingAreaDir()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager:<clinit>()
org.apache.hadoop.io.serializer.SerializationFactory:<clinit>()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:write(int)
org.apache.hadoop.registry.client.binding.RegistryUtils:homePathForCurrentUser()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:loadTaskManifest(org.apache.hadoop.util.JsonSerialization,org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:stopThreads()
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_syncRead_jmhTest:syncRead_SingleShotTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPUOrderingPolicyUnderUtilizedPreemptionDelay(long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.ErrorBlock:renderPartial()
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getContainerReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainerReportRequest)
org.apache.hadoop.mapred.SequenceFileAsTextInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.yarn.api.protocolrecords.ReservationListResponse:newInstance(java.util.List)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor:<clinit>()
org.apache.hadoop.tools.CopyListing:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:close()
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.select.SelectInputStream$1:visit(com.amazonaws.services.s3.model.SelectObjectContentEvent$EndEvent)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$SyncEdit:logEdit()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getSnapshottableDirListing()
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo)
org.apache.hadoop.fs.ftp.FTPFileSystem$1:hasCapability(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeNewApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.sls.SLSRunner:stop()
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$6:getUrl()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:run(java.lang.String[])
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.DFSClient:setKeyProvider(org.apache.hadoop.crypto.key.KeyProvider)
org.apache.hadoop.mapred.LocalJobRunner:setLocalMaxRunningReduces(org.apache.hadoop.mapreduce.JobContext,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:removeQueue(java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getTaskAttemptFilesystem(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSAdmin:getCurrentTrashDir()
org.apache.hadoop.mapreduce.Job:getTaskCompletionEvents(int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:rename(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.sls.SLSRunner$1:addIfService(java.lang.Object)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:decreasePending(org.apache.hadoop.yarn.api.records.Resource,int)
org.apache.hadoop.mapreduce.lib.chain.ChainReducer:addMapper(org.apache.hadoop.mapreduce.Job,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$12:getUrl()
org.apache.hadoop.streaming.PipeCombiner:envline(java.lang.String)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:enterState(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState,org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState)
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:toResponse(java.lang.Throwable)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:loadCache(boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$20:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.fs.azurebfs.Abfss:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:initializeContainer(org.apache.hadoop.yarn.server.api.ContainerInitializationContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeProxyCACert(java.security.cert.X509Certificate,java.security.PrivateKey)
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:canDelete(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:toUrl(org.apache.hadoop.hdfs.web.resources.HttpOpParam$Op,org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.web.resources.Param[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:activateApp(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getEffectiveMaxCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:accept(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$CacheVisitor)
org.apache.hadoop.ipc.WritableRpcEngine$Server:refreshCallQueue(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:rollEdits()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getDestS3AFS()
org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read()
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.yarn.service.provider.ProviderUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:stop()
org.apache.hadoop.fs.FsShellPermissions$Chmod:expandArguments(java.util.LinkedList)
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1:run()
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getCurrentKey(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArguments(java.util.LinkedList)
org.apache.hadoop.registry.conf.RegistryConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueBlock:render()
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:compare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getNodeIds(java.lang.String)
org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.dynamometer.SimulatedDataNodes:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:decReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.service.launcher.LaunchableService:close()
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:start()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getGroupsForUser(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getCurrentEditLogTxid()
org.apache.hadoop.fs.s3a.S3AInputStream:resetConnection()
org.apache.hadoop.fs.azurebfs.Abfs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:allowSnapshot(java.lang.String)
org.apache.hadoop.hdfs.DFSClient:create(java.lang.String,boolean,short,long)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:stop()
org.apache.hadoop.fs.shell.Delete$Expunge:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:getAdmissionPolicy(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getSnapshottableDirListing()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getUsed()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getUsed()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:start()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:setHeadroom(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:lookupFlowContext(org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowRowKey,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getDiagnostics(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetDiagnosticsRequest)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:processDeleteOnExit()
org.apache.hadoop.examples.terasort.TeraValidate$ValidateReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getVolumeInfo()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupImage:rollEditLog(int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:versionRequest()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:pullPreviousAttemptContainers()
org.apache.hadoop.fs.sftp.SFTPFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:close()
org.apache.hadoop.mapred.Counters:findCounter(java.lang.String,int,java.lang.String)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat:createSplits(java.util.Map,java.util.Map,java.util.Map,long,long,long,long,java.util.List)
org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.ReduceTask:updateResourceCounters()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:close()
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:<clinit>()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.sftp.SFTPFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:getReservationAcls(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.aliyun.oss.OSSListResult:logAtDebug(org.slf4j.Logger)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:start()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logCloseFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:isFileClosed(java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:updateSchedulerConfiguration(org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:decPending(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getMaximumApplicationLifetime(java.lang.String)
org.apache.hadoop.registry.client.api.DNSOperations:stop()
org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:removeAclFeature(int)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.ftp.FTPFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.FairCallQueue:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem$1:hsync()
org.apache.hadoop.mapred.JobConf:getInts(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateFencedState()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getTrashRoots(boolean)
org.apache.hadoop.io.DoubleWritable:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.Truncate:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.util.ResourceCalculatorPlugin:<clinit>()
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:setFilesystemProperties(java.util.Hashtable,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.fs.cosn.CosNFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider:toResponse(java.lang.Throwable)
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:<clinit>()
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:serviceStop()
org.apache.hadoop.hdfs.server.namenode.BackupNode:reconfigureSPSModeEvent(java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.FsUsage$Dus:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getAllErasureCodingCodecs()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkGetACLS(java.lang.String)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:blockChecksum(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.protocol.BlockChecksumOptions)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.fs.LocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getCanonicalServiceName()
org.apache.hadoop.security.alias.CredentialShell:run(java.lang.String[])
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:open(java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock:getCallerUGI()
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refreshUserToGroupsMappings()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:start()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:addIfService(java.lang.Object)
org.apache.hadoop.oncrpc.RpcCall:<clinit>()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:<clinit>()
org.apache.hadoop.fs.HarFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.yarn.util.resource.Resources:isInvalidDivisor(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.fs.FsShell$Help:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:setModificationTime(long,int)
org.apache.hadoop.yarn.sls.SLSRunner$1:close()
org.apache.hadoop.mapred.lib.CombineTextInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getTotalBlocks()
org.apache.hadoop.yarn.server.api.impl.pb.client.CollectorNodemanagerProtocolPBClientImpl:close()
org.apache.hadoop.fs.FsShellPermissions$Chmod:run(java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:render(java.lang.Class)
org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.checkpoint.CheckpointService:open(org.apache.hadoop.mapreduce.checkpoint.CheckpointID)
org.apache.hadoop.fs.s3a.S3AFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.net.DFSNetworkTopology:recommissionNode(org.apache.hadoop.net.Node)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:<init>()
org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:startPeriodic()
org.apache.hadoop.hdfs.KeyProviderCache$1:onRemoval(org.apache.hadoop.thirdparty.com.google.common.cache.RemovalNotification)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render(java.lang.Class)
org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMS:getKeysMetadata(java.util.List)
org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getMountPoints(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:deleteTaskWorkingPathQuietly(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.azurebfs.Abfss:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getInts(java.lang.String)
org.apache.hadoop.io.file.tfile.BCFile:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:<clinit>()
org.apache.hadoop.yarn.util.AbstractLivelinessMonitor:<clinit>()
org.apache.hadoop.yarn.server.api.AuxiliaryLocalPathHandler:getLocalPathForWrite(java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:createTupleWritable()
org.apache.hadoop.registry.conf.RegistryConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.fs.local.LocalFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.AlignedPlannerWithGreedy:<clinit>()
org.apache.hadoop.yarn.server.timeline.DomainLogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:rollNewVersion(java.lang.String)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverter:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.azure.Wasbs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapred.lib.CombineTextInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.webapp.view.JQueryUI:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitor:stop()
org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:prefetchOperationCompleted()
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.oncrpc.security.CredentialsSys:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.security.authentication.client.KerberosAuthenticator:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:stop()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage$AllApplicationsBlock:getCallerUGI()
org.apache.hadoop.tools.rumen.ReduceAttempt20LineHistoryEventEmitter:emitterCore(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:<clinit>()
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:nodeHeartbeat(org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest)
org.apache.hadoop.mapred.nativetask.serde.DoubleWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.io.SetFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.fs.azure.Wasb:getDelegationTokens(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:reorderSchedulableEntity(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)
org.apache.hadoop.examples.WordStandardDeviation$WordStandardDeviationReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3:<init>(org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:<clinit>()
org.apache.hadoop.yarn.util.FSDownload$1:load(java.lang.Object)
org.apache.hadoop.mapred.JobContext:getCacheFiles()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:runAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set)
org.apache.hadoop.registry.server.services.MicroZookeeperService:close()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.MoveCommands$Rename:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:<clinit>()
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceHandlerImpl:<clinit>()
org.apache.hadoop.fs.shell.Truncate:displayError(java.lang.Exception)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:nextKeyValue()
org.apache.hadoop.ipc.WritableRpcEngine$Server:call(org.apache.hadoop.io.Writable,long)
org.apache.hadoop.registry.conf.RegistryConfiguration:getInts(java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:serviceStart()
org.apache.hadoop.hdfs.server.namenode.sps.SPSService:start(org.apache.hadoop.hdfs.protocol.HdfsConstants$StoragePolicySatisfierMode)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils:normalizeRequest(org.apache.hadoop.yarn.api.records.ResourceRequest,org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:deleteReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationDeleteRequest)
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.Hdfs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:<clinit>()
org.apache.hadoop.hdfs.shortcircuit.ClientMmap:close()
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getNodeUsage()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.SequenceFileAsTextInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:flush()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:serviceStop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenIdentifier:getTrackingId()
org.apache.hadoop.mapred.SequenceFileInputFilter:setFilterClass(org.apache.hadoop.conf.Configuration,java.lang.Class)
org.apache.hadoop.fs.http.HttpFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobStatus:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:getTrashRoots(boolean)
org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.examples.terasort.TeraScheduler:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.yarn.applications.distributedshell.PlacementSpec:<clinit>()
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder)
org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication()
org.apache.hadoop.fs.Hdfs:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.webapp.view.NavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetReplicationOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:setUser(java.lang.String,int)
org.apache.hadoop.applications.mawo.server.common.AbstractTask:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.conf.ConfigurationWithLogging:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.s3a.DefaultS3ClientFactory:<clinit>()
org.apache.hadoop.fs.s3a.S3A:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePluginManager:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$19:getUrl()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.fs.SWebHdfs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:start()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:close()
org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.client.HdfsUtils:isHealthy(java.net.URI)
org.apache.hadoop.fs.cosn.CosNFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.monitor.probe.PortProbe:<clinit>()
org.apache.hadoop.registry.server.services.AddingCompositeService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.tools.CacheAdmin:main(java.lang.String[])
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.ReduceTaskStatus:setStartTime(long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.Abfss:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.OpensslCipher:<clinit>()
org.apache.hadoop.streaming.PipeMapper:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.AboutBlock:renderPartial()
org.apache.hadoop.mapreduce.lib.input.TextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:close()
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:noteFailure(java.lang.Exception)
org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.security.Groups:<clinit>()
org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter:init(org.apache.commons.daemon.DaemonContext)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:setDropBehind(java.lang.Boolean)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage:render(java.lang.Class)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:serviceStart()
org.apache.hadoop.hdfs.server.federation.router.DFSRouter:<clinit>()
org.apache.hadoop.fs.shell.Display$Text:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setPermission(java.lang.String,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker$2:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1:load(java.lang.Object)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:requestShortCircuitFds(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId,int,boolean)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getPercentUsed()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KMS:reencryptEncryptedKeys(java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetQuotaByStorageType(java.lang.String,long,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.SetReplication:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getKeysMetadata(java.lang.String[])
org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)
org.apache.hadoop.fs.shell.XAttrCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:deleteFilesWithDanglingTempData(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:waitFor(java.util.function.Supplier)
org.apache.hadoop.crypto.key.kms.KMSClientProvider:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:rollEdits()
org.apache.hadoop.mapreduce.security.TokenCache:<clinit>()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:seek(long)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.AbstractContainersLauncher:close()
org.apache.hadoop.fs.WebHdfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:augmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.mapred.ShuffleHandler$Shuffle$2:operationComplete(org.jboss.netty.channel.ChannelFuture)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:updateApplicationPriority(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationPriorityRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizedWhileRunningTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:equals(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:deleteFile(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getProps()
org.apache.hadoop.mapred.RunningJob:getFailureInfo()
org.apache.hadoop.hdfs.server.datanode.DataNode:createSocketAddr(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getTotalKillableResource(java.lang.String)
org.apache.hadoop.hdfs.server.federation.resolver.order.RandomResolver:<clinit>()
org.apache.hadoop.mapred.ReduceTaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob:createValueAggregatorJob(java.lang.String[],java.lang.Class[])
org.apache.hadoop.fs.sftp.SFTPFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.HarFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppAttempts(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.fs.s3a.impl.GetContentSummaryOperation:apply()
org.apache.hadoop.fs.s3a.S3AFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:processDeleteOnExit()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:initMaximumResourceCapability(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutPage:render(java.lang.Class)
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:<clinit>()
org.apache.hadoop.streaming.PipeMapper:addJobConfToEnvironment(org.apache.hadoop.mapred.JobConf,java.util.Properties)
org.apache.hadoop.tools.dynamometer.blockgenerator.XMLParserMapper:<clinit>()
org.apache.hadoop.fs.FSInputStream:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getSnapshotDiffReport(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.aliyun.oss.OSS:getServerDefaults()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:getDirecorySize()
org.apache.hadoop.fs.azurebfs.services.AbfsListStatusRemoteIterator:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDelegationToken(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getCanonicalServiceName()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:getSharedLogCTime()
org.apache.hadoop.fs.azure.Wasb:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:start()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptFailedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:<clinit>()
org.apache.hadoop.fs.Hdfs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.http.HttpsFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.shell.Count:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigArgumentHandler:<clinit>()
org.apache.hadoop.mapred.JobContext:getArchiveClassPaths()
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render()
org.apache.hadoop.fs.viewfs.ViewFs$1$1:apply(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:close()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setMemoryPerNode(java.lang.String,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:updateNMTokens(java.util.Collection)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:serviceStart()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.CustomOutputCommitter:commitJob(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityTableRW:<clinit>()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getDiagnostics(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetDiagnosticsRequestProto)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getInstantaneousMaxCapacity(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:getCallerUGI()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:setVcoreSeconds(long)
org.apache.hadoop.fs.s3a.S3AFileSystem:getHomeDirectory()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:pickupReplicaSet(java.util.Collection,java.util.Collection,java.util.Map)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitor:start()
org.apache.hadoop.hdfs.DFSOutputStream:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeAcl(java.lang.String)
org.apache.hadoop.yarn.sls.SLSRunner$1:createDispatcher()
org.apache.hadoop.hdfs.DFSStripedInputStream:getBlockAt(long)
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:render()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:listManifests()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceStart()
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:requestShortCircuitShm(java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:getBytes()
org.apache.hadoop.fs.azurebfs.Abfs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.streaming.PipeMapRunner:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuNodeResourceUpdateHandler:getAvgNodeGpuUtilization()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.lang.String)
org.apache.hadoop.mapred.join.OverrideRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.SWebHdfs:getHomeDirectory()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManagerService:<clinit>()
org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:removeChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)
org.apache.hadoop.mapred.join.OverrideRecordReader:createKey()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy:isChildPolicyAllowed(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy)
org.apache.hadoop.mapred.gridmix.ReplayJobFactory$ReplayReaderThread:run()
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:serviceStart()
org.apache.hadoop.fs.shell.CopyCommands$Cp:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityTableRW:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:stop()
org.apache.hadoop.fs.viewfs.NflyFSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,java.util.EnumSet,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:unbuffer()
org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)
org.apache.hadoop.fs.viewfs.NflyFSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.fs.WebHdfs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long)
org.apache.hadoop.yarn.sls.synthetic.SynthJob:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:setGroup(java.lang.String,int)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileReaderTask:<clinit>()
org.apache.hadoop.tools.mapred.CopyCommitter:getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapred.YarnOutputFiles:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskAsyncLazyPersistService:<clinit>()
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:getProgress()
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:readFully(long,byte[],int,int)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:verifyFileExists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupImage:finalizeUpgrade(boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.Credentials:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getErasureCodingCodecs()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[])
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:getLocalDestination(java.util.LinkedList)
org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherThread:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getSnapshottableDirListing()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.dao.MapTaskAttemptInfo:<init>(org.apache.hadoop.mapreduce.v2.app.job.TaskAttempt)
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowRunEntityReader:lookupFlowContext(org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowRowKey,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.cosn.CosNFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,float)
org.apache.hadoop.examples.terasort.TeraGen$SortGenMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getReplicatedBlockStats()
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.azure.Wasb:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.mapreduce.util.ProcessTree:destroy(java.lang.String,long,boolean,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:markContainerForPreemption(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:getPreemptionIterator()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppUtil:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:registerApplicationMasterForDistributedScheduling(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getContainers(org.apache.hadoop.yarn.api.protocolrecords.GetContainersRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:completedContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,boolean)
org.apache.hadoop.hdfs.server.balancer.Balancer:main(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getVolumeFailuresTotal()
org.apache.hadoop.fs.FsShell$Help:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigUserLimit(java.lang.String,int)
org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFs:getCanonicalServiceName()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FTPFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MapTask:write(java.io.DataOutput)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:copyBlockdata(java.net.URI)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLSecretManagerRetriableHandlerImpl:<clinit>()
org.apache.hadoop.yarn.server.router.webapp.RouterController:federation()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:serviceStop()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:readRelationship(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:<clinit>()
org.apache.hadoop.tools.util.RetriableCommand:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.nativetask.StatusReportChecker:<init>(org.apache.hadoop.mapred.Task$TaskReporter)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getWorkingDirectory()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setReservationAdmissionPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getEffectiveCapacity(java.lang.String)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.Job:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.CustomOutputCommitter:abortJob(org.apache.hadoop.mapred.JobContext,int)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listEncryptionZones(long)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:getCallerUGI()
org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:<clinit>()
org.apache.hadoop.yarn.service.utils.ServiceUtils:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:provisionEZTrash(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.util.Times:<clinit>()
org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:decrReserveResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.GetConf:main(java.lang.String[])
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$CompressAwarePath:makeQualified(org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetGenstampV1Op:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore:<clinit>()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)
org.apache.hadoop.hdfs.server.namenode.ImageServlet:<clinit>()
org.apache.hadoop.fs.statistics.IOStatisticsContext:snapshot()
org.apache.hadoop.examples.SecondarySort$MapClass:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.applications.mawo.server.common.Task:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:clear()
org.apache.hadoop.streaming.mapreduce.StreamInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.local.LocalFs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor:<clinit>()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:<init>(org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.fs.azurebfs.Abfs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.io.retry.RetryInvocationHandler:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:init(int,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.sls.SLSRunner,long,long,java.lang.String,java.lang.String,boolean,java.lang.String,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map,java.util.Map)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.fs.cosn.CosNFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:<clinit>()
org.apache.hadoop.fs.Hdfs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:start()
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:close()
org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,long)
org.apache.hadoop.fs.shell.AclCommands:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.conf.YarnConfiguration:unset(java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseStorage4Block(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,java.util.List,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:snapshotDiffReportListingRemoteIterator(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.ConfTest:main(java.lang.String[])
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getPartitionQueueMetrics(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.sps.BlockMoveTaskHandler:submitMoveTask(org.apache.hadoop.hdfs.server.protocol.BlockStorageMovementCommand$BlockMovingInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getQueueMaxResource(java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:asyncContainerRelease(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read()
org.apache.hadoop.yarn.client.SCMAdmin:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.Object)
org.apache.hadoop.fs.azure.Wasb:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier:getUser()
org.apache.hadoop.fs.viewfs.NflyFSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.server.common.MetricsLoggerTask:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:generateApplicationTable(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block,org.apache.hadoop.security.UserGroupInformation,java.util.Collection)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.mapred.JobClient:getDefaultMaps()
org.apache.hadoop.fs.FileSystem$Statistics:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:doFinalizeOfSharedLog()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getFileLinkInfo(java.lang.String)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:rollNewVersion(java.lang.String,byte[])
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:render()
org.apache.hadoop.conf.ConfigurationWithLogging:getPassword(java.lang.String)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getAvailableVirtualMemorySize()
org.apache.hadoop.fs.AvroFSInput:seek(long)
org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush()
org.apache.hadoop.yarn.metrics.EventTypeMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:flush()
org.apache.hadoop.fs.HarFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Count:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.DFSStripedOutputStream:write(int)
org.apache.hadoop.mapreduce.lib.output.BindingPathOutputCommitter:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.ftp.FTPInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMUnregisteredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onContainerReInitialize(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:setSignalled(boolean)
org.apache.hadoop.registry.server.services.RegistryAdminService:initUserRegistry(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:getTaskManifestPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:updateModificationTime(long,int)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.conf.RegistryConfiguration:unset(java.lang.String)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator$1:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:getApplicationAttemptsReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptsRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getCpuUsagePercent()
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:close()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.GridmixJob:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerLoadCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:<clinit>()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:hasCapability(java.lang.String)
org.apache.hadoop.io.erasurecode.coder.ErasureCoder:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.server.HttpFSExceptionProvider:log(javax.ws.rs.core.Response$Status,java.lang.Throwable)
org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver:<clinit>()
org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB:close()
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader:close()
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowTableRW:<clinit>()
org.apache.hadoop.hdfs.DFSClient:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.fs.adl.AdlFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.WebHdfs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.fs.http.HttpsFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$JsonOutputter:save(java.io.File)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.server.namenode.ha.ActiveState:setState(org.apache.hadoop.hdfs.server.namenode.ha.HAContext,org.apache.hadoop.hdfs.server.namenode.ha.HAState)
org.apache.hadoop.fs.s3a.WriteOperationHelper:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:getHomeDirectory()
org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:recoverLease(java.lang.String,java.lang.String)
org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close()
org.apache.hadoop.minikdc.MiniKdc:<clinit>()
org.apache.hadoop.fs.local.RawLocalFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.DistTool:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:reInitializeContainer(org.apache.hadoop.yarn.api.protocolrecords.ReInitializeContainerRequest)
org.apache.hadoop.fs.shell.CopyCommands$Merge:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode:isSecurityEnabled()
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,boolean,boolean)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:jobCounters()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:serviceStop()
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream:hflush()
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider:<clinit>()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:failTaskAttempt(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$FailTaskAttemptRequestProto)
org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:renderText(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:getContainers(org.apache.hadoop.yarn.api.protocolrecords.GetContainersRequest)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:about()
org.apache.hadoop.fs.sftp.SFTPFileSystem$2:hsync()
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getDiskBalancerStatus()
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:nextKeyValue()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getCurrentEditLogTxid()
org.apache.hadoop.hdfs.net.DFSNetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.mapreduce.lib.output.TextOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.service.ClientAMService$1:run()
org.apache.hadoop.fs.FsShell$Help:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFs$1:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Expunge:runAll()
org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.Wasb:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:hasAccess(org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.io.compress.SplittableCompressionCodec:getDefaultExtension()
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands:run(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logModifyCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,boolean)
org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.FixedLengthInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:maybeCreateSuccessMarker(org.apache.hadoop.mapreduce.JobContext,java.util.List,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager:determineTimestampsAndCacheVisibilities(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getReservedResources()
org.apache.hadoop.hdfs.StripedDataStreamer:run()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:serviceStart()
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.JobConf:getMapDebugScript()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerStartedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.FileOutputCommitter:commitJob(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.sps.FileCollector:scanAndCollectFiles(long)
org.apache.hadoop.fs.azure.BlockBlobInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:reset()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:modifyAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.namenode.snapshot.DiffListBySkipList:<clinit>()
org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String)
org.apache.hadoop.tools.mapred.CopyOutputFormat:getDefaultWorkFile(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:makeCompositeCrcResult()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewNavBlock:render(java.lang.Class)
org.apache.hadoop.util.JvmPauseMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:initLog()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getListing(java.lang.String,byte[],boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.DFSStripedOutputStream:close()
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:start()
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:close()
org.apache.hadoop.io.SetFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.compress.CodecPool:<clinit>()
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:processDeleteOnExit()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:allowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshServiceAcls(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshServiceAclsRequest)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setGlobalMaximumApplicationsPerQueue(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getNodeIds(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.StoragePolicyAdmin:main(java.lang.String[])
org.apache.hadoop.mapred.nativetask.serde.BoolWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.mapred.FileOutputCommitter:cleanupJob(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaNodeResource:<clinit>()
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo$1:value()
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ActivitiesInfo:<clinit>()
org.apache.hadoop.fs.s3a.ProgressableProgressListener:progressChanged(com.amazonaws.event.ProgressEvent)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.streaming.PipeCombiner:createInputWriter()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.protocol.ReencryptionStatusIterator:hasNext()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:toStringStable()
org.apache.hadoop.hdfs.tools.DFSHAAdmin:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:serviceStart()
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.azurebfs.commit.AzureManifestCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumOfBlocksPendingReplication()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:<clinit>()
org.apache.hadoop.fs.s3a.S3ARetryPolicy:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:readClusterInfo(org.apache.commons.cli.CommandLine)
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.yarn.conf.YarnConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:taskCounters()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator:close()
org.apache.hadoop.yarn.conf.HAUtil:isAutomaticFailoverEnabledAndEmbedded(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:updateHeartbeatState(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)
org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[])
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:opt(java.lang.String,long)
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:init(int,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.sls.SLSRunner,long,long,java.lang.String,java.lang.String,boolean,java.lang.String,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map,java.util.Map)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:render(java.lang.Class)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:close()
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.examples.SecondarySort$IntPair$Comparator:newKey()
org.apache.hadoop.yarn.sls.SLSRunner:<clinit>()
org.apache.hadoop.fs.HarFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:hashCode()
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$16:getUrl()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:serviceStart()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:parseFile(java.lang.String)
org.apache.hadoop.hdfs.client.HdfsDataOutputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.http.HttpServer2:<clinit>()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.sls.SLSRunner$1:doSecureLogin()
org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieve(java.lang.String,long)
org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider:getToken()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.BlockReader:readFully(byte[],int,int)
org.apache.hadoop.fs.shell.CopyCommands$Merge:run(java.lang.String[])
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getAclStatus(java.lang.String)
org.apache.hadoop.mapred.JvmContext:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getCapacity()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:<clinit>()
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.fs.s3a.S3AFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSyncAll()
org.apache.hadoop.examples.MultiFileWordCount$MyInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.local.RawLocalFs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver$NoSplitTextInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapred.JobConf:setMaxTaskFailuresPerTracker(int)
org.apache.hadoop.hdfs.server.federation.resolver.order.HashFirstResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppNewlySavingTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.SchedulerPageUtil$QueueBlockUtil:render()
org.apache.hadoop.yarn.security.DockerCredentialTokenIdentifier:getTrackingId()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:close()
org.apache.hadoop.tools.rumen.Anonymizer:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.Abfs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.impl.pb.RejectedSchedulingRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DeprecatedUTF8:writeString(java.io.DataOutput,java.lang.String)
org.apache.hadoop.fs.shell.Concat:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.fs.cosn.CosN:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:getResourceInformation(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getTrashRoots(boolean)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.ftp.FTPFileSystem:getUsed()
org.apache.hadoop.util.IndexedSorter:sort(org.apache.hadoop.util.IndexedSortable,int,int)
org.apache.hadoop.fs.ftp.FTPFileSystem$1:hflush()
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:getJson()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:serviceStart()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)
org.apache.hadoop.tools.rumen.ZombieJob:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.mapred.gridmix.SubmitterUserResolver:<clinit>()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:serviceStop()
org.apache.hadoop.fs.LocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:isInCurrentState()
org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture:<clinit>()
org.apache.hadoop.mapreduce.OutputCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.StripedDataStreamer:nextBlockOutputStream()
org.apache.hadoop.mapreduce.v2.hs.PartialJob:<clinit>()
org.apache.hadoop.fs.ftp.FtpFs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.shell.Test:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getServerDefaults()
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:setJobOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration,float)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:displayError(java.lang.Exception)
org.apache.hadoop.fs.cosn.CosNFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Tail:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.s3a.impl.DirectoryPolicyImpl:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.filecache.DistributedCache:addArchiveToClassPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:serviceStart()
org.apache.hadoop.fs.http.HttpsFileSystem:getUsed()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:setDouble(java.lang.String,double)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:doEditTransaction(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp)
org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:getProgress()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getErasureCodingPolicy(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increaseAllocated(org.apache.hadoop.yarn.api.records.Resource,int)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listXAttrs(java.lang.String)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$13:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.fs.LocalFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.http.HttpsFileSystem:getServerDefaults()
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:doRecover(org.apache.hadoop.hdfs.server.common.Storage$StorageState)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.shell.Truncate:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.balancer.Balancer:run(java.util.Collection,org.apache.hadoop.hdfs.server.balancer.BalancerParameters,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:onText(org.eclipse.jetty.websocket.api.Session,java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.shell.Concat:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.input.DelegatingMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.viewfs.ChRootedFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationTableRW:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:write(byte[],int,int)
org.apache.hadoop.fs.ftp.FTPFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:<init>(org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.Job:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:start()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:<clinit>()
org.apache.hadoop.hdfs.web.AuthFilter:initializeSecretProvider(javax.servlet.FilterConfig)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:<clinit>()
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:toString()
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:getECTopologyVerifierResult(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.util.Collection)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:deleteAsUser(org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext)
org.apache.hadoop.yarn.service.monitor.probe.HttpProbe:ping(org.apache.hadoop.yarn.service.component.instance.ComponentInstance)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenIdentifier:getUser()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LegacyReader:readOp(boolean)
org.apache.hadoop.mapred.TaskAttemptContext:setStatus(java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:putAll(java.util.List,boolean,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:initializeLeafQueueConfigs(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:<clinit>()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:doUnregistration()
org.apache.hadoop.fs.s3a.S3AFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM:retrievePassword(org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getTaskOutput(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.adl.AdlFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseLocalOrFavoredStorage(org.apache.hadoop.net.Node,boolean,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.fs.ftp.FtpFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.LocalFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:serviceStop()
org.apache.hadoop.mapred.join.OuterJoinRecordReader:add(org.apache.hadoop.mapred.join.ComposableRecordReader)
org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner:setKeyFieldPartitionerOptions(org.apache.hadoop.mapreduce.Job,java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:stop()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getInitialWorkingDirectory()
org.apache.hadoop.fs.shell.XAttrCommands:displayError(java.lang.Exception)
org.apache.hadoop.yarn.service.utils.ConfigHelper:<clinit>()
org.apache.hadoop.yarn.conf.YarnConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getDefaultBlockSize()
org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.PeerCache:close()
org.apache.hadoop.yarn.service.utils.SliderFileSystem:verifyClusterDirectoryNonexistent(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadDriver:main(java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getStoragePolicies()
org.apache.hadoop.yarn.server.router.webapp.NodesBlock:render()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getXAttrs(java.lang.String,java.util.List)
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:close()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getTransactionID()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:stop()
org.apache.hadoop.fs.s3a.S3AInputStream:<clinit>()
org.apache.hadoop.yarn.sls.SLSRunner$1:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:<init>(org.apache.hadoop.yarn.event.Dispatcher,org.apache.hadoop.yarn.server.nodemanager.DeletionService,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService,org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.fs.FileContext,long)
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:createPassword(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:forceKillApplication(org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest)
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:pauseReencryptUpdaterForTesting()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$8:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:<clinit>()
org.apache.hadoop.fs.s3a.S3ADataBlocks$BlockUploadData:close()
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:stop()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:serviceStart()
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm:allocAndRegisterSlot(org.apache.hadoop.hdfs.ExtendedBlockId)
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:recordOutput(org.apache.commons.text.TextStringBuilder,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:close()
org.apache.hadoop.fs.cosn.CosNInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.service.client.ServiceClient:addIfService(java.lang.Object)
org.apache.hadoop.fs.azure.Wasbs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.crypto.key.kms.KMSClientProvider:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:clearPendingContainerCache()
org.apache.hadoop.tools.mapred.RetriableFileCopyCommand:<clinit>()
org.apache.hadoop.examples.terasort.TeraScheduler:main(java.lang.String[])
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:setSequenceFileOutputKeyClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.streaming.mapreduce.StreamInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.registry.client.api.DNSOperations:start()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:getTrackingId()
org.apache.hadoop.mapred.FadvisedChunkedFile:nextChunk()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:finalize()
org.apache.hadoop.fs.local.LocalFs:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:serviceStart()
org.apache.hadoop.mapred.gridmix.SubmitterUserResolver:<init>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:<clinit>()
org.apache.hadoop.mapred.lib.MultipleOutputs:setCountersEnabled(org.apache.hadoop.mapred.JobConf,boolean)
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.DFSClient:open(java.lang.String,int,boolean,org.apache.hadoop.fs.FileSystem$Statistics)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.fs.ftp.FtpFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Rmdir:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:close()
org.apache.hadoop.registry.server.services.AddingCompositeService:serviceStart()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getTokenInfoFromZK(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.TaskAttemptContext:getLocalCacheArchives()
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:apply()
org.apache.hadoop.fs.azure.Wasbs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:initFileSystem(java.net.URI)
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:hsync()
org.apache.hadoop.security.token.Token$PrivateToken:decodeFromUrlString(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.RegistryAdminService:bind(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord,int)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:close()
org.apache.hadoop.mapreduce.v2.app.speculate.LegacyTaskRuntimeEstimator:contextualize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)
org.apache.hadoop.fs.azure.Wasbs:getServerDefaults()
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade$CacheLoaderImpl:load(java.lang.Object)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getSlowDatanodeStats()
org.apache.hadoop.fs.LocalFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:normalizeResourceRequests(java.util.List,java.lang.String)
org.apache.hadoop.fs.LocalFileSystem:getServerDefaults()
org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<clinit>()
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.hdfs.DFSStripedInputStream:read(long,java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatistics:seekForwards(long)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:<init>(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler,org.apache.hadoop.yarn.util.Clock)
org.apache.hadoop.fs.s3a.audit.AuditIntegration:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:getMultiple(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getReplicatedBlockStats()
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:getProxy()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:read()
org.apache.hadoop.hdfs.server.common.JspHelper:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewNavBlock:render()
org.apache.hadoop.io.FastByteComparisons:<clinit>()
org.apache.hadoop.fs.azure.NativeFileSystemStore:updateFolderLastModifiedTime(java.lang.String,java.util.Date,org.apache.hadoop.fs.azure.SelfRenewingLease)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$5:getUrl()
org.apache.hadoop.hdfs.DFSClient:exists(java.lang.String)
org.apache.hadoop.mapreduce.filecache.DistributedCache:getCacheArchives(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:start()
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getCanonicalServiceName()
org.apache.hadoop.oncrpc.SimpleTcpClientHandler:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.SnapshotCommands:processRawArguments(java.util.LinkedList)
org.apache.hadoop.security.token.Token$PrivateToken:decodeIdentifier()
org.apache.hadoop.hdfs.server.federation.store.protocol.RefreshSuperUserGroupsConfigurationResponse:newInstance(boolean)
org.apache.hadoop.hdfs.protocol.CachePoolIterator:hasNext()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<init>(java.lang.String[],float,float,long)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobCounters(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptStartedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.client.impl.DfsClientConf:<clinit>()
org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<clinit>()
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.pipes.Application:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeOrUpdateAMRMTokenSecretManager(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState,boolean)
org.apache.hadoop.mapred.SortedRanges:add(org.apache.hadoop.mapred.SortedRanges$Range)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.registry.conf.RegistryConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:killAllAppsInQueue(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$11:getUrl()
org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialProvider:init()
org.apache.hadoop.fs.http.HttpFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:serviceStart()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getTrashRoots(boolean)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:clearWriteAccessors()
org.apache.hadoop.fs.s3a.S3AInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.Hdfs:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getUserAMResourceLimitPerPartition(java.lang.String,java.lang.String)
org.apache.hadoop.mapred.join.CompositeRecordReader:key(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory:<clinit>()
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:callTryEvict()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:setCountersEnabled(org.apache.hadoop.mapreduce.Job,boolean)
org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.shell.SetReplication:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.v2.hs.CompletedJob:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:decreaseReserved(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render(java.lang.Class)
org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.mapred.FadvisedChunkedFile:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AddApplicationToSchedulerTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:<clinit>()
org.apache.hadoop.yarn.server.router.webapp.NodesBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.net.DFSNetworkTopology:decommissionNode(org.apache.hadoop.net.Node)
org.apache.hadoop.registry.server.dns.PrivilegedRegistryDNSStarter:destroy()
org.apache.hadoop.fs.shell.Delete$Rmr:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:updatePreemptedSecondsForCustomResources(org.apache.hadoop.yarn.api.records.Resource,long)
org.apache.hadoop.fs.shell.Delete$Expunge:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor:getSocketAddress4Transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ResourceUtilizationTracker:addContainerResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)
org.apache.hadoop.fs.shell.find.Find:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.registry.conf.RegistryConfiguration:write(java.io.DataOutput)
org.apache.hadoop.mapred.ShuffleHandler:stop()
org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:stop()
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputBoundingQuery()
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.fs.adl.AdlFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.service.launcher.LaunchableService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$CorruptedDir:getName()
org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress:<clinit>()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getApplicationAttemptReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptReportRequest)
org.apache.hadoop.registry.server.services.RegistryAdminService:buildConnectionString()
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.fs.s3a.S3AFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.JobHistory:close()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject)
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:hashCode()
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:serviceStop()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenIdentifier:getUser()
org.apache.hadoop.mapred.JobConf:addResource(java.lang.String,boolean)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getNodeHttpAddress(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getConnection()
org.apache.hadoop.util.FileBasedIPList:<clinit>()
org.apache.hadoop.yarn.server.router.Router:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.sps.ExternalStoragePolicySatisfier:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.conf.ConfigurationWithLogging:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.util.functional.RemoteIterators:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:getCanonicalServiceName()
org.apache.hadoop.mapreduce.CryptoUtils:wrapIfNecessary(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.azurebfs.Abfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,float)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSStripedOutputStream:hsync(java.util.EnumSet)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.conf.RegistryConfiguration:setInt(java.lang.String,int)
org.apache.hadoop.hdfs.server.federation.router.Router:serviceStop()
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJob(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getNodeContainers(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:<clinit>()
org.apache.hadoop.mapred.JobConf:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:main(java.lang.String[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getUsed()
org.apache.hadoop.registry.conf.RegistryConfiguration:getProps()
org.apache.hadoop.yarn.util.WindowsBasedProcessTree:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:mknode(java.lang.String,boolean)
org.apache.hadoop.fs.viewfs.NflyFSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:get(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:selectInputStreams(java.util.Collection,long,boolean)
org.apache.hadoop.fs.MultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.federation.router.Router)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:commitPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.fs.Hdfs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:renameSnapshot(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:getCallerUGI()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainerLogFile(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean)
org.apache.hadoop.yarn.server.router.webapp.RouterController:renderJSON(java.lang.Object)
org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.MoveCommands$Rename:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.LocalFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSClient:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.conf.YarnConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.registry.server.services.RegistryAdminService:supplyBindingInformation()
org.apache.hadoop.fs.s3a.impl.MkdirOperation:apply()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.oncrpc.RegistrationClient$RegistrationClientHandler:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:removeGlobalConstraint(java.util.Set)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:stop()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:isMovable(java.util.Collection,org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:getGroupsForUser(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:isInCurrentState()
org.apache.hadoop.ipc.WritableRpcEngine$Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier:getTrackingId()
org.apache.hadoop.ipc.metrics.RpcMetrics:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.tools.GetConf$BackupNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceAllocator:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:killTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskAttemptRequest)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.mapred.nativetask.serde.LongWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:getProxy()
org.apache.hadoop.mapred.IndexCache:removeMap(java.lang.String)
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:recordOutput(org.apache.commons.text.TextStringBuilder,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NavBlock:render()
org.apache.hadoop.mapreduce.task.reduce.EventFetcher:<clinit>()
org.apache.hadoop.hdfs.tools.DebugAdmin:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:about()
org.apache.hadoop.examples.terasort.TeraChecksum$ChecksumMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getLocatedFileInfo(java.lang.String,boolean)
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:checkAndGetApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.yarn.applications.distributedshell.Client:sendStopSignal()
org.apache.hadoop.fs.shell.find.Name:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:updateReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumExpiredNamenodes()
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:write(int)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:size()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$DeletionStateIterator:next()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsCountersPage:render(java.lang.Class)
org.apache.hadoop.hdfs.server.diskbalancer.connectors.DBNameNodeConnector:<clinit>()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:main(java.lang.String[])
org.apache.hadoop.fs.LocalFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.appmaster.AMSimulator:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getStoragePolicies()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:setCurrentKeyId(int)
org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:serviceStart()
org.apache.hadoop.yarn.client.AMRMClientUtils:<clinit>()
org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:removeChild(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode,int)
org.apache.hadoop.fs.MultipartUploaderBuilder:opt(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.NameNodeProxiesClient:<clinit>()
org.apache.hadoop.fs.shell.Tail:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.ipc.Server:<clinit>()
org.apache.hadoop.fs.Hdfs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest)
org.apache.hadoop.mapred.JobContext:getUser()
org.apache.hadoop.mapred.JobContext:getMapOutputValueClass()
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:hashCode()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:completedContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:reorderScheduleEntities()
org.apache.hadoop.mapred.jobcontrol.Job:toString()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueBlock:renderPartial()
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:refresh(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.azurebfs.Abfss:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.server.util.timeline.TimelineServerUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:activeQueue()
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider:<clinit>()
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosN:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.SequenceFileInputFilter$RegexFilter:setPattern(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getErasureCodingCodecs()
org.apache.hadoop.yarn.server.resourcemanager.webapp.ErrorBlock:getCallerUGI()
org.apache.hadoop.fs.viewfs.NflyFSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:serviceStop()
org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.audit.AuditManagerS3A:stop()
org.apache.hadoop.mapred.YARNRunner:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobContext:getJar()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaCachingGetSpaceUsed:<init>(org.apache.hadoop.hdfs.server.datanode.FSCachingGetSpaceUsed$Builder)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getBlacklistedNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.fs.HarFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor:getSocketAddress4Transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:getCallerUGI()
org.apache.hadoop.mapred.JobConf:deleteLocalFiles()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateFencedState()
org.apache.hadoop.examples.dancing.Pentomino:main(java.lang.String[])
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getLastPromisedEpoch()
org.apache.hadoop.security.http.RestCsrfPreventionFilter:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.hdfs.server.namenode.INodeFile:removeAclFeature(int)
org.apache.hadoop.hdfs.server.datanode.FinalizedReplica:getDataInputStream(long)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.DtUtilShell$Edit:validate()
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl:<clinit>()
org.apache.hadoop.lib.service.scheduler.SchedulerService:<clinit>()
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.s3a.S3AUtils:ensureOutputParameterInRange(java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:start()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:truncate(java.lang.String,long,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:close()
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set)
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.qjournal.server.JournalNode:main(java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:stop()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsAdmin:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:refreshSuperUserGroupsConfiguration()
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:stopThreads()
org.apache.hadoop.tools.mapred.CopyCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:close()
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteInputStream:<clinit>()
org.apache.hadoop.hdfs.server.common.blockaliasmap.BlockAliasMap$Reader:resolve(org.apache.hadoop.hdfs.protocol.Block)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DiskResourceHandler:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.registry.server.services.RegistryAdminService:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.CentralizedOpportunisticContainerAllocator:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:close()
org.apache.hadoop.fs.http.HttpsFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl:<clinit>()
org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:stopPeriodic()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:nodePartitionUpdated(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:stop()
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:populatePathNames(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)
org.apache.hadoop.portmap.RpcProgramPortmap:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:disableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,int)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:iterator()
org.apache.hadoop.fs.shell.FsUsage$Dus:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.ftp.FtpFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:iterator()
org.apache.hadoop.fs.cosn.CosN:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.Hdfs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)
org.apache.hadoop.mapred.gridmix.SleepJob$SleepMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.HarFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.JobConf:setOutputCommitter(java.lang.Class)
org.apache.hadoop.fs.Hdfs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:prepareForLaunch(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:normalizeResourceRequests(java.util.List)
org.apache.hadoop.fs.aliyun.oss.OSS:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.azure.AzureFileSystemThreadPoolExecutor:<clinit>()
org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:close()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.mapred.JobContext:getCombinerKeyGroupingComparator()
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:firstStep()
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:stop()
org.apache.hadoop.fs.shell.MoveCommands$Rename:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromStream(java.io.InputStream)
org.apache.hadoop.hdfs.tools.GetConf$NameNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:recordModification(int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.oncrpc.security.SecurityHandler:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.tools.MarkerTool:<clinit>()
org.apache.hadoop.security.authentication.client.AuthenticatedURL:<clinit>()
org.apache.hadoop.yarn.server.timeline.TimelineStore:stop()
org.apache.hadoop.mapred.QueueManager:<init>(java.lang.String,boolean)
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.nfs.nfs3.Nfs3Base:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:displayError(java.lang.Exception)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$CancelledAfterReinitTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateFencedState()
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getTrashRoots(boolean)
org.apache.hadoop.mapred.lib.CombineTextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.SequenceFileInputFilter:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTaskAttemptState(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl$ProviderBlockIteratorImpl:close()
org.apache.hadoop.hdfs.server.federation.router.Quota:<clinit>()
org.apache.hadoop.hdfs.server.datanode.checker.AsyncChecker:schedule(org.apache.hadoop.hdfs.server.datanode.checker.Checkable,java.lang.Object)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskAttemptContext:getMapperClass()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.http.HttpFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:initiateJobOperation(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.OptionsParser:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueTemplateMaxCapacity(java.lang.String,java.lang.String,float)
org.apache.hadoop.hdfs.server.sps.ExternalSPSBlockMoveTaskHandler$2:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:displayError(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:<clinit>()
org.apache.hadoop.mapred.JobQueueInfo:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.viewfs.NflyFSystem:getCanonicalServiceName()
org.apache.hadoop.mapred.JobClient:displayTasks(org.apache.hadoop.mapreduce.Job,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.StripedDataStreamer:updatePipeline(long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpFSFileSystem$2:hashCode()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setCapacityByLabel(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.impl.StoreContext:createTempFile(java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:mkOneDir(java.io.File)
org.apache.hadoop.fs.azurebfs.Abfs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$ETagChangeDetectionPolicy:onChangeDetected(java.lang.String,java.lang.String,java.lang.String,long,java.lang.String,long)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getUsed()
org.apache.hadoop.fs.HarFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumInMaintenanceLiveDataNodes()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:doImportCheckpoint(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.mapreduce.v2.app.webapp.JobBlock:render()
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:createValue()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:maybeCreateSuccessMarker(org.apache.hadoop.mapreduce.JobContext,java.util.List,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot)
org.apache.hadoop.hdfs.server.federation.metrics.NamenodeBeanMetrics$1:load(java.lang.Object)
org.apache.hadoop.streaming.PipeCombiner:startOutputThreads(org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshSuperUserGroupsConfiguration(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshSuperUserGroupsConfigurationRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest:getDeltaCapacity()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRemoveCacheDirectiveInfo(java.lang.Long,boolean)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.Hdfs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getServerDefaults()
org.apache.hadoop.fs.SWebHdfs:getCanonicalServiceName()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:runAll()
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:close()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater:close()
org.apache.hadoop.mapred.nativetask.serde.BoolWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:killReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.shell.CopyCommands$Cp:displayError(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor:<clinit>()
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:localize(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId,java.util.Map)
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getAppAttempt(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler$2:close()
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.fs.s3a.S3AFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.services.AbfsLease:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org.apache.hadoop.mapreduce.lib.db.FloatSplitter:<clinit>()
org.apache.hadoop.mapreduce.jobhistory.HistoryViewer:<init>(java.lang.String,org.apache.hadoop.conf.Configuration,boolean)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:allowSnapshot(java.lang.String)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.mapreduce.Job:setMaxReduceAttempts(int)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeNewApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.nodemanager.NodeManagerMXBean:isSecurityEnabled()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:unset(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:incrNodeTypeAggregations(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker:<clinit>()
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage$AllContainersBlock:render()
org.apache.hadoop.fs.http.HttpFileSystem:getUsed()
org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.mapred.JobConf:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setReplication(java.lang.String,short)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:compareTo(java.lang.Object)
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listReencryptionStatus(long)
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<clinit>()
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:must(java.lang.String,long)
org.apache.hadoop.fs.http.HttpsFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:sanitizeEnv(java.util.Map,org.apache.hadoop.fs.Path,java.util.List,java.util.List,java.util.List,java.util.Map,org.apache.hadoop.fs.Path,java.util.Set)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo:<clinit>()
org.apache.hadoop.security.token.DtUtilShell$Renew:validate()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listReencryptionStatus(long)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:<clinit>()
org.apache.hadoop.examples.terasort.TeraGen$SortGenMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ftp.FTPFileSystem$1:close()
org.apache.hadoop.fs.adl.Adl:getCanonicalServiceName()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:processDeleteOnExit()
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readFully(long,byte[],int,int)
org.apache.hadoop.security.alias.JavaKeyStoreProvider:getInputStreamForFile()
org.apache.hadoop.io.compress.DirectDecompressionCodec:createDirectDecompressor()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:close()
org.apache.hadoop.fs.shell.Tail:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:serviceStop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.io.SecureIOUtils:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RollbackContainerTransition:transition(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl,org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent)
org.apache.hadoop.mapred.TaskAttemptContext:getCacheArchives()
org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:<clinit>()
org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.shell.Delete$Expunge:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.adl.AdlFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:decUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:formatNonFileJournals(org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,boolean)
org.apache.hadoop.fs.s3a.S3AFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.s3a.S3AFileSystem:toString()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:submitApp(java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsAdmin:getErasureCodingPolicies()
org.apache.hadoop.yarn.service.ServiceMaster:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:serviceStart()
org.apache.hadoop.fs.azurebfs.Abfss:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumLiveDataNodes()
org.apache.hadoop.mapred.Counters:addGroup(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setCachedPending(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:selectInputStreams(java.util.Collection,long,boolean)
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingAnalyzer:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:serviceStop()
org.apache.hadoop.fs.s3a.S3AFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputClass()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:stopCacheUpdateService()
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:cachePut(int,java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewBlock:render()
org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)
org.apache.hadoop.hdfs.server.federation.store.RecordStore:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.mapred.SequenceFileInputFilter$PercentFilter:setFrequency(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:setRecordLength(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodesInPath:<clinit>()
org.apache.hadoop.fs.store.LogExactlyOnce:error(java.lang.String,java.lang.Object[])
org.apache.hadoop.streaming.PipeCombiner:maybeLogRecord()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.ha.ZKFailoverController:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:createContainerLocalizer(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.List,org.apache.hadoop.fs.FileContext)
org.apache.hadoop.util.SignalLogger:register(org.apache.commons.logging.Log)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.azurebfs.Abfss:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.cosn.CosNFileSystem:mkdir(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:activeQueue()
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:cancelPrefetches()
org.apache.hadoop.security.token.Token$PrivateToken:privateClone(org.apache.hadoop.io.Text)
org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.s3a.S3AFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:echo()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage$AllApplicationsBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeNewReservation(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:fsyncDirectory()
org.apache.hadoop.yarn.service.webapp.ApiServer:getService(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.ContainerPage:render(java.lang.Class)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getServerDefaults()
org.apache.hadoop.yarn.service.utils.HttpUtil:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:serviceStart()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.aliyun.oss.OSS:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azure.BlockBlobAppendStream:<clinit>()
org.apache.hadoop.mapreduce.filecache.DistributedCache:addArchiveToClassPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppAttemptPage:render(java.lang.Class)
org.apache.hadoop.mapred.join.ResetableIterator:add(org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.csi.adaptor.DefaultCsiAdaptorImpl:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getEditsFromTxid(long)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$6:run()
org.apache.hadoop.yarn.client.cli.SchedConfCLI:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:renameMeta(java.net.URI)
org.apache.hadoop.mapreduce.SharedCacheConfig:<clinit>()
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterPermissionChecker:checkPermissionWithContext(org.apache.hadoop.hdfs.server.namenode.INodeAttributeProvider$AuthorizationContext)
org.apache.hadoop.tools.rumen.TraceBuilder:<clinit>()
org.apache.hadoop.fs.SWebHdfs:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.protocol.CacheDirectiveIterator:next()
org.apache.hadoop.yarn.server.resourcemanager.ClusterMonitor:removeNode(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:<init>(org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.YarnChild$3:run()
org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,int,java.lang.String)
org.apache.hadoop.fs.s3a.S3A:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.Wasbs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl:<init>(org.apache.hadoop.yarn.event.Dispatcher,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor,org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor,org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer,org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager,org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager,org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM,org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM)
org.apache.hadoop.mapred.MapTaskAttemptImpl:resolveHosts(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsNavBlock:renderPartial()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics:<clinit>()
org.apache.hadoop.ha.SshFenceByTcpPort:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render()
org.apache.hadoop.fs.sftp.SFTPFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:getReservedResources(long,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.TaskAttemptContext:getSortComparator()
org.apache.hadoop.fs.http.server.HttpFSServer:postRoot(java.io.InputStream,javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowRunEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<clinit>()
org.apache.hadoop.security.IdMappingServiceProvider:getGroupName(int,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAddBlock(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:constructFilterListBasedOnFields(java.util.Set)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.event.AsyncDispatcher:close()
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listCacheDirectives(long,org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)
org.apache.hadoop.mapreduce.checkpoint.CheckpointID:write(java.io.DataOutput)
org.apache.hadoop.nfs.nfs3.FileHandle:<init>(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.tools.rumen.Job20LineHistoryEventEmitter:emitterCore(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String)
org.apache.hadoop.examples.DBCountPageView:<clinit>()
org.apache.hadoop.fs.SWebHdfs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)
org.apache.hadoop.mapred.ShuffleHandler:close()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:incReserved(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.webapp.example.MyApp$MyView:render(java.lang.Class)
org.apache.hadoop.mapred.Merger:merge(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,java.util.List,int,int,org.apache.hadoop.fs.Path,org.apache.hadoop.io.RawComparator,org.apache.hadoop.util.Progressable,boolean,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logModifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:symLink(java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:stop()
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.crypto.key.UserProvider:getKeysMetadata(java.lang.String[])
org.apache.hadoop.mapred.lib.db.DBInputFormat$DBRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.mapreduce.lib.db.DBWritable)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:replaceExpiredDelegationToken()
org.apache.hadoop.mapred.ShuffleHandler:start()
org.apache.hadoop.conf.ConfigurationWithLogging:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:serviceStop()
org.apache.hadoop.service.LoggingStateChangeListener:<init>()
org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.server.services.AddingCompositeService:noteFailure(java.lang.Exception)
org.apache.hadoop.io.compress.SplittableCompressionCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:<clinit>()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getStorageBytesRead()
org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getAclStatus(java.lang.String)
org.apache.hadoop.mapred.lib.MultipleOutputs:<clinit>()
org.apache.hadoop.mapred.lib.db.DBOutputFormat:getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:selectInputStreams(java.util.Collection,long,boolean,boolean)
org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:flush()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getAdditionalTokenIssuers()
org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:initFileSystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.Hdfs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.SysInfoLinux:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:maybeCreate(java.lang.String,org.apache.zookeeper.CreateMode,java.util.List,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.StripedDataStreamer:queuePacket(org.apache.hadoop.hdfs.DFSPacket)
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:overrideExpiredRecord(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getCanonicalServiceName()
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData)
org.apache.hadoop.conf.ReconfigurationServlet:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:removeXAttrFeature(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:registerCustomResources()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SingleConstraintAppPlacementAllocator:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$Rename:runAll()
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.WebHdfs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ServiceScheduler:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:toString()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getStoragePolicy(java.lang.String)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:beforeMarshalling(com.amazonaws.AmazonWebServiceRequest)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:createStreams(boolean,org.apache.hadoop.util.DataChecksum)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getInputFile(int)
org.apache.hadoop.yarn.util.Records:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.local.LocalFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:setAccessTime(long,int,boolean)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:<clinit>()
org.apache.hadoop.mapred.ResourceMgrDelegate:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.yarn.client.RMFailoverProxyProvider:performFailover(java.lang.Object)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:start()
org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput)
org.apache.hadoop.fs.azurebfs.services.ShellDecryptionKeyProvider:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.SequenceFileInputFilter:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapred.nativetask.serde.FloatWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:<init>()
org.apache.hadoop.mapred.lib.ChainReducer:setReducer(org.apache.hadoop.mapred.JobConf,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapred.JobClient:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppAttemptPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setInt(java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:clear()
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.security.client.ClientHSTokenSelector:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:cleanupExpiredQueues(java.lang.String,boolean,java.util.Set,java.lang.String)
org.apache.hadoop.fs.s3a.tools.MarkerTool:run(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:readDirAsUser(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.health.TimedHealthReporterService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.fs.shell.TouchCommands$Touch:runAll()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,boolean,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:disableECPolicy(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:unsetErasureCodingPolicy(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.RamManager:unreserve(int)
org.apache.hadoop.registry.conf.RegistryConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.DominantResourceFairnessPolicy:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapreduce.ReduceContext$ValueIterator:reset()
org.apache.hadoop.yarn.service.client.ApiServiceClient:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.webapp.view.NavBlock:renderPartial()
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.cosmosdb.CosmosDBDocumentStoreReader:close()
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:serviceStart()
org.apache.hadoop.fs.s3a.audit.OperationAuditor:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:addChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.client.LocalizationProtocolPBClientImpl:close()
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:copyBlockdata(java.net.URI)
org.apache.hadoop.io.BloomMapFile$Reader:midKey()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.RegistryAdminService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.TaskAttemptContext:getProfileParams()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:stop()
org.apache.hadoop.fs.Hdfs:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:decrReserveResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:serviceStop()
org.apache.hadoop.yarn.service.client.ServiceClient:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:getContainerPid()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:serviceStop()
org.apache.hadoop.fs.azurebfs.oauth2.IdentityTransformer:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:stop()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:loadBlockIterator(java.lang.String,java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.io.FloatWritable$Comparator:newKey()
org.apache.hadoop.io.SequenceFile:<clinit>()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:stop()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getSnapshotDiffReportListing(java.lang.String,java.lang.String,java.lang.String,byte[],int)
org.apache.hadoop.conf.ConfigurationWithLogging:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:chooseRandom(java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KillWaitTaskCompletedTransition:checkJobAfterTaskCompletion(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.mapred.LocalContainerLauncher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.SetFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.ftp.FTPFileSystem:getDefaultBlockSize()
org.apache.hadoop.service.launcher.ServiceLauncher:warn(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:nextValidOp()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.fs.HarFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.InstrumentedWriteLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)
org.apache.hadoop.mapred.MapTaskAttemptImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.ExitUtil:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getTotalPendingResourcesConsideringUserLimit(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:<clinit>()
org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:<clinit>()
org.apache.hadoop.hdfs.util.StripedBlockUtil:<clinit>()
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onFailure(java.lang.Throwable)
org.apache.hadoop.mapreduce.lib.db.MySQLDataDrivenDBRecordReader:nextKeyValue()
org.apache.hadoop.fs.shell.Display$Text:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.gridmix.LoadJob:toString()
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:<init>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:listPendingUploadsToAbort(org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.fs.ftp.FTPFileSystem$1:abort()
org.apache.hadoop.hdfs.server.datanode.DataNode:reconfigureProperty(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStop()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getAppNameMappingPlacementRule()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkUpdate(java.lang.String,byte[])
org.apache.hadoop.mapred.nativetask.serde.IKVSerializer:deserializeKV(org.apache.hadoop.mapred.nativetask.buffer.DataInputStream,org.apache.hadoop.mapred.nativetask.util.SizedWritable,org.apache.hadoop.mapred.nativetask.util.SizedWritable)
org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setMaximumAMResourcePercentPerPartition(java.lang.String,java.lang.String,float)
org.apache.hadoop.examples.pi.DistSum$ReduceSide$SummingReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.shell.Display$Text:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.qjournal.protocolPB.InterQJournalProtocolTranslatorPB:close()
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.fs.http.HttpsFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.s3a.S3AFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:isInSafeMode()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:needsPassword()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:channelOpen(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ChannelStateEvent)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:start()
org.apache.hadoop.mapred.join.OverrideRecordReader:<init>(int,org.apache.hadoop.mapred.JobConf,int,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:getCallerUGI()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext:startActiveServices()
org.apache.hadoop.mapred.ReduceTaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CancelDelegationTokenOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getOutputFieldCount()
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:setFilterClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:removeAll()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.viewfs.NflyFSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.KerberosUgiAuthenticator$1:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.mapred.lib.LazyOutputFormat$LazyRecordWriter:write(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.HttpsFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.LocalFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.webapp.view.HeaderBlock:render()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:<clinit>()
org.apache.hadoop.registry.server.dns.PrivilegedRegistryDNSStarter:init(org.apache.commons.daemon.DaemonContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.ftp.FTPInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:<init>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:isFileClosed(java.lang.String)
org.apache.hadoop.mapred.YarnChild:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:serviceStop()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.conf.YarnConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.HarFs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:noteFailure(java.lang.Exception)
org.apache.hadoop.conf.ConfigurationWithLogging:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)
org.apache.hadoop.examples.WordCount$IntSumReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.LocalFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.mapreduce.Job$4:run()
org.apache.hadoop.fs.ftp.FTPFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.sps.SPSService:stopGracefully()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:init(org.apache.hadoop.yarn.util.Clock,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,java.util.Collection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getResourceLimitForActiveUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:saveNamespace()
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getMinimumAllocation()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage:render(java.lang.Class)
org.apache.hadoop.yarn.util.ConverterUtils:getYarnUrlFromURI(java.net.URI)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.conf.ConfigurationWithLogging:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.RunningJob:isRetired()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalUnReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:doUpgradeOfSharedLog()
org.apache.hadoop.mapred.JobConf:getNumTasksToExecutePerJvm()
org.apache.hadoop.security.token.DtUtilShell:<clinit>()
org.apache.hadoop.hdfs.client.HdfsAdmin:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[])
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.fs.viewfs.ViewFs:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getNormalizedResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,double)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy:reorderScheduleEntities()
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:close()
org.apache.hadoop.io.ArrayFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.namenode.sps.SPSService:stop(boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$6:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:run(java.lang.String[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.planner.GreedyPlanner:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:allowPrivilegedContainerExecution(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.fs.shell.Delete$Rmdir:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier:getBytes()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.placement.SecondaryGroupExistingPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)
org.apache.hadoop.fs.ftp.FTPFileSystem:close()
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getHomeDirectory()
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:serviceStop()
org.apache.hadoop.fs.azure.Wasbs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:destroy()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:initRecordStorage(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger:<init>()
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:returnCompressor(org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.shell.XAttrCommands:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.shell.Delete$Rmdir:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:serviceStart()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getCanonicalServiceName()
org.apache.hadoop.yarn.webapp.GenericExceptionHandler:<clinit>()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getNumProcessors()
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:hashCode()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getResourceProfiles(org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceProfilesRequest)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:getLocalDestination(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.nativetask.handlers.BufferPusher:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:addGlobalConstraint(java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint,boolean)
org.apache.hadoop.fs.ftp.FTPFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:requestPrefetch(int)
org.apache.hadoop.util.InstrumentedReadLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getUsed()
org.apache.hadoop.yarn.server.timelineservice.documentstore.collection.document.flowrun.FlowRunDocument:<clinit>()
org.apache.hadoop.io.compress.zlib.ZlibCompressor:<clinit>()
org.apache.hadoop.fs.shell.TouchCommands$Touch:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.server.common.sps.BlockStorageMovementTracker:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getMaximumApplicationLifetime(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ContainerTokensStateIterator:next()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listOpenFiles(long,java.util.EnumSet,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.MemoryMappedBlock:close()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:close()
org.apache.hadoop.resourceestimator.translator.impl.NativeSingleLineParser:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:putAuxiliaryServices(javax.servlet.http.HttpServletRequest,org.apache.hadoop.yarn.server.nodemanager.containermanager.records.AuxServiceRecords)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:commitTaskInternal(org.apache.hadoop.mapreduce.TaskAttemptContext,java.util.List,org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy:validate(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:removeAclFeature(int)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:readFully(long,byte[])
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:renderText(java.lang.String)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:read(byte[],int,int)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:allContainers()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getKeyVersion(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock:render(java.lang.Class)
org.apache.hadoop.yarn.security.ContainerTokenSelector:<clinit>()
org.apache.hadoop.fs.Hdfs:getAllStoragePolicies()
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.JobOrTaskStage:apply(java.lang.Object)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:render()
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.Set,long,java.util.List,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:close()
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder)
org.apache.hadoop.fs.aliyun.oss.OSS:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.cosn.CosNFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.common.BaseTableRW:getTableName(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.util.functional.CommonCallableSupplier:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:<clinit>()
org.apache.hadoop.io.compress.DeflateCodec:createInputStream(java.io.InputStream)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listOpenFiles(long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.lib.MethodMetric:<clinit>()
org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier:getBytes()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputTableName()
org.apache.hadoop.mapreduce.lib.partition.InputSampler:<clinit>()
org.apache.hadoop.fs.s3a.S3AInstrumentation$CommitterStatisticsImpl:incCounter(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy$FairShareComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.NavBlock:getCallerUGI()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:addQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:deleteSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:fsinfo(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:start()
org.apache.hadoop.hdfs.server.federation.router.ConnectionManager:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:destroy()
org.apache.hadoop.conf.ConfigurationWithLogging:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.fs.Hdfs:getFsStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:close()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:logExpireTokens(java.util.Collection)
org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser:<clinit>()
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:processDeleteOnExit()
org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.adl.Adl:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:restoreFailedStorage(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateDNBlockInfosReducer:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:markContainerForKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:doPut(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:render()
org.apache.hadoop.hdfs.FileChecksumHelper:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:internalRemoveFromClusterNodeLabels(java.util.Collection)
org.apache.hadoop.hdfs.server.mover.Mover:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:incrReserveResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.shell.Truncate:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.examples.MultiFileWordCount:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.security.token.DtFileOperations:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.adl.AdlFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir:main(java.lang.String[])
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.files.PersistentCommitData:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.AbstractContainersLauncher:start()
org.apache.hadoop.hdfs.web.TokenAspect:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.mapreduce.lib.input.MultipleInputs:addInputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueTemplateCapacityByLabel(java.lang.String,java.lang.String,float)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsNavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementCounter(java.lang.String)
org.apache.hadoop.yarn.server.webproxy.ProxyCA$3:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:render()
org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getOwnerForPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.TrashPolicyDefault:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl:toString()
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:close()
org.apache.hadoop.mapred.ResourceMgrDelegate:serviceStart()
org.apache.hadoop.hdfs.server.federation.router.RouterUserProtocol:<clinit>()
org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsDataOutputStream:abort()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getServiceStatus()
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsConfPage:render(java.lang.Class)
org.apache.hadoop.util.DiskChecker:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.net.URL)
org.apache.hadoop.mapred.join.ComposableInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Head:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsMemoryResourceHandlerImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:recoverContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueUsersInfoBlock:render()
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.shell.TouchCommands$Touch:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:setResourceValue(int,long)
org.apache.hadoop.mapred.TaskAttemptContext:getReducerClass()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:readFully(long,byte[])
org.apache.hadoop.metrics2.impl.MetricsConfig:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:stopPeriodic()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:renewLease(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$UpdateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.shell.Truncate:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAccessibleNodeLabels(java.lang.String,java.util.Set)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getRunCommandForOther(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalIncrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:finishAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,boolean,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$EndLogSegmentOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:stateChanged(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:close()
org.apache.hadoop.mapred.gridmix.FileQueue:read()
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$PropertiesOutputter:save(java.io.File)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:close()
org.apache.hadoop.fs.s3a.S3AFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.AddingCompositeService:stop()
org.apache.hadoop.fs.adl.AdlFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:serviceStart()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)
org.apache.hadoop.streaming.AutoInputFormat:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:runAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setErasureCodingPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:retrivePasswordInternal(org.apache.hadoop.yarn.security.NMTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listOpenFiles(long)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getClusterNodeLabels(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodeLabelsRequest)
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:doLogAggregationOutOfBand()
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl:toString()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceUpdaterImpl:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPassword(java.lang.String)
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockCommand:execute()
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream:flush(boolean)
org.apache.hadoop.mapred.ReduceTask:taskCleanup(org.apache.hadoop.mapred.TaskUmbilicalProtocol)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getOutputStreamForKeystore()
org.apache.hadoop.examples.pi.DistSum$ReduceSide$PartitionMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[])
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:compareTo(org.apache.hadoop.yarn.api.records.ResourceRequest)
org.apache.hadoop.hdfs.web.AuthFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:getContainerReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainerReportRequest)
org.apache.hadoop.fs.s3a.prefetch.S3ARemoteObject:<clinit>()
org.apache.hadoop.hdfs.server.datanode.FSCachingGetSpaceUsed:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getLabelsToNodes(org.apache.hadoop.yarn.api.protocolrecords.GetLabelsToNodesRequest)
org.apache.hadoop.mapreduce.lib.chain.Chain$MapRunner:run()
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:run(java.lang.String[])
org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:close()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$9:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getRemainingCapacityBigInt()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities)
org.apache.hadoop.util.KMSUtil:<clinit>()
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.util.InstrumentedWriteLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:setShouldUnregister(boolean)
org.apache.hadoop.mapred.LocalJobRunner:<clinit>()
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.zk.CuratorService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Mkdir:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:createKey()
org.apache.hadoop.fs.LocalFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[],java.util.EnumSet)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getKeys()
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractReservationSystem:<clinit>()
org.apache.hadoop.fs.azurebfs.Abfss:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getTotalKillableResource(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.JobBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$IntermediateMemoryToMemoryMerger:run()
org.apache.hadoop.fs.cosn.CosNFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskReport:write(java.io.DataOutput)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)
org.apache.hadoop.util.concurrent.AsyncGetFuture:get()
org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)
org.apache.hadoop.io.erasurecode.CodecUtil:<clinit>()
org.apache.hadoop.mapreduce.checkpoint.CheckpointService:create()
org.apache.hadoop.security.SaslInputStream:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean)
org.apache.hadoop.fs.ftp.FTPFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.fpga.FpgaResourceHandlerImpl:<clinit>()
org.apache.hadoop.mapred.gridmix.JobFactory$1:getMapTaskAttemptInfoAdjusted(int,int,int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:metaSave(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:launchContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:rollEdits()
org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl:<init>(org.apache.hadoop.yarn.event.Dispatcher,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.Credentials,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getProps()
org.apache.hadoop.mapred.Counters$GenericGroup:addCounter(org.apache.hadoop.mapreduce.Counter)
org.apache.hadoop.io.nativeio.NativeIO$POSIX:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:getBoundingValsQuery()
org.apache.hadoop.mapred.TaskAttemptContext:getJar()
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:start()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupImage:doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:killReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore$EntityDeletionThread:run()
org.apache.hadoop.mapred.gridmix.JobFactory$1:getNumberReduces()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceStop()
org.apache.hadoop.fs.adl.Adl:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:setResourceValue(java.lang.String,long)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:computeUserLimitAndSetHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.azurebfs.security.AbfssDtFetcher:addDelegationTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setReserved(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppActivitiesInfo:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getMissingReplOneBlocksCount()
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:requestContainerResourceChange(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getLifelineRpcServerAddress(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreCollectionCreator:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:toString()
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager:<clinit>()
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy,long,long)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.http.HttpFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getKeysMetadata(java.lang.String[])
org.apache.hadoop.util.VersionInfo:main(java.lang.String[])
org.apache.hadoop.io.compress.lz4.Lz4Compressor:<clinit>()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.tools.NNHAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.yarn.service.utils.ConfigHelper:loadFromResource(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:noteFailure(java.lang.Exception)
org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setMaximumResourceRequirement(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:setShouldUnregister(boolean)
org.apache.hadoop.fs.adl.Adl:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier:getTrackingId()
org.apache.hadoop.io.IOUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:unregisterAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.oncrpc.RegistrationClient:<clinit>()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:close()
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:<clinit>()
org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.DFSStripedInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:updateTotalResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:getCallerUGI()
org.apache.hadoop.mapred.MapFileOutputFormat:getReaders(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:render(java.lang.Class)
org.apache.hadoop.yarn.conf.YarnConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$Cp:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getDeadNodes()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:stopThreads()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getTrashRoots(boolean)
org.apache.hadoop.hdfs.DeadNodeDetector:run()
org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.DelegationTokenRenewer:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigConverterMain:main(java.lang.String[])
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:serviceStop()
org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addTags(java.util.Properties)
org.apache.hadoop.mapred.JobConf:addResource(java.net.URL)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:must(java.lang.String,double)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:get(java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallback:onSuccess(java.lang.Object)
org.apache.hadoop.fs.Hdfs:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:render(java.lang.Class)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:stop()
org.apache.hadoop.mapred.IFile:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:initDriver()
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.XAttrCommands:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$NoChangeDetection:onChangeDetected(java.lang.String,java.lang.String,java.lang.String,long,java.lang.String,long)
org.apache.hadoop.hdfs.server.federation.router.Router:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:delete(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.DeleteOpParam,org.apache.hadoop.hdfs.web.resources.RecursiveParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:read(long,java.nio.ByteBuffer)
org.apache.hadoop.fs.s3a.tools.MarkerTool:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getKeyVersion(java.lang.String)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close()
org.apache.hadoop.mapreduce.v2.app.webapp.NavBlock:getCallerUGI()
org.apache.hadoop.fs.http.HttpsFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$StoppedAfterUpgradeTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:readClusterInfo(org.apache.commons.cli.CommandLine)
org.apache.hadoop.fs.shell.Delete$Rmdir:displayError(java.lang.Exception)
org.apache.hadoop.mapred.JobClient$NetworkedJob:<init>(org.apache.hadoop.mapred.JobStatus,org.apache.hadoop.mapreduce.Cluster)
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:hsync()
org.apache.hadoop.hdfs.tools.DiskBalancerCLI:<clinit>()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream:write(byte[],int,int)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitterFactory:getDestinationFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.fs.shell.Count:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.webapp.view.LipsumBlock:render()
org.apache.hadoop.mapred.TaskAttemptContext:getUser()
org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory:buildAmazonS3EncryptionClient(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters)
org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(byte[],int,int,byte[],int,int)
org.apache.hadoop.fs.adl.AdlFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.resourcemanager.placement.SpecifiedPlacementRule:<clinit>()
org.apache.hadoop.examples.pi.DistSum$MapSide$SummingMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.io.BooleanWritable$Comparator:newKey()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)
org.apache.hadoop.hdfs.DFSStripedOutputStream:writeChunk(java.nio.ByteBuffer,int,byte[],int,int)
org.apache.hadoop.fs.s3a.tools.MarkerTool:execMarkerTool(org.apache.hadoop.fs.s3a.tools.MarkerTool$ScanArgs)
org.apache.hadoop.fs.cosn.CosNFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.StripedDataStreamer:setupPipelineInternal(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.azurebfs.oauth2.RefreshTokenBasedTokenProvider:getToken()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getBlockPoolId()
org.apache.hadoop.ipc.WritableRpcEngine$Server:addAuxiliaryListener(int)
org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock()
org.apache.hadoop.fs.WebHdfs:getDelegationTokens(java.lang.String)
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getDecompressor()
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.DFSStripedInputStream:unbuffer()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.services.AbfsLease$1:onSuccess(java.lang.Object)
org.apache.hadoop.examples.pi.DistSum$MapSide$SummingMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:poll(long,java.util.concurrent.TimeUnit)
org.apache.hadoop.ipc.CallQueueManager:<clinit>()
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerErrorPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$24:connect(java.net.URL)
org.apache.hadoop.fs.azurebfs.Abfs:finalize()
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:stop()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:useDelHint(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo,java.util.List,java.util.Collection,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.mapred.TaskAttemptContext:getCacheFiles()
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.fs.WebHdfs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob:main(java.lang.String[])
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:notifyAMContainerLaunched(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:get(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ResourceUtilizationTracker:subtractContainerResource(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:opt(java.lang.String,double)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getStorageBytesRead()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.mapreduce.v2.app.webapp.AppController:<clinit>()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getBlocksTotal()
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$9:getUrl()
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:setMapperClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter:buildNamespace(java.io.InputStream,java.util.List)
org.apache.hadoop.net.TableMapping:reloadCachedMappings()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:stop()
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,int)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.resourcemanager.placement.UserGroupMappingPlacementRule:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider:<clinit>()
org.apache.hadoop.hdfs.server.namenode.AuditLogger:initialize(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.NflyFSystem:processDeleteOnExit()
org.apache.hadoop.mapred.Merger$MergeQueue:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.RawComparator,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainerLogs(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,boolean,boolean)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.adl.Adl:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.Ls$Lsr:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.azurebfs.Abfs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:stop()
org.apache.hadoop.fs.FsShell$Usage:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:<clinit>()
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:stop()
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setDeprecatedProperties()
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger:run()
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.HdfsDtFetcher:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.ReservationInvariantsChecker:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.webapp.example.MyApp$MyController:render(java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStop()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:rename(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:setOutputPath(java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:getDefaultBlockSize()
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer$SaslServerCallbackHandler:handle(javax.security.auth.callback.Callback[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.WebHdfs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.TimelineClient:serviceStart()
org.apache.hadoop.mapred.lib.db.DBInputFormat$DBRecordReader:nextKeyValue()
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:syncFs()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.yarn.conf.YarnConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.shell.Test:runAll()
org.apache.hadoop.yarn.webapp.view.JQueryUI:getCallerUGI()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:acquireLease(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:setKeyFieldComparatorOptions(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getTrashRoots(boolean)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:deactivate()
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.JobHistory:getJobFileInfo(org.apache.hadoop.mapreduce.v2.api.records.JobId)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.mapred.JobConf:unset(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$16:connect(java.net.URL)
org.apache.hadoop.mapred.JobConf:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl)
org.apache.hadoop.fs.viewfs.NflyFSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreateChildQueueEnabled(java.lang.String,boolean)
org.apache.hadoop.lib.service.Scheduler:schedule(java.util.concurrent.Callable,long,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapreduce.lib.input.NLineInputFormat:setNumLinesPerSplit(org.apache.hadoop.mapreduce.Job,int)
org.apache.hadoop.conf.ConfigurationWithLogging:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:run(java.lang.String[])
org.apache.hadoop.yarn.sls.SLSRunner$1:isSecurityEnabled()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:unreserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.nativetask.serde.ByteWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetAclOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:start()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:read(byte[],int,int)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:<clinit>()
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:must(java.lang.String,float)
org.apache.hadoop.io.erasurecode.ErasureCodeNative:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean:isSecurityEnabled()
org.apache.hadoop.fs.DU:init()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)
org.apache.hadoop.fs.HarFs:getServerDefaults()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.JobConf:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getQueueUserAclInfo(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.log.LogLevel:main(java.lang.String[])
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobConf(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setDefaultLifetimePerQueue(java.lang.String,long)
org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceStop()
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:<clinit>()
org.apache.hadoop.mapred.ReduceTaskStatus:write(java.io.DataOutput)
org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:<clinit>()
org.apache.hadoop.mapred.pipes.Submitter:jobSubmit(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.adl.AdlFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.cosn.CosNFileReadTask:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getLiveNodes()
org.apache.hadoop.mapred.Counters$FSGroupImpl:findCounter(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:serviceStop()
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.cosn.CosNFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getHistoryResourceSkyline(java.lang.String,java.lang.String)
org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.registry.conf.RegistryConfiguration:getLong(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:serviceStop()
org.apache.hadoop.mapred.lib.MultipleOutputs:addNamedOutput(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:removeRMContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.mapred.jobcontrol.Job:getJobConf()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:renameData(java.net.URI)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:start()
org.apache.hadoop.hdfs.util.LightWeightHashSet:<clinit>()
org.apache.hadoop.mapred.JobConf:getValByRegex(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.hdfs.server.federation.resolver.order.AvailableSpaceResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:close()
org.apache.hadoop.mapred.JobClient:getDelegationToken(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.api.ServerRMProxy:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:readFully(long,byte[])
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:skip(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAllResourceProfilesResponsePBImpl:hashCode()
org.apache.hadoop.hdfs.DFSUtil:getWebAddressesForNameserviceId(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getCurrentKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.http.HttpFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.fs.adl.AdlFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.shell.MoveCommands$Rename:displayError(java.lang.Exception)
org.apache.hadoop.fs.adl.AdlFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setReservationWindow(java.lang.String,long)
org.apache.hadoop.hdfs.server.namenode.BackupImage:recoverTransitionRead(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)
org.apache.hadoop.fs.azure.Wasb:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.mapred.ResourceMgrDelegate:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getCumulativeCpuTime()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logDisallowSnapshot(java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.SWebHdfs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapred.JobConf:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:putDomain(org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:resetBlocks()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getAppAttempts(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMLaunchedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.FsShell$Usage:processArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.mapreduce.lib.output.PartialOutputCommitter:cleanUpPartialOutputForTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.io.compress.DirectDecompressionCodec:createOutputStream(java.io.OutputStream)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPreemptionDisabled(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:computeContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getEditsFromTxid(long)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:addIfService(java.lang.Object)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getMinimumAllocation()
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:optLong(java.lang.String,long)
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:removeQueue(java.lang.String)
org.apache.hadoop.mapred.TaskAttemptContext:getJobName()
org.apache.hadoop.service.CompositeService:<clinit>()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:initFileSystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.fs.azurebfs.utils.IdentityHandler:lookupForLocalUserIdentity(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getReservedResources()
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:getPreemptedVcoreSeconds()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAggressive:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.DF,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.yarn.client.api.impl.RemoteRequestsTable:<clinit>()
org.apache.hadoop.yarn.service.ServiceMaster:<clinit>()
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:getDefaultWorkFile(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:createTmpFile(java.lang.String,org.apache.hadoop.hdfs.protocol.Block)
org.apache.hadoop.examples.MultiFileWordCount$MyInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceStop()
org.apache.hadoop.fs.LocalFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close()
org.apache.hadoop.yarn.conf.YarnConfiguration:main(java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:disallowSnapshot(java.lang.String)
org.apache.hadoop.security.authentication.server.CompositeAuthenticationHandler:destroy()
org.apache.hadoop.hdfs.server.datanode.FSCachingGetSpaceUsed$Builder:getJitter()
org.apache.hadoop.hdfs.DFSUtil:getNameServiceIdFromAddress(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.processor.VolumeAMSProcessor:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:<clinit>()
org.apache.hadoop.mapred.Counters$FSGroupImpl:addCounter(java.lang.String,java.lang.String,long)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.s3a.auth.MarshalledCredentialBinding:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logLegacyGenerationStamp(long)
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptFailedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl,org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:start()
org.apache.hadoop.yarn.conf.YarnConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:close()
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:close()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppRejectedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice:<clinit>()
org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getProps()
org.apache.hadoop.fs.shell.FsUsage$Df:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:finishApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:tryResolveInRegexMountpoint(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.ImageWriter$Options:codec(java.lang.String)
org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String)
org.apache.hadoop.mapreduce.Cluster:getAllJobs()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.security.ApplicationACLsManager:<init>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:put(org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin:main(java.lang.String[])
org.apache.hadoop.tools.HadoopArchives:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:calculateAndGetAMResourceLimitPerPartition(java.lang.String)
org.apache.hadoop.fs.s3a.S3AInstrumentation$OutputStreamStatistics:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:unsetErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.client.HdfsAdmin:setSpaceQuota(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:start()
org.apache.hadoop.yarn.sls.SLSRunner$1:createScheduler()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:decUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$FinalSavingTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:getProgress()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.service.utils.PublishedConfiguration:asConfigurationXML()
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.Counters$Group:getCounter(int,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.placement.AppNameMappingPlacementRule:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.mapreduce.Job:setJobSetupCleanupNeeded(boolean)
org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys$2:removeEldestEntry(java.util.Map$Entry)
org.apache.hadoop.fs.LocalFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobContext:getPartitionerClass()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:shutdown()
org.apache.hadoop.hdfs.server.namenode.sps.BlockMovementListener:notifyMovementTriedBlocks(org.apache.hadoop.hdfs.protocol.Block[])
org.apache.hadoop.fs.HarFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumNamenodes()
org.apache.hadoop.mapred.join.OverrideRecordReader:createInternalValue()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logStartRollingUpgrade(long)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:getBytes()
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:scanNextOp()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.cosn.CosNFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getDelegationToken(java.lang.String)
org.apache.hadoop.yarn.server.router.clientrm.AbstractClientRequestInterceptor:<clinit>()
org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest)
org.apache.hadoop.mapreduce.v2.hs.HistoryContext:getUser()
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:<init>(org.apache.hadoop.fs.s3a.S3AFileSystem)
org.apache.hadoop.fs.s3a.impl.StoreContext:getBucketLocation()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitterFactory:getDestinationFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeProxyCACert(java.security.cert.X509Certificate,java.security.PrivateKey)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:serviceStop()
org.apache.hadoop.yarn.server.federation.policies.router.RejectRouterPolicy:getActiveSubclusters()
org.apache.hadoop.mapred.Merger$MergeQueue:insert(java.lang.Object)
org.apache.hadoop.fs.audit.CommonAuditContext:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerSafeMode:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.mapreduce.lib.reduce.LongSumReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AUtils:validateOutputStreamConfiguration(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CopyCommands$Merge:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:addToCluserNodeLabelsWithDefaultExclusivity(java.util.Set)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAboutPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getStringCollection(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.RunningJob:mapProgress()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:close()
org.apache.hadoop.yarn.server.scheduler.DistributedOpportunisticContainerAllocator:<clinit>()
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:getLength(java.lang.Object)
org.apache.hadoop.fs.shell.find.Find:expandArgument(java.lang.String)
org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString()
org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)
org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.ReduceTaskStatus:setDiagnosticInfo(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:serviceStop()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMController:overview()
org.apache.hadoop.mapred.JobConf:getQueueName()
org.apache.hadoop.registry.server.services.MicroZookeeperService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:stop()
org.apache.hadoop.registry.conf.RegistryConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:initializeLeafQueueConfigs(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetResourceProfileResponsePBImpl:hashCode()
org.apache.hadoop.net.DNSToSwitchMappingWithDependency:reloadCachedMappings(java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:stop()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getUserGroupMappingPlacementRule()
org.apache.hadoop.examples.QuasiMonteCarlo$QmcReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapreduce.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:runAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getSnapshotDiffReportListing(java.lang.String,java.lang.String,java.lang.String,byte[],int)
org.apache.hadoop.mapreduce.JobSubmissionFiles:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:getPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.fs.http.HttpsFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:getDisabledNameservices(org.apache.hadoop.hdfs.server.federation.store.protocol.GetDisabledNameservicesRequest)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:render(java.lang.Class)
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getPhysicalMemorySize()
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor:run()
org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM:createPassword(org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:decReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.federation.store.CachedRecordStore:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceStart()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listCachePools(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.token.Token$PrivateToken:renew(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.Wasb:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.VEDeviceDiscoverer:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:copyMetadata(java.net.URI)
org.apache.hadoop.fs.azure.Wasbs:getHomeDirectory()
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.serde.NullWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:initOutboundBandwidthResourceHandler(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.tools.GetGroups:getUgmProtocol()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:cleanupContainerFiles(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.HarFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.s3a.S3A:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NavBlock:getCallerUGI()
org.apache.hadoop.fs.ftp.FtpFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.LocalFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.security.authentication.util.KerberosName:resetDefaultRealm()
org.apache.hadoop.mapred.lib.db.DBInputFormat:getCountQuery()
org.apache.hadoop.examples.WordStandardDeviation$WordStandardDeviationReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.local.LocalFs:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.mapred.LocalDistributedCacheManager:<clinit>()
org.apache.hadoop.fs.azurebfs.Abfss:getHomeDirectory()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalDecrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:deleteTaskWorkingPathQuietly(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:registerCustomResources()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$6:connect(java.net.URL)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:addEntry(org.apache.hadoop.hdfs.server.federation.store.records.MountTable)
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:start()
org.apache.hadoop.yarn.nodelabels.RMNodeAttribute:addNode(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getNetworkBytesWritten()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime:<clinit>()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:close()
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerShellWebSocket:<clinit>()
org.apache.hadoop.hdfs.server.balancer.KeyManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:addApplicationSync(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.Credentials,boolean,java.lang.String)
org.apache.hadoop.fs.azure.Wasbs:getCanonicalServiceName()
org.apache.hadoop.fs.shell.Delete$Rmdir:runAll()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:setInt(java.lang.String,int)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:isSecurityEnabled()
org.apache.hadoop.yarn.webapp.view.InfoBlock:render()
org.apache.hadoop.fs.shell.Tail:run(java.lang.String[])
org.apache.hadoop.hdfs.server.mover.Mover$Processor:scheduleMoveReplica(org.apache.hadoop.hdfs.server.balancer.Dispatcher$DBlock,org.apache.hadoop.hdfs.server.mover.Mover$MLocation,java.util.List)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getDefaultBlockSize()
org.apache.hadoop.examples.BaileyBorweinPlouffe:<clinit>()
org.apache.hadoop.fs.s3a.S3AInstrumentation$MetricsUpdatingIOStatisticsStore:addSample(java.lang.String,long)
org.apache.hadoop.hdfs.StatefulStripeReader:readChunk(org.apache.hadoop.hdfs.protocol.LocatedBlock,int)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[])
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:unRegisterNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.UnRegisterNodeManagerRequest)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:displayError(java.lang.Exception)
org.apache.hadoop.fs.adl.AdlFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createPermissionStatus(org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setOverrideWithQueueMappings(boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeNewApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.util.AsyncDiskService:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.SWebHdfs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapred.CleanupQueue:<init>()
org.apache.hadoop.fs.local.LocalFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.s3a.commit.MagicCommitIntegration:<clinit>()
org.apache.hadoop.fs.azure.ClientThrottlingIntercept:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptStoredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.WebHdfs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:renderPartial()
org.apache.hadoop.service.launcher.ServiceLauncher:error(java.lang.String,java.lang.Throwable)
org.apache.hadoop.fs.s3a.S3AFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpsFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.fs.http.HttpFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.examples.dancing.DancingLinks:<clinit>()
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeXAttr(java.lang.String,org.apache.hadoop.fs.XAttr)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<init>(java.lang.String[],long)
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkMkParentPath(java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueuesBlock:render(java.lang.Class)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:submitResourceCommitRequest(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment)
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.Head:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.HadoopArchiveLogs:main(java.lang.String[])
org.apache.hadoop.fs.SWebHdfs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getRemoteAppLogDir(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainer(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:listPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.fs.Hdfs:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.mapred.lib.db.DBInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalUnReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:rollEditLog(int)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:hasCapability(java.lang.String)
org.apache.hadoop.yarn.service.provider.docker.DockerProviderService:buildContainerLaunchContext(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:start()
org.apache.hadoop.yarn.server.sharedcachemanager.metrics.CleanerMetrics:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:serviceStop()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:tasks()
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:getDomains(java.lang.String)
org.apache.hadoop.util.InstrumentedReadLock:lock()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.lib.db.BigDecimalSplitter:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:close()
org.apache.hadoop.yarn.api.records.impl.pb.ResourceOptionPBImpl:toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.cosn.CosNOutputStream:close()
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorMapper:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:renameSnapshot(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.mapred.lib.KeyFieldBasedComparator:<init>()
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.JobConfPage:render(java.lang.Class)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processOptions(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:updateModificationTime(long,int)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumOfBlocksUnderReplicated()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$14:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:createResourceCommitRequest(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment)
org.apache.hadoop.mapred.lib.CombineTextInputFormat$TextRecordReaderWrapper:createValue()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.HadoopArchiveLogsRunner:<clinit>()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveDigestAndRenameCheckpointImage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,long,org.apache.hadoop.io.MD5Hash)
org.apache.hadoop.hdfs.DFSUtil:getNNServiceRpcAddresses(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:processDeleteOnExit()
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:stop()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:blockReceivedAndDeleted(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageReceivedDeletedBlocks[])
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.AclCommands:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.fs.ftp.FTPFileSystem:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.BlockingThreadPoolExecutorService:<clinit>()
org.apache.hadoop.fs.HarFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.api.impl.pb.client.SCMUploaderProtocolPBClientImpl:close()
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptSucceededTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KillWaitTaskCompletedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.SnapshotCommands:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.WebHdfs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap$LevelDBStore:close()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getECTopologyResultForPolicies(java.lang.String[])
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.shell.TouchCommands$Touch:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.DeletionService:stop()
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:init(org.apache.hadoop.lib.server.Server)
org.apache.hadoop.mapreduce.v2.util.MRApps:setEnvFromInputString(java.util.Map,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:hasAccess(org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.mapred.JobConf:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.hdfs.DataStreamer$2:onRemoval(org.apache.hadoop.thirdparty.com.google.common.cache.RemovalNotification)
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:makeFinalResult()
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:runAll()
org.apache.hadoop.yarn.nodelabels.store.StoreOp:recover(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.adl.AdlFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:configureJob(org.apache.hadoop.mapreduce.Job)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:open(org.apache.hadoop.fs.PathHandle,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.appcatalog.controller.AppDetailsController:upgradeApp(java.lang.String,org.apache.hadoop.yarn.service.api.records.Service)
org.apache.hadoop.registry.client.types.Endpoint$Marshal:load(java.io.File)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)
org.apache.hadoop.fs.shell.Head:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refresh(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listEncryptionZones(long)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.conf.ConfigurationWithLogging:getProps()
org.apache.hadoop.hdfs.server.namenode.NameNode:main(java.lang.String[])
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerBecomeReadyTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetQuota(java.lang.String,long,long)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)
org.apache.hadoop.crypto.key.kms.server.KMS:createKey(java.util.Map)
org.apache.hadoop.fs.MultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.shortcircuit.ClientMmap:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:<clinit>()
org.apache.hadoop.mapred.JobConf:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.adl.AdlFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,boolean,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getInotifyEventStream(long)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetXAttrOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:getRemoteDestination(java.util.LinkedList)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.QueueConfigurations:getEffectiveMaxCapacity()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.services.AbfsClient:setAcl(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.ipc.WritableRpcEngine$Server:stop()
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:start()
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:stop()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getPendingReplicationBlocks()
org.apache.hadoop.ipc.Client:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$StartedResourcesIterator:hasNext()
org.apache.hadoop.fs.shell.AclCommands:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager:<clinit>()
org.apache.hadoop.yarn.api.records.QueueConfigurations:getConfiguredMaxCapacity()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:removeReservation(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:get(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Head:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:saveNamespace(long,long)
org.apache.hadoop.fs.s3a.auth.STSClientFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)
org.apache.hadoop.examples.terasort.TeraValidate$ValidateReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.FsShell$Help:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.S3A:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.nodelabels.RMNodeLabel:hashCode()
org.apache.hadoop.tools.DistCh:main(java.lang.String[])
org.apache.hadoop.util.CrcComposer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.mapreduce.lib.db.DBOutputFormat:<clinit>()
org.apache.hadoop.fs.ftp.FtpFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations:<clinit>()
org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:reinit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:setUser(java.lang.String,int)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:allocateResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getPathName(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobContext:getLocalCacheArchives()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.LocalSASKeyGeneratorImpl:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.Job:setProfileParams(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:getMetadataInputStream(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:run()
org.apache.hadoop.yarn.client.api.async.NMClientAsync:commitLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolTranslatorPB:close()
org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.chain.ChainReducer:setReducer(org.apache.hadoop.mapreduce.Job,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPassword(java.lang.String)
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:toString()
org.apache.hadoop.hdfs.tools.ECAdmin:main(java.lang.String[])
org.apache.hadoop.fs.shell.SnapshotCommands:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:start()
org.apache.hadoop.fs.shell.AclCommands:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateDNBlockInfosReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.streaming.PipeMapper:addEnvironment(java.util.Properties,java.lang.String)
org.apache.hadoop.tools.mapred.CopyCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:start()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:stop()
org.apache.hadoop.tools.mapred.CopyCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:renderPartial()
org.apache.hadoop.fs.azurebfs.Abfss:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:decrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:updatePipeline(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])
org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:serviceStop()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:seek(long)
org.apache.hadoop.hdfs.server.common.sps.BlockDispatcher:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getQuotaUsage(java.lang.String)
org.apache.hadoop.tools.rumen.Folder:<clinit>()
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:nextValidOp()
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:updateModificationTime(long,int)
org.apache.hadoop.io.SetFile$Reader:midKey()
org.apache.hadoop.hdfs.server.diskbalancer.planner.PlannerFactory:<clinit>()
org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:initializePlan(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getContainer(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.ipc.Client$Connection$1:run()
org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation:<clinit>()
org.apache.hadoop.mapred.JobConf:addResource(java.io.InputStream)
org.apache.hadoop.io.ReadaheadPool:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getNodeHttpAddress(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.minikdc.MiniKdc$1:run()
org.apache.hadoop.yarn.conf.YarnConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.RuncImageTagToManifestPlugin:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:setFloat(java.lang.String,float)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,long)
org.apache.hadoop.yarn.appcatalog.controller.AppListController:delete(java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:failTaskAttempt(org.apache.hadoop.mapreduce.v2.api.protocolrecords.FailTaskAttemptRequest)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:stop()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorConservative:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.DF,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:hashCode()
org.apache.hadoop.fs.shell.Delete$Expunge:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:<clinit>()
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:getCallerUGI()
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:handleInternal(io.netty.channel.ChannelHandlerContext,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:performFailover(java.lang.Object)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:allowHostPidNamespace(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getKeyProvider()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetQuotaOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,float)
org.apache.hadoop.fs.sftp.SFTPFileSystem$2:hasCapability(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:truncate(java.lang.String,long,java.lang.String)
org.apache.hadoop.hdfs.protocol.ReencryptionStatusIterator:next()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getServerDefaults()
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputConditions()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumOfMissingBlocks()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getLinkTarget(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo:populateQueueCapacities(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueCapacities,org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas)
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:close()
org.apache.hadoop.fs.s3a.S3A:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ServiceMaster:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.webapp.AppAttemptBlock:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.sps.ExternalSPSContext:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddCachePoolOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getSnapshotDiffReport(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)
org.apache.hadoop.fs.HarFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.service.client.ApiServiceClient:start()
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalRebootTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)
org.apache.hadoop.metrics2.util.MBeans:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:start()
org.apache.hadoop.fs.sftp.SFTPFileSystem$2:abort()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$7:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:start()
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:checkExists(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapred.MapTaskStatus:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.yarn.server.resourcemanager.placement.DefaultPlacementRule:setConfig(org.w3c.dom.Element)
org.apache.hadoop.hdfs.server.namenode.EditsDoubleBuffer:<clinit>()
org.apache.hadoop.examples.WordMean:main(java.lang.String[])
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:start()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:initStore(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.nodelabels.store.AbstractFSNodeStore$StoreSchema,java.lang.Object)
org.apache.hadoop.util.ShutdownHookManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:initiateJobOperation(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getResourceLimitForActiveUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:getCallerUGI()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:getAgent(java.lang.String)
org.apache.hadoop.mapred.nativetask.HadoopPlatform:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:addBlock(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],long,java.lang.String[],java.util.EnumSet)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.utils.YarnServerSecurityUtils:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DeletionService:close()
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:start()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getEffectiveMaxCapacity(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:relaunchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:sortCSConfigurations()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CopyCommands$Cp:run(java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:fsync(java.lang.String,long,java.lang.String,long)
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:populatePathNames(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)
org.apache.hadoop.lib.servlet.HostnameFilter:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:logOutput(java.lang.String)
org.apache.hadoop.hdfs.DFSClient:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor:<clinit>()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:seekToNewSource(long)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:renderPartial()
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:generateEncryptedKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:removeApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:defaultAugmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.cosn.CosN:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.WebHdfsDtFetcher:isTokenRequired()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:incAMLimit(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:getNodeList(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:setAccessTime(long,int,boolean)
org.apache.hadoop.streaming.AutoInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:getMatchingRequests(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.ExecutionType,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminMonitorBase:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:renameData(java.net.URI)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreDriver:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBRecordReader:getSelectQuery()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$3:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:createPassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)
org.apache.hadoop.hdfs.tools.NNHAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:updateStorageVersion()
org.apache.hadoop.fs.shell.FsUsage$Df:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.s3a.S3AInputStream:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient:<clinit>()
org.apache.hadoop.yarn.client.cli.RMAdminCLI:getAllServiceState()
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorReducer:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:expandArgument(java.lang.String)
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:<init>(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,long,int,long,org.apache.hadoop.yarn.api.records.Priority,long)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getDataNodeStats(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension$1:load(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.SchedulerPageUtil$QueueBlockUtil:render(java.lang.Class)
org.apache.hadoop.filecache.DistributedCache:setLocalArchives(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapred.lib.KeyFieldBasedComparator:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.oauth2.UserPasswordTokenProvider:getToken()
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:run()
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:getZoneStatus(java.lang.String)
org.apache.hadoop.registry.server.dns.ApplicationServiceRecordProcessor:<clinit>()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:write(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaDiscoverer:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.StoreFile)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobClient$NetworkedJob:toString()
org.apache.hadoop.fs.local.LocalFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString()
org.apache.hadoop.mapred.TextInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:incrementDelegationTokenSeqNum()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:noteFailure(java.lang.Exception)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:rollNewVersion(java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:serviceStart()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:allocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.fs.ftp.FTPFileSystem:getServerDefaults()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ClearNSQuotaOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getDefaultBlockSize()
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$MkdirOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.s3a.commit.files.SuccessData:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getProps()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveNamespace(long,long,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.fs.shell.Command:<clinit>()
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.web.HostRestrictingAuthorizationFilterHandler:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setErasureCodingPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:start()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:run(java.lang.String[])
org.apache.hadoop.mapred.nativetask.serde.IntWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:cleanupContainerFiles(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSHAAdmin:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.lib.db.DBInputFormat:createConnection()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ClientAMService:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getCanonicalServiceName()
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:must(java.lang.String,float)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:serviceStop()
org.apache.hadoop.fs.s3a.S3AFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.ReduceTask:write(java.io.DataOutput)
org.apache.hadoop.streaming.PipeMapRed:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:removeQueue(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:start()
org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:<clinit>()
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:writeSpillFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3a.prefetch.S3ABlockManager:requestPrefetch(int)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:start()
org.apache.hadoop.streaming.PipeMapRunner:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler:handle(javax.security.auth.callback.Callback[])
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceStop()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:listPendingUploadsToAbort(org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:rollNewVersion(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSymlink(java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.server.namenode.INodeSymlink,boolean)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logDeleteSnapshot(java.lang.String,java.lang.String,boolean,long)
org.apache.hadoop.mapred.join.OverrideRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:initRecordStorage(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getServiceRpcServerAddress(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMRegisteredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.JobContext:getInputFormatClass()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:modifyAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.yarn.service.client.ApiServiceClient:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:start()
org.apache.hadoop.hdfs.server.namenode.BackupImage:format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,boolean)
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getRemaining()
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer:getLogURL()
org.apache.hadoop.yarn.client.api.impl.TimelineConnector$TimelineURLConnectionFactory:getHttpURLConnection(java.net.URL)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:decPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.api.records.URL:fromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ApplicationStateIterator:hasNext()
org.apache.hadoop.fs.HarFs:getHomeDirectory()
org.apache.hadoop.mapred.SequenceFileInputFilter:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.adl.Adl:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getBatchedListing(java.lang.String[],byte[],boolean)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:renderPartial()
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)
org.apache.hadoop.yarn.server.webproxy.ProxyCA$1:checkServerTrusted(java.security.cert.X509Certificate[],java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listEncryptionZones(long)
org.apache.hadoop.fs.shell.FsUsage$Dus:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:<clinit>()
org.apache.hadoop.crypto.key.CachingKeyProvider:getKeys()
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor:close()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector$TAFairOrderingComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApplicationsReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest)
org.apache.hadoop.fs.aliyun.oss.OSS:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.adl.AdlFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskAttemptContext:getCombinerKeyGroupingComparator()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$5:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getSnapshottableDirectoryList()
org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)
org.apache.hadoop.fs.s3a.S3AUtils:lookupPassword(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.util.InstrumentedReadLock:tryLock(long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.azurebfs.services.AbfsThrottlingInterceptFactory:<clinit>()
org.apache.hadoop.security.alias.JavaKeyStoreProvider:getOutputStreamForKeystore()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:start()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:constructFilterListBasedOnFilters()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:isRollingUpgrade()
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getSpillIndexFileForWrite(int,long)
org.apache.hadoop.fs.s3a.prefetch.S3ABlockManager:get(int)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeReservation(java.lang.String,java.lang.String)
org.apache.hadoop.streaming.StreamUtil:slurpHadoop(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.fs.http.HttpFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.HarFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:serviceStart()
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:stop()
org.apache.hadoop.mapreduce.CounterGroup:readFields(java.io.DataInput)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$LeafQueueBlock:getCallerUGI()
org.apache.hadoop.mapred.gridmix.SerialJobFactory:start()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:stopPeriodic()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:errorsAndWarnings()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerAllocationProposal:toString()
org.apache.hadoop.mapred.TaskAttemptListenerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:start()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:close()
org.apache.hadoop.mapred.Counters$GenericGroup:findCounter(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.Job:setMapSpeculativeExecution(boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:addConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set,org.apache.hadoop.yarn.api.resource.PlacementConstraint,boolean)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:enterClosedState()
org.apache.hadoop.hdfs.server.diskbalancer.connectors.JsonNodeConnector:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:startAllocatorThread()
org.apache.hadoop.fs.LocalFileSystem:getHomeDirectory()
org.apache.hadoop.mapred.MapTask:updateResourceCounters()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.client.api.async.NMClientAsync:increaseContainerResourceAsync(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setTimes(java.lang.String,long,long)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkPathMustExist(java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:markContainerForPreemption(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:submitApplication(org.apache.hadoop.yarn.api.protocolrecords.SubmitApplicationRequest)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream:flush()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:doUnregistration()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:deleteSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.fs.WebHdfs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapred.TaskAttemptListenerImpl:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:serviceStart()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList:addDiff(int,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$ParentQueueBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:render()
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:seek(long)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:abortJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:run(java.lang.String[])
org.apache.hadoop.fs.s3a.S3AFileSystem:getServerDefaults()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.fs.shell.Truncate:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.s3a.impl.V2Migration:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:populatePathNames(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)
org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer:<clinit>()
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:getRemoteAppLogDir(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:stop()
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:run(java.lang.String[])
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:listManifests()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Mkdir:run(java.lang.String[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem:<clinit>()
org.apache.hadoop.mapred.JobConf:setJobEndNotificationCustomNotifierClass(java.lang.String)
org.apache.hadoop.crypto.key.KeyProviderTokenIssuer:getDelegationToken(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:fsyncDirectory()
org.apache.hadoop.crypto.CryptoInputStream:read()
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapred.DeprecatedQueueConfigurationParser:loadResource(java.io.InputStream)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:listPendingUploadsToCommit(org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.fs.adl.AdlFsInputStream:toString()
org.apache.hadoop.fs.FsShell$Help:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:startScheduler(long,long)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.mapred.gridmix.JobMonitor:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.service.ServiceOperations:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeFile:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:get(java.lang.Class)
org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.fs.shell.find.Find:displayError(java.lang.Exception)
org.apache.hadoop.fs.shell.Tail:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.security.token.DistributedSQLCounter:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode:<clinit>()
org.apache.hadoop.tools.mapred.CopyMapper:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:serviceStart()
org.apache.hadoop.mapred.nativetask.handlers.BufferPuller:<clinit>()
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.webapp.WebApp:routeWithoutDefaultView(java.lang.String,java.lang.Class,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:<clinit>()
org.apache.hadoop.mapreduce.Job:addArchiveToSharedCache(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<clinit>()
org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:recoverLease(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.api.impl.pb.client.DistributedSchedulingAMProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:releaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.service.ServiceMaster:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getAppAttempts(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementNeeded:<clinit>()
org.apache.hadoop.util.CombinedIPWhiteList:<clinit>()
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$FilterRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.tools.rumen.datatypes.ClassName:needsAnonymization(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.select.SelectTool:run(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFs$3:hasNext()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$KillAllocatedAMTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueueCandidatesSelector:<clinit>()
org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionStrategy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:serviceStart()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:init(java.util.Properties)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:createQueueMetricsForCustomResources()
org.apache.hadoop.yarn.server.nodemanager.webapp.NavBlock:render(java.lang.Class)
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:skip(long)
org.apache.hadoop.streaming.AutoInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$19:connect(java.net.URL)
org.apache.hadoop.mapred.JobConf:size()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)
org.apache.hadoop.fs.s3a.auth.delegation.DelegationTokenProvider:getFsDelegationToken()
org.apache.hadoop.hdfs.server.common.ECTopologyVerifier:<clinit>()
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:render()
org.apache.hadoop.io.NullWritable:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:<clinit>()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:mkdir(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.streaming.mapreduce.StreamInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getNameServiceId(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.local.LocalFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.fs.azure.Wasbs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.GridmixJob$RawBytesOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationReportRequest)
org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:displayError(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat:setSequenceFileOutputValueClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:incUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.net.URI)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:stopPeriodic()
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:initialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)
org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)
org.apache.hadoop.io.SetFile$Reader:reset()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$12:connect(java.net.URL)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:aboutToComplete(java.lang.String,java.util.List,long,org.apache.hadoop.fs.statistics.IOStatistics)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedAtFinalStateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:logExpireTokens(java.util.Collection)
org.apache.hadoop.security.authentication.examples.RequestLoggerFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache$StreamMonitor:run()
org.apache.hadoop.fs.shell.Concat:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3a.commit.CommitUtils:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:getRecordClass(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.QueuePriorityContainerCandidateSelector:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:<init>()
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:read(byte[],int,int)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getUsed()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:destroy()
org.apache.hadoop.fs.ftp.FTPInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:stopPeriodic()
org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run()
org.apache.hadoop.fs.viewfs.ViewFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithSuccessTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getCanonicalServiceName()
org.apache.hadoop.tools.RegexpInConfigurationFilter:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Test:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[])
org.apache.hadoop.fs.adl.Adl:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.service.ClientAMService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum,org.apache.hadoop.hdfs.server.datanode.CachingStrategy,boolean,boolean,boolean[],java.lang.String,java.lang.String[])
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat:setSequenceFileOutputKeyClass(org.apache.hadoop.mapred.JobConf,java.lang.Class)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.util.CombinedHostsFileReader:<clinit>()
org.apache.hadoop.fs.SafeMode:setSafeMode(org.apache.hadoop.fs.SafeModeAction)
org.apache.hadoop.hdfs.server.namenode.BackupNode:checkHaStateChange(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:msync()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:isAtLeastReservationThreshold(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$TemplateOutputter:save(java.io.File)
org.apache.hadoop.mapred.gridmix.JobSubmitter:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.cosn.CosN:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSStripedInputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:reencryptEncryptedKeys(java.util.List)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.conf.ConfigurationWithLogging:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterDistributedSchedulingAMResponsePBImpl:toString()
org.apache.hadoop.fs.shell.FsUsage$Df:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:serviceStart()
org.apache.hadoop.yarn.service.utils.SliderFileSystem:getBaseApplicationPath()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ModifyCachePoolOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:isInLatestSnapshot(int)
org.apache.hadoop.fs.azure.Wasb:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.util.InstrumentedWriteLock:unlock()
org.apache.hadoop.fs.ftp.FtpFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:checkAndGetApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.fs.s3a.S3AFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:must(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SerialNumberManager:<clinit>()
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.store.MountTableStore:<clinit>()
org.apache.hadoop.mapred.JobClient:getChildQueues(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getQueueUserAclInfo(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeProxyCACert(java.security.cert.X509Certificate,java.security.PrivateKey)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveErasureCodingPolicyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:recoverLease(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close()
org.apache.hadoop.hdfs.qjournal.server.JournalNode:<clinit>()
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:start()
org.apache.hadoop.hdfs.server.namenode.BackupNode:createRpcServer(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.fs.azurebfs.services.AbfsClient:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.security.authentication.server.LdapAuthenticationHandler:<clinit>()
org.apache.hadoop.hdfs.web.AuthFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.fs.shell.Ls$Lsr:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.examples.dancing.DistributedPentomino$PentMap:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:recoverContainersOnNode(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.io.compress.snappy.SnappyCompressor:<clinit>()
org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.Object)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:getUser()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:setWorkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:close()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:<clinit>()
org.apache.hadoop.mapreduce.ReduceContext$ValueIterator:clearMark()
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:finalizeUpgrade(boolean)
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<clinit>()
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.nativetask.serde.IntWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.adl.Adl:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:renderPartial()
org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:noteFailure(java.lang.Exception)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)
org.apache.hadoop.fs.local.LocalFs:getHomeDirectory()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:isRollingUpgrade()
org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.mapred.IndexCache:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:updateReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationUpdateRequest)
org.apache.hadoop.mapred.QueueConfigurationParser:<clinit>()
org.apache.hadoop.mapred.JobClient:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.nativetask.serde.IntWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getECTopologyResultForPolicies(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.FileOutputCommitter:<clinit>()
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:serviceStop()
org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:getUser()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:getChild(byte[],int)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumNameservices()
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobContext:getGroupingComparator()
org.apache.hadoop.mapred.pipes.Submitter:setKeepCommandFile(org.apache.hadoop.mapred.JobConf,boolean)
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:<init>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobTaskAttemptId(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:<init>()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:afterError(com.amazonaws.Request,com.amazonaws.Response,java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:registerNodeManager(org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest)
org.apache.hadoop.fs.adl.AdlFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:serviceStop()
org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.crypto.key.UserProvider:getCurrentKey(java.lang.String)
org.apache.hadoop.typedbytes.TypedBytesWritableInput:readSortedMap()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setReplication(java.lang.String,short)
org.apache.hadoop.fs.viewfs.NflyFSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:deleteMetadata()
org.apache.hadoop.mapred.MapTaskStatus:statusUpdate(org.apache.hadoop.mapred.TaskStatus$State,float,java.lang.String,org.apache.hadoop.mapred.TaskStatus$Phase,long)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:fromResource(java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsNavBlock:getCallerUGI()
org.apache.hadoop.mapred.JobConf:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getSnapshotDiffReport(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:start()
org.apache.hadoop.yarn.service.utils.SliderFileSystem:buildKeytabPath(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.lib.InputSampler:run(java.lang.String[])
org.apache.hadoop.yarn.server.router.webapp.AppsBlock:render(java.lang.Class)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.protocolPB.DatanodeLifelineProtocolClientSideTranslatorPB:close()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:waitFor(java.util.function.Supplier,int)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.sftp.SFTPFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:prepareContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext)
org.apache.hadoop.registry.server.services.RegistryAdminService:registerPathListener(org.apache.hadoop.registry.client.impl.zk.PathListener)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AUtils:setBucketOption(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:listManifests()
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager:httpServerTemplateForRM(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.net.InetSocketAddress,java.lang.String)
org.apache.hadoop.mapreduce.filecache.DistributedCache:getFileTimestamps(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:close()
org.apache.hadoop.mapred.JobConf:addResource(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:cleanupBeforeRelaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.tools.mapred.lib.DynamicInputFormat:<clinit>()
org.apache.hadoop.fs.s3a.audit.OperationAuditor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:get(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:getLocalizationStatuses(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:fsstat(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getStrings(java.lang.String)
org.apache.hadoop.fs.Hdfs:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:getEntity(java.lang.String,java.lang.String,java.util.EnumSet)
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceStop()
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:truncateBlock(long)
org.apache.hadoop.fs.viewfs.NflyFSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:isFileClosed(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Display$Text:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider:close()
org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.conf.YarnConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutputFiles:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.examples.dancing.OneSidedPentomino:getSplits(int)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getRpcServerBindHost(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.Cluster:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.mapred.join.InnerJoinRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceStart()
org.apache.hadoop.hdfs.protocolPB.ReconfigurationProtocolTranslatorPB:close()
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:generateEncryptedKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:close()
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:<clinit>()
org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.mapreduce.Job:setTaskOutputFilter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.Job$TaskStatusFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:decReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:parseEntity(org.apache.hadoop.hbase.client.Result)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(java.lang.Object)
org.apache.hadoop.yarn.sls.SLSRunner$1:createClientRMService()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationStateSynchronously(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture)
org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding:<clinit>()
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:getNewReservationId()
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:serviceStart()
org.apache.hadoop.fs.shell.CopyCommands$Cp:getLocalDestination(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.util.Set,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.util.Set)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getReconfigurationStatus()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getBlockKeys()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumInMaintenanceDeadDataNodes()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage:render(java.lang.Class)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getBytesWithFutureGenerationStamps()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask:deletionTaskFinished()
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:close()
org.apache.hadoop.util.JvmPauseMonitor:close()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setDeprecatedProperties()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:start()
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:close()
org.apache.hadoop.fs.s3a.S3AUtils:clearBucketOption(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.DFSStripedInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:<init>(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$CSQInfo)
org.apache.hadoop.hdfs.server.namenode.BackupNode:queueExternalCall(org.apache.hadoop.ipc.ExternalCall)
org.apache.hadoop.fs.adl.AdlFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsAdmin:getEncryptionZoneForPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigDefaultNodeLabelExpression(java.lang.String,java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:toString()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:removeLabelsFromNode(java.util.Map)
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:incrementDelegationTokenSeqNum()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getThreadSpecificIOStatisticsContext(long)
org.apache.hadoop.mapreduce.CounterGroup:incrAllCounters(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission()
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:<init>(int,org.apache.hadoop.conf.Configuration,int,java.lang.Class)
org.apache.hadoop.yarn.service.utils.PublishedConfiguration:<init>(java.lang.String,java.lang.Iterable,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])
org.apache.hadoop.mapreduce.task.reduce.Fetcher:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.ClientServiceDelegate:<clinit>()
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:accept(org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector,org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.FsShellPermissions$Chmod:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.DFSStripedOutputStream:flushBuffer(boolean,boolean)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$FinalTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getServerDefaults()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:removeSnapshot(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,java.lang.String,long)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:getDestinationForPath(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.fs.azurebfs.Abfss:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.resourceestimator.service.ShutdownHook:run()
org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.fs.cosn.CosN:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getKeyVersions(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:stop()
org.apache.hadoop.crypto.key.KeyProviderTokenIssuer:getKeyProvider()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumDeadNodes()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.TouchCommands$Touch:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getNodeHttpAddress(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:getAdmissionPolicy(java.lang.String)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:serviceStart()
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:<clinit>()
org.apache.hadoop.mapred.ReduceTaskStatus:statusUpdate(org.apache.hadoop.mapred.TaskStatus$State,float,java.lang.String,org.apache.hadoop.mapred.TaskStatus$Phase,long)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:createJournal(java.net.URI)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:createSchedContainerChangeRequests(java.util.List,boolean)
org.apache.hadoop.io.compress.DefaultCodec:<clinit>()
org.apache.hadoop.security.authentication.server.CompositeAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:supplyBindingInformation()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:decrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceStop()
org.apache.hadoop.fs.azure.SyncableDataOutputStream:close()
org.apache.hadoop.fs.LocalFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:stopWriter(long)
org.apache.hadoop.hdfs.server.namenode.ImageWriter:writeSnapshotDiffSection()
org.apache.hadoop.mapred.ShuffleHandler$ReduceMapFileCount:operationComplete(org.jboss.netty.channel.ChannelFuture)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:write(java.io.DataOutput)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:disallowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:getConnection()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:allocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:allowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.webapp.view.FooterBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$KillTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.YarnOutputFiles:<init>()
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:stop()
org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$Native:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueUsersInfoBlock:render(java.lang.Class)
org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:serviceStart()
org.apache.hadoop.fs.viewfs.NflyFSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.Hdfs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.fs.http.HttpsFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$1:load(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getUserIdInfo(java.lang.String)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier:getUser()
org.apache.hadoop.fs.shell.Concat:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.io.retry.AsyncCallHandler:<clinit>()
org.apache.hadoop.hdfs.DFSStripedOutputStream:hflush()
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.mapreduce.CounterGroup:findCounter(java.lang.String,boolean)
org.apache.hadoop.tools.mapred.CopyCommitter:getCommittedTaskPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSMaxRunningAppsEnforcer:<clinit>()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:create(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.SLSRunner$1:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:resumeContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:<init>(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl,long,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.MapOutputFile,int,boolean,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:activeQueue()
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:toString()
org.apache.hadoop.fs.CompositeCrcFileChecksum:hashCode()
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getEffectiveMaxCapacity(java.lang.String)
org.apache.hadoop.mapred.MergeSorter:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapred.SequenceFileAsTextInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)
org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders:<clinit>()
org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.client.HdfsDataOutputStream:hflush()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.mapred.JobConf:getJobLocalDir()
org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseLocalRack(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:serviceStart()
org.apache.hadoop.fs.Hdfs:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MapTaskAttemptImpl:resolveHost(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:relaunchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.mapred.gridmix.SleepJob:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:close()
org.apache.hadoop.fs.shell.Delete$Rmdir:expandArgument(java.lang.String)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,double)
org.apache.hadoop.yarn.conf.YarnConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:listReservations(org.apache.hadoop.yarn.api.protocolrecords.ReservationListRequest)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:stop()
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.HarFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.shell.Display$Checksum:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.mapred.QueueManager:<init>()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:close()
org.apache.hadoop.fs.azurebfs.services.ShellDecryptionKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.hdfs.server.federation.router.RouterClientProtocol:getDelegationTokens(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:stop()
org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])
org.apache.hadoop.yarn.webapp.log.AggregatedLogsNavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:deactivateApp(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getQuotaUsage(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login()
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigCapacity(java.lang.String,float)
org.apache.hadoop.fs.shell.Delete$Rmdir:run(java.lang.String[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setDeprecatedProperties()
org.apache.hadoop.yarn.server.router.webapp.NavBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.AclCommands:expandArgument(java.lang.String)
org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.cosn.CosNInputStream:read()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeXAttr(java.lang.String,org.apache.hadoop.fs.XAttr)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:stopDispatcher()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ConcatDeleteOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString()
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:run()
org.apache.hadoop.fs.impl.prefetch.FilePosition:toString()
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:accept(org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector,org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.registry.client.api.DNSOperations:delete(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord)
org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksBlock:renderPartial()
org.apache.hadoop.fs.shell.Ls$Lsr:expandArgument(java.lang.String)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:writeToLog(org.apache.hadoop.yarn.nodelabels.store.op.FSNodeStoreLogOp)
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl:toString()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:execContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerExecContext)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getTaskReports(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportsRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)
org.apache.hadoop.hdfs.client.impl.BlockReaderRemote:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:checkAddLabelsToNode(java.util.Map)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:size()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getEffectiveCapacity(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$EnableErasureCodingPolicyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.s3a.select.SelectTool:close()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueUsersInfoBlock:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:run()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:prepareForLaunch(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.ipc.ProtobufRpcEngine2:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster:main(java.lang.String[])
org.apache.hadoop.hdfs.util.AtomicFileOutputStream:<clinit>()
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getCurrentLagTxns()
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:setGroup(java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:deactivateApp(java.lang.String)
org.apache.hadoop.fs.http.server.HttpFSExceptionProvider:toResponse(java.lang.Throwable)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:read()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getAvailablePhysicalMemorySize()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getNodeIds(java.lang.String)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getKeyVersion(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:serviceStop()
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:serviceStart()
org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.fs.cosn.auth.COSCredentialsProviderList:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.ha.ShellCommandFencer:<clinit>()
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.Tail:runAll()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:serviceStart()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:getDataInputStream(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:decReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.tools.GetGroups:run(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:close()
org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable)
org.apache.hadoop.fs.HarFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.compress.DirectDecompressionCodec:createInputStream(java.io.InputStream)
org.apache.hadoop.io.file.tfile.TFile$Reader$1:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getSchedulerConfiguration(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:rollEditLog()
org.apache.hadoop.fs.http.HttpFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.BytesWritable$Comparator:newKey()
org.apache.hadoop.yarn.conf.YarnConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:finishApp(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.service.launcher.AbstractLaunchableService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FTPFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.ImageWriter:writeFilesUCSection()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:reserveIncreasedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl:toString()
org.apache.hadoop.hdfs.server.datanode.DiskBalancer:<clinit>()
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:readClusterInfo(org.apache.commons.cli.CommandLine)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$FinalSavingTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.BackupNode:transitionToActive()
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaResourcePlugin:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:activateApp(java.lang.String)
org.apache.hadoop.registry.server.services.RegistryAdminService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl:hashCode()
org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.util.Preconditions:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory:createProxy(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,boolean)
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:retrievePasswordInternal(org.apache.hadoop.yarn.security.ContainerTokenIdentifier,org.apache.hadoop.yarn.server.security.MasterKeyData)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:setUser(java.lang.String,int)
org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.client.api.impl.DirectTimelineWriter:doPostingObject(java.lang.Object,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.yarn.server.timeline.RollingLevelDB:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.NvidiaDockerV1CommandPlugin:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationTableRW:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext:<clinit>()
org.apache.hadoop.mapred.FadvisedFileRegion:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAddErasureCodingPolicy(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy,boolean)
org.apache.hadoop.fs.LocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:submitDirectory(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.Task$CombineValuesIterator:next()
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:read()
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.rumen.ZombieJobProducer:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.rumen.ZombieCluster,org.apache.hadoop.conf.Configuration,long)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:serviceStart()
org.apache.hadoop.mapred.ReduceTaskStatus:setStateString(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refreshNodes()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeCachePool(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.server.services.AddingCompositeService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceStop()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:setPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getNetworkBytesWritten()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org.apache.hadoop.mapreduce.filecache.DistributedCache:addFileToClassPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.fs.local.RawLocalFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetResourceProfileResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getIsNamespaceEnabled(org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getErasureCodingPolicies()
org.apache.hadoop.fs.shell.Display$Checksum:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.shell.Display$Text:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.statistics.S3AMultipartUploaderStatistics:instantiated()
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:destroy()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$ReplicatedBlockChecksumComputer:getBlockInputStream(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.mapred.lib.CombineTextInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:serviceStop()
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionPendingInodeIdCollector:traverseDirInt(long,org.apache.hadoop.hdfs.server.namenode.INode,java.util.List,org.apache.hadoop.hdfs.server.namenode.FSTreeTraverser$TraverseInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:updateSchedulingRequests(java.util.List)
org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$Cp:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getActivedAppDiagnosticMessage(java.lang.StringBuilder)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.fs.HarFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.fs.LocalFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.io.nativeio.NativeIO:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logCancelDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$XmlOutputter:save(java.io.OutputStream)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:finishApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation$AbfsHttpOperationWithFixedResult:processResponse(byte[],int,int)
org.apache.hadoop.mapred.nativetask.serde.LongWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:expandArgument(java.lang.String)
org.apache.hadoop.fs.s3a.S3AUtils:createAwsConf(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.ftp.FTPFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapred.nativetask.handlers.NativeCollectorOnlyHandler:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getDatanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.examples.WordMean$WordMeanReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl:<clinit>()
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.join.InnerJoinRecordReader:createInternalValue()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:removeXAttrFeature(int)
org.apache.hadoop.mapred.JobConf:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputOrderBy()
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage$ApplicationBlock:getCallerUGI()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getCanonicalServiceName()
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:listManifests()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:must(java.lang.String,double)
org.apache.hadoop.fs.shell.FsUsage$Dus:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.S3AFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Mkdir:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:close()
org.apache.hadoop.hdfs.DFSStripedInputStream:read(long,byte[],int,int)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSDirectory:resolvePath(java.lang.String,org.apache.hadoop.hdfs.server.namenode.FSDirectory)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:recoverFromStore()
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:close()
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.azure.Wasb:getCanonicalServiceName()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getMinimumAllocation()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.conf.RegistryConfiguration:getBoolean(java.lang.String,boolean)
org.apache.hadoop.io.BloomMapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:setService(java.lang.Class)
org.apache.hadoop.tools.rumen.ZombieCluster:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.rumen.MachineNode,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:expandArgument(java.lang.String)
org.apache.hadoop.mapred.join.OverrideRecordReader:createValue()
org.apache.hadoop.fs.WebHdfs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.GetSpaceUsed$Builder:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,double)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl$2:postStart()
org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.MapFile:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:pullNewlyDecreasedContainers()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap)
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:getTempTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.ftp.FTPFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat:createSplits(java.util.Map,java.util.Map,java.util.Map,long,long,long,long,java.util.List)
org.apache.hadoop.fs.s3a.audit.impl.NoopSpan:close()
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)
org.apache.hadoop.mapreduce.checkpoint.CheckpointService:abort(org.apache.hadoop.mapreduce.checkpoint.CheckpointService$CheckpointWriteChannel)
org.apache.hadoop.io.FloatWritable:<clinit>()
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)
org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:serviceStop()
org.apache.hadoop.io.BloomMapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRunApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRenameSnapshot(java.lang.String,java.lang.String,java.lang.String,boolean,long)
org.apache.hadoop.fs.shell.find.Find$3:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainTableRW:<clinit>()
org.apache.hadoop.fs.cosn.CosNInputStream:toString()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:optLong(java.lang.String,long)
org.apache.hadoop.fs.azurebfs.Abfss:finalize()
org.apache.hadoop.mapred.RunningJob:getJobState()
org.apache.hadoop.fs.azurebfs.oauth2.RefreshTokenBasedTokenProvider:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.ByteBufferWrapper:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:monitorHealth()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutBlock:renderPartial()
org.apache.hadoop.mapreduce.task.reduce.LocalFetcher:copyFromHost(org.apache.hadoop.mapreduce.task.reduce.MapHost)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$11:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.hdfs.server.namenode.NNUpgradeUtil:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:isInCurrentState()
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:getCallerUGI()
org.apache.hadoop.hdfs.server.datanode.BlockScanner:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:createWrappedCommitter(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.adl.AdlFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFsInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.router.webapp.RESTRequestInterceptor:get()
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getDelegationToken(java.lang.String)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:stopThreads()
org.apache.hadoop.mapred.TaskAttemptContext:getInputFormatClass()
org.apache.hadoop.tools.dynamometer.Client:main(java.lang.String[])
org.apache.hadoop.yarn.api.records.ResourceRequest$ResourceRequestComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.HarFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.mapred.nativetask.serde.ByteWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:serviceStart()
org.apache.hadoop.hdfs.server.aliasmap.InMemoryAliasMap:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition:getUsedDeductAM()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappedBlock:close()
org.apache.hadoop.yarn.service.timelineservice.ServiceMetricsSink:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$7:connect(java.net.URL)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.net.URL)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:removeChild(org.apache.hadoop.hdfs.server.namenode.INode,int)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:stop()
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowActivityTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.fs.local.RawLocalFs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$1:run()
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:<clinit>()
org.apache.hadoop.mapred.TaskAttemptContext:getMapOutputValueClass()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AppendOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner()
org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobTaskAttempts(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.ServiceMaster:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.mapred.lib.MultipleOutputs:<init>(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.blacklist.SimpleBlacklistManager:<clinit>()
org.apache.hadoop.fs.shell.Display$Checksum:expandArgument(java.lang.String)
org.apache.hadoop.fs.LocalFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.util.CombinedIPList:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:completedContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,boolean)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:undoRename4ScrParent(org.apache.hadoop.hdfs.server.namenode.INodeReference,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.fs.local.LocalFs:getServerDefaults()
org.apache.hadoop.fs.FileContext:<clinit>()
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:init(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.fs.sftp.SFTPFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.Invoker:quietly(java.lang.String,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getWorkingDirectory()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerStartCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:renameMeta(java.net.URI)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDefaultReplication()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.net.TableMapping:isSingleSwitchByScriptPolicy()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.hdfs.KeyProviderCache:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$17:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.fs.viewfs.NflyFSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:size()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:pullNewlyIncreasedContainers()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:setAvailableResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:appattempt()
org.apache.hadoop.mapred.join.InnerJoinRecordReader:<init>(int,org.apache.hadoop.mapred.JobConf,int,java.lang.Class)
org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.apache.commons.logging.Log)
org.apache.hadoop.mapred.RunningJob:getHistoryUrl()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long)
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:getProxy()
org.apache.hadoop.filecache.DistributedCache:getTimestamp(org.apache.hadoop.conf.Configuration,java.net.URI)
org.apache.hadoop.yarn.appcatalog.controller.AppDetailsController:restartApp(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$CompletedResourcesIterator:next()
org.apache.hadoop.streaming.PipeMapper:envPut(java.util.Properties,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.DFSStripedInputStream:actualGetFromOneDataNode(org.apache.hadoop.hdfs.DFSInputStream$DNAddrPair,long,long,java.nio.ByteBuffer,org.apache.hadoop.hdfs.DFSUtilClient$CorruptedBlocks)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:removeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:enableIOStatisticsContext()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:getTokenService(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode:reconfigurePropertyImpl(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:stopContainer(org.apache.hadoop.yarn.server.api.ContainerTerminationContext)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.tools.DFSAdmin:getCurrentTrashDir(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.fs.s3a.InconsistentAmazonS3Client:putObject(com.amazonaws.services.s3.model.PutObjectRequest)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:getAssignmentIterator(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.IteratorSelector)
org.apache.hadoop.fs.sftp.SFTPFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.security.HadoopKerberosName:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:getSpillFileCB(org.apache.hadoop.fs.Path,java.io.InputStream,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)
org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat$LazyRecordWriter:close(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.JobClient:getCounter(org.apache.hadoop.mapreduce.Counters,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeFilter)
org.apache.hadoop.hdfs.server.federation.store.impl.MountTableStoreImpl:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult)
org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamContext:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.mapred.nativetask.serde.ByteWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:seekToNewSource(long)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:getAgent(java.lang.String)
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:initializeSecretProvider(javax.servlet.FilterConfig)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:commitPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:blockGroupChecksum(org.apache.hadoop.hdfs.protocol.StripedBlockInfo,org.apache.hadoop.security.token.Token,long,org.apache.hadoop.hdfs.protocol.BlockChecksumOptions)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.AddingCompositeService:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:isRecoverySupported(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.nodemanager.webapp.NavBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.CapacitySchedulerPreemptionContext:getPartitionResource(java.lang.String)
org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:<clinit>()
org.apache.hadoop.examples.terasort.TeraValidate$ValidateMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:<clinit>()
org.apache.hadoop.security.token.DtUtilShell:run(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.sftp.SFTPFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.registry.server.services.RegistryAdminService:serviceStop()
org.apache.hadoop.yarn.webapp.view.JQueryUI:render()
org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3:main(java.lang.String[])
org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:displayError(java.lang.Exception)
org.apache.hadoop.fs.WebHdfs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.server.timeline.EntityLogInfo:parseForStore(org.apache.hadoop.yarn.server.timeline.TimelineDataManager,org.apache.hadoop.fs.Path,boolean,com.fasterxml.jackson.core.JsonFactory,com.fasterxml.jackson.databind.ObjectMapper,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:leaveSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.LeaveSafeModeRequest)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppLogAggregationStatusPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploader:<init>(org.apache.hadoop.yarn.api.records.LocalResource,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.api.SCMUploaderProtocol)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerStopCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.mapred.JobConf:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:createSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$VerifyActiveStatusThread:run()
org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:noteFailure(java.lang.Exception)
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.yarn.csi.client.CsiGrpcClient:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getCanonicalServiceName()
org.apache.hadoop.fs.azure.BlockBlobInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:compare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.sftp.SFTPInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.federation.router.Router:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.CopyCommands$Merge:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getAverageCapacity(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:resync()
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:openFileForRead(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:getConstraint(org.apache.hadoop.yarn.api.records.ApplicationId,java.util.Set)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter:getNodeName(long)
org.apache.hadoop.hdfs.client.impl.BlockReaderLocalLegacy:<clinit>()
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo:getSchedulerResourceTypes()
org.apache.hadoop.fs.LocalFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:setResourceInformation(int,org.apache.hadoop.yarn.api.records.ResourceInformation)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.JobClient:getJobsFromQueue(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.ReduceTaskStatus:statusUpdate(float,java.lang.String,org.apache.hadoop.mapred.Counters)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalIncrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader:start()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$15:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ApplicationStateIterator:next()
org.apache.hadoop.examples.terasort.TeraInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.util.ProcessTree:sigQuitProcessGroup(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupImage:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int)
org.apache.hadoop.tools.dynamometer.blockgenerator.XMLParserMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:isSingleSwitchByScriptPolicy()
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:serviceStop()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.DFSStripedInputStream:readWithStrategy(org.apache.hadoop.hdfs.ReaderStrategy)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:setupConfigurableCapacities(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.streaming.io.KeyOnlyTextOutputReader:initialize(org.apache.hadoop.streaming.PipeMapRed)
org.apache.hadoop.mapreduce.jobhistory.HistoryEvent:setDatum(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:<init>(org.apache.hadoop.yarn.event.Dispatcher,org.apache.hadoop.yarn.server.nodemanager.DeletionService,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService,org.apache.hadoop.fs.Path,java.util.Map,org.apache.hadoop.yarn.api.records.LogAggregationContext,org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.fs.FileContext,long,long)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:read()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskAttemptContext:getArchiveTimestamps()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathBooleanRunner:getUrl()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:createValue()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAddCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo,boolean)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getEZForPath(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMErrorsAndWarningsPage:render(java.lang.Class)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:stop()
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:start()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficControlBandwidthHandlerImpl:<clinit>()
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:compareTo(org.apache.hadoop.yarn.api.records.ReservationRequest)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFs$2:next()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:handleMaxCapacityPercentage(java.lang.String)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$1:run()
org.apache.hadoop.fs.shell.MoveCommands$Rename:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<clinit>()
org.apache.hadoop.net.DNSToSwitchMappingWithDependency:reloadCachedMappings()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileInputStream:read(byte[],int,int)
org.apache.hadoop.fs.http.HttpFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerVolumeCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.mapred.JobConf:getJobSubmitHostName()
org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:run(java.lang.String[])
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:runAll()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getClusterId()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:close()
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:createTupleWritable()
org.apache.hadoop.metrics2.MetricsJsonBuilder:<clinit>()
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:memoryAllocated(int)
org.apache.hadoop.examples.RandomWriter:main(java.lang.String[])
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getCanonicalServiceName()
org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate()
org.apache.hadoop.fs.http.HttpFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:recoverDrainingState()
org.apache.hadoop.mapreduce.filecache.DistributedCache:getLocalCacheArchives(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:serviceStart()
org.apache.hadoop.fs.HarFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:startPeriodic()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.api.records.impl.pb.SchedulingRequestPBImpl:hashCode()
org.apache.hadoop.net.TableMapping:<clinit>()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getEstimatedResourceAllocation(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<init>(java.lang.String[],float)
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:start()
org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getPropertySources(java.lang.String)
org.apache.hadoop.registry.server.services.RegistryAdminService:buildSecurityDiagnostics()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:read(long,java.nio.ByteBuffer)
org.apache.hadoop.lib.service.scheduler.SchedulerService:init(org.apache.hadoop.lib.server.Server)
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:innerClose()
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:main(java.lang.String[])
org.apache.hadoop.fs.s3a.WriteOperations:retry(java.lang.String,java.lang.String,boolean,org.apache.hadoop.util.functional.CallableRaisingIOE)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.tools.HDFSConcat:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageDelimitedTextWriter:<init>(java.io.PrintStream,java.lang.String,java.lang.String)
org.apache.hadoop.fs.cosn.CosNFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.mapreduce.lib.db.DBWritable)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:setUser(java.lang.String,int)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:stashOriginalFilePermissions()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:getProcessorList(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$10:connect(java.net.URL)
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:resumeContainer()
org.apache.hadoop.fs.adl.AdlFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:close()
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:synchronizePlan(java.lang.String,boolean)
org.apache.hadoop.fs.sftp.SFTPInputStream:read(long,byte[],int,int)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat$TextRecordReaderWrapper:nextKeyValue()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:removeChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathRunner:connect(java.net.URL)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:<clinit>()
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:transferBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.fs.StorageType[],java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getSchedulerConfigurationVersion(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:start()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:removeAll(java.lang.Class)
org.apache.hadoop.ipc.WritableRpcEngine$Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityOverTimePolicy:availableResources(org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,java.lang.String,org.apache.hadoop.yarn.api.records.ReservationId,long,long)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:flushBuffer(boolean,boolean)
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:getProxyInternal()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.yarn.server.timelineservice.reader.filter.TimelineFilterUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.shell.Test:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.ManifestStoreOperationsThroughFileSystem:storePreservesEtagsThroughRenames(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String,java.lang.String,java.util.LinkedHashSet)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext:getHomeDirectory()
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy:notifyOfResponse(org.apache.hadoop.yarn.server.federation.store.records.SubClusterId,org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:stop()
org.apache.hadoop.mapreduce.lib.map.RegexMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:moveAllApps(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.hdfs.server.federation.store.impl.DisabledNameserviceStoreImpl:loadCache(boolean)
org.apache.hadoop.fs.FileUtil:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getTotalKillableResource(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createEncryptionZone(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getApplications(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest)
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob:createValueAggregatorJob(java.lang.String[],java.lang.Class[],java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:getProxyAddresses(java.net.URI,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.resourceestimator.service.ShutdownHook:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueuesBlock:renderPartial()
org.apache.hadoop.fs.shell.Head:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedPartitioner:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainer(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeReservation(java.lang.String,java.lang.String)
org.apache.hadoop.mapred.JobConf:writeXml(java.io.Writer)
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:<clinit>()
org.apache.hadoop.tools.rumen.ParsedJob:dumpParsedJob()
org.apache.hadoop.mapred.JobConf:setMaxVirtualMemoryForTask(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.tools.CLI:main(java.lang.String[])
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:doPostingObject(java.lang.Object,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager:<clinit>()
org.apache.hadoop.mapreduce.lib.db.OracleDateSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:initNodeLabelStore(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:setInputDirRecursive(org.apache.hadoop.mapreduce.Job,boolean)
org.apache.hadoop.yarn.util.YarnVersionInfo:main(java.lang.String[])
org.apache.hadoop.fs.sftp.SFTPFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobClient:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:getRecoveredMappableBlock(java.io.File,java.lang.String,byte)
org.apache.hadoop.fs.shell.Display$Text:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:retrievePassword(org.apache.hadoop.yarn.security.ContainerTokenIdentifier)
org.apache.hadoop.fs.shell.SnapshotCommands:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.shell.find.Print:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.util.curator.ZKCuratorManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:start()
org.apache.hadoop.fs.local.LocalFs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:middleStep()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:stop()
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils$1:write(java.io.OutputStream)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AboutBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.fs.cosn.CosNOutputStream:write(int)
org.apache.hadoop.yarn.nodelabels.store.AbstractFSNodeStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.NodeAllocation:<clinit>()
org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.metrics2.MetricsSystemMXBean:startMetricsMBeans()
org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.QueueManager:refreshQueues(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.QueueRefresher)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:markContainerForKillable(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getLong(java.lang.String,long)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.azurebfs.utils.IdentityHandler:lookupForLocalGroupIdentity(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getContainersReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainersRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getMaxShare()
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:readOp()
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:beforeExecution(com.amazonaws.AmazonWebServiceRequest)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider:<clinit>()
org.apache.hadoop.streaming.PipeCombiner:reduce(java.lang.Object,java.util.Iterator,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.s3a.commit.files.PendingSet:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:optLong(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:createTupleWritable()
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.tools.NNHAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:addBlockPool(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMContainerCrashedBeforeRunningTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getMostRecentCheckpointTxId()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.cli.SchedConfCLI$1:getHttpURLConnection(java.net.URL)
org.apache.hadoop.examples.WordMedian$WordMedianMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:getDelegationKey(int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:close()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:<clinit>()
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,int)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,double)
org.apache.hadoop.fs.HarFs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setStoragePolicy(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:killAllAppsInQueue(java.lang.String)
org.apache.hadoop.crypto.random.OsSecureRandom:finalize()
org.apache.hadoop.tools.rumen.Task20LineHistoryEventEmitter:emitterCore(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:render()
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:noteFailure(java.lang.Exception)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String)
org.apache.hadoop.fs.shell.Display$Text:runAll()
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:close()
org.apache.hadoop.service.launcher.ServiceShutdownHook:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:checkVersion()
org.apache.hadoop.crypto.key.kms.server.KMS:getKey(java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Rmdir:processRawArguments(java.util.LinkedList)
org.apache.hadoop.mapred.ShuffleHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$4:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.HarFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.HarFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:discardSegments(long)
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:run(java.lang.String[])
org.apache.hadoop.fs.cosn.CosN:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:run()
org.apache.hadoop.mapred.TextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:startDispatcher()
org.apache.hadoop.yarn.webapp.util.WebAppUtils:setNMWebAppHostNameAndPort(org.apache.hadoop.conf.Configuration,java.lang.String,int)
org.apache.hadoop.registry.server.services.MicroZookeeperService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppKilledTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.v2.app.job.impl.MapTaskImpl:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:put(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.fs.s3a.S3AFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getResourceUsageReport()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:close()
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.mapred.JobConf:getJobPriority()
org.apache.hadoop.yarn.conf.YarnConfiguration:getRaw(java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:setSteadyFairShare(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close()
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineReader:stop()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:key(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getInt(java.lang.String,int)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String,java.lang.String,java.util.LinkedHashSet)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:getContainerPid()
org.apache.hadoop.mapred.ReduceTaskStatus:setFinishTime(long)
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:close()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobCounters(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.snapshot.FileDiffList:checkAndAddLatestSnapshotDiff(int,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:stop()
org.apache.hadoop.fs.azure.BlockBlobInputStream:toString()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:<init>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:createEncryptionZone(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseLocalOrFavoredStorage(org.apache.hadoop.net.Node,boolean,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:restoreFailedStorage(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getRpcServerAddress(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.RunningJob:getCounters()
org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock:getCallerUGI()
org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:startOperation(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController$3:run()
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:initConfig()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:stop()
org.apache.hadoop.fs.azure.Wasb:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator:<clinit>()
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.net.URL)
org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpsFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.registry.conf.RegistryConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:delete(java.lang.String,boolean)
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:serviceStart()
org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:getApplicationOwner(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getMinShare()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:setWorkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.MD5Hash$Comparator:newKey()
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceStop()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.net.URL)
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:addIfService(java.lang.Object)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:stop()
org.apache.hadoop.mapred.JobConf:getMaxReduceTaskFailuresPercent()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRun(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy:<clinit>()
org.apache.hadoop.mapred.JobClient:getReduceTaskReports(java.lang.String)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:stop()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getLocatedFileInfo(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.provisioner.VolumeProvisioningTask:<clinit>()
org.apache.hadoop.fs.shell.Stat:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.api.records.impl.pb.QueueConfigurationsPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:createProxyIfNeeded(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getContainer(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:start()
org.apache.hadoop.hdfs.server.namenode.BackupImage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.azure.Wasbs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:serviceStop()
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocolTranslatorPB:close()
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:createCommitContextForTesting(org.apache.hadoop.fs.Path,java.lang.String,int)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:addNodes(java.util.List)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:attemptAllocationOnNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.SchedulingRequest,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.minikdc.MiniKdc:main(java.lang.String[])
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:deleteDestinationPaths(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceMappingManager:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:serviceStart()
org.apache.hadoop.fs.viewfs.NflyFSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.provider.AbstractProviderService:<clinit>()
org.apache.hadoop.oncrpc.security.Credentials:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:<clinit>()
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:start()
org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:createSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:createDir(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:<clinit>()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:updatePreemptedForCustomResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.MetricsInvariantChecker:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.fs.azurebfs.Abfs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.mapred.FixedLengthInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$2:onRemoval(org.apache.hadoop.thirdparty.com.google.common.cache.RemovalNotification)
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.client.LocalizationProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData:toString()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:init(org.apache.commons.configuration2.SubsetConfiguration)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:releaseContainers(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.JobClient:getConfiguration(java.lang.String)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:setInput(org.apache.hadoop.mapreduce.Job,java.lang.Class,java.lang.String,java.lang.String,java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)
org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:readEntities(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory:createS3Client(java.net.URI,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3A:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:<clinit>()
org.apache.hadoop.security.UserGroupInformation$TestingGroups:refresh()
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:recordOutput(org.apache.commons.text.TextStringBuilder,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:getCallerUGI()
org.apache.hadoop.net.ScriptBasedMappingWithDependency:isSingleSwitchByScriptPolicy()
org.apache.hadoop.fs.cosn.CosNFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setWorkflowPriorityMappings(java.util.List)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setOrderingPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:serviceStart()
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,java.util.List,boolean)
org.apache.hadoop.fs.shell.Concat:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.webapp.view.RobotsTextPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DiskResourceHandler:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:updatePlacementRules()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.service.ClientAMService:stop()
org.apache.hadoop.crypto.key.kms.ValueQueue$1:load(java.lang.Object)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:getApplicationAcls(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getMinimumAllocation()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getTrashRoots(boolean)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:serviceStart()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices$2:write(java.io.OutputStream)
org.apache.hadoop.fs.ftp.FTPFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:disallowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.WriteOperations:revertCommit(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:start()
org.apache.hadoop.fs.s3a.impl.ChangeTracker:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable,java.net.InetSocketAddress[])
org.apache.hadoop.fs.azurebfs.Abfs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:stop()
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setTimes(java.lang.String,long,long)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.util.GSetByHashMap:<clinit>()
org.apache.hadoop.crypto.CryptoStreamUtils:<clinit>()
org.apache.hadoop.hdfs.server.namenode.NNStorage:format()
org.apache.hadoop.fs.sftp.SFTPFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerVolumeSet:<clinit>()
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:put(org.apache.hadoop.yarn.api.records.timeline.TimelineDomain)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.ipc.WritableRpcEngine$Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.mapred.TaskAttemptContext:getCounter(java.lang.Enum)
org.apache.hadoop.hdfs.tools.DFSAdmin:close()
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode:transitionToObserver()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:serviceStart()
org.apache.hadoop.fs.http.HttpsFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:renderText(java.lang.String)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.DF,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)
org.apache.hadoop.fs.shell.Test:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueuesBlock:renderPartial()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumInServiceLiveDataNodes()
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onContainersReceivedFromPreviousAttempts(java.util.List)
org.apache.hadoop.examples.QuasiMonteCarlo:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.examples.RandomTextWriter$RandomTextMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:start()
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$StoragePurger:markStale(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructor:reconstruct()
org.apache.hadoop.fs.s3a.S3AFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:submitApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,boolean)
org.apache.hadoop.yarn.service.impl.pb.client.ClientAMProtocolPBClientImpl:close()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:serviceStart()
org.apache.hadoop.mapred.Merger:merge(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path,org.apache.hadoop.io.RawComparator,org.apache.hadoop.util.Progressable,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.mapred.Counters$Counter,org.apache.hadoop.util.Progress)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:syncYarnSysFS(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.client.Get,java.util.List)
org.apache.hadoop.fs.cosn.CosNFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MultiFileInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManager:getDefaultProfile()
org.apache.hadoop.hdfs.client.HdfsAdmin:createEncryptionZone(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.tools.dynamometer.ApplicationMaster:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:createDelegationToken(java.util.Optional,org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getVerifyECWithTopologyResult()
org.apache.hadoop.contrib.utils.join.DataJoinReducerBase:report()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:moveAllApps(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setMinimumResourceRequirement(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.cosn.CosNFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getLinkTarget(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getPreferredBlockSize(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.tools.DiskBalancerCLI:main(java.lang.String[])
org.apache.hadoop.hdfs.DFSStripedInputStream:chooseDataNode(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection,boolean)
org.apache.hadoop.hdfs.client.impl.BlockReaderFactory:<clinit>()
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:<init>(org.apache.hadoop.mapred.lib.CombineFileSplit,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.Reporter,java.lang.Integer)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet)
org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<clinit>()
org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeOrUpdateAMRMTokenSecretManager(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState,boolean)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:setAccessTime(long,int,boolean)
org.apache.hadoop.fs.cosn.CosN:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.handlers.IDataLoader:close()
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:isLeader()
org.apache.hadoop.fs.shell.TouchCommands$Touch:run(java.lang.String[])
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getNumProcessors()
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String)
org.apache.hadoop.fs.shell.FsUsage$Dus:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setNodes(java.lang.String[])
org.apache.hadoop.nfs.nfs3.Nfs3Interface:remove(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.mapreduce.lib.chain.ChainMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.viewfs.MountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:close()
org.apache.hadoop.mapred.JobConf:getLongBytes(java.lang.String,long)
org.apache.hadoop.fs.s3a.impl.RenameOperation:apply()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getMaximumResourceCapability()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat$TextRecordReaderWrapper:<init>(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.Integer)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:close()
org.apache.hadoop.yarn.server.nodemanager.NodeManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3Utils:getNamenodeId(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:finishAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,boolean,java.lang.String)
org.apache.hadoop.tools.mapred.CopyMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:serviceStop()
org.apache.hadoop.mapred.RunningJob:getJobName()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortPendingUploadsInCleanup(boolean,org.apache.hadoop.fs.s3a.commit.impl.CommitContext)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapred.FileOutputCommitter:isCommitJobRepeatable(org.apache.hadoop.mapred.JobContext)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$WindowsSecureWrapperScriptBuilder:writeLocalWrapperScript(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.shell.AclCommands:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:<clinit>()
org.apache.hadoop.fs.WebHdfs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render()
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:toString()
org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.metrics2.sink.KafkaSink:<clinit>()
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:readClusterInfo(org.apache.commons.cli.CommandLine)
org.apache.hadoop.tools.DistCp:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.tools.DistCpOptions)
org.apache.hadoop.mapred.ResourceMgrDelegate:serviceStop()
org.apache.hadoop.hdfs.server.namenode.BackupImage:saveLegacyOIVImage(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,org.apache.hadoop.hdfs.util.Canceler)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameOldOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.federation.resolver.order.LocalResolver:getFirstNamespace(java.lang.String,org.apache.hadoop.hdfs.server.federation.resolver.PathLocation)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:flush(boolean)
org.apache.hadoop.fs.cosn.CosNFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.fs.http.HttpFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.jobcontrol.Job:submit()
org.apache.hadoop.yarn.util.Apps:getEnvVarsFromInputProperty(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithFailureTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.ShuffleHandler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.client.api.RegistryOperationsFactory:createAuthenticatedInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:decrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue:<clinit>()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getKeyProvider()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$StartAppAttemptTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:isInLatestSnapshot(int)
org.apache.hadoop.mapred.ReduceTaskAttemptImpl:resolveHosts(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:localize(org.apache.hadoop.yarn.api.protocolrecords.ResourceLocalizationRequest)
org.apache.hadoop.mapreduce.v2.app.webapp.TasksBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:<clinit>()
org.apache.hadoop.nfs.nfs3.FileHandle:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getBlacklistedNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler$AsyncScheduleThread:run()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStart()
org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:registerApplicationMasterForDistributedScheduling(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.HarFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsNavBlock:render()
org.apache.hadoop.fs.viewfs.RegexMountPoint:<clinit>()
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:onCompleteLazyPersist(java.lang.String,long,long,java.io.File[],org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getAvailableResources()
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:createValue()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:decAMLimit(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.HarFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setReservable(java.lang.String,boolean)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin:noteFailure(java.lang.Exception)
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo()
org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.VisitedResourceRequestTracker:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:nodeUpdate(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode)
org.apache.hadoop.mapreduce.security.token.JobTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.s3a.S3AUtils:<clinit>()
org.apache.hadoop.fs.azurebfs.utils.TracingContext:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:<init>()
org.apache.hadoop.fs.azurebfs.services.AbfsIoUtils:<clinit>()
org.apache.hadoop.io.ArrayFile$Reader:seek(long)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService$2:load(java.lang.Object)
org.apache.hadoop.fs.shell.Test:expandArguments(java.util.LinkedList)
org.apache.hadoop.registry.server.dns.ServiceRecordProcessor:initTypeToInfoMapping(org.apache.hadoop.registry.client.types.ServiceRecord)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigMaxCapacity(java.lang.String,float)
org.apache.hadoop.fs.aliyun.oss.OSS:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.azure.Wasb:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.security.client.YARNDelegationTokenIdentifier:<init>(org.apache.hadoop.yarn.proto.YarnSecurityTokenProtos$YARNDelegationTokenIdentifierProto$Builder)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.util.InstrumentedWriteLock:lockInterruptibly()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:mkdirs(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.s3a.impl.SDKStreamDrainer:applyRaisingException()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getQueueUserAcls(org.apache.hadoop.yarn.api.protocolrecords.GetQueueUserAclsInfoRequest)
org.apache.hadoop.fs.shell.Concat:run(java.lang.String[])
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:close()
org.apache.hadoop.fs.ftp.FTPFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:putAll(java.util.List,boolean,boolean)
org.apache.hadoop.ipc.WritableRpcEngine$Server:start()
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:close(org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.shell.Delete$Expunge:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.examples.SecondarySort$Reduce:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.crypto.key.kms.server.KMSAudit:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.containerlaunch.ClasspathConstructor:yarnApplicationClasspath(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.net.SocketIOWithTimeout:<clinit>()
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:removeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.GetDelegationTokenRequest)
org.apache.hadoop.yarn.client.api.TimelineV2Client:putSubAppEntities(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:copyBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:sendHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,boolean,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KilledExternallyTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:nextRaw(java.util.List)
org.apache.hadoop.fs.ftp.FtpFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.mapred.JobContext:getJobName()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FsShellPermissions$Chmod:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setQuota(java.lang.String,long,long,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.sftp.SFTPFileSystem$2:setDropBehind(java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:iterator()
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:serviceStop()
org.apache.hadoop.registry.conf.RegistryConfiguration:get(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:checkRemoveLabelsFromNode(java.util.Map)
org.apache.hadoop.yarn.client.RMProxy:createRMProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,org.apache.hadoop.yarn.client.RMProxy)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:addIfService(java.lang.Object)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRunCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.mapred.FadvisedChunkedFile:close()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:setDelegationTokenSeqNum(int)
org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:start()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.cosn.CosNUtils:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDelegationToken()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.service.client.ServiceClient:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:stopDaemons()
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.webapp.example.HelloWorld$Hello:echo()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:<init>(org.apache.hadoop.mapreduce.v2.app.webapp.App,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.webapp.Controller$RequestContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:close()
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:serviceStart()
org.apache.hadoop.streaming.io.TextOutputReader:initialize(org.apache.hadoop.streaming.PipeMapRed)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.ftp.FtpFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.client.ServiceClient:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.ArrayFile$Reader:midKey()
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.adl.AdlFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.federation.RouterAdmin:main(java.lang.String[])
org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:deductUnallocatedResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.gridmix.Gridmix$Component:start()
org.apache.hadoop.mapred.lib.CombineTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getFileSystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getECBlockGroupStats()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getEffectiveMaxCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:unregister()
org.apache.hadoop.mapred.SkipBadRecords:setAttemptsToStartSkipping(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.PathCapabilities:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.cosn.CosNInputStream:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs$RandomInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.registry.server.services.RegistryAdminService:list(java.lang.String)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:write(byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:close()
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:createPassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)
org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:<clinit>()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:main(java.lang.String[])
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:waitFor(java.util.function.Supplier,int,int)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPassword(java.lang.String)
org.apache.hadoop.streaming.PipeCombiner:mapRedFinished()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$18:getUrl()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:lookupFlowContext(org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowRowKey,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$CompressAwarePath:getFileSystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:close()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserveIncreasedContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.contrib.utils.join.DataJoinReducerBase:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:stop()
org.apache.hadoop.yarn.webapp.view.NavBlock:render()
org.apache.hadoop.hdfs.tools.DFSAdmin:<clinit>()
org.apache.hadoop.security.alias.JavaKeyStoreProvider:needsPassword()
org.apache.hadoop.yarn.service.ServiceMaster:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.security.NMTokenIdentifier:getTrackingId()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:errorsAndWarnings()
org.apache.hadoop.yarn.service.component.Component$CompletedAfterUpgradeTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:main(java.lang.String[])
org.apache.hadoop.yarn.server.federation.store.utils.FederationApplicationHomeSubClusterStoreInputValidator:<clinit>()
org.apache.hadoop.mapreduce.Job:getInstance(org.apache.hadoop.mapreduce.Cluster,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FTPFileSystem$1:setDropBehind(java.lang.Boolean)
org.apache.hadoop.mapred.QueueManager:<clinit>()
org.apache.hadoop.fs.Hdfs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:run(java.lang.String[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.fs.s3a.S3AInstrumentation$MetricsUpdatingIOStatisticsStore:incrementCounter(java.lang.String,long)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:createSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.azure.ClientThrottlingAnalyzer:<clinit>()
org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolTranslatorPB:close()
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:start()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.mapred.gridmix.GridmixRecord$Comparator:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceStart()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:memoryFreed(int)
org.apache.hadoop.fs.cosn.CosNFileSystem:close()
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()
org.apache.hadoop.yarn.util.timeline.TimelineUtils:getTimelineServiceVersion(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.tools.mapred.lib.DynamicInputChunk:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.AppPriorityACLsManager:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.mapred.RunningJob:waitForCompletion()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.fs.ftp.FtpFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.serde.LongWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])
org.apache.hadoop.mapreduce.Cluster:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.mapreduce.lib.input.FileInputFormat:setInputPathFilter(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.hdfs.tools.GetConf$JournalNodeCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.io.BooleanWritable:<clinit>()
org.apache.hadoop.examples.terasort.TeraChecksum$ChecksumReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.security.alias.JavaKeyStoreProvider:flush()
org.apache.hadoop.mapred.JobConf:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.JobConf:setMaxReduceTaskFailuresPercent(int)
org.apache.hadoop.examples.RandomWriter$RandomMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.DFSClient$2:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:enableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:serviceStop()
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getMetadata(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin:serviceStart()
org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File)
org.apache.hadoop.mapred.nativetask.serde.ByteWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.io.Text:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:stop()
org.apache.hadoop.tools.RegexCopyFilter:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$FinalTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:<clinit>()
org.apache.hadoop.fs.s3a.impl.HeaderProcessing:<clinit>()
org.apache.hadoop.streaming.PipeCombiner:createInputWriter(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.INodeFile:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.fs.http.HttpFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:fetchData()
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeReservation(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.webapp.LogWebService:getContainerLogsInfo(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,boolean,java.lang.String,boolean)
org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:initOutput(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.mapred.JobContext:getMaxMapAttempts()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:serviceStart()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)
org.apache.hadoop.fs.shell.FsUsage$Df:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getSnapshotDiffReportListing(java.lang.String,java.lang.String,java.lang.String,byte[],int)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:logaggregationstatus()
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:<clinit>()
org.apache.hadoop.ipc.WritableRpcEngine$Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.Abfss:getCanonicalServiceName()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:startCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.hdfs.tools.GetConf$NNRpcAddressesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:getCallerUGI()
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:read(byte[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:updateBlockForPipeline(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.yarn.server.router.Router:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:incrementCurrentKeyId()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.shell.Delete$Rmr:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:createReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setNodeLocalityDelay(int)
org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.FileContext:getFileContext()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:close()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:startThreads()
org.apache.hadoop.fs.shell.FsUsage$Dus:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumDecomDeadDataNodes()
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:<init>(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.fs.LocalFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:<clinit>()
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:bumpReplicaGS(long)
org.apache.hadoop.fs.s3a.s3guard.S3Guard:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:setDropBehind(java.lang.Boolean)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:getTempPathForCluster(java.lang.String)
org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:start()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getUsageStats(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.metrics.ClientSCMMetrics:<clinit>()
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)
org.apache.hadoop.mapred.gridmix.IntermediateRecordFactory:<init>(long,long,int,org.apache.hadoop.mapred.gridmix.GridmixKey$Spec,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:clear()
org.apache.hadoop.crypto.key.kms.server.KMSConfiguration:<clinit>()
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:close()
org.apache.hadoop.yarn.service.webapp.ApiServer:<clinit>()
org.apache.hadoop.hdfs.protocol.HdfsFileStatus:readFields(java.io.DataInput)
org.apache.hadoop.resourceestimator.skylinestore.api.SkylineStore:addHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getTrashRoots(boolean)
org.apache.hadoop.mapreduce.v2.app.webapp.JobBlock:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.AbstractContainersLauncher:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplica:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:reorderSchedulableEntity(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)
org.apache.hadoop.fs.s3a.S3AFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:abortCurrentLogSegment()
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getDecomNodes()
org.apache.hadoop.io.BloomMapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier:getTrackingId()
org.apache.hadoop.hdfs.qjournal.server.JournalMetrics:getLastWriterEpoch()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setOwner(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:remove(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:unreserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.web.AuthFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter:<clinit>()
org.apache.hadoop.mount.MountInterface:nullOp(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics:render()
org.apache.hadoop.applications.mawo.server.common.TaskStatus:setStartTime()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowRunEntityReader:defaultAugmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.shell.Delete$Rmr:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.security.WhitelistBasedResolver:<clinit>()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:<init>()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:echo()
org.apache.hadoop.yarn.client.cli.ClusterCLI:main(java.lang.String[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.LocalContainerLauncher:<init>(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.mapred.TaskUmbilicalProtocol)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:start()
org.apache.hadoop.conf.ConfigurationWithLogging:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshNodes(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesRequest)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:close()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:available()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getTaskAttemptFilesystem(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.util.function.Supplier)
org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.FunctionRaisingIOE)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateDNBlockInfosReducer:setup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadDriver:<clinit>()
org.apache.hadoop.hdfs.nfs.nfs3.WriteManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setAMLimit(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:serviceStart()
org.apache.hadoop.mapred.JobConf:setKeyFieldPartitionerOptions(java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:breakLease(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementManager:<clinit>()
org.apache.hadoop.hdfs.server.aliasmap.InMemoryLevelDBAliasMapServer:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMSWebApp:contextDestroyed(javax.servlet.ServletContextEvent)
org.apache.hadoop.hdfs.server.namenode.FSTreeWalk$FSTreeIterator:onAccept(org.apache.hadoop.hdfs.server.namenode.TreePath,long)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:innerClose()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:commitPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:close()
org.apache.hadoop.mapred.join.InnerJoinRecordReader:createKey()
org.apache.hadoop.fs.shell.TouchCommands$Touch:processRawArguments(java.util.LinkedList)
org.apache.hadoop.mapred.lib.LazyOutputFormat$LazyRecordWriter:close(org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:open(java.lang.String)
org.apache.hadoop.fs.shell.Truncate:run(java.lang.String[])
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.router.webapp.NodesPage:render(java.lang.Class)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:stop()
org.apache.hadoop.fs.ftp.FTPFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getNodeIdToUnreserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator)
org.apache.hadoop.fs.ftp.FTPFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getAvailableResources()
org.apache.hadoop.fs.azurebfs.oauth2.RefreshTokenBasedTokenProvider:isTokenAboutToExpire()
org.apache.hadoop.yarn.sls.SLSRunner$1:serviceStart()
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:reserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.SequenceFileAsTextInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:prepareAtomicFolderRename(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainer(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:close()
org.apache.hadoop.mapred.JobConf:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.mapred.join.OuterJoinRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.mapred.join.TupleWritable)
org.apache.hadoop.fs.s3a.impl.MultiObjectDeleteSupport:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupElasticMemoryController:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:serviceStop()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:stop()
org.apache.hadoop.mapred.ReduceTaskAttemptImpl:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPFileSystem$2:hflush()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.filter.FilterList)
org.apache.hadoop.fs.viewfs.ChRootedFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeAcl(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:opt(java.lang.String,int)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logTruncate(java.lang.String,java.lang.String,java.lang.String,long,long,org.apache.hadoop.hdfs.protocol.Block)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getReservationAdmissionPolicy(java.lang.String)
org.apache.hadoop.mapred.YarnChild:main(java.lang.String[])
org.apache.hadoop.fs.HarFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:start()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:<clinit>()
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:nodelabels()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.rumen.ParsedJob:<clinit>()
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponentInstance(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.service.api.records.Container)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.lib.db.DBInputFormat:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getResourceTypeInfo(org.apache.hadoop.yarn.api.protocolrecords.GetAllResourceTypeInfoRequest)
org.apache.hadoop.io.SetFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:<clinit>()
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter$KMSResponse:sendError(int,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicy:getAssignmentIterator(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.IteratorSelector)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.utils.BuilderUtils:newLocalResource(java.net.URI,org.apache.hadoop.yarn.api.records.LocalResourceType,org.apache.hadoop.yarn.api.records.LocalResourceVisibility,long,long,boolean)
org.apache.hadoop.util.Progress:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:incrementCurrentKeyId()
org.apache.hadoop.mapred.JobConf:setJobEndNotificationURI(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getStats()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsLogsPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain:<clinit>()
org.apache.hadoop.filecache.DistributedCache:addLocalFiles(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:close()
org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getNextSPSPath()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getDestinationFS(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3:startDaemons()
org.apache.hadoop.fs.viewfs.NflyFSystem:getTrashRoots(boolean)
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:seek(long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.streaming.PipeCombiner:addEnvironment(java.util.Properties,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:noteFailure(java.lang.Exception)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:incrNodeTypeAggregations(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)
org.apache.hadoop.fs.http.HttpsFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.federation.store.impl.ZookeeperFederationStateStore:<clinit>()
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.FsUrlConnection:getInputStream()
org.apache.hadoop.mapred.JobConf:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.fs.ftp.FTPFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seekToNewSource(long)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:start()
org.apache.hadoop.yarn.webapp.view.JQueryUI:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:finishAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationId,boolean,java.lang.String)
org.apache.hadoop.mapred.JobContext:getOutputValueClass()
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:<clinit>()
org.apache.hadoop.mapred.TaskAttemptContext:getFileClassPaths()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getJobReport(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetJobReportRequestProto)
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.streaming.PipeCombiner:getDoPipe()
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat:setOutputCompressionType(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:close()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.util.MRApps:<clinit>()
org.apache.hadoop.registry.server.services.RegistryAdminService:zkUpdate(java.lang.String,byte[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.shell.CopyCommands$Cp:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getQueueMaxResource(java.lang.String)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy:checkIfUsageOverFairShare(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getContainerReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainerReportRequest)
org.apache.hadoop.mapred.lib.NLineInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.registry.conf.RegistryConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$AdminCommandHandler:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.hdfs.server.namenode.CacheManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.local.LocalFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.FsUsage$Dus:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:start()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNameservices()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputQuery()
org.apache.hadoop.fs.shell.SetReplication:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksPage:render(java.lang.Class)
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.registry.conf.RegistryConfiguration:getPassword(java.lang.String)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer:serialize(java.lang.Object,com.fasterxml.jackson.core.JsonGenerator,com.fasterxml.jackson.databind.SerializerProvider)
org.apache.hadoop.fs.shell.FsUsage$Df:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AFileSystem:setInputPolicy(org.apache.hadoop.fs.s3a.S3AInputPolicy)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getRollingUpgradeStatus()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:addChild(org.apache.hadoop.hdfs.server.namenode.INode,boolean,int)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:getBytes()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.mapred.gridmix.EchoUserResolver:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:serviceStop()
org.apache.hadoop.mapred.QueueManager:getQueueAcls(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.tools.mapred.CopyCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:toString()
org.apache.hadoop.fs.local.RawLocalFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:must(java.lang.String,long)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:setAvailable(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:decreaseAllocated(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.tools.mapred.CopyMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.WriteOperations:listMultipartUploads(java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])
org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Delete$Expunge:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:notifyAMContainerLaunched(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:incAMUsed(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.JobConf:main(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getEZForPath(java.lang.String)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.yarn.sls.scheduler.CapacitySchedulerMetrics:init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getLinkTarget(java.lang.String)
org.apache.hadoop.hdfs.PeerCache:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DeleteSnapshotOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:<clinit>()
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache:<clinit>()
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:stop()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:stop()
org.apache.hadoop.mapred.JobConf:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:getTaskAttemptPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getDefaultBlockSize()
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.RejectAMRMProxyPolicy:getActiveSubclusters()
org.apache.hadoop.fs.s3a.S3AFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.local.RawLocalFs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:getRaw(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:concat(java.lang.String,java.lang.String[])
org.apache.hadoop.tools.RegexpInConfigurationFilter:<clinit>()
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:toString()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:addIfService(java.lang.Object)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.Test:processArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.FilePool:<clinit>()
org.apache.hadoop.mapred.JobConf:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.s3a.commit.magic.MagicCommitTracker:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfier:<clinit>()
org.apache.hadoop.mapred.lib.CombineTextInputFormat$TextRecordReaderWrapper:createKey()
org.apache.hadoop.mapred.gridmix.JobFactory:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:resolve(java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getServerDefaults()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage$AllContainersBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:metadataExists()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.http.HttpsFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO:write(org.apache.hadoop.io.SequenceFile$Writer,java.util.Collection,boolean)
org.apache.hadoop.mapreduce.lib.db.DateSplitter:<clinit>()
org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:getFileSystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapreduce.Job:setProfileTaskRange(boolean,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.router.webapp.NavBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenameSnapshotOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.azurebfs.oauth2.UserPasswordTokenProvider:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:serviceStart()
org.apache.hadoop.util.HostsFileReader:refresh()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:write(byte[],int,int)
org.apache.hadoop.yarn.util.RackResolver:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getECTopologyResultForPolicies(java.lang.String[])
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:getNodes(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService:<clinit>()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getCumulativeCpuTime()
org.apache.hadoop.fs.FsShellPermissions$Chmod:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:setupConfigurableCapacities()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeCachePool(java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerFactory:<clinit>()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:restartContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:metadataExists()
org.apache.hadoop.mapreduce.lib.map.WrappedMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.fs.cosn.CosN:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy:getPreemptionIterator()
org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getCurrentIOStatisticsContext()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:setContainerCompletedStatus(int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nvidia.NvidiaGPUPluginForRuntimeV2:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:serviceStop()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkPathExists(java.lang.String)
org.apache.hadoop.ipc.WritableRpcEngine:<clinit>()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ContainerStateIterator:hasNext()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1:run()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.examples.DBCountPageView$PageviewMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.security.JniBasedUnixGroupsMapping:logError(int,java.lang.String)
org.apache.hadoop.yarn.server.federation.failover.FederationRMFailoverProxyProvider$1:run()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:complete(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LocalizationFailedToDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.WebHdfs:getCanonicalServiceName()
org.apache.hadoop.fs.azure.BlockBlobInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsBlkioResourceHandlerImpl:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem:<clinit>()
org.apache.hadoop.examples.MultiFileWordCount$MyInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.sls.SLSRunner$1:createServiceManager()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$15:getUrl()
org.apache.hadoop.fs.cosn.CosNFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:refreshMountTableEntries(org.apache.hadoop.hdfs.server.federation.store.protocol.RefreshMountTableEntriesRequest)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$15:connect(java.net.URL)
org.apache.hadoop.yarn.service.ClientAMService:serviceStart()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration:isReservable(java.lang.String)
org.apache.hadoop.mapred.lib.TotalOrderPartitioner:getPartitionFile(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache:<clinit>()
org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.STSClientFactory:builder(org.apache.hadoop.conf.Configuration,java.lang.String,com.amazonaws.auth.AWSCredentialsProvider)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:<clinit>()
org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:listManifests()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRemoveCachePool(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementDispatcher:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getJobAttemptFileSystem(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.tools.mapred.CopyCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.CachingKeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.viewfs.ChRootedFs:getFsStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.LocalFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.S3ADataBlocks:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:<clinit>()
org.apache.hadoop.yarn.appcatalog.controller.AppListController:deploy(java.lang.String,org.apache.hadoop.yarn.service.api.records.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueueOrderingPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:start()
org.apache.hadoop.mapreduce.lib.db.MySQLDataDrivenDBRecordReader:getSelectQuery()
org.apache.hadoop.io.compress.SplittableCompressionCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.io.BloomMapFile$Reader:reset()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:close()
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:isLockSupported()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:serviceStart()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:resetSchedulerMetrics()
org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[])
org.apache.hadoop.fs.cosn.CosNFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.resolver.PathLocation:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:start()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.azure.StorageInterface:getCredentials()
org.apache.hadoop.service.launcher.LaunchableService:stop()
org.apache.hadoop.hdfs.client.HdfsAdmin:clearQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.crypto.key.kms.server.KMS:deleteKey(java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:close()
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:messageReceived(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.MessageEvent)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:isRollingUpgrade()
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:getCallerUGI()
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:<clinit>()
org.apache.hadoop.examples.terasort.TeraSort:<clinit>()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:close()
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.HdfsWriter:channelInactive(io.netty.channel.ChannelHandlerContext)
org.apache.hadoop.mapred.pipes.Submitter:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:<clinit>()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:run()
org.apache.hadoop.fs.shell.SnapshotCommands:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:serviceStart()
org.apache.hadoop.mapred.join.InnerJoinRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:read(long,byte[],int,int)
org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)
org.apache.hadoop.fs.shell.Display$Text:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$16:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.http.HttpFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:close()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListResponsePBImpl:toString()
org.apache.hadoop.fs.s3a.Invoker:quietlyEval(java.lang.String,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)
org.apache.hadoop.fs.s3a.commit.LocalTempDir:tempPath(org.apache.hadoop.conf.Configuration,java.lang.String,long)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode:initialize(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.service.client.ApiServiceClient:<clinit>()
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:close()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:shutdown()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setCachedUsed(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.Test:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:restoreFailedStorage(java.lang.String)
org.apache.hadoop.fs.ftp.FTPFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:startThreads()
org.apache.hadoop.mapred.lib.db.DBOutputFormat:setOutput(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.String[])
org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationConsumer(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.ConsumerRaisingIOE)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:put(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:getOffsetStr(long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager:<clinit>()
org.apache.hadoop.hdfs.client.HdfsAdmin:disallowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:start()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setBalancerBandwidth(long)
org.apache.hadoop.mapreduce.Cluster:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:start()
org.apache.hadoop.fs.s3a.S3AFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSUtils:<clinit>()
org.apache.hadoop.mapred.lib.InputSampler$Sampler:getSample(org.apache.hadoop.mapred.InputFormat,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.util.ApplicationClassLoader:loadClass(java.lang.String)
org.apache.hadoop.fs.cosn.CosNativeFileSystemStore:<clinit>()
org.apache.hadoop.fs.shell.Concat:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.Job:getFinishTime()
org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.Count:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getDestS3AFS()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp:setup()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getNodeIds(java.lang.String)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:setMapOutputCompressionEmulationRatio(org.apache.hadoop.conf.Configuration,float)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:stop()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:defaultAugmentParams(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.yarn.server.AMRMClientRelayer:<clinit>()
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:completedContainerInternal(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:journal(long,int,byte[])
org.apache.hadoop.yarn.conf.YarnConfiguration:getPassword(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:removeXAttrFeature(int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.sftp.SFTPFileSystem:<clinit>()
org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.SWebHdfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPUOrderingPolicyUnderUtilizedPreemptionMoveReservation(boolean)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi:check(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.BackupNode:join()
org.apache.hadoop.mapred.LocalContainerLauncher:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:setHeadroom(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.federation.store.utils.FederationPolicyStoreInputValidator:<clinit>()
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:<clinit>()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:applicationFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationFinishData)
org.apache.hadoop.fs.s3a.S3A:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:enableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CreateSnapshotOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService$1:onRemoval(org.apache.hadoop.thirdparty.com.google.common.cache.RemovalNotification)
org.apache.hadoop.yarn.server.router.webapp.AboutPage:render(java.lang.Class)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:addIfService(java.lang.Object)
org.apache.hadoop.registry.server.services.MicroZookeeperService:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setDeprecatedProperties()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$5:write(java.io.OutputStream)
org.apache.hadoop.fs.local.LocalFs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getMostRecentCheckpointTxId()
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.cosn.CosNFileSystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:asyncContainerRelease(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.ipc.WritableRpcEngine$Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)
org.apache.hadoop.fs.shell.Truncate:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:renderJSON(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getRaw(java.lang.String)
org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:loadLastPartialChunkChecksum()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:reserveResource(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetAllResourceProfilesResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getFree()
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.AdminService:serviceStop()
org.apache.hadoop.fs.WebHdfs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:equals(java.lang.Object)
org.apache.hadoop.mapreduce.v2.hs.JobHistory:start()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.examples.Join:main(java.lang.String[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.security.AMRMTokenIdentifier:<clinit>()
org.apache.hadoop.contrib.utils.join.DataJoinMapperBase:report()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:aggregate(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.yarn.server.timelineservice.storage.TimelineAggregationTrack)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:close()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getMaximumAllocation()
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.yarn.security.AMRMTokenSelector:<clinit>()
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:close()
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:start()
org.apache.hadoop.ipc.ProxyCombiner:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.mount.MountInterface:mnt(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderManager:close()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:preValidateMoveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.fs.FsShell$Usage:run(java.lang.String[])
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:createDelegationToken(java.util.Optional,org.apache.hadoop.fs.s3a.auth.delegation.EncryptionSecrets,org.apache.hadoop.io.Text)
org.apache.hadoop.fs.http.HttpsFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.resourceestimator.skylinestore.validator.SkylineStoreValidator:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction)
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:renderPartial()
org.apache.hadoop.yarn.api.records.impl.pb.ReservationAllocationStatePBImpl:hashCode()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.AoclDiagnosticOutputParser:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.util.ReadOnlyList$Util$2:toArray()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:rename2(java.lang.String,java.lang.String,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.viewfs.NflyFSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.LeveldbConfigurationStore:<clinit>()
org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:moveAllApps(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:read(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:serviceStart()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:close()
org.apache.hadoop.mapred.SkipBadRecords:getAttemptsToStartSkipping(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:pauseContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:isInCurrentState()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:rollMasterKey()
org.apache.hadoop.hdfs.server.namenode.JournalManager:hasSomeData()
org.apache.hadoop.examples.terasort.TeraValidate:main(java.lang.String[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.Counters:addGroup(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:recoverContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:getDataOutputStream(boolean)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:close()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:isGoodDatanode(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,int,boolean,java.util.List,boolean)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.azurebfs.services.AbfsClient:close()
org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.HarFs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.shell.Head:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:createAppDir(java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.examples.WordMean$WordMeanMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.FutureDataInputStreamBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.util.concurrent.AsyncGetFuture:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.service.client.ServiceClient:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:read(long,byte[],int,int)
org.apache.hadoop.mapreduce.v2.app.speculate.ExponentiallySmoothedTaskRuntimeEstimator:contextualize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext)
org.apache.hadoop.fs.azurebfs.Abfs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:<clinit>()
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.client.HdfsUtils:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileEncryptionInfo(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AboutPage:render(java.lang.Class)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getKeyVersions(java.lang.String)
org.apache.hadoop.hdfs.web.AuthFilter:destroy()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:getStoragePolicyID()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMController:echo()
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:enableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.EnableNameserviceRequest)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:addRMContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorDescriptor:configure(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:start()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$21:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkPathMustExist(java.lang.String)
org.apache.hadoop.hdfs.tools.DFSHAAdmin:runCmd(java.lang.String[])
org.apache.hadoop.mapreduce.Job:getInstance(org.apache.hadoop.mapreduce.Cluster)
org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.server.namenode.StartupProgressServlet:getUGI(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:keystoreExists()
org.apache.hadoop.mapred.nativetask.NativeMapOutputCollectorDelegator:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.LocalFileSystem:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:validateContainerState()
org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:getBytes()
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStop()
org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream:flush()
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:close()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getStorageBytesWritten()
org.apache.hadoop.examples.RandomTextWriter$RandomTextMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.JobConf:getProps()
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:startThreads()
org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:reencryptEncryptionZone(org.apache.hadoop.fs.Path,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction)
org.apache.hadoop.hdfs.server.namenode.ha.WrappedFailoverProxyProvider:createProxyIfNeeded(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo)
org.apache.hadoop.hdfs.client.HdfsAdmin:setQuota(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.mapreduce.lib.db.DBWritable)
org.apache.hadoop.fs.shell.CopyCommands$Cp:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.adl.AdlFileSystem:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:moveAppTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.fs.s3a.audit.AuditManagerS3A:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:<clinit>()
org.apache.hadoop.hdfs.server.sps.ExternalStoragePolicySatisfier:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateFencedState()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logMkDir(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.hdfs.server.federation.store.impl.MembershipStoreImpl:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult)
org.apache.hadoop.mapred.TextInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:sendLifelineForTests()
org.apache.hadoop.hdfs.server.namenode.Checkpointer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:<clinit>()
org.apache.hadoop.hdfs.server.datanode.DataNode:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:truncateFileWithRetries(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,long)
org.apache.hadoop.fs.cosn.CosN:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.webapp.example.HelloWorld$Hello:render(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:resumeContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:createCredentials(org.apache.hadoop.security.UserGroupInformation,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode:setRpcLifelineServerAddress(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getUsed()
org.apache.hadoop.hdfs.server.namenode.FixedBlockMultiReplicaResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$NMTokensStateIterator:hasNext()
org.apache.hadoop.fs.shell.find.Find:run(java.lang.String[])
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:close()
org.apache.hadoop.mapreduce.lib.input.DelegatingMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.S3AInstrumentation:close()
org.apache.hadoop.hdfs.server.namenode.FSDirectory:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getSlowDatanodeReport()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:deleteBlockData()
org.apache.hadoop.hdfs.server.federation.router.DFSRouter:main(java.lang.String[])
org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInputDirCallback:onSuccess(java.lang.Object)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.mapred.JobConf:getMaxPhysicalMemoryForTask()
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:serviceStop()
org.apache.hadoop.fs.shell.SetReplication:run(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:copyBlockdata(java.net.URI)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.viewfs.NflyFSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:serviceStart()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$RetroactiveFailureTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:render()
org.apache.hadoop.yarn.appcatalog.controller.AppDetailsController:getStatus(java.lang.String)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setCapacity(java.lang.String,java.lang.String)
org.apache.hadoop.net.TableMapping:resolve(java.util.List)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:write(int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.yarn.service.ClientAMService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:loadFSImageFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)
org.apache.hadoop.fs.sftp.SFTPFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.job.impl.ReduceTaskImpl:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listCorruptFileBlocks(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:commitLastReInitialization(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getPendingDeletionBlocks()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.LocalContainerLauncher:start()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobContext:getLocalCacheFiles()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:read(long,java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractAutoCreatedLeafQueue:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1:getUserName()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:rollMasterKey()
org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.NMProtoUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.AboutBlock:render()
org.apache.hadoop.mapred.jobcontrol.Job:getJobName()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal)
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsVolumeSpi:getUsageStats(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:stop()
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:blockDataExists()
org.apache.hadoop.streaming.PipeCombiner:createOutputReader(java.lang.Class)
org.apache.hadoop.fs.azurebfs.Abfs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:initializeWriter(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerContext)
org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.service.client.ServiceClient:<clinit>()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:openInputStream(long)
org.apache.hadoop.mapred.JobACLsManager:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:stop()
org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:canonicalizeUri(java.net.URI)
org.apache.hadoop.mapred.ShuffleHandler:serviceStop()
org.apache.hadoop.crypto.key.CachingKeyProvider:flush()
org.apache.hadoop.fs.local.LocalFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)
org.apache.hadoop.fs.local.RawLocalFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean:getSlowPeersReport()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$UpdateBlocksOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.cosn.CosN:getHomeDirectory()
org.apache.hadoop.fs.adl.Adl:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:serviceStop()
org.apache.hadoop.fs.sftp.SFTPFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ServiceScheduler:<clinit>()
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$1:removeEldestEntry(java.util.Map$Entry)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:<init>(org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.registry.server.services.RegistryAdminService:<clinit>()
org.apache.hadoop.yarn.service.ServiceMaster:close()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:isInLatestSnapshot(int)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:closeFSStore()
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:setPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:initRPC()
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:close()
org.apache.hadoop.oncrpc.RpcProgram:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:recoverFilesWithDanglingTempData(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.impl.RouterStoreImpl:loadCache(boolean)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:serviceStop()
org.apache.hadoop.hdfs.tools.GetGroups:run(java.lang.String[])
org.apache.hadoop.hdfs.HdfsKMSUtil:getKeyProvider(org.apache.hadoop.crypto.key.KeyProviderTokenIssuer,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:<clinit>()
org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close()
org.apache.hadoop.yarn.server.uam.UnmanagedAMPoolManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.JobConf:setMapOutputCompressorClass(java.lang.Class)
org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$UserResourcesIterator:next()
org.apache.hadoop.filecache.DistributedCache:addLocalArchives(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.HarFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST:init(java.lang.String)
org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.TasksPage:render(java.lang.Class)
org.apache.hadoop.mapred.FileOutputCommitter:recoverTask(org.apache.hadoop.mapred.TaskAttemptContext)
org.apache.hadoop.hdfs.server.namenode.BackupImage:loadFSImageFile(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,org.apache.hadoop.hdfs.server.namenode.FSImageStorageInspector$FSImageFile,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue:<clinit>()
org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close()
org.apache.hadoop.hdfs.server.namenode.BackupNode:startReconfigurationTask()
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:initFileSystem(java.net.URI)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$ParentQueueBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:normalizeResourceRequests(java.util.List)
org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlock:startUpload()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateApplicationStateSynchronously(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture)
org.apache.hadoop.fs.s3a.prefetch.S3ABlockManager:<init>(org.apache.hadoop.fs.s3a.prefetch.S3ARemoteObjectReader,org.apache.hadoop.fs.impl.prefetch.BlockData)
org.apache.hadoop.yarn.webapp.view.ErrorPage:render(java.lang.Class)
org.apache.hadoop.yarn.service.provider.tarball.TarballProviderService:buildContainerRetry(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,org.apache.hadoop.yarn.service.component.instance.ComponentInstance)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList)
org.apache.hadoop.service.launcher.LaunchableService:start()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[])
org.apache.hadoop.fs.store.EtagChecksum:hashCode()
org.apache.hadoop.yarn.util.RackResolver:resolve(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:stop()
org.apache.hadoop.mapreduce.v2.app.job.impl.ReduceTaskImpl:internalError(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:addToClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsRequest)
org.apache.hadoop.record.RecordComparator:newKey()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.registry.conf.RegistryConfiguration:set(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:registerApplicationMasterForDistributedScheduling(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:must(java.lang.String,int)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.services.ListingSupport:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:satisfyStoragePolicy(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:getCumulativeCpuTime()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathRunner:getUrl()
org.apache.hadoop.examples.WordCount$IntSumReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:readEntities(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getDatanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:setPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String)
org.apache.hadoop.fs.SWebHdfs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.mapred.TaskAttemptContext:getGroupingComparator()
org.apache.hadoop.hdfs.web.TokenAspect:ensureTokenInitialized()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.nfs.mount.Mountd:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.join.InnerJoinRecordReader:add(org.apache.hadoop.mapred.join.ComposableRecordReader)
org.apache.hadoop.yarn.server.router.webapp.AppsBlock:render()
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.client.api.TimelineV2Client:putSubAppEntitiesAsync(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity[])
org.apache.hadoop.fs.azurebfs.Abfss:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:renameSnapshot(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock:<init>(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.webapp.View$ViewContext,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.AbstractContainersLauncher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.log.Log4Json:format(org.apache.log4j.spi.LoggingEvent)
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:<init>(long,long,int,int,java.lang.String,java.lang.String,boolean)
org.apache.hadoop.mapred.JobConf:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.registry.conf.RegistryConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.records.RouterState:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:getTargetPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:validateSpillIndexFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)
org.apache.hadoop.crypto.key.KeyShell:run(java.lang.String[])
org.apache.hadoop.fs.azurebfs.Abfs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.io.SetFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getJobAttemptFileSystem(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapred.MapTask:isCommitRequired()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)
org.apache.hadoop.hdfs.client.HdfsAdmin:listCacheDirectives(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:increaseContainersResource(org.apache.hadoop.yarn.api.protocolrecords.IncreaseContainersResourceRequest)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.adl.AdlFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner:setOffsets(org.apache.hadoop.conf.Configuration,int,int)
org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azure.BlockBlobAppendStream$UploadBlockCommand:dump()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.registry.conf.RegistryConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.fs.cosn.CosNFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInitialInputPathCallback:onFailure(java.lang.Throwable)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:stop()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:optLong(java.lang.String,long)
org.apache.hadoop.fs.shell.SnapshotCommands:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.namenode.INodeFile:updateModificationTime(long,int)
org.apache.hadoop.fs.FsShellPermissions$Chmod:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.sps.SPSService:init(org.apache.hadoop.hdfs.server.namenode.sps.Context)
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<clinit>()
org.apache.hadoop.yarn.server.webapp.LogServlet:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:activateSavedReplica(java.lang.String,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker$RamDiskReplica)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:handleContainerExitCode(int,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.SequenceFileInputFilter$MD5Filter:setFrequency(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.server.namenode.BackupImage:saveNamespace(long,long,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.conf.ConfigurationWithLogging:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappedBlock:close()
org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.metrics.RouterMBean:getBlockPoolId()
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeOrUpdateAMRMTokenSecretManager(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState,boolean)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:contextInitialized(javax.servlet.ServletContextEvent)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.util.ProcfsBasedProcessTree:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.security.token.Token$PrivateToken:toString()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:loadManagerFromEditLog(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.SequenceFileInputFilter$Filter:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.MemoryPlacementConstraintManager:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getUsed()
org.apache.hadoop.net.DNS:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:deleteTaskAttemptPathQuietly(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setReplication(java.lang.String,short)
org.apache.hadoop.hdfs.server.namenode.NamenodeFsck:<clinit>()
org.apache.hadoop.yarn.sls.SLSRunner$1:createSchedulerEventDispatcher()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.NavBlock:renderPartial()
org.apache.hadoop.io.ShortWritable:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:setEffectiveMaxResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandomWithStorageType(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.mapred.gridmix.SleepJob$SleepMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.DataNodeUGIProvider:clearCache()
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:renderPartial()
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)
org.apache.hadoop.fs.cosn.CosNFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getRouters()
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:pullNewlyIncreasedContainers()
org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.io.SequenceFile$RecordCompressWriter:hsync()
org.apache.hadoop.fs.shell.Display$Checksum:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:put(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord,boolean,boolean)
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:pauseReencryptForTesting()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueueMappingEntities(java.util.List,java.lang.String)
org.apache.hadoop.fs.azure.security.WasbTokenRenewer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStop()
org.apache.hadoop.hdfs.server.federation.metrics.RouterMBean:isSecurityEnabled()
org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.adl.AdlFileSystem:getCanonicalServiceName()
org.apache.hadoop.fs.viewfs.NflyFSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:updateSchedulingRequests(java.util.List)
org.apache.hadoop.fs.shell.MoveCommands$Rename:expandArguments(java.util.LinkedList)
org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:iterator()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:preValidateMoveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.webapp.Controller:index()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterNetworkTopologyServlet:getUGI(javax.servlet.http.HttpServletRequest,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:reapContainer()
org.apache.hadoop.mapred.LocalContainerLauncher:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.ConvertedConfigValidator:<clinit>()
org.apache.hadoop.fs.azure.ShellDecryptionKeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:renderPartial()
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:start()
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$7:getUrl()
org.apache.hadoop.fs.s3a.WriteOperations:createSpan(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:resync()
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:start()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseLocalOrFavoredStorage(org.apache.hadoop.net.Node,boolean,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.tools.DFSAdmin:getTrash()
org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.crypto.key.CachingKeyProvider:getKeysMetadata(java.lang.String[])
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getDomains(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddBlockOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setMaximumApplicationMasterResourcePerQueuePercent(java.lang.String,float)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:setEntitlement(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.fs.azure.Wasb:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskUmbilicalProtocol:setCheckpointID(org.apache.hadoop.mapred.TaskID,org.apache.hadoop.mapreduce.checkpoint.TaskCheckpointID)
org.apache.hadoop.mapreduce.lib.db.DBRecordReader:<clinit>()
org.apache.hadoop.yarn.server.federation.policies.amrmproxy.LocalityMulticastAMRMProxyPolicy:<clinit>()
org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.federation.store.records.MountTable:newInstance(java.lang.String,java.util.Map,long,long)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskAttemptContext:getMaxMapAttempts()
org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.TouchCommands$Touch:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:initS3AFileSystem(java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:<clinit>()
org.apache.hadoop.mapred.JobConf:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:close()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:finishApp(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState)
org.apache.hadoop.hdfs.client.HdfsAdmin:removeCachePool(java.lang.String)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:clear()
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitter:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getSlowDatanodeReport()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl:<init>(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.conf.Configuration,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.NflyFSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:blockAddedToFileCache()
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getEffectiveMaxCapacity(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:incrNodeTypeAggregations(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType)
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter$FilterRecordReader:nextKeyValue()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.numa.NumaResourceHandlerImpl:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logGetDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)
org.apache.hadoop.fs.adl.Adl:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeAttributesManagerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkList(java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:resetDefaultFactory()
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider$RequestHedgingInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueuePreemptionComputePlugin:computeAppsIdealAllocation(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempQueuePerPartition,java.util.Map,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,float)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:listManifests()
org.apache.hadoop.crypto.JceAesCtrCryptoCodec:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAllocateBlockId(long)
org.apache.hadoop.registry.client.binding.RegistryUtils:<clinit>()
org.apache.hadoop.tools.mapred.CopyCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.aliyun.oss.OSS:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.client.cli.NodeCLI:main(java.lang.String[])
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.mapred.nativetask.util.OutputUtil:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.WebHdfs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listCorruptFileBlocks(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:updateNMTokens(java.util.Collection)
org.apache.hadoop.fs.azure.Wasb:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getHAServiceState()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppFinishedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.webapp.view.TwoColumnCssLayout:render(java.lang.Class)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.fs.cosn.CosNFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.cosn.NativeFileSystemStore:retrieve(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.qjournal.server.GetJournalEditServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:incrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.jobhistory.HistoryViewer:print()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigMaximumAllocation(java.lang.String,java.lang.String)
org.apache.hadoop.net.DNSDomainNameResolver:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:bumpReplicaGS(long)
org.apache.hadoop.yarn.server.nodemanager.DeletionService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:verifyDriverReady()
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RenewDelegationTokenOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.webapp.example.HelloWorld$HelloView:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setPermission(java.lang.String,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.local.RawLocalFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.hdfs.net.DFSNetworkTopology:chooseRandom(java.lang.String)
org.apache.hadoop.fs.cosn.CosNFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.WritableRpcEngine$Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getCanonicalServiceName()
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:initialize(org.apache.hadoop.hdfs.server.datanode.DNConf)
org.apache.hadoop.tools.mapred.CopyCommitter:<clinit>()
org.apache.hadoop.yarn.client.AHSProxy:<clinit>()
org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:incrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor$1:run()
org.apache.hadoop.fs.LocalFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.SWebHdfs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.Abfss:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:updateMountTableEntry(org.apache.hadoop.hdfs.server.federation.store.protocol.UpdateMountTableEntryRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.NflyFSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueueBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean)
org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])
org.apache.hadoop.registry.conf.RegistryConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.mapred.pipes.Submitter:submitJob(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:<init>()
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:runAll()
org.apache.hadoop.yarn.client.SCMAdmin:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.web.resources.ExceptionHandler:toResponse(java.lang.Throwable)
org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.LocalFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:close()
org.apache.hadoop.yarn.server.webproxy.ProxyUtils:<clinit>()
org.apache.hadoop.mapred.lib.MultipleTextOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:getLength(org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.shell.Tail:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionLevel(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:stop()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat:getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:listManifests()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:<init>()
org.apache.hadoop.crypto.key.CachingKeyProvider:deleteKey(java.lang.String)
org.apache.hadoop.fs.s3a.S3A:getCanonicalServiceName()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.queuemanagement.GuaranteedOrZeroCapacityOverTimePolicy:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refreshCallQueue()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RollbackContainerTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getServerDefaults()
org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:getContainerReport(org.apache.hadoop.yarn.api.protocolrecords.GetContainerReportRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:findNodeToUnreserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.util.function.Supplier)
org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.sls.scheduler.CapacitySchedulerMetrics:addAMRuntime(org.apache.hadoop.yarn.api.records.ApplicationId,long,long,long,long)
org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer$3:initChannel(io.netty.channel.Channel)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:close()
org.apache.hadoop.mapred.JobContext:getProfileParams()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.SWebHdfs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.CompositeCrcFileChecksum:equals(java.lang.Object)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:<clinit>()
org.apache.hadoop.hdfs.protocol.BlockStoragePolicy:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:setPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AUtils:getBucketOption(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)
org.apache.hadoop.streaming.PipeMapper:createOutputReader(java.lang.Class)
org.apache.hadoop.hdfs.util.ByteArrayManager:<clinit>()
org.apache.hadoop.hdfs.HAUtil:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver:<clinit>()
org.apache.hadoop.fs.shell.Stat:runAll()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.fs.cosn.CosNFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:rollNewVersion(java.lang.String,byte[])
org.apache.hadoop.yarn.server.nodemanager.webapp.NavBlock:getCallerUGI()
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:renderPartial()
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_syncRead_jmhTest:syncRead_Throughput(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlows(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.security.alias.JavaKeyStoreProvider:initFileSystem(java.net.URI)
org.apache.hadoop.yarn.service.ServiceScheduler:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.fs.ftp.FtpFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getTotalCapacity()
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.mapreduce.lib.db.MySQLDBRecordReader:getSelectQuery()
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:set(java.lang.String,java.lang.String)
org.apache.hadoop.examples.MultiFileWordCount$MapClass:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:open(org.apache.hadoop.fs.PathHandle,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)
org.apache.hadoop.net.NetUtils:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:getStringCollection(java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:warnOnActiveUploads(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMController:renderText(java.lang.String)
org.apache.hadoop.fs.azurebfs.Abfss:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.io.DoubleWritable$Comparator:newKey()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:close()
org.apache.hadoop.ipc.RpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)
org.apache.hadoop.fs.Hdfs:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.MultipleOutputs$InternalFileOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.SchedulerPlacementProcessor:finishApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterResponse)
org.apache.hadoop.fs.s3a.S3A:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:containerIncreasedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.conf.YarnConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.federation.store.impl.DisabledNameserviceStoreImpl:overrideExpiredRecord(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.viewfs.NflyFSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:extractMagicFileLength(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:processDeleteOnExit()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveNamespace(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.util.Canceler)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getLifelineRpcServerBindHost(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystem$1:resolve(java.lang.String,boolean)
org.apache.hadoop.fs.s3a.select.SelectInputStream:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.ContainerPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.service.utils.ServiceApiUtil:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfigValidator:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.yarn.service.provider.tarball.TarballClientProvider:validateConfigFiles(java.util.List,java.lang.String,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapreduce.v2.app.webapp.NavBlock:render()
org.apache.hadoop.fs.viewfs.NflyFSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler:<clinit>()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:start()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppNodeUpdateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromFile(java.io.File)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsNavBlock:renderPartial()
org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshAdminAcls(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshAdminAclsRequest)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$20:connect(java.net.URL)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:killAllAppsInQueue(java.lang.String)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:opt(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:killTask(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskRequestProto)
org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:setOvercommitTimeOut(long)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:setBoundingQuery(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getProps()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:checkVersion()
org.apache.hadoop.fs.shell.Delete$Expunge:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapreduce.JobSubmitter:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:close()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:<init>(org.apache.hadoop.yarn.server.nodemanager.Context,org.apache.hadoop.yarn.server.nodemanager.ResourceView,org.apache.hadoop.yarn.webapp.WebApp,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:<clinit>()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromJson(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:stop()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getFloat(java.lang.String,float)
org.apache.hadoop.mapred.SortedRanges:remove(org.apache.hadoop.mapred.SortedRanges$Range)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.util.resource.Resources$FixedValueResource:getResources()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.AbstractPreemptableResourceCalculator$TQComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.client.HdfsAdmin:addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:getCallerUGI()
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapred.Task:<clinit>()
org.apache.hadoop.fs.local.RawLocalFs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:close()
org.apache.hadoop.util.JvmPauseMonitor:serviceStart()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getNumCores()
org.apache.hadoop.mapred.MapTask:taskCleanup(org.apache.hadoop.mapred.TaskUmbilicalProtocol)
org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:delete(org.apache.hadoop.fs.Path,boolean,boolean)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FileJournalManager:getLogFile(java.io.File,long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean)
org.apache.hadoop.hdfs.client.HdfsAdmin:getInotifyEventStream()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getNumCores()
org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer:close(org.apache.hadoop.hdfs.protocol.datatransfer.IOStreamPair)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AboutBlock:getCallerUGI()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:hflush()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:getDataOutputStream(boolean)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:synchronizePlan(org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,boolean)
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils:<clinit>()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:nullProcedure()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable:updateDemand()
org.apache.hadoop.fs.shell.find.Find:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupImage:saveFSImage(org.apache.hadoop.hdfs.server.namenode.SaveNamespaceContext,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.FederationInterceptor:<clinit>()
org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogReader:readAcontainerLogs(java.io.DataInputStream,java.io.Writer)
org.apache.hadoop.conf.ConfigurationWithLogging:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerQueueManager:<clinit>()
org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:recoverDrainingState()
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.Test:displayError(java.lang.Exception)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputFieldNames()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:mapAttributesToNodes(org.apache.hadoop.yarn.server.api.protocolrecords.NodesToAttributesMappingRequest)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.Reducer:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:saveFSImage(org.apache.hadoop.hdfs.server.namenode.SaveNamespaceContext,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.security.authentication.util.RandomSignerSecretProvider:startScheduler(long,long)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$10:getUrl()
org.apache.hadoop.hdfs.server.namenode.BackupImage:doImportCheckpoint(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.hdfs.DFSClient:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.examples.dancing.Sudoku:main(java.lang.String[])
org.apache.hadoop.fs.sftp.SFTPFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AFileSystem:initializeClass()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setFloat(java.lang.String,float)
org.apache.hadoop.yarn.service.provider.tarball.TarballProviderService:buildContainerLaunchContext(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext)
org.apache.hadoop.io.BloomMapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)
org.apache.hadoop.yarn.webapp.view.InfoBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:deleteMetadata()
org.apache.hadoop.streaming.PipeCombiner:addJobConfToEnvironment(org.apache.hadoop.mapred.JobConf,java.util.Properties)
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:setOutputPath(java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getClusterNodes(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesRequest)
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)
org.apache.hadoop.fs.s3a.S3AFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapreduce.lib.chain.ChainMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getStringCollection(java.lang.String)
org.apache.hadoop.yarn.webapp.WebApp:route(java.lang.String,java.lang.Class,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:handleStoreEvent(org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEvent)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:removeRMContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logEdit(int,byte[])
org.apache.hadoop.yarn.server.resourcemanager.reservation.InMemoryPlan:<clinit>()
org.apache.hadoop.yarn.security.NMTokenSelector:<clinit>()
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache:<clinit>()
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:<clinit>()
org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:hashCode()
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:close()
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:opt(java.lang.String,int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getContentSummary(java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.mapred.JobContext:getMapOutputKeyClass()
org.apache.hadoop.yarn.server.api.impl.pb.client.SCMUploaderProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:flush()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.shell.FsUsage$Df:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersion(java.lang.String)
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:isFileClosed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.MoveCommands$Rename:getTargetPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.FileInputFormat:<clinit>()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumDecomLiveDataNodes()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refreshSuperUserGroupsConfiguration()
org.apache.hadoop.yarn.api.records.impl.pb.ResourceSizingPBImpl:hashCode()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:start()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:stop()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:start()
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream:write(byte[],int,int)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.webapp.util.WebAppUtils:getResolvedRemoteRMWebAppURLWithScheme(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:getDomain(java.lang.String)
org.apache.hadoop.yarn.service.ServiceMaster:serviceStop()
org.apache.hadoop.hdfs.web.TokenAspect:initDelegationToken(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:deleteBlockData()
org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter:stop()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$21:connect(java.net.URL)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:initFileOutputCommitterOptions(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.lib.map.RegexMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.JobContext:getNumReduceTasks()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getEditLogManifest(long)
org.apache.hadoop.registry.conf.RegistryConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:<clinit>()
org.apache.hadoop.yarn.server.webapp.LogWebService:getContainerLogFile(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String,boolean)
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.client.RMHAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:serviceStop()
org.apache.hadoop.io.compress.DeflateCodec:createCompressor()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:registerSubordinateNamenode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getPartitionQueueMetrics(java.lang.String)
org.apache.hadoop.tools.DistTool:readFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.component.Component$ContainerAllocatedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.cosn.CosN:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:releaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.MoveCommands$Rename:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:unreserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.cosn.CosNFileSystem:<clinit>()
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:must(java.lang.String,long)
org.apache.hadoop.yarn.service.component.Component$NeedsUpgradeTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.HttpFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.util.ConverterUtils:getYarnUrlFromPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileInputStream:read()
org.apache.hadoop.security.ssl.KeyStoresFactory:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.LocalFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:startPeriodic()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:<clinit>()
org.apache.hadoop.crypto.CryptoCodec:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:refreshMaximumAllocation(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:open(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$UserResourcesIterator:hasNext()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getUserMetrics(java.lang.String)
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:initializeSecretProvider(javax.servlet.FilterConfig)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:<clinit>()
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:decommissionNode(org.apache.hadoop.net.Node)
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.store.EtagChecksum:equals(java.lang.Object)
org.apache.hadoop.fs.s3a.S3AFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor:init(java.lang.String)
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:addSpillIndexFileCB(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:removeChildQueue(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getRaw(java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.mapred.nativetask.NativeRuntime:<clinit>()
org.apache.hadoop.fs.FsShell$Help:expandArgument(java.lang.String)
org.apache.hadoop.ha.HealthMonitor$MonitorDaemon$1:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.fs.http.HttpsFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:verifyDirectoryWriteAccess(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSClient:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$GetDelegationTokenOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Rmr:run(java.lang.String[])
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.nativetask.serde.BoolWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceAllocationCommitter:tryCommit(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean)
org.apache.hadoop.yarn.server.router.Router:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.services.AbfsCounters:formString(java.lang.String,java.lang.String,java.lang.String,boolean)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:<clinit>()
org.apache.hadoop.mapred.Counters$Group:getCounter(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueStateManager:<clinit>()
org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:compileReport(java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler)
org.apache.hadoop.fs.adl.AdlFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:finalizeUpgrade()
org.apache.hadoop.hdfs.web.TokenAspect:removeRenewAction()
org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.fs.http.HttpFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:size()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:sendLifeline(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:write(java.io.DataOutput)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getDefaultBlockSize()
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout()
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:serviceStart()
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.net.URL)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)
org.apache.hadoop.fs.viewfs.NflyFSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl:toString()
org.apache.hadoop.fs.LocalFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:truncateBlock(long)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:isMultiThreadNecessary(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:setFairShare(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.security.authentication.server.CompositeAuthenticationHandler:init(java.util.Properties)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsResourceCalculator:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.mapred.FixedLengthInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:errorReport(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,int,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:moveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.io.compress.SplittableCompressionCodec:createCompressor()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:get(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RemoteMethod:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:<init>(org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor$LoadComparator)
org.apache.hadoop.mapred.gridmix.ClusterSummarizer:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:reencryptEncryptionZone(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:runCmd(java.lang.String[])
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:testResetReadBufferManager(int,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.filecache.DistributedCache:getLocalCacheFiles(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.examples.terasort.TeraSort:setUseSimplePartitioner(org.apache.hadoop.mapreduce.Job,boolean)
org.apache.hadoop.fs.azure.Wasbs:getDelegationTokens(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:close()
org.apache.hadoop.mapred.Counters$FSGroupImpl:findCounter(java.lang.String,java.lang.String)
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:returnCompressor(org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:closeWriter()
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:disableNameservice(org.apache.hadoop.hdfs.server.federation.store.protocol.DisableNameserviceRequest)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:load(java.io.File)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getOutputTableName()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getAllErasureCodingPolicies()
org.apache.hadoop.mapred.lib.db.DBInputFormat:closeConnection()
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:create(java.lang.String)
org.apache.hadoop.yarn.server.router.webapp.NavBlock:getCallerUGI()
org.apache.hadoop.fs.adl.AdlFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:<init>(org.apache.hadoop.mapreduce.v2.app.webapp.App,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:put(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord,boolean,boolean)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.chain.ChainMapper:addMapper(org.apache.hadoop.mapreduce.Job,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.TFileAggregatedLogsBlock:render()
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:close()
org.apache.hadoop.mapred.join.OuterJoinRecordReader:<init>(int,org.apache.hadoop.mapred.JobConf,int,java.lang.Class)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:close()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.protocolPB.ReconfigurationProtocolTranslatorPB:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:createEncryptionZone(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsBinaryLoader:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:close()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapred.lib.LazyOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:enterSafeMode(org.apache.hadoop.hdfs.server.federation.store.protocol.EnterSafeModeRequest)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:stop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getECTopologyResultForPolicies(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.AboutBlock:getCallerUGI()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:<clinit>()
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.s3a.S3A:getDelegationTokens(java.lang.String)
org.apache.hadoop.io.ArrayFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.cosmosdb.CosmosDBDocumentStoreWriter:close()
org.apache.hadoop.fs.shell.Concat:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:listFilesAndEmptyDirectories(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.crypto.key.KeyShell$RollCommand:validate()
org.apache.hadoop.hdfs.server.federation.resolver.order.HashResolver:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:submitApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,boolean)
org.apache.hadoop.yarn.service.provider.docker.DockerClientProvider:validateConfigFiles(java.util.List,java.lang.String,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:serviceStart()
org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker$ResultHandler:onSuccess(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:<clinit>()
org.apache.hadoop.examples.terasort.TeraValidate$ValidateMapper:cleanup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.LocalJobRunner:getLocalMaxRunningReduces(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$AbstractBlockChecksumComputer:compute()
org.apache.hadoop.yarn.server.router.webapp.RouterWebServices:getContainer(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:<clinit>()
org.apache.hadoop.fs.FsShell$Usage:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.mapred.lib.KeyFieldBasedComparator:newKey()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:app()
org.apache.hadoop.yarn.sls.nodemanager.NMSimulator:init(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,int,int,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,float)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:initialize()
org.apache.hadoop.mapred.JobConf:setMaxMapTaskFailuresPercent(int)
org.apache.hadoop.hdfs.web.resources.ExceptionHandler:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector:<init>(org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.mapreduce.lib.map.RegexMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:endCheckpoint(org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)
org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:maybeCreateSuccessMarkerFromCommits(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.record.RecordComparator:<init>(java.lang.Class)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:getPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.fs.azure.PageBlobOutputStream:close()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:serviceStart()
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:start()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.streaming.mapreduce.StreamInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.fs.FSInputChecker:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:submitApp(java.lang.String)
org.apache.hadoop.fs.local.LocalFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.find.Find:<clinit>()
org.apache.hadoop.mapred.MapTask:keepTaskFiles(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.fs.sftp.SFTPFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.tools.offlineImageViewer.FSImageHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:setupConfigurableCapacities(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.fs.cosn.CosNCopyFileTask:<clinit>()
org.apache.hadoop.mapred.BufferSorter:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.azure.Wasbs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:main(java.lang.String[])
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.tools.dynamometer.ApplicationMaster:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getCorruptBlocksCount()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getStatus()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:singleJobCounter()
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:buildNamespace(java.io.InputStream,java.util.List)
org.apache.hadoop.mapred.gridmix.StressJobFactory:start()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[])
org.apache.hadoop.examples.MultiFileWordCount$MyInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:close()
org.apache.hadoop.fs.viewfs.ViewFs$1:tryResolveInRegexMountpoint(java.lang.String,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getTaskAttemptCompletionEvents(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskAttemptCompletionEventsRequest)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getNodes(org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeFilter)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(byte[])
org.apache.hadoop.fs.local.LocalFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:waitFor(java.util.function.Supplier,int)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:addChild(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode,boolean,int)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getConnectAddress()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:stop()
org.apache.hadoop.hdfs.DFSStripedInputStream:getAllBlocks()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension$3:load(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:<init>()
org.apache.hadoop.hdfs.server.namenode.BackupNode:loadNamesystem(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:connect(java.net.URL)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getListing(java.lang.String,byte[],boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:toString()
org.apache.hadoop.yarn.conf.YarnConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.crypto.CryptoInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:reapContainer()
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.s3a.audit.AuditManagerS3A:start()
org.apache.hadoop.hdfs.server.common.Storage:<clinit>()
org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:runAll()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getStrings(java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.adl.AdlFsInputStream:readFully(long,byte[])
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.api.records.impl.pb.RejectedSchedulingRequestPBImpl:hashCode()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.tools.mapred.CopyCommitter:getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.DFSStripedOutputStream:enqueueCurrentPacketFull()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerTokenSelector:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:createProxyIfNeeded(org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider$NNProxyInfo)
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:stashOriginalFilePermissions()
org.apache.hadoop.yarn.server.router.RouterServerUtil:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$Merge:expandArgument(java.lang.String)
org.apache.hadoop.fs.cosn.CosNFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.yarn.service.webapp.ApiServer:updateService(javax.servlet.http.HttpServletRequest,java.lang.String,org.apache.hadoop.yarn.service.api.records.Service)
org.apache.hadoop.yarn.server.nodemanager.scheduler.DistributedScheduler:<clinit>()
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:render()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:sanitizeEnv(java.util.Map,org.apache.hadoop.fs.Path,java.util.List,java.util.List,java.util.List,java.util.Map,org.apache.hadoop.fs.Path,java.util.Set)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:addNamedOutput(org.apache.hadoop.mapreduce.Job,java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:decPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:monitorHealth()
org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat$LazyRecordWriter:write(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceStart()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.allocation.AllocationFileParser:<clinit>()
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.http.HttpFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:setAccessTime(long,int,boolean)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:queueExternalCall(org.apache.hadoop.ipc.ExternalCall)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerContext:getMaximumResourceCapability()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.RunningJob:cleanupProgress()
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet:<clinit>()
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker:<clinit>()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:decUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:render(java.lang.Class)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:<clinit>()
org.apache.hadoop.mapred.gridmix.SleepJob$SleepReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.tools.mapred.CopyCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:getUser()
org.apache.hadoop.crypto.key.kms.server.KMS:rolloverKey(java.lang.String,java.util.Map)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:close()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FtpFs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:stopContainers(org.apache.hadoop.yarn.api.protocolrecords.StopContainersRequest)
org.apache.hadoop.fs.aliyun.oss.OSS:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.fs.shell.Ls$Lsr:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:containerLaunchedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.api.records.NodeId)
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.security.token.Token$PrivateToken:encodeToUrlString()
org.apache.hadoop.mapred.TaskStatus:<clinit>()
org.apache.hadoop.fs.shell.Test:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getTotal()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.CachingGetSpaceUsed:<clinit>()
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read()
org.apache.hadoop.mapreduce.lib.partition.InputSampler:main(java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat$TextRecordReaderWrapper:getProgress()
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorCombiner:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapred.gridmix.SerialJobFactory$SerialReaderThread:run()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:storeNewClusterNodeLabels(java.util.List)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.S3AFileSystem:getUsed()
org.apache.hadoop.fs.cosn.CosNFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3a.statistics.impl.AwsStatisticsCollector:collectMetrics(com.amazonaws.Request,com.amazonaws.Response)
org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$NMReportedContainerChangeIsDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:serviceStart()
org.apache.hadoop.tools.HadoopArchives:main(java.lang.String[])
org.apache.hadoop.fs.shell.Delete$Expunge:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.MultipartUploaderBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.MutableCSConfigurationProvider:<clinit>()
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerRequestPBImpl:hashCode()
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:call(org.apache.hadoop.io.Writable,long)
org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:getContainerStatuses(org.apache.hadoop.yarn.api.protocolrecords.GetContainerStatusesRequest)
org.apache.hadoop.mapred.join.TupleWritable:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getUsedCapacityBigInt()
org.apache.hadoop.examples.QuasiMonteCarlo$QmcMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.AbstractComparatorOrderingPolicy:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:get(java.lang.Class)
org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:moveReservedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterPermissionChecker:<init>(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.shell.AclCommands:run(java.lang.String[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:getNodeList(java.lang.String)
org.apache.hadoop.util.SysInfoWindows:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:checkReplaceLabelsOnNode(java.util.Map)
org.apache.hadoop.fs.s3a.commit.files.SuccessData:<clinit>()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:<clinit>()
org.apache.hadoop.applications.mawo.server.common.MawoConfiguration:<init>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.fs.shell.Ls$Lsr:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:isContainerAlive(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerLivenessContext)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppPage:render(java.lang.Class)
org.apache.hadoop.mapred.JobClient:getSystemDir()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:close()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerNullStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.split.JobSplitWriter:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.JobsBlock:getCallerUGI()
org.apache.hadoop.mapred.jobcontrol.Job:getJobClient()
org.apache.hadoop.yarn.conf.HAUtil:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:update(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:close()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:selectInputStreams(long,long,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreSerializer:getSerializer()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getDelegationToken(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:listManifests()
org.apache.hadoop.hdfs.PositionStripeReader:readChunk(org.apache.hadoop.hdfs.protocol.LocatedBlock,int)
org.apache.hadoop.hdfs.client.impl.metrics.BlockReaderIoProvider:<clinit>()
org.apache.hadoop.fs.shell.Delete$Rmdir:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:deleteReservation(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:deleteDestinationPaths(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.cosn.CosNFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedFileOutputStream:write(byte[])
org.apache.hadoop.fs.FsShell$Usage:displayError(java.lang.Exception)
org.apache.hadoop.examples.terasort.TeraSort:setOutputReplication(org.apache.hadoop.mapreduce.Job,int)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:getReplanner(java.lang.String)
org.apache.hadoop.fs.LocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica:getDataInputStream(long)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:getResults(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.filter.FilterList)
org.apache.hadoop.fs.LocalFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.FileOutputCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getNumVCoresUsed()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage:render(java.lang.Class)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$25:getUrl()
org.apache.hadoop.io.compress.DirectDecompressionCodec:createCompressor()
org.apache.hadoop.yarn.sls.SLSRunner$1:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getEffectiveCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.examples.MultiFileWordCount$MapClass:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:reapContainer()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.db.DBInputFormat:<clinit>()
org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.contrib.utils.join.DataJoinMapperBase:close()
org.apache.hadoop.fs.adl.AdlFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.FileOutputCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:start()
org.apache.hadoop.util.functional.TaskPool:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeNewApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt)
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:getCallerUGI()
org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:maybeIgnore(boolean,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)
org.apache.hadoop.yarn.webapp.view.InfoBlock:getCallerUGI()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload()
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer:<clinit>()
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:start()
org.apache.hadoop.hdfs.server.namenode.BackupNode:getSlowPeersReport()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:run(java.lang.String[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:checkClosed()
org.apache.hadoop.fs.LocalFileSystem:processDeleteOnExit()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:start()
org.apache.hadoop.util.Shell$1:run()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getCurrentKey(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:must(java.lang.String,float)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getHAServiceState()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:registerDatanode(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration)
org.apache.hadoop.mapred.TextInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.yarn.api.records.impl.LightWeightResource:getResourceInformation(int)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.local.RawLocalFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:incReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:<clinit>()
org.apache.hadoop.fs.azurebfs.Abfss:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor:toString()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreHeartbeat:<clinit>()
org.apache.hadoop.fs.Hdfs:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner$1:connect(java.net.URL)
org.apache.hadoop.fs.s3a.S3A:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:modifyAclEntries(java.lang.String,java.util.List)
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:isEnabled(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getNormalizedResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.cosn.CosN:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler:run()
org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.NameNodeUtils:<clinit>()
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.registry.client.binding.RegistryUtils:extractServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeAcl(java.lang.String)
org.apache.hadoop.fs.azurebfs.services.AbfsOutputStream:waitForPendingUploads()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithSuccessToDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:completedContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:renderPartial()
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeNewApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController:getOlderRemoteAppLogDir(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.fs.s3a.MultipartUtils:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getInts(java.lang.String)
org.apache.hadoop.util.InstrumentedReadLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)
org.apache.hadoop.mapred.MapTaskStatus:setStateString(java.lang.String)
org.apache.hadoop.hdfs.LocatedBlocksRefresher:run()
org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:sanitizeEnv(java.util.Map,org.apache.hadoop.fs.Path,java.util.List,java.util.List,java.util.List,java.util.Map,org.apache.hadoop.fs.Path,java.util.Set)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalAllocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.fs.shell.SetReplication:expandArgument(java.lang.String)
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNodeInfoMXBean:isSecurityEnabled()
org.apache.hadoop.util.VersionInfo:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.examples.pi.DistSum:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.metrics.DataNodeDiskMetrics:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoIntraQueuePreemptionPlugin:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getDouble(java.lang.String,double)
org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider:rollSecret()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CombinedResourceCalculator:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:setConfiguredMinResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:rollNewVersion(java.lang.String,byte[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerKillCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.FsShell:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:moveAppFrom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.tools.DistCh:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:validateContainerState()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorPercentage:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.DF,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNodeUsage()
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$DomainLogFD:prepareForWrite()
org.apache.hadoop.yarn.webapp.WebApps:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)
org.apache.hadoop.registry.server.services.AddingCompositeService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getFilesTotal()
org.apache.hadoop.hdfs.DFSClient:reportChecksumFailure(java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNamenodes()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$24:getUrl()
org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext:<clinit>()
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:killTask(org.apache.hadoop.mapreduce.v2.api.protocolrecords.KillTaskRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin$1:load(java.lang.Object)
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:start()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:bumpReplicaGS(long)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$LaunchFailedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:queueCall(org.apache.hadoop.ipc.Server$Call)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:task()
org.apache.hadoop.yarn.util.YarnVersionInfo:<clinit>()
org.apache.hadoop.fs.shell.Tail:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:<clinit>()
org.apache.hadoop.mapred.JobContext:getWorkingDirectory()
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:serviceStart()
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.router.webapp.RouterController:about()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getMaximumAllocation()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.CustomOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.local.RawLocalFs:getDelegationTokens(java.lang.String)
org.apache.hadoop.registry.server.dns.PrivilegedRegistryDNSStarter:start()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:killContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.s3a.select.SelectInputStream:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$5:connect(java.net.URL)
org.apache.hadoop.mapred.gridmix.ReplayJobFactory:<clinit>()
org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.mapred.JobConf:getTrimmed(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:cancelDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.CancelDelegationTokenRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:getMultiple(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:addRMContainer(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:optLong(java.lang.String,long)
org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:start()
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:readBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token,java.lang.String,long,long,boolean,org.apache.hadoop.hdfs.server.datanode.CachingStrategy)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:renderData(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:refreshNodes()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ChecksummedReader:readOp(boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:<clinit>()
org.apache.hadoop.yarn.sls.SLSRunner$1:createAndStartZKManager(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender:close()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:fsync(java.lang.String,long,java.lang.String,long)
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:getProxyInternal()
org.apache.hadoop.ha.ActiveStandbyElector:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedAtFinalSavingTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFilter:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.shell.FsUsage$Df:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:postRemove(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.SchedulerPlacementProcessor:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:start()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.util.MD5FileUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.api.records.QueueConfigurations:getEffectiveMinCapacity()
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_asyncRead_jmhTest:asyncRead_Throughput(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.util.Shell:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:commitJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.yarn.conf.YarnConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.fs.SWebHdfs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapred.TextOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:updateResourceForReleasedContainer(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.HarFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.local.RawLocalFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.shell.SetReplication:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:serviceStart()
org.apache.hadoop.mapred.lib.MultipleOutputs:addMultiNamedOutput(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.Class,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.web.KerberosUgiAuthenticator$1:getUserName()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:renderJSON(java.lang.Object)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getMissingBlocksCount()
org.apache.hadoop.hdfs.tools.GetConf$SecondaryNameNodesCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapred.TaskAttemptContext:getPartitionerClass()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.azurebfs.services.AbfsClient:renewLease(java.lang.String,java.lang.String,org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:releaseShortCircuitFds(org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$SlotId)
org.apache.hadoop.mapred.JobConf:getJobPriorityAsInteger()
org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor:init(java.lang.String)
org.apache.hadoop.mapred.TaskAttemptContext:getNumReduceTasks()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:needsPassword()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocator:getCSAssignmentFromAllocateResult(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocation,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)
org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getNextSPSPath()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:signalContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext)
org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close()
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:<clinit>()
org.apache.hadoop.mapreduce.filecache.DistributedCache:addCacheFile(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.AbstractFileSystem:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logEnableErasureCodingPolicy(java.lang.String,boolean)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.FsShellPermissions$Chmod:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.appcatalog.application.AppCatalogInitializer:<clinit>()
org.apache.hadoop.examples.BaileyBorweinPlouffe:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:truncateBlock(long)
org.apache.hadoop.mapreduce.jobhistory.EventReader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:fileSystemExists()
org.apache.hadoop.fs.http.HttpsFileSystem:getTrashRoots(boolean)
org.apache.hadoop.hdfs.server.datanode.web.HostRestrictingAuthorizationFilterHandler:<init>()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:noteFailure(java.lang.Exception)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(java.lang.Object)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:createSchedulerProxy()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.server.namenode.FSDirectory$InitQuotaTask:compute()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:run(java.lang.String[])
org.apache.hadoop.security.authentication.examples.RequestLoggerFilter:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:copyMetadata(java.net.URI)
org.apache.hadoop.security.SaslRpcClient:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:disallowSnapshot(java.lang.String)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter:doFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.SWebHdfs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:notifyStoreOperationFailed(java.lang.Exception)
org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider:getCredentials()
org.apache.hadoop.yarn.server.resourcemanager.DecommissioningNodesWatcher:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:removeXAttrFeature(int)
org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:getSteadyFairShare()
org.apache.hadoop.mapred.gridmix.SleepJob:toString()
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:write(int)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceStart()
org.apache.hadoop.fs.impl.FSBuilderSupport:<clinit>()
org.apache.hadoop.fs.FsShellPermissions$Chmod:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:close()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getTrashRoots(boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystemHelper:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:startReconfiguration()
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getAppAttempt(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3A:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory()
org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerStoppedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.webapp.View:<clinit>()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenIdentifier:getTrackingId()
org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory:<clinit>()
org.apache.hadoop.fs.s3a.select.SelectTool:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMDelegatedNodeLabelsUpdater:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.examples.terasort.TeraInputFormat$1:run()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetGenstampV2Op:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:addAuxiliaryListener(int)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:serviceStop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkSet(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List,boolean)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setBoolean(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getSubAppEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTaskAttemptIdCounters(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getTopUserOpCounts()
org.apache.hadoop.fs.Globber:<clinit>()
org.apache.hadoop.fs.http.server.HttpFSServer:delete(java.lang.String,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.registry.conf.RegistryConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl:hashCode()
org.apache.hadoop.tools.FileBasedCopyListing:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollector:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listXAttrs(java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeFile:setGroup(java.lang.String,int)
org.apache.hadoop.fs.http.server.HttpFSServer:getRoot(javax.ws.rs.core.UriInfo,org.apache.hadoop.fs.http.server.HttpFSParametersProvider$OperationParam,org.apache.hadoop.lib.wsrs.Parameters,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.mapreduce.split.JobSplitWriter:createSplitFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.util.List)
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:getToken()
org.apache.hadoop.fs.http.HttpsFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager:<clinit>()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:serviceStart()
org.apache.hadoop.examples.terasort.TeraGen:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:updateContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapred.JobContext:getCacheArchives()
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$Uploads:run(java.lang.String[])
org.apache.hadoop.registry.conf.RegistryConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceSizingPBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.ha.StandbyCheckpointer$CheckpointerThread$1:run()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumBlocks()
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl:hashCode()
org.apache.hadoop.fs.ftp.FTPInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.fs.shell.Display$Checksum:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$25:connect(java.net.URL)
org.apache.hadoop.yarn.util.ApplicationClassLoader:getResource(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$4:getUrl()
org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.security.IdMappingServiceProvider:getUid(java.lang.String)
org.apache.hadoop.fs.FsShell$Help:runAll()
org.apache.hadoop.hdfs.server.namenode.FSTreeWalk:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.filecache.DistributedCache:addFileToClassPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,boolean)
org.apache.hadoop.fs.statistics.IOStatisticsLogging:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.hdfs.server.namenode.NNStorage:setStorageDirectories(java.util.Collection,java.util.Collection)
org.apache.hadoop.hdfs.client.HdfsAdmin:clearSpaceQuota(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:serviceStart()
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[])
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetPermissionsOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.SLSRunner:main(java.lang.String[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$WrappingAuditSpan:close()
org.apache.hadoop.mapred.RamManager:reserve(int,java.io.InputStream)
org.apache.hadoop.yarn.util.resource.ResourceUtils:<clinit>()
org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter:start()
org.apache.hadoop.yarn.conf.YarnConfiguration:iterator()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:stop()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateFencedState()
org.apache.hadoop.yarn.server.federation.resolver.SubClusterResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.tools.dynamometer.blockgenerator.XMLParserMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnRWHelper:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueueBlock:render()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getUsed()
org.apache.hadoop.yarn.service.monitor.probe.MonitorUtils:<clinit>()
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateDNBlockInfosReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.blockmanagement.FSClusterStats,org.apache.hadoop.net.NetworkTopology,org.apache.hadoop.hdfs.server.blockmanagement.Host2NodesMap)
org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader:<clinit>()
org.apache.hadoop.io.MapFile:<clinit>()
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)
org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[])
org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:preCommitJob(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.registry.server.dns.RegistryDNS:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:unset(java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.util.JvmPauseMonitor:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getAllStoragePolicies()
org.apache.hadoop.fs.shell.Truncate:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:pullNewlyDecreasedContainers()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceStart()
org.apache.hadoop.fs.HarFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:getTaskAttemptFilesystem(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getNodeApps(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:serviceStop()
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:setMemorySeconds(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.azurebfs.oauth2.LocalIdentityTransformer:<clinit>()
org.apache.hadoop.mapred.Partitioner:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:serviceStop()
org.apache.hadoop.registry.conf.RegistryConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.shell.Head:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumDecommissioningDataNodes()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:getServer(org.apache.hadoop.yarn.ipc.YarnRPC,org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:container()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyVersions(java.lang.String)
org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)
org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.String,java.lang.Object[])
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:submitApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:getCurrentKey(java.lang.String)
org.apache.hadoop.mapred.nativetask.serde.IKVSerializer:serializePartitionKV(org.apache.hadoop.mapred.nativetask.buffer.DataOutputStream,int,org.apache.hadoop.mapred.nativetask.util.SizedWritable,org.apache.hadoop.mapred.nativetask.util.SizedWritable)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)
org.apache.hadoop.mapred.JobConf:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.applications.mawo.server.common.CompositeTask:<init>(org.apache.hadoop.applications.mawo.server.common.Task)
org.apache.hadoop.fs.cosn.CosNFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.io.NullWritable$Comparator:newKey()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getNodeToLabels(org.apache.hadoop.yarn.api.protocolrecords.GetNodesToLabelsRequest)
org.apache.hadoop.fs.MultipartUploader:startUpload(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.LineRecordReader:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobClient:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:close()
org.apache.hadoop.hdfs.client.HdfsAdmin:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.security.ssl.SSLFactory:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadSortComparator:<init>()
org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:canonicalizeUri(java.net.URI)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:recoverResourceRequestsForContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ContainerRequest)
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:resolveDuplicateReplicas(java.lang.String,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap)
org.apache.hadoop.fs.sftp.SFTPFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:serviceStart()
org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.shell.AclCommands:displayError(java.lang.Exception)
org.apache.hadoop.fs.azurebfs.services.KeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineReaderImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<clinit>()
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:renderPartial()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:setInput(org.apache.hadoop.mapreduce.Job,java.lang.Class,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2:run()
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream:hsync()
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceStop()
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])
org.apache.hadoop.conf.ConfigurationWithLogging:size()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore:deleteFilesystem(org.apache.hadoop.fs.azurebfs.utils.TracingContext)
org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.EntityLogInfo:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:read(java.nio.ByteBuffer)
org.apache.hadoop.fs.s3a.audit.AuditManagerS3A:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.RuncManifestToResourcesPlugin:close()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceStart()
org.apache.hadoop.fs.LocalFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:getOutputCompressorClass(org.apache.hadoop.mapreduce.JobContext,java.lang.Class)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.conf.YarnConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB:close()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)
org.apache.hadoop.fs.azurebfs.services.ReadBufferManager:<clinit>()
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:updateApplicationTimeouts(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationTimeoutsRequest)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$13:connect(java.net.URL)
org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader:next(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setUserAMLimit(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:startLogSegmentAndWriteHeaderTxn(long,int)
org.apache.hadoop.mapred.pipes.DownwardProtocol:authenticate(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:start()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.web.AuthFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getRaw(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.EditLogBackupOutputStream:flush(boolean)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.regionserver.InternalScanner,org.apache.hadoop.hbase.regionserver.ScanType,org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics:setCustomResourceCapability(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addTags(java.util.Properties)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:generateOverview(org.apache.hadoop.yarn.api.records.ApplicationAttemptReport,java.util.Collection,org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.protocol.RefreshSuperUserGroupsConfigurationRequest:newInstance()
org.apache.hadoop.fs.s3a.select.SelectInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.server.namenode.ha.IPFailoverProxyProvider:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI)
org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.registry.server.services.RegistryAdminService:start()
org.apache.hadoop.fs.local.LocalFs:getFsStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.lib.service.scheduler.SchedulerService:destroy()
org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager:createPassword(org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier)
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:createKey()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl:close()
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.azure.Wasb:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.util.FindClass:main(java.lang.String[])
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted(java.net.InetAddress)
org.apache.hadoop.mapreduce.CryptoUtils:<clinit>()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.NamedCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getPendingResources()
org.apache.hadoop.util.BlockingThreadPoolExecutorService$3:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPriorityAcls(java.lang.String,org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.api.records.Priority,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:maybeIgnore(boolean,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)
org.apache.hadoop.yarn.api.records.impl.pb.RejectedSchedulingRequestPBImpl:toString()
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:start()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:parseEntity(org.apache.hadoop.hbase.client.Result)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponents(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.protocol.ReencryptionStatus:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:run()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:render()
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:flush()
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.LeaseRecoverable:recoverLease(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:getCountQuery()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:rename(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.s3a.S3A:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappableBlockLoader:<clinit>()
org.apache.hadoop.mapred.jobcontrol.Job:<clinit>()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl:toString()
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:renderPartial()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refreshServiceAcl()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl$KillWaitAttemptFailedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:clear()
org.apache.hadoop.security.authorize.ServiceAuthorizationManager:<clinit>()
org.apache.hadoop.fs.shell.Tail:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:setFairShare(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.conf.ConfigurationWithLogging:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.IntWritable$Comparator:newKey()
org.apache.hadoop.oncrpc.SimpleTcpClientHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:loadServices()
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$KillWaitTaskCompletedTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:_dec(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractResourceUsage$ResourceType,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.federation.router.FederationUtil:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:init()
org.apache.hadoop.fs.cosn.CosNFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.federation.router.security.RouterSecurityManager:<clinit>()
org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCompressor()
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.adl.Adl:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.fs.impl.prefetch.BlockOperations:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:refreshServiceAcls(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.net.URL)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setDeprecatedProperties()
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:addIfService(java.lang.Object)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getContainers(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:rollEditLog()
org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElector:stop()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:runAll()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,java.util.List,boolean)
org.apache.hadoop.crypto.key.kms.server.KMS:generateEncryptedKeys(java.lang.String,java.lang.String,int)
org.apache.hadoop.fs.s3a.commit.impl.CommitContext:commit(org.apache.hadoop.fs.s3a.commit.files.SinglePendingCommit,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:echo()
org.apache.hadoop.mapred.lib.db.DBConfiguration:configureDB(org.apache.hadoop.mapred.JobConf,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:prepareForLaunch(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:get()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:canAssignToUser(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.mapred.gridmix.Gridmix$Shutdown:run()
org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.auth.delegation.AbstractDelegationTokenBinding$TokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:insert(java.lang.Object)
org.apache.hadoop.mapred.lib.CombineTextInputFormat$TextRecordReaderWrapper:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.ReduceTaskStatus:readFields(java.io.DataInput)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getBlockPoolUsed(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.fs.s3a.impl.DirMarkerTracker:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:recordModification(int)
org.apache.hadoop.fs.http.server.HttpFSServer:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getAvailableResources()
org.apache.hadoop.tools.rumen.Folder:main(java.lang.String[])
org.apache.hadoop.mapred.TaskStatus:createTaskStatus(java.io.DataInput,org.apache.hadoop.mapred.TaskAttemptID,float,int,org.apache.hadoop.mapred.TaskStatus$State,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.mapred.TaskStatus$Phase,org.apache.hadoop.mapred.Counters)
org.apache.hadoop.mapred.JobConf:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$Rename:run(java.lang.String[])
org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.com.nec.NECVEPlugin:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:cleanupExpiredQueues(java.lang.String,boolean,java.util.Set,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:resumeReencryptForTesting()
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.conf.YarnConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.http.HttpFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.Adl:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getServiceStatus()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getCpuUsagePercentage()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:closeConnection()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:<init>(org.apache.hadoop.yarn.webapp.View$ViewContext,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager)
org.apache.hadoop.yarn.webapp.view.InfoBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:hasCapability(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:getCallerUGI()
org.apache.hadoop.mapred.TaskAttemptContext:getOutputFormatClass()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:isUpgradeFinalized()
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:close()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:setPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:setPreemptedVcoreSeconds(long)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:registerBackupNode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getServiceStatus()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync$RpcEdit:logEdit()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkStat(java.lang.String)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:runAll()
org.apache.hadoop.tools.GlobbedCopyListing:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.mapred.MapFileOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:init(java.util.Properties)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAutoCreatedLeafQueueConfigUserLimitFactor(java.lang.String,float)
org.apache.hadoop.fs.http.HttpFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.SLSRunner$1:createReservationSystem()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ConfigurationMutationACLPolicyFactory:<clinit>()
org.apache.hadoop.portmap.Portmap:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:<clinit>()
org.apache.hadoop.mapreduce.filecache.DistributedCache:setCacheFiles(java.net.URI[],org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:getMetadataInputStream(long)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:toString()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.registry.server.services.RegistryAdminService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,long,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:disableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.SchedulerPageUtil$QueueBlockUtil:getCallerUGI()
org.apache.hadoop.fs.s3a.WriteOperations:select(org.apache.hadoop.fs.Path,com.amazonaws.services.s3.model.SelectObjectContentRequest,java.lang.String)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceAllocationRequestPBImpl:toString()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:serviceStop()
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:checkExists(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapred.InputSplitWithLocationInfo:write(java.io.DataOutput)
org.apache.hadoop.io.compress.GzipCodec:createCompressor()
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:recalculateQueueUsageRatio(org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.yarn.conf.YarnConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsBinaryInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.adl.AdlFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkMkParentPath(java.lang.String,java.util.List)
org.apache.hadoop.mapreduce.lib.db.TextSplitter:split(org.apache.hadoop.conf.Configuration,java.sql.ResultSet,java.lang.String)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:addMountTableEntry(org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryRequest)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getMetadata(java.lang.String)
org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KillingToDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:appLaunched(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp,long)
org.apache.hadoop.fs.shell.CopyCommands$Merge:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore:loadFromMirror(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.placement.UserPlacementRule:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$LaunchTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:listManifests()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationStateSynchronously(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture)
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:close()
org.apache.hadoop.hdfs.server.namenode.INodeFile:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,float)
org.apache.hadoop.io.compress.DeflateCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:putRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.PutOpParam,org.apache.hadoop.hdfs.web.resources.DestinationParam,org.apache.hadoop.hdfs.web.resources.OwnerParam,org.apache.hadoop.hdfs.web.resources.GroupParam,org.apache.hadoop.hdfs.web.resources.PermissionParam,org.apache.hadoop.hdfs.web.resources.UnmaskedPermissionParam,org.apache.hadoop.hdfs.web.resources.OverwriteParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ReplicationParam,org.apache.hadoop.hdfs.web.resources.BlockSizeParam,org.apache.hadoop.hdfs.web.resources.ModificationTimeParam,org.apache.hadoop.hdfs.web.resources.AccessTimeParam,org.apache.hadoop.hdfs.web.resources.RenameOptionSetParam,org.apache.hadoop.hdfs.web.resources.CreateParentParam,org.apache.hadoop.hdfs.web.resources.TokenArgumentParam,org.apache.hadoop.hdfs.web.resources.AclPermissionParam,org.apache.hadoop.hdfs.web.resources.XAttrNameParam,org.apache.hadoop.hdfs.web.resources.XAttrValueParam,org.apache.hadoop.hdfs.web.resources.XAttrSetFlagParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.CreateFlagParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StoragePolicyParam,org.apache.hadoop.hdfs.web.resources.ECPolicyParam,org.apache.hadoop.hdfs.web.resources.NameSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageSpaceQuotaParam,org.apache.hadoop.hdfs.web.resources.StorageTypeParam)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.BackupStore:<clinit>()
org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher:propagateUsernameInInsecureCluster()
org.apache.hadoop.mapred.join.OuterJoinRecordReader:createInternalValue()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:jobCompleted(boolean)
org.apache.hadoop.fs.LocalFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.router.webapp.RouterController:nodes()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$23:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:allocateResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.resourceestimator.translator.impl.LogParserUtil:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuNodeResourceUpdateHandler:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$LeafQueueBlock:render(java.lang.Class)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:close()
org.apache.hadoop.fs.shell.Truncate:runAll()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getReconfigurationStatus()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:serviceStart()
org.apache.hadoop.fs.shell.Display$Checksum:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.TaskManifest:<clinit>()
org.apache.hadoop.mapred.Merger:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewNavBlock:getCallerUGI()
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService:postInit()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getNodesToAttributes(org.apache.hadoop.yarn.api.protocolrecords.GetNodesToAttributesRequest)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.BytesWritable:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeCachePool(java.lang.String)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:list(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:getEntityTimelines(java.lang.String,java.util.SortedSet,java.lang.Long,java.lang.Long,java.lang.Long,java.util.Set)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:getNodes(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:incAMUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.conf.YarnConfiguration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.app.job.impl.ReduceTaskImpl:handle(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEvent)
org.apache.hadoop.conf.ConfigurationWithLogging:writeXml(java.io.Writer)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshQueues(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshQueuesRequest)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.server.namenode.ImageWriter:writeSecretManagerSection()
org.apache.hadoop.yarn.api.records.ReservationRequest$ReservationRequestComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:createSchedContainerChangeRequests(java.util.List,boolean)
org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getVolumeMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)
org.apache.hadoop.fs.HarFs:getCanonicalServiceName()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeCacheDirective(long)
org.apache.hadoop.security.authentication.server.MultiSchemeAuthenticationHandler:<clinit>()
org.apache.hadoop.io.compress.PassthroughCodec:<clinit>()
org.apache.hadoop.mapred.JobConf:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumLiveNodes()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FsShell$Help:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getInstances(java.lang.String,java.lang.Class)
org.apache.hadoop.fs.shell.Mkdir:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.mapred.join.InnerJoinRecordReader:createValue()
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:init(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getAllocatedResources()
org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction,boolean)
org.apache.hadoop.mapred.LocalContainerLauncher:serviceStart()
org.apache.hadoop.yarn.server.timeline.util.LeveldbUtils:<clinit>()
org.apache.hadoop.fs.shell.Display$Text:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.webapp.util.WebServiceClient$1:getHttpURLConnection(java.net.URL)
org.apache.hadoop.tools.HadoopArchiveLogs:<clinit>()
org.apache.hadoop.yarn.util.resource.Resources$FixedValueResource:getResourceInformation(java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getTrashRoots(boolean)
org.apache.hadoop.conf.ConfigurationWithLogging:readFields(java.io.DataInput)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestPrinter:main(java.lang.String[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:batchedListLocatedStatusIterator(java.util.List)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:close()
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$TemplateOutputter:save(java.io.OutputStream)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumDecomDeadNodes()
org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl:<clinit>()
org.apache.hadoop.fs.shell.Display$Text:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.FpgaNodeResourceUpdateHandler:<clinit>()
org.apache.hadoop.mapred.JobContext:getFileTimestamps()
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:getNodes(java.lang.String)
org.apache.hadoop.io.Stringifier:toString(java.lang.Object)
org.apache.hadoop.mapred.MapTaskStatus:readFields(java.io.DataInput)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:createAttemptHeadRoomTable(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:serviceStop()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkRead(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.ImageServlet:doPut(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:getLocalizationStatuses(org.apache.hadoop.yarn.api.protocolrecords.GetLocalizationStatusesRequest)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveCacheDirectiveInfoOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getState(java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ServiceScheduler$NMClientCallback:onContainerReInitializeError(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.Throwable)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readAndDiscard(int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.AllocationBasedResourceUtilizationTracker:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage$AllApplicationsBlock:render(java.lang.Class)
org.apache.hadoop.security.HttpCrossOriginFilterInitializer:<clinit>()
org.apache.hadoop.examples.dancing.DistributedPentomino$PentMap:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Tail:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:<clinit>()
org.apache.hadoop.mapreduce.v2.app.webapp.JobBlock:getCallerUGI()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.fs.s3a.S3AFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)
org.apache.hadoop.fs.azure.security.WasbDelegationTokenIdentifier:getUser()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerQueue:hasAccess(org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addTags(java.util.Properties)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getStats()
org.apache.hadoop.fs.adl.AdlFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setOwner(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.Ls$Lsr:run(java.lang.String[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setErasureCodingPolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:rollEditLog(int)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.MapFileOutputFormat:getEntry(org.apache.hadoop.io.MapFile$Reader[],org.apache.hadoop.mapred.Partitioner,org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:addIfService(java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.mapreduce.v2.hs.JobHistory:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getKeyVersions(java.lang.String)
org.apache.hadoop.registry.client.api.RegistryOperations:close()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.sftp.SFTPInputStream:toString()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setOrderingPolicyParameter(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,boolean)
org.apache.hadoop.yarn.conf.YarnConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:getOlderRemoteAppLogDir(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.crypto.random.OpensslSecureRandom:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:close()
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:stop()
org.apache.hadoop.hdfs.util.LightWeightHashSet:<init>(int)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.fs.adl.Adl:getHomeDirectory()
org.apache.hadoop.fs.cosn.CosN:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.registry.conf.RegistryConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.yarn.service.client.ApiServiceClient:close()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)
org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.ha.WrappedFailoverProxyProvider:getProxyAddresses(java.net.URI,java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:<init>(org.apache.hadoop.hdfs.web.WebHdfsFileSystem$UnresolvedUrlOpener,org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlOpener)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.hdfs.DFSUtilClient:isHDFSEncryptionEnabled(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getApplicationAttempts(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationAttemptsRequest)
org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException)
org.apache.hadoop.fs.LocalFileSystem:<init>()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.ftp.FTPFileSystem:processDeleteOnExit()
org.apache.hadoop.fs.cosn.CosN:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceManagerAdministrationProtocolPBClientImpl:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp:<clinit>()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:renameMeta(java.net.URI)
org.apache.hadoop.fs.adl.AdlFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.fs.http.HttpsFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.cosn.CosNFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:start()
org.apache.hadoop.fs.shell.TouchCommands$Touch:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl:hashCode()
org.apache.hadoop.lib.service.instrumentation.InstrumentationService:postInit()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$ErrorMetrics:getCallerUGI()
org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:init(int,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.sls.SLSRunner,long,long,java.lang.String,java.lang.String,boolean,java.lang.String,long,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,java.util.Map,java.util.Map)
org.apache.hadoop.hdfs.DistributedFileSystem:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:refreshEntries(java.util.Collection)
org.apache.hadoop.mapred.RunningJob:reduceProgress()
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:setSignalled(boolean)
org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run()
org.apache.hadoop.mapred.JobClient:getMapTaskReports(java.lang.String)
org.apache.hadoop.yarn.server.router.webapp.NodesBlock:getCallerUGI()
org.apache.hadoop.fs.ftp.FtpFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage$AllContainersBlock:getCallerUGI()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:start()
org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:stop()
org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerDisabledTracker:addReport(java.lang.String,java.lang.String,org.apache.hadoop.hdfs.server.protocol.OutlierMetrics)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:initialize(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.tools.GetConf$PrintConfKeyCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.fs.http.HttpFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumMd5CrcReconstructor:getSocketAddress4Transfer(org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:removeMountTableEntry(org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest)
org.apache.hadoop.hdfs.SWebHdfsDtFetcher:isTokenRequired()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logOpenFile(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean,boolean)
org.apache.hadoop.fs.HarFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeProxyCACert(java.security.cert.X509Certificate,java.security.PrivateKey)
org.apache.hadoop.fs.http.HttpsFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.cosn.CosN:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystemStore:purge(java.lang.String)
org.apache.hadoop.fs.s3a.tools.MarkerTool:initS3AFileSystem(java.lang.String)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor:<clinit>()
org.apache.hadoop.fs.HarFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.client.util.YarnClientUtils:<clinit>()
org.apache.hadoop.mapred.JobConf:getJobSubmitHostAddress()
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils:<clinit>()
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int)
org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:createSymlink(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkTagMappingManagerFactory:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:metaSave(java.lang.String)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getCounters(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetCountersRequestProto)
org.apache.hadoop.fs.shell.Delete$Rmr:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:endCheckpoint(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration,org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:isTrusted()
org.apache.hadoop.yarn.service.ServiceScheduler$AMRMClientCallback:onRequestsRejected(java.util.List)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.nativetask.NativeBatchProcessor:<clinit>()
org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.fs.azure.Wasbs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:recordModification(int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:start()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.FsUsage$Df:runAll()
org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade:reinitialize(org.apache.hadoop.yarn.server.federation.store.FederationStateStore,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.io.retry.RetryPolicies:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:unsetErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.webapp.RouterController:renderText(java.lang.String)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getDelegationKey(int)
org.apache.hadoop.fs.s3a.statistics.S3AInputStreamStatistics:blockRemovedFromFileCache()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.discovery.FPGADiscoveryStrategy:discover()
org.apache.hadoop.fs.WebHdfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.join.CompositeRecordReader:accept(org.apache.hadoop.mapred.join.CompositeRecordReader$JoinCollector,org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.hdfs.DFSUtilClient:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController:postWrite(org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileControllerContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:canAssignToThisQueue(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.LocalFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapreduce.v2.app.job.impl.MapTaskImpl:internalError(org.apache.hadoop.mapreduce.v2.app.job.event.TaskEventType)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.hdfs.server.namenode.BackupImage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem,long,org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext)
org.apache.hadoop.registry.conf.RegistryConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.fs.ChecksumFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:<clinit>()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:serviceStart()
org.apache.hadoop.ipc.WritableRpcEngine$Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:fillJoinCollector(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.crypto.key.kms.server.KMSAudit$1:onRemoval(org.apache.hadoop.thirdparty.com.google.common.cache.RemovalNotification)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerQueueManager:reinitializeQueues(org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSchedulerConfiguration)
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$2:operationComplete(io.netty.util.concurrent.Future)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.lib.db.MySQLDBRecordReader:createValue()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$StartedResourcesIterator:next()
org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String,java.util.LinkedHashSet)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitterFactory:getDestinationFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:recalculateQueueUsageRatio(org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RecoveredContainerTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.lib.BinaryPartitioner:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:failApplicationAttempt(org.apache.hadoop.yarn.api.protocolrecords.FailApplicationAttemptRequest)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.pipes.PipesReducer:close()
org.apache.hadoop.hdfs.DFSStripedInputStream:openInfo(boolean)
org.apache.hadoop.resourceestimator.translator.impl.BaseLogParser:<clinit>()
org.apache.hadoop.yarn.server.webapp.ContainerBlock:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:serviceStart()
org.apache.hadoop.mapred.nativetask.handlers.BufferPullee:<init>(java.lang.Class,java.lang.Class,org.apache.hadoop.mapred.RawKeyValueIterator,org.apache.hadoop.mapred.nativetask.NativeDataTarget)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.fs.azure.Wasbs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.streaming.PipeMapper:createInputWriter(java.lang.Class)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getRpcPort()
org.apache.hadoop.yarn.webapp.view.LipsumBlock:getCallerUGI()
org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapreduce.lib.input.FileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:abort()
org.apache.hadoop.yarn.service.provider.defaultImpl.DefaultProviderService:buildContainerRetry(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,org.apache.hadoop.yarn.service.component.instance.ComponentInstance)
org.apache.hadoop.tools.mapred.lib.DynamicRecordReader:<clinit>()
org.apache.hadoop.yarn.event.AsyncDispatcher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:main(java.lang.String[])
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:start()
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:readMetrics(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity,org.apache.hadoop.hbase.client.Result,org.apache.hadoop.yarn.server.timelineservice.storage.common.ColumnPrefix)
org.apache.hadoop.applications.mawo.server.common.TaskStatus:setEndTime()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.ipc.ProtobufRpcEngine:<clinit>()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.JobConf:addResource(java.net.URL,boolean)
org.apache.hadoop.hdfs.client.HdfsAdmin:removeCacheDirective(long)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueueBlock:renderPartial()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:createPreemptionThread()
org.apache.hadoop.resourceestimator.solver.impl.LpSolver:toRecurringRDL(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.Reconfigurable:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String)
org.apache.hadoop.yarn.client.ServerProxy$1:run()
org.apache.hadoop.mapred.TaskAttemptContext:getMaxReduceAttempts()
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:<clinit>()
org.apache.hadoop.fs.audit.CommonAuditContext:reset()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:getApplicationReport(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String[],java.security.cert.X509Certificate)
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:fromInstance(java.lang.Object)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getUsed()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:pathconf(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService:close()
org.apache.hadoop.io.compress.SplittableCompressionCodec:createDecompressor()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:decAMUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.YarnConfigurationStoreFactory:<clinit>()
org.apache.hadoop.yarn.server.federation.policies.router.LocalityRouterPolicy:<clinit>()
org.apache.hadoop.fs.HarFileSystem:getUsed()
org.apache.hadoop.hdfs.server.balancer.KeyManager$BlockKeyUpdater:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getStringCollection(java.lang.String)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.webapp.view.NavBlock:getCallerUGI()
org.apache.hadoop.fs.HarFs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:toString()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseLocalOrFavoredStorage(org.apache.hadoop.net.Node,boolean,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:start()
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:close()
org.apache.hadoop.metrics2.MetricsSystemMXBean:start()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:setupConfigurableCapacities()
org.apache.hadoop.hdfs.server.namenode.DfsServlet:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AddCacheDirectiveInfoOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:<clinit>()
org.apache.hadoop.io.BloomMapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:stop()
org.apache.hadoop.fs.s3a.auth.delegation.AbstractS3ATokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:containerStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerStartData)
org.apache.hadoop.fs.ftp.FTPFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$22:getUrl()
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getClusterNodeAttributes(org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodeAttributesRequest)
org.apache.hadoop.metrics2.lib.MutableRates:<clinit>()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.s3a.impl.GetContentSummaryOperation:<clinit>()
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:must(java.lang.String,boolean)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:setReadahead(java.lang.Long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:render()
org.apache.hadoop.yarn.sls.SLSRunner$1:createAdminService()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetNSQuotaOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:getPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:removeApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setPUOrderingPolicyUnderUtilizedPreemptionEnabled(boolean)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:refreshCallQueue(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.net.DFSTopologyNodeImpl:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getDelegationTokens(java.lang.String)
org.apache.hadoop.yarn.client.ClientRMProxy:<clinit>()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,boolean)
org.apache.hadoop.mapreduce.util.MRJobConfUtil:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:format(org.apache.hadoop.hdfs.server.namenode.FSNamesystem,java.lang.String,boolean)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:serviceStart()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.v2.app.webapp.TasksBlock:getCallerUGI()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addTags(java.util.Properties)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:toString()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.FsUsage$Df:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:optLong(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeNewReservation(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:applicationAttemptFinished(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptFinishData)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:allowSnapshot(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeCachePool(java.lang.String)
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.examples.dancing.DistributedPentomino$PentMap:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_asyncRead_jmhTest:asyncRead_AverageTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:stop()
org.apache.hadoop.fs.shell.MoveCommands$Rename:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.timeline.security.TimelineAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool$BucketInfo:run(java.lang.String[])
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapred.nativetask.ICombineHandler:combine()
org.apache.hadoop.examples.WordMedian$WordMedianMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getContainerExecutorExecutablePath(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.SWebHdfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:optLong(java.lang.String,long)
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:returnCompressor(org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.NodeLabelsUtils:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logReassignLease(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.fs.shell.Stat:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkCreate(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAddCacheDirectiveInfo(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,boolean)
org.apache.hadoop.mapred.LocatedFileStatusFetcher$ProcessInputDirCallback:onFailure(java.lang.Throwable)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:createRbwFile(java.lang.String,org.apache.hadoop.hdfs.protocol.Block)
org.apache.hadoop.hdfs.server.mover.Mover:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner$1:getUrl()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:noteFailure(java.lang.Exception)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString()
org.apache.hadoop.fs.cosn.CosNFileSystem:getUsed()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:expandArgument(java.lang.String)
org.apache.hadoop.yarn.service.client.ApiServiceClient:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:setAvailableResourcesToUser(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.mapred.gridmix.GenerateData$GenDataMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.namenode.BackupNode:reconfigureProperty(java.lang.String,java.lang.String)
org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable)
org.apache.hadoop.fs.ftp.FTPFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$UpgradeTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.shell.Stat:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,int)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:startOperation(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceStart()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider$NodeAttributeScriptRunner:run()
org.apache.hadoop.yarn.LocalConfigurationProvider:close()
org.apache.hadoop.hdfs.DFSClient:delete(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:recover()
org.apache.hadoop.mapred.RunningJob:getTaskCompletionEvents(int)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getTokenInfoFromZK(java.lang.String,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:createRunnableLauncher(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt,org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType)
org.apache.hadoop.mapred.JobConf:getTrimmed(java.lang.String,java.lang.String)
org.apache.hadoop.examples.WordStandardDeviation$WordStandardDeviationMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:setupConfigurableCapacities(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.cosn.CosNInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:initiateCsiClients(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setScheduleAynschronously(boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.s3a.S3A:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getTransactionID()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:validateSubmitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.FileOutputFormat:setOutputCompressorClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org.apache.hadoop.oncrpc.SimpleTcpServer:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.yarn.client.api.impl.TimelineWriter:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getWorkingDirectory()
org.apache.hadoop.hdfs.server.datanode.metrics.OutlierDetector:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:close()
org.apache.hadoop.registry.server.services.RegistryAdminService:instantiateCacheForRegistry()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addTags(java.util.Properties)
org.apache.hadoop.streaming.StreamXmlRecordReader:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.mapred.FileSplit,org.apache.hadoop.mapred.Reporter,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:getPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:serviceStop()
org.apache.hadoop.hdfs.server.namenode.INodeFile:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.fs.shell.CopyCommands$Merge:runAll()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumInMaintenanceDeadDataNodes()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:rollMasterKey()
org.apache.hadoop.fs.adl.Adl:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:requestCreated(com.amazonaws.AmazonWebServiceRequest)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setStoragePolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.AHSClientImpl:serviceStop()
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferServer:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:pauseContainer()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:opt(java.lang.String,int)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:createReleaseCache()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.service.ServiceScheduler:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long)
org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobClient:run(java.lang.String[])
org.apache.hadoop.streaming.StreamInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.http.server.HttpFSReleaseFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:startPeriodic()
org.apache.hadoop.mount.MountdBase:<clinit>()
org.apache.hadoop.yarn.client.cli.RMAdminCLI:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.storage.application.ApplicationTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveCachePoolOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Stat:processRawArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getNNStartedTimeInMillis()
org.apache.hadoop.registry.server.services.RegistryAdminService:zkPathExists(java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$Merge:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.S3AInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:activateApplications()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$13:getUrl()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.datanode.DataStorage:fullyDelete(java.io.File)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkSet(java.lang.String,org.apache.zookeeper.CreateMode,byte[],java.util.List,boolean)
org.apache.hadoop.yarn.api.records.impl.pb.ReservationRequestPBImpl:compareTo(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapred.pipes.PipesReducer:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DeletionService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.SyncableDataOutputStream:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRunApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:nmlogs()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:killAllAppsInQueue(java.lang.String)
org.apache.hadoop.examples.SecondarySort$IntPair:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)
org.apache.hadoop.mapred.pipes.PipesMapRunner:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.ChecksumFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$HealthBlock:getCallerUGI()
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapred.nativetask.serde.FloatWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.fs.sftp.SFTPFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.Hdfs:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:releaseBackupStream(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.yarn.server.router.webapp.NodesBlock:renderPartial()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeNewApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:stopThreads()
org.apache.hadoop.mapred.nativetask.handlers.CombinerHandler:<clinit>()
org.apache.hadoop.yarn.sls.SLSRunner$1:createRMSecretManagerService()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getFileInfo(java.lang.String)
org.apache.hadoop.registry.conf.RegistryConfiguration:addTags(java.util.Properties)
org.apache.hadoop.yarn.server.router.webapp.AppsBlock:getCallerUGI()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:readlink(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:setOutputPath(java.lang.String)
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getStorageBytesWritten()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.webapp.JobsBlock:render(java.lang.Class)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:reencryptEncryptedKeys(java.util.List)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.hdfs.server.federation.store.impl.MountTableStoreImpl:overrideExpiredRecord(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:instantiateCacheForRegistry()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext:stopActiveServices()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:removeAclFeature(int)
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DiskResourceHandler:postComplete(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.client.impl.LeaseRenewer:toString()
org.apache.hadoop.registry.client.binding.RegistryUtils$ServiceRecordMarshal:fromBytes(byte[])
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.PositionedReadable:readFully(long,byte[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:restoreFailedStorage(java.lang.String)
org.apache.hadoop.fs.azure.KeyProvider:getStorageAccountKey(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.SequenceFile$BlockCompressWriter:syncFs()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.yarn.conf.YarnConfiguration:size()
org.apache.hadoop.yarn.server.router.webapp.AppsPage:render(java.lang.Class)
org.apache.hadoop.registry.cli.RegistryCli:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.PmemMappedBlock:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:close()
org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:optLong(java.lang.String,long)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org.apache.hadoop.yarn.server.nodemanager.DeletionService:start()
org.apache.hadoop.hdfs.server.namenode.BackupNode:monitorHealth()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$OnDiskMerger:run()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logEdit(org.apache.hadoop.hdfs.server.namenode.FSEditLogOp)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.mapred.IFileInputStream:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:render()
org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy:<clinit>()
org.apache.hadoop.hdfs.client.HdfsAdmin:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo,java.util.EnumSet)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:decrReserveResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:setAccessTime(long,int,boolean)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setXAttr(java.lang.String,org.apache.hadoop.fs.XAttr,java.util.EnumSet)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher:serviceStart()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:setOutputPath(java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:maybeCreateSuccessMarker(org.apache.hadoop.mapreduce.JobContext,java.util.List,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listReconfigurableProperties()
org.apache.hadoop.mapreduce.util.ProcessTree:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:start()
org.apache.hadoop.hdfs.server.federation.router.Router:shutDown()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setSafeMode(org.apache.hadoop.fs.SafeModeAction,boolean)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:close()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getSingleTaskCounters(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage:render(java.lang.Class)
org.apache.hadoop.yarn.api.impl.pb.client.CsiAdaptorProtocolPBClientImpl:close()
org.apache.hadoop.fs.HarFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.security.KDiag:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:start()
org.apache.hadoop.hdfs.server.datanode.FSCachingGetSpaceUsed$Builder:build()
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.NameNodeStatusMXBean:getSlowDisksReport()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.webapp.SchedulerPageUtil$QueueBlockUtil:renderPartial()
org.apache.hadoop.fs.LocalFileSystem:getUsed()
org.apache.hadoop.fs.shell.AclCommands:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logDisableErasureCodingPolicy(java.lang.String,boolean)
org.apache.hadoop.fs.viewfs.NflyFSystem:getHomeDirectory()
org.apache.hadoop.fs.shell.Display$Checksum:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.filecache.DistributedCache:getArchiveClassPaths(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:closeWriter(org.apache.hadoop.mapreduce.v2.api.records.JobId)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:storeNewApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt)
org.apache.hadoop.mapreduce.v2.app.job.impl.MapTaskImpl:canCommit(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)
org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler:<clinit>()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromBytes(java.lang.String,byte[],java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getHeadroom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager$User,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp)
org.apache.hadoop.fs.azurebfs.services.AbfsInputStreamStatistics:seekBackwards(long)
org.apache.hadoop.registry.conf.RegistryConfiguration:writeXml(java.io.OutputStream)
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:append(java.lang.String,java.lang.String,org.apache.hadoop.io.EnumSetWritable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:updateClusterResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.hdfs.server.namenode.FSEditLog:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:stat(java.lang.String)
org.apache.hadoop.fs.shell.SetReplication:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.map.InverseMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.adl.AdlFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.Router:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.net.unix.DomainSocket:<clinit>()
org.apache.hadoop.mapred.JobConf:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:close()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.ReduceTask:<clinit>()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.hdfs.protocol.OpenFilesIterator:next()
org.apache.hadoop.fs.shell.CopyCommands$Cp:getTargetPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:removeAclFeature(int)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.federation.utils.FederationRegistryClient:getAllApplications()
org.apache.hadoop.crypto.random.OpensslSecureRandom:<init>()
org.apache.hadoop.fs.adl.AdlFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:unsetErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.mapred.CopyCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:unregister()
org.apache.hadoop.mapred.nativetask.serde.IntWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.mapred.ReduceTaskAttemptImpl:resolveHost(java.lang.String)
org.apache.hadoop.yarn.util.AdHocLogDumper:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.net.SocketInputStream:waitForReadable()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setBalancerBandwidth(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$5:run()
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:allocateForDistributedScheduling(org.apache.hadoop.yarn.server.api.protocolrecords.DistributedSchedulingAllocateRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:<init>(org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.TaskAttemptID,org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl,long,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.MapOutputFile,int,boolean)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)
org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent)
org.apache.hadoop.fs.adl.AdlFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:concat(java.lang.String,java.lang.String[])
org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:sourceNext()
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:<clinit>()
org.apache.hadoop.hdfs.server.datanode.web.HostRestrictingAuthorizationFilterHandler:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.yarn.client.AMRMClientUtils$1:run()
org.apache.hadoop.fs.s3a.S3AFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:destroy()
org.apache.hadoop.yarn.service.client.ApiServiceClient:serviceStart()
org.apache.hadoop.yarn.service.ServiceScheduler:close()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor:<clinit>()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:obtainReference()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getDelegationToken(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.lib.output.TextOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.HarFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:recordOutput(org.apache.commons.text.TextStringBuilder,java.lang.String)
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_asyncRead_jmhTest:asyncRead_SingleShotTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.mapred.IFileInputStream:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager:close()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.InputSampler:writePartitionFile(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.lib.InputSampler$Sampler)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:stop()
org.apache.hadoop.fs.shell.Ls$Lsr:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebAppFilter:doFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumStaleNodes()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:serviceStop()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:loadManagerFromEditLog(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:renewDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.RenewDelegationTokenRequest)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:hasCapability(java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSStripedOutputStream:createPacket(int,int,long,long,boolean)
org.apache.hadoop.examples.QuasiMonteCarlo$QmcMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.SWebHdfs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$RawKVIteratorReader:nextRawKey(org.apache.hadoop.io.DataInputBuffer)
org.apache.hadoop.fs.s3a.S3A:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.registry.server.services.MicroZookeeperService:start()
org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:startUpload()
org.apache.hadoop.lib.wsrs.ExceptionProvider:log(javax.ws.rs.core.Response$Status,java.lang.Throwable)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:close()
org.apache.hadoop.fs.shell.Tail:displayError(java.lang.Exception)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappedBlock:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$14:connect(java.net.URL)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,boolean)
org.apache.hadoop.mapreduce.Job:setProfileEnabled(boolean)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DeviceResourceDockerRuntimePluginImpl:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getXAttrs(java.lang.String,java.util.List)
org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo:setMemory(int)
org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.shell.Display$Checksum:processArguments(java.util.LinkedList)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:getPrediction(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobTask(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner:getUrl()
org.apache.hadoop.streaming.LoadTypedBytes:main(java.lang.String[])
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:start()
org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)
org.apache.hadoop.mapred.LocalContainerLauncher:<clinit>()
org.apache.hadoop.fs.http.server.HttpFSServer:getHttpUGI(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.examples.SecondarySort:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodeLabelsPage$NodeLabelsBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock:<init>(org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:serviceStop()
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.sftp.SFTPConnectionPool:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:close()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics)
org.apache.hadoop.mapred.gridmix.SleepJob$SleepMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.streaming.StreamJob:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:incrReserveResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.Counters$FrameworkGroupImpl:incrAllCounters(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:createQueueMetricsForCustomResources()
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCMBean:getRpcClientConnections()
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:chooseRandom(java.lang.String,java.util.Collection)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:render()
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$2:run()
org.apache.hadoop.mapred.JobConf:setNumTasksToExecutePerJvm(int)
org.apache.hadoop.yarn.sls.SLSRunner:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$UnexpectedAMRegisteredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:main(java.lang.String[])
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:start()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.datanode.ReplicaInfo:getDataOutputStream(boolean)
org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService:close()
org.apache.hadoop.mapreduce.tools.CLI:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.LocalFileSystem:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.documentstore.writer.TimelineCollectionWriter:<clinit>()
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:must(java.lang.String,int)
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.mapred.LocatedFileStatusFetcher:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:stop()
org.apache.hadoop.mapred.nativetask.NativeRuntime:registerLibrary(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getQueueUserAclInfo()
org.apache.hadoop.fs.shell.Delete$Rmr:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:run()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:addWriteAccessor(java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.Head:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.AppPlacementAllocator:precheckNode(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.azurebfs.Abfss:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.shell.Display$Checksum:runAll()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:waitFor(java.util.function.Supplier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getInts(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.metrics.GenericEventTypeMetrics:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands:run(java.lang.String[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.lang.String,boolean)
org.apache.hadoop.mapred.MapTaskStatus:statusUpdate(org.apache.hadoop.mapred.TaskStatus)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.INode:<clinit>()
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenIdentifier:getBytes()
org.apache.hadoop.mapreduce.Job:getPriority()
org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:refresh(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.BackupNode:verifyToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,byte[])
org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream:flush(boolean)
org.apache.hadoop.fs.HarFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.DirectTimelineWriter:<clinit>()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:serviceStop()
org.apache.hadoop.minikdc.KerberosSecurityTestcase:startMiniKdc()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:access(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getTotalPendingRequestsPerPartition()
org.apache.hadoop.examples.pi.DistSum$ReduceSide$PartitionMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTrimmedStringCollection(java.lang.String)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:equals(java.lang.Object)
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:serviceStart()
org.apache.hadoop.tools.mapred.CopyOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setMaximumLifetimePerQueue(java.lang.String,long)
org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:validate()
org.apache.hadoop.fs.s3a.S3AInstrumentation:dump(java.lang.String,java.lang.String,java.lang.String,boolean)
org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeOrUpdateAMRMTokenSecretManager(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState,boolean)
org.apache.hadoop.mapred.JobContext:getProfileTaskRange(boolean)
org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$1:run()
org.apache.hadoop.fs.shell.find.Find:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.aliyun.oss.OSS:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.shell.AclCommands:runAll()
org.apache.hadoop.fs.FutureDataInputStreamBuilder:opt(java.lang.String,java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.MRAppMaster:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:createContainerToken(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.yarn.api.records.NodeId,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Priority,long)
org.apache.hadoop.ha.HealthMonitor:<clinit>()
org.apache.hadoop.mapred.gridmix.GridmixKey$Comparator:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.LocalFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore$EntityDeletionThread:run()
org.apache.hadoop.registry.server.services.DeleteCompletionCallback:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:getCurrentKeyId()
org.apache.hadoop.mapred.JobContext:getCombinerClass()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:enterClosedState()
org.apache.hadoop.mapred.Counters:log(org.slf4j.Logger)
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteArrayBlock:close()
org.apache.hadoop.fs.shell.TouchCommands$Touch:displayError(java.lang.Exception)
org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close()
org.apache.hadoop.fs.azurebfs.Abfss:getServerDefaults()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.KeyProviderTokenIssuer:getCanonicalServiceName()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ExitedWithFailureToDoneTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime$NMContainerPolicyUtils:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:createSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:getProxy()
org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.mapred.TaskLogAppender:activateOptions()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJob(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler:<init>(org.apache.hadoop.security.http.RestCsrfPreventionFilter)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:setConfiguredMaxResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.ReduceContext$ValueIterator:mark()
org.apache.hadoop.mapred.MapTask:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.SWebHdfsDtFetcher:addDelegationTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.ipc.RetryCache:<clinit>()
org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC:<clinit>()
org.apache.hadoop.examples.WordStandardDeviation$WordStandardDeviationMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:getValByRegex(java.lang.String)
org.apache.hadoop.streaming.AutoInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:verifyDriverReady()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FifoPolicy:<clinit>()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:enterState(org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState,org.apache.hadoop.fs.s3a.S3ADataBlocks$DataBlock$DestState)
org.apache.hadoop.fs.s3a.InconsistentAmazonS3Client:deleteObject(com.amazonaws.services.s3.model.DeleteObjectRequest)
org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:run()
org.apache.hadoop.fs.aliyun.oss.OSS:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.fs.adl.Adl:getDelegationTokens(java.lang.String)
org.apache.hadoop.registry.server.services.RegistryAdminService:serviceStart()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:stop()
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue:toString()
org.apache.hadoop.fs.sftp.SFTPFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getTaskReport(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskReportRequestProto)
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.Job:addArchiveToClassPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.db.DBInputFormat:getConnection()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logAllowSnapshot(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getUserMetrics(java.lang.String)
org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController$1:run()
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:resetTokenBindingToDT(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:finalize()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[])
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:<clinit>()
org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner:getKeyFieldPartitionerOption(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractManagedParentQueue:<clinit>()
org.apache.hadoop.mapred.gridmix.JobFactory$1:getJobConf()
org.apache.hadoop.hdfs.DFSStripedInputStream:toString()
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl:close()
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:read(byte[])
org.apache.hadoop.io.compress.DirectDecompressionCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.fs.azure.Wasb:getServerDefaults()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:requestContainerResourceChange(org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.crypto.key.kms.server.KMS:getCurrentVersion(java.lang.String)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterPage:render(java.lang.Class)
org.apache.hadoop.fs.shell.SnapshotCommands:runAll()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getDataOutputStream(boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getInts(java.lang.String)
org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthScriptRunner:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.input.SequenceFileAsTextInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getNormalizedResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getTotalCapacityBigInt()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setBalancerBandwidth(long)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:serviceStop()
org.apache.hadoop.fs.shell.Ls$Lsr:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.conf.ConfigurationWithLogging:clear()
org.apache.hadoop.examples.AggregateWordHistogram:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider:<clinit>()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.FSBuilder:build()
org.apache.hadoop.mapreduce.v2.app.webapp.NavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.client.ClientRMProxy:getRMAddress(org.apache.hadoop.yarn.conf.YarnConfiguration,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$AcquiredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.api.impl.pb.client.SCMAdminProtocolPBClientImpl:close()
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:flush()
org.apache.hadoop.yarn.util.FSDownload:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:close()
org.apache.hadoop.tools.mapred.CopyOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:firstStep()
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl:toString()
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:setModificationTime(long,int)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$18:connect(java.net.URL)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:read()
org.apache.hadoop.fs.LocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:start()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumEnteringMaintenanceDataNodes()
org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTasks(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.ClientAMService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)
org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread:run()
org.apache.hadoop.yarn.server.uam.UnmanagedApplicationManager:<clinit>()
org.apache.hadoop.mapreduce.v2.app.TaskHeartbeatHandler:<clinit>()
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.tools.mapred.UniformSizeInputFormat:<clinit>()
org.apache.hadoop.fs.shell.find.Find:runAll()
org.apache.hadoop.yarn.webapp.view.HeaderBlock:renderPartial()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:addIfService(java.lang.Object)
org.apache.hadoop.yarn.service.ClientAMService:start()
org.apache.hadoop.fs.DFCachingGetSpaceUsed:init()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:reserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerLeafQueueInfo:getChildQueues(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)
org.apache.hadoop.mapred.RunningJob:setJobPriority(java.lang.String)
org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1:postStart()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:unsetErasureCodingPolicy(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.RMInfoMXBean:isSecurityEnabled()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.SWebHdfs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:<clinit>()
org.apache.hadoop.mapred.JobClient:getQueues()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock:render()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listOpenFiles(long,java.util.EnumSet,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:<clinit>()
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:getMountTableEntries(org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesRequest)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:setAcl(java.lang.String,java.util.List)
org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:conf()
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshUserToGroupsMappings()
org.apache.hadoop.mapred.lib.LazyOutputFormat:getRecordWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf,java.lang.String,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:close()
org.apache.hadoop.fs.Hdfs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:serviceStop()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:close()
org.apache.hadoop.fs.azurebfs.Abfs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.client.HdfsAdmin:allowSnapshot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webapp.LogWebService$1:getHttpURLConnection(java.net.URL)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.subapplication.SubApplicationTableRW:getResultScanner(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Scan)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.fs.cosn.CosN:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.mapred.jobcontrol.Job:killJob()
org.apache.hadoop.mapred.JobConf:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobTaskAttemptIdCounters(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:serviceStart()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:createKey()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:errorsAndWarnings()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.yarn.server.federation.failover.FederationProxyProviderUtil:<clinit>()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.hdfs.server.namenode.BackupState:setStateInternal(org.apache.hadoop.hdfs.server.namenode.ha.HAContext,org.apache.hadoop.hdfs.server.namenode.ha.HAState)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterAdminServer:stop()
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:flush()
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodeLabelsPage$NodeLabelsBlock:renderPartial()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:killJob(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillJobRequestProto)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createResilientCommitSupport(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalReleaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.api.records.impl.pb.ReservationAllocationStatePBImpl:equals(java.lang.Object)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock:render(java.lang.Class)
org.apache.hadoop.fs.azurebfs.Abfss:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setStoragePolicy(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.sls.synthetic.SynthTraceJobProducer:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:serviceStop()
org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpsFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:sortCSConfigurations()
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,long)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.Head:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:serviceStop()
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger:startMerge(java.util.Set)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getContainer(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.mapred.gridmix.JobFactory$1:getInputSplits()
org.apache.hadoop.mapred.JobContext:getSortComparator()
org.apache.hadoop.oncrpc.RpcProgram:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.NavBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.fs.shell.find.Find:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler:start()
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:<clinit>()
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DeletionTask:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitorManager:<clinit>()
org.apache.hadoop.hdfs.web.AuthFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getDatanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:computeContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairSchedulerPlanFollower:<clinit>()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)
org.apache.hadoop.hdfs.DFSStripedInputStream:close()
org.apache.hadoop.streaming.HadoopStreaming:main(java.lang.String[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$XmlOutputter:save(java.io.File)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:calculateAndGetAMResourceLimit()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource:<clinit>()
org.apache.hadoop.yarn.client.cli.NodeAttributesCLI$CommandHandler:handleCommand(org.apache.commons.cli.CommandLine)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService:stop()
org.apache.hadoop.fs.MultipartUploaderBuilder:must(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:<init>(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:maybeCreateSuccessMarkerFromCommits(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getServiceRpcServerBindHost(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,double)
org.apache.hadoop.portmap.Portmap:<clinit>()
org.apache.hadoop.fs.cosn.BufferPool:<clinit>()
org.apache.hadoop.mapred.JobConf:getMaxVirtualMemoryForTask()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getGroupsForUser(java.lang.String)
org.apache.hadoop.mapred.ReduceTask$SkippingReduceValuesIterator:next()
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:isStarved()
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:logCurrentHadoopUser()
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:close()
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:create(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.shell.Mkdir:expandArgument(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.protocol.NamenodeHeartbeatRequest:newInstance(org.apache.hadoop.hdfs.server.federation.store.records.MembershipState)
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseStorageMonitor:stop()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManager:getAMContainer(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.local.RawLocalFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:buildResourcePath(java.lang.String)
org.apache.hadoop.ha.FailoverController:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listReencryptionStatus(long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:getCallerUGI()
org.apache.hadoop.fs.viewfs.NflyFSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage:render(java.lang.Class)
org.apache.hadoop.hdfs.server.federation.store.protocol.GetDestinationRequest:newInstance(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:copyHdfsFileToLocal(org.apache.hadoop.fs.Path,java.io.File)
org.apache.hadoop.fs.LocalFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsAttemptsPage$FewAttemptsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.examples.DBCountPageView$PageviewMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$UnmanagedAMAttemptSavedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close()
org.apache.hadoop.yarn.server.nodemanager.NodeManager:createNewNodeManager()
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.gridmix.JobFactory$1:getTaskInfo(org.apache.hadoop.mapreduce.TaskType,int)
org.apache.hadoop.io.BloomMapFile$Writer:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:getMetadata(java.lang.String)
org.apache.hadoop.fs.HarFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.NodeManager:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:close()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumStaleDataNodes()
org.apache.hadoop.util.XMLUtils:<clinit>()
org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault()
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:getNodeList(java.lang.String)
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.ipc.WritableRpcEngine$Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics:<clinit>()
org.apache.hadoop.fs.http.server.HttpFSExceptionProvider:<clinit>()
org.apache.hadoop.mapred.uploader.FrameworkUploader:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:allocateResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewNavBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.placement.PlacementFactory:<clinit>()
org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider:<clinit>()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getUsedCapacity()
org.apache.hadoop.fs.sftp.SFTPFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.datanode.DataNode:main(java.lang.String[])
org.apache.hadoop.fs.http.HttpFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.crypto.key.kms.server.KMSACLs:<clinit>()
org.apache.hadoop.yarn.webapp.Router:<clinit>()
org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:updateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.fs.s3a.FailureInjectionPolicy:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:<clinit>()
org.apache.hadoop.security.token.Token$PrivateToken:cancel(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3AInputStream:read()
org.apache.hadoop.conf.ConfigurationWithLogging:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService:writeAsync(org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.router.rmadmin.RouterRMAdminService:stop()
org.apache.hadoop.fs.http.server.HttpFSAuthenticationFilter:destroy()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getMaximumResourceCapability()
org.apache.hadoop.ha.PowerShellFencer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getAllocatedResources()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$RandomTextDataMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.LeaseManager:getINodeWithLeases()
org.apache.hadoop.yarn.server.resourcemanager.placement.PrimaryGroupPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:<init>(java.util.Properties,org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.ConversionOptions)
org.apache.hadoop.fs.http.HttpsFileSystem:getHomeDirectory()
org.apache.hadoop.fs.viewfs.NflyFSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSync()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:finishApp(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.tools.GetConf$PrintConfKeyCommandHandler:doWorkInternal(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.yarn.server.timelineservice.storage.reader.FlowActivityEntityReader:readEntities(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:setAvailableResourcesToUser(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:getToken()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.webapp.example.HelloWorld:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.service.ServiceMaster:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:selectInputStreams(long,long,org.apache.hadoop.hdfs.server.namenode.MetaRecoveryContext,boolean,boolean)
org.apache.hadoop.yarn.server.federation.store.records.SubClusterState:<clinit>()
org.apache.hadoop.hdfs.DFSUtil:<clinit>()
org.apache.hadoop.fs.Hdfs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getPendingResources()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:start()
org.apache.hadoop.fs.cosn.CosNFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenManager:close()
org.apache.hadoop.hdfs.server.federation.resolver.MultipleDestinationMountTableResolver:lookupLocation(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.examples.RandomWriter$RandomMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.examples.DBCountPageView:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.GpuDeviceInformationParser:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getBoolean(java.lang.String,boolean)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:scanForSpeculations()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:buildSecurityDiagnostics()
org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.fs.s3a.impl.RenameOperation:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:getServerDefaults()
org.apache.hadoop.mapreduce.Job:<init>()
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getSpillIndexFile(int)
org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.cosn.CosNFileSystem:processDeleteOnExit()
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,long)
org.apache.hadoop.hdfs.server.federation.store.impl.DisabledNameserviceStoreImpl:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult)
org.apache.hadoop.fs.ftp.FtpFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.appmaster.AMSimulator$3:run()
org.apache.hadoop.yarn.server.timelineservice.storage.domain.DomainTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getInts(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseStorage4Block(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,java.util.List,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.mapred.pipes.DownwardProtocol:setJobConf(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.io.LongWritable$DecreasingComparator:newKey()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$25:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:recoverLease(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.http.CrossOriginFilter:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refreshSuperUserGroupsConfiguration()
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:serviceStart()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getEditLogManifest(long)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getMaximumResourceCapability()
org.apache.hadoop.streaming.PipeMapper:waitOutputThreads()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSorter:serviceStart()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.io.compress.SplittableCompressionCodec:getDecompressorType()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:finalize()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.utils.DateTimeUtils:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeLabelsProvider:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.util.DataChecksum:<clinit>()
org.apache.hadoop.mapred.JobContext:getProfileEnabled()
org.apache.hadoop.fs.cosn.CosNFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.examples.AggregateWordCount$WordCountPlugInClass:configure(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getDataNodeStats()
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.io.file.tfile.TFile:<clinit>()
org.apache.hadoop.yarn.conf.YarnConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.utils.JsonSerDeser:<clinit>()
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.IntraQueuePreemptionComputePlugin:getResourceDemandFromAppsPerQueue(java.lang.String,java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:getDouble(java.lang.String,double)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:removeAclFeature(int)
org.apache.hadoop.registry.server.dns.ServiceRecordProcessor:manageDNSRecords(org.apache.hadoop.registry.server.dns.RegistryDNS$RegistryCommand)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$ParentQueueBlock:getCallerUGI()
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.shell.Ls$Lsr:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.security.YarnAuthorizationProvider:destroy()
org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.hdfs.server.namenode.JournalSet:<clinit>()
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:skipUntil(long)
org.apache.hadoop.hdfs.server.datanode.BlockPoolManager$1:run()
org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider:close()
org.apache.hadoop.yarn.server.timeline.LogInfo:<clinit>()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobs(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.hdfs.server.balancer.Dispatcher:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.checker.TimeoutFuture:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.HarFileSystem:getTrashRoots(boolean)
org.apache.hadoop.hdfs.DFSStripedOutputStream:closeImpl()
org.apache.hadoop.security.authentication.server.AuthenticationFilter:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:teardown()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:getCommittedTaskPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.oncrpc.SimpleTcpClientHandler:channelActive(io.netty.channel.ChannelHandlerContext)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:read(java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:logAndThrowInvalidInputException(org.slf4j.Logger,java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileSystemImpl:remove(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord)
org.apache.hadoop.fs.s3a.S3A:finalize()
org.apache.hadoop.yarn.server.scheduler.ResourceRequestSet:addAndOverrideRRSet(org.apache.hadoop.yarn.server.scheduler.ResourceRequestSet)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:getAppAttempt(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:skip(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapred.JobConf:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.FsShellPermissions$Chmod:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:main(java.lang.String[])
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:releaseContainers(java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.ftp.FTPFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:serviceStart()
org.apache.hadoop.yarn.util.resource.Resources$FixedValueResource:getResourceInformation(int)
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:formatSchedulerConfiguration(javax.servlet.http.HttpServletRequest)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:saveChild2Snapshot(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,org.apache.hadoop.hdfs.server.namenode.INode,int,org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapred.RunningJob:killTask(org.apache.hadoop.mapred.TaskAttemptID,boolean)
org.apache.hadoop.hdfs.client.HdfsAdmin:unsetErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer:main(java.lang.String[])
org.apache.hadoop.mapred.Counters$GenericGroup:incrAllCounters(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:container()
org.apache.hadoop.mapreduce.Job:addFileToClassPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode:transitionToStandby()
org.apache.hadoop.mapreduce.lib.input.LineRecordReader:<clinit>()
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.registry.server.services.AddingCompositeService:start()
org.apache.hadoop.yarn.server.utils.BuilderUtils:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addSample(java.lang.String,long)
org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl:setResourceInformation(int,org.apache.hadoop.yarn.api.records.ResourceInformation)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.fs.ftp.FtpFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshUserToGroupsMappings(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshUserToGroupsMappingsRequest)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceStop()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:renewLease(java.lang.String)
org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter:start()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity$UgiInfo:toString()
org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.fs.adl.AdlFileSystem:getServerDefaults()
org.apache.hadoop.fs.azure.WasbRemoteCallHelper:<clinit>()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$UpdateMasterKeyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl:<clinit>()
org.apache.hadoop.yarn.client.api.impl.FileSystemTimelineWriter$LogFDsCache:close()
org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType)
org.apache.hadoop.io.BloomMapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor$WarningSpan:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.DryRunResultHolder:<clinit>()
org.apache.hadoop.io.IntWritable:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:removeXAttrFeature(int)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:getVolumeMap(java.lang.String,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReplicaMap,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker)
org.apache.hadoop.hdfs.client.HdfsAdmin:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.TouchCommands$Touch:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.oncrpc.RpcUtil$RpcFrameDecoder:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.LocalFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RemoveXAttrOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getNextSPSPath()
org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:listManifests()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.reservation.PeriodicRLESparseResourceAllocation:getMaximumPeriodicCapacity(long,long)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:reserveResource(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:opt(java.lang.String,long)
org.apache.hadoop.mapred.nativetask.serde.LongWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.yarn.server.utils.BuilderUtils:newContainerLaunchContext(java.util.Map,java.util.Map,java.util.List,java.util.Map,java.nio.ByteBuffer,java.util.Map)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increaseAggregatedPreemptedSeconds(org.apache.hadoop.yarn.api.records.Resource,long)
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ProvidedVolumeImpl:newBlockIterator(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:unreserve(org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.NavBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:stop()
org.apache.hadoop.fs.Hdfs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.JobClient:getCleanupTaskReports(org.apache.hadoop.mapred.JobID)
org.apache.hadoop.mapred.JobConf:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:decPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.Trash:<clinit>()
org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler:initializeState(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:serviceStart()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:stop()
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:hflush()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.mapred.gridmix.LoadJob:<clinit>()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:renderPartial()
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:close()
org.apache.hadoop.fs.impl.prefetch.ResourcePool:release(java.lang.Object)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:runAll()
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.HarFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.LocatedBlocksRefresher:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:updateResourceForReleasedContainer(org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadSortComparator:newKey()
org.apache.hadoop.mapred.join.ComposableRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.LeaseManager:runLeaseChecks()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFs:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeFile:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.yarn.webapp.Dispatcher:service(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.Counters$FrameworkGroupImpl:addCounter(org.apache.hadoop.mapreduce.Counter)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:closeFSStore()
org.apache.hadoop.fs.azurebfs.Abfs:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:updateBlockForPipeline(org.apache.hadoop.hdfs.protocol.ExtendedBlock,java.lang.String)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:close()
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,java.lang.String)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.client.HdfsAdmin:getErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.hdfs.server.datanode.web.webhdfs.WebHdfsHandler:exceptionCaught(io.netty.channel.ChannelHandlerContext,java.lang.Throwable)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$17:getUrl()
org.apache.hadoop.registry.server.services.MicroZookeeperService:stop()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.mapred.RunningJob:getJobStatus()
org.apache.hadoop.mapreduce.filecache.DistributedCache:getFileClassPaths(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.portmap.RpcProgramPortmap:channelRead(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMContainerBlock:renderPartial()
org.apache.hadoop.fs.azurebfs.Abfs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMContainerCrashedAtRunningTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.impl.EntryFileIO:createReader(java.io.File)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:getInputStream()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:setAvailableResourcesToQueue(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.router.webapp.RouterController:echo()
org.apache.hadoop.mapred.ShuffleHandler$Shuffle:exceptionCaught(org.jboss.netty.channel.ChannelHandlerContext,org.jboss.netty.channel.ExceptionEvent)
org.apache.hadoop.mapred.nativetask.Platforms:<clinit>()
org.apache.hadoop.hdfs.server.federation.store.protocol.GetRouterRegistrationRequest:newInstance(java.lang.String)
org.apache.hadoop.tools.mapred.CopyCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:prepareContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerPrepareContext)
org.apache.hadoop.hdfs.client.HdfsAdmin:getKeyProvider()
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.lang.String)
org.apache.hadoop.yarn.conf.YarnConfiguration:getProps()
org.apache.hadoop.fs.viewfs.ViewFileSystem$1$1:apply(java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:saveNamespace(long,long)
org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.Hdfs:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalRebootTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:close()
org.apache.hadoop.yarn.server.nodemanager.NodeManager:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.deviceframework.DevicePluginAdapter:<clinit>()
org.apache.hadoop.fs.shell.find.Name$Iname:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.JobConf:getKeyFieldPartitionerOption()
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:serviceStart()
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<clinit>()
org.apache.hadoop.fs.http.server.HttpFSServerWebServer:stop()
org.apache.hadoop.fs.s3a.impl.StoreContext:decrementGauge(org.apache.hadoop.fs.s3a.Statistic,long)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.yarn.server.resourcemanager.ClientRMService:serviceStop()
org.apache.hadoop.fs.http.HttpsFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManager:rollbackLastReInitialization(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.yarn.server.federation.policies.router.LocalityRouterPolicy:getHomeSubcluster(org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:init(org.apache.hadoop.yarn.util.Clock,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,java.util.Collection)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:searchChild(org.apache.hadoop.hdfs.server.namenode.INode)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:<clinit>()
org.apache.hadoop.contrib.utils.join.DataJoinReducerBase:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:hasAccess(org.apache.hadoop.yarn.api.records.QueueACL,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.tools.DistCp:<clinit>()
org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getDefaultBlockSize()
org.apache.hadoop.fs.azurebfs.oauth2.LocalIdentityTransformer:transformIdentityForGetRequest(java.lang.String,boolean,java.lang.String)
org.apache.hadoop.applications.mawo.server.common.TaskStatus:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.registry.conf.RegistryConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.mapred.ResourceMgrDelegate:<clinit>()
org.apache.hadoop.crypto.key.CachingKeyProvider:getKeyVersions(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.OutboundBandwidthResourceHandler:teardown()
org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheUploaderService:stop()
org.apache.hadoop.registry.server.dns.RegistryDNS:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.web.KerberosUgiAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.service.launcher.ServiceLauncher:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowApps(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromInstance(java.lang.Object)
org.apache.hadoop.yarn.sls.resourcemanager.MockAMLauncher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.BlockReader:readAll(byte[],int,int)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.mapred.Counters$FrameworkGroupImpl:addCounter(java.lang.String,java.lang.String,long)
org.apache.hadoop.mapred.nativetask.INativeHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:sendHeartbeat(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary,boolean,org.apache.hadoop.hdfs.server.protocol.SlowPeerReports,org.apache.hadoop.hdfs.server.protocol.SlowDiskReports)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMWebServer:serviceStart()
org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:nextKeyValue()
org.apache.hadoop.fs.shell.Stat:run(java.lang.String[])
org.apache.hadoop.fs.adl.AdlFsInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.services.SharedKeyCredentials:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$2:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobs(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.NameCache:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:close()
org.apache.hadoop.hdfs.server.namenode.TransferFsImage:<clinit>()
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTaskAttempts(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager$CachedResolver:close()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl:serviceStart()
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)
org.apache.hadoop.metrics2.sink.GraphiteSink:<clinit>()
org.apache.hadoop.ipc.RpcMultiplexer:getAndAdvanceCurrentIndex()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getFileInfo(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice$AddReplicaProcessor:compute()
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService:serviceStop()
org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys$1:removeEldestEntry(java.util.Map$Entry)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumInMaintenanceLiveDataNodes()
org.apache.hadoop.conf.ConfigurationWithLogging:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationHistoryProtocolPBClientImpl:close()
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:getProxyAddresses(java.net.URI,java.lang.String)
org.apache.hadoop.mapred.JobClient:getTaskOutputFilter(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:resetSchedulerMetrics()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.Schedulable:assignContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode)
org.apache.hadoop.mapred.LineRecordReader$LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$AllocationReloadListener:onCheck()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:must(java.lang.String,double)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerLauncherRouter:start()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromBytes(java.lang.String,byte[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl:hashCode()
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:close()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:getTargetPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)
org.apache.hadoop.registry.client.api.DNSOperations:close()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:allocate(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.fs.shell.Display$Text:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.client.AutoRefreshNoHARMFailoverProxyProvider:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.client.RMProxy,java.lang.Class)
org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner:setLeftOffset(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.hdfs.server.federation.router.Router:close()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AttemptFailedFinalStateSavedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.azure.AzureNativeFileSystemStore:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.JobHistory:serviceStart()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRemoveXAttrs(java.lang.String,java.util.List,boolean)
org.apache.hadoop.hdfs.nfs.nfs3.Nfs3HttpServer:getServerURI()
org.apache.hadoop.fs.viewfs.ViewFs:getCanonicalServiceName()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.tools.GetGroups:getUgmProtocol()
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:setOutputPath(java.lang.String)
org.apache.hadoop.mapred.lib.db.DBConfiguration:getInputCountQuery()
org.apache.hadoop.io.compress.CompressionCodecFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalReleaseResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:recoverUnclosedStreams()
org.apache.hadoop.hdfs.server.diskbalancer.command.QueryCommand:create(java.lang.String)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler$UpdateThread:run()
org.apache.hadoop.fs.cosn.CosNFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setFloat(java.lang.String,float)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increasePending(org.apache.hadoop.yarn.api.records.Resource,int)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:commitJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.examples.MultiFileWordCount$CombineFileLineRecordReader:<init>(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.Integer)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.conf.ConfigurationWithLogging:writeXml(java.io.OutputStream)
org.apache.hadoop.util.HostsFileReader:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.compress.DeflateCodec:createOutputStream(java.io.OutputStream)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.OutboundBandwidthResourceHandler:preStart(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapreduce.v2.hs.HistoryContext:getAllJobs()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueResourceQuotas:setEffectiveMinResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getQuotaUsage(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:deductUnallocatedResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:runAll()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:close()
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getInputStreamForFile()
org.apache.hadoop.fs.HarFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.LightWeightResizableGSet:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.logaggregation.tracker.NMLogAggregationStatusTracker:<clinit>()
org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher$2:run()
org.apache.hadoop.hdfs.server.namenode.ha.WrappedFailoverProxyProvider:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI)
org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$RollingUpgradeStartOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalDecrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.HarFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TruncateOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(byte[],int,int)
org.apache.hadoop.registry.server.services.RegistryAdminService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock:renderPartial()
org.apache.hadoop.hdfs.server.namenode.BackupNode:loginAsNameNodeUser(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.commit.files.PersistentCommitData:saveFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.s3a.commit.files.PersistentCommitData,org.apache.hadoop.util.JsonSerialization,boolean)
org.apache.hadoop.registry.conf.RegistryConfiguration:onlyKeyExists(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:updateModificationTime(long,int)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getNodeContainer(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.mapred.gridmix.Statistics:<clinit>()
org.apache.hadoop.fs.HarFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.placement.RejectPlacementRule:setConfig(java.lang.Object)
org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:incrReserveResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.FsShell$Help:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:close()
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMOverviewPage$SCMOverviewBlock:getCallerUGI()
org.apache.hadoop.streaming.StreamXmlRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.shell.MoveCommands$Rename:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumMd5CrcReconstructor:initDecoderIfNecessary()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:preValidateMoveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getSnapshotDiffReport(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:read(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:getEffectiveCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:cleanupStagingDirs()
org.apache.hadoop.hdfs.server.namenode.INodeFile:setUser(java.lang.String,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.LoadJob:buildSplits(org.apache.hadoop.mapred.gridmix.FilePool)
org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl:isProcessTreeOverLimit(org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree,java.lang.String,long)
org.apache.hadoop.crypto.key.kms.ValueQueue$2:run()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:onClose(org.eclipse.jetty.websocket.api.Session,int,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:getResults(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.filter.FilterList)
org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close()
org.apache.hadoop.mapred.JobClient:setTaskOutputFilter(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.JobClient$TaskStatusFilter)
org.apache.hadoop.conf.Configuration:main(java.lang.String[])
org.apache.hadoop.mapred.join.OuterJoinRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppAttemptBlock:createTablesForAttemptMetrics(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getInotifyEventStream()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.examples.WordCount$TokenizerMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ftp.FTPFileSystem:getCanonicalServiceName()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:setContainerCompletedStatus(int)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getProvidedSpace()
org.apache.hadoop.mapreduce.v2.app.webapp.SingleCounterBlock:renderPartial()
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:startAllocatorThread()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.NflyFSystem:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.S3A:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:readFully(long,byte[])
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:<clinit>()
org.apache.hadoop.mapreduce.Job:setCancelDelegationTokenUponJobCompletion(boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:write(int)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setErasureCodingPolicy(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:setPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.cosn.CosNFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.GreedyReservationAgent:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getJobTasks(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<clinit>()
org.apache.hadoop.fs.SWebHdfs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:addQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue)
org.apache.hadoop.mapred.FixedLengthInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:deleteBlockData()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:updateApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsNavBlock:render()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:serviceStart()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSStripedOutputStream:writeChunk(byte[],int,int,byte[],int,int)
org.apache.hadoop.streaming.PipeCombiner:waitOutputThreads()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:reacquireContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerReacquisitionContext)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:resetSchedulerMetrics()
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:start()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRenewDelegationToken(org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier,long)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.fs.WebHdfs:getHomeDirectory()
org.apache.hadoop.examples.dancing.DistributedPentomino:main(java.lang.String[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:validateAndGetSchedulerConfiguration(org.apache.hadoop.yarn.webapp.dao.SchedConfUpdateInfo,javax.servlet.http.HttpServletRequest)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:initializePlan(java.lang.String)
org.apache.hadoop.lib.servlet.HostnameFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:enableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.mapreduce.task.reduce.MergeThread:<clinit>()
org.apache.hadoop.yarn.server.webproxy.WebAppProxy:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.webapp.view.HtmlPage$Page:subView(java.lang.Class)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readFully(long,byte[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.filecache.DistributedCache:setCacheArchives(java.net.URI[],org.apache.hadoop.conf.Configuration)
org.apache.hadoop.service.launcher.IrqHandler:<clinit>()
org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader:<clinit>()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.tools.GetGroups:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands:runAll()
org.apache.hadoop.yarn.util.DockerClientConfigHandler:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:checkForDecommissioningNodes(org.apache.hadoop.yarn.server.api.protocolrecords.CheckForDecommissioningNodesRequest)
org.apache.hadoop.fs.local.LocalFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.hdfs.DFSClient:create(java.lang.String,boolean,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DisableErasureCodingPolicyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:getDataNodeProxy(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:cleanSubtree(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,int,int)
org.apache.hadoop.lib.service.hadoop.FileSystemAccessService$3:run()
org.apache.hadoop.fs.ftp.FtpFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.common.BaseTableRW:createTable(org.apache.hadoop.hbase.client.Admin,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.router.RouterFsck:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:dumpRegistryRobustly(boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:serviceStop()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.mapred.JobConf:setUseNewReducer(boolean)
org.apache.hadoop.security.IdMappingServiceProvider:getGid(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getProcessId(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.hdfs.server.namenode.BackupNode:reconfigureSlowNodesParameters(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:enterClosedState()
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getKeyProviderUri()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:activeQueue()
org.apache.hadoop.mapred.ShuffleHandler:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processArguments(java.util.LinkedList)
org.apache.hadoop.mapred.IFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.mapred.Counters$Counter)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setReservationAgent(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.utils.ClientRegistryBinder:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseDataNode(java.lang.String,java.util.Collection,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.HarFileSystem:getCanonicalServiceName()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:initMaximumResourceCapability(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:hashCode()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:start()
org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.io.InputStream)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.cli.TopCLI:main(java.lang.String[])
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:removeXAttrFeature(int)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand)
org.apache.hadoop.registry.server.services.AddingCompositeService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:stop()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<clinit>()
org.apache.hadoop.registry.client.binding.RegistryUtils:listServiceRecords(org.apache.hadoop.registry.client.api.RegistryOperations,java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:main(java.lang.String[])
org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagementProtocolPBClientImpl:close()
org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:incPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:preValidateMoveApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.oncrpc.SimpleUdpServer:<clinit>()
org.apache.hadoop.fs.WebHdfs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.SLSRunner$1:createQueueACLsManager(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.azurebfs.security.AbfsDelegationTokenIdentifier:<init>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:render()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listCachePools(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination:<clinit>()
org.apache.hadoop.fs.FSInputChecker:read(long,byte[],int,int)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.CopyCommands$Cp:expandArgument(java.lang.String)
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBRecordReader:nextKeyValue()
org.apache.hadoop.fs.sftp.SFTPFileSystem:getDefaultBlockSize()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:handleContainerExitWithFailure(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.fs.Path,java.lang.StringBuilder)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.client.api.AMRMClient:registerApplicationMaster(java.lang.String,int,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:hasRollbackFSImage()
org.apache.hadoop.mapred.jobcontrol.Job:setJobName(java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:loadManifest(org.apache.hadoop.fs.FileStatus)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logFinalizeRollingUpgrade(long)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:fsyncDirectory()
org.apache.hadoop.fs.shell.FsUsage$Dus:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.protocolPB.JournalProtocolTranslatorPB:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy$FairComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.HttpFSServer:<init>()
org.apache.hadoop.fs.FsShell$Usage:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupImage:doUpgrade(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl:hashCode()
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:setAccessTime(long,int,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:getRemoteDestination(java.util.LinkedList)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream:flush()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.provider.defaultImpl.DefaultProviderService:buildContainerLaunchContext(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.yarn.service.api.records.Service,org.apache.hadoop.yarn.service.component.instance.ComponentInstance,org.apache.hadoop.yarn.service.utils.SliderFileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Container,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext)
org.apache.hadoop.fs.Hdfs:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.server.datanode.BlockSender:<clinit>()
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock:<init>(org.apache.hadoop.yarn.webapp.View$ViewContext,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenIdentifier:getBytes()
org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.viewfs.ViewFs$3:next()
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:initFileSystem(java.net.URI)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:deleteRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.DeleteOpParam,org.apache.hadoop.hdfs.web.resources.RecursiveParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam)
org.apache.hadoop.mapreduce.lib.output.NamedCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.SWebHdfs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.ActivitiesManager:close()
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.azure.BlockBlobInputStream:readFully(long,byte[])
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:zkDelete(java.lang.String,boolean,org.apache.curator.framework.api.BackgroundCallback)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapred.lib.db.DBOutputFormat$DBRecordWriter:close(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier:getUser()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:initJournalsForWrite()
org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore:<clinit>()
org.apache.hadoop.fs.shell.Delete$Rmdir:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:buildResourcePath(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:verifyBlockPlacement(org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int)
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:close()
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileBaseImpl:<clinit>()
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream:abort()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.SerialJobFactory:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromJson(java.lang.String)
org.apache.hadoop.mapred.join.CompositeRecordReader:skip(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMErrorsAndWarningsPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:stop()
org.apache.hadoop.resourceestimator.skylinestore.api.SkylineStore:addEstimation(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$CompletedResourcesIterator:hasNext()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:setPinning(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TempAppPerPartition:assignPreemption(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.adl.AdlFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.examples.dancing.OneSidedPentomino:main(java.lang.String[])
org.apache.hadoop.util.Classpath:main(java.lang.String[])
org.apache.hadoop.io.ArrayFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:serviceStart()
org.apache.hadoop.fs.Hdfs$1:hasNext()
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo()
org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)
org.apache.hadoop.io.erasurecode.CodecRegistry:<clinit>()
org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB:<clinit>()
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:toString()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:teardown()
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext:stopStandbyServices()
org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.ipc.YarnRPC:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:close()
org.apache.hadoop.yarn.service.ServiceScheduler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.Hdfs:getHomeDirectory()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:runAll()
org.apache.hadoop.mapreduce.Counters:findCounter(java.lang.String,org.apache.hadoop.mapreduce.FileSystemCounter)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.PlanCommand:getNodes(java.lang.String)
org.apache.hadoop.yarn.server.api.impl.pb.client.DistributedSchedulingAMProtocolPBClientImpl:close()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getBatchedListing(java.lang.String[],byte[],boolean)
org.apache.hadoop.hdfs.WebHdfsDtFetcher:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupJobStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.StripedDataStreamer:waitAndQueuePacket(org.apache.hadoop.hdfs.DFSPacket)
org.apache.hadoop.yarn.conf.YarnConfiguration:write(java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo:getSchedulerResourceTypes()
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KilledForReInitializationTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.util.ThreadUtil:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close()
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$LeafQueueBlock:renderPartial()
org.apache.hadoop.tools.util.ProducerConsumer:blockingTake()
org.apache.hadoop.fs.FileUtil:fullyDelete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.JobConf:getJarUnpackPattern()
org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.DFSStripedInputStream:reportLostBlock(org.apache.hadoop.hdfs.protocol.LocatedBlock,java.util.Collection)
org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm:<clinit>()
org.apache.hadoop.fs.LocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.Command:<clinit>()
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:bindToFileSystem(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalAllocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.monitor.RMAppLifetimeMonitor:<clinit>()
org.apache.hadoop.examples.BaileyBorweinPlouffe$BbpReducer:cleanup(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getMaximumApplicationLifetime(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueuesBlock:render()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$21:getUrl()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$CloseOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobBlock:renderPartial()
org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.TaskReport:getCounters()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeCacheDirective(long)
org.apache.hadoop.mapred.JobContext:getOutputFormatClass()
org.apache.hadoop.hdfs.server.diskbalancer.command.HelpCommand:create(java.lang.String)
org.apache.hadoop.util.PerformanceAdvisory:<clinit>()
org.apache.hadoop.mapred.nativetask.serde.IKVSerializer:updateLength(org.apache.hadoop.mapred.nativetask.util.SizedWritable,org.apache.hadoop.mapred.nativetask.util.SizedWritable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:updateNodeToLabelsMappings(java.util.Map)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:<init>(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler,org.apache.hadoop.yarn.util.Clock)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:metaSave(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FileSystemImage:main(java.lang.String[])
org.apache.hadoop.fs.s3a.S3A:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueueBlock:getCallerUGI()
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:close()
org.apache.hadoop.security.authentication.util.KerberosName:<clinit>()
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTaskAttemptId(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:updateApplicationTimeouts(org.apache.hadoop.yarn.api.protocolrecords.UpdateApplicationTimeoutsRequest)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getQueueInfo(org.apache.hadoop.yarn.api.protocolrecords.GetQueueInfoRequest)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:close()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ExpiredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:recoverDrainingState()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setInstantaneousMaxCapacity(java.lang.String,float)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:must(java.lang.String,boolean)
org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator:close()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getJobTask(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$8:connect(java.net.URL)
org.apache.hadoop.mapred.ReduceTaskStatus:statusUpdate(org.apache.hadoop.mapred.TaskStatus)
org.apache.hadoop.mapred.ShuffleHandler$Shuffle$1:load(java.lang.Object)
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.mapreduce.v2.app.webapp.JobPage:render(java.lang.Class)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.tools.mapred.CopyMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.tools.dynamometer.Client:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:run(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:incReservedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.Mkdir:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:reinitialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.RMContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FairOrderingPolicy:reorderSchedulableEntity(org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.SchedulableEntity)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getApp(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.CSQueueMetricsForCustomResources:increaseAggregatedPreempted(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.cosn.CosNFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.service.ServiceMaster:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:<clinit>()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:allocateContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,boolean)
org.apache.hadoop.fs.shell.Delete$Rmr:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getRunCommandForOther(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.metrics.RBFMetrics:<clinit>()
org.apache.hadoop.fs.azure.WasbFsck:main(java.lang.String[])
org.apache.hadoop.fs.azure.Wasbs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.examples.terasort.TeraChecksum:main(java.lang.String[])
org.apache.hadoop.registry.server.dns.ReverseZoneUtils:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:serviceStart()
org.apache.hadoop.mapred.LineRecordReader$LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listReconfigurableProperties()
org.apache.hadoop.mapred.JobConf:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream:nextValidOp()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:createContainerLogDirs(java.lang.String,java.lang.String,java.util.List,java.lang.String)
org.apache.hadoop.fs.s3a.impl.MkdirOperation:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getNormalizedResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getUsed()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.fs.Hdfs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.nfs.NfsExports:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getDelegationToken(java.lang.String)
org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:getDomain(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:close()
org.apache.hadoop.yarn.server.router.Router:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager:<clinit>()
org.apache.hadoop.fs.WebHdfs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.fieldsel.FieldSelectionMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:createRestartMetaStream()
org.apache.hadoop.fs.s3a.impl.CallableSupplier:<clinit>()
org.apache.hadoop.mapred.lib.db.DBConfiguration:getOutputFieldNames()
org.apache.hadoop.mapred.JobConf:setMaxPhysicalMemoryForTask(long)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setOverCommitTimeoutPerNode(java.lang.String,int)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.sps.BlockStorageMovementAttemptedItems:<clinit>()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:updateModificationTime(long,int)
org.apache.hadoop.hdfs.web.URLConnectionFactory:<clinit>()
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:asyncContainerRelease(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getServerDefaults()
org.apache.hadoop.mapred.JobConf:getReduceDebugScript()
org.apache.hadoop.fs.http.HttpsFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:close()
org.apache.hadoop.fs.http.HttpFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getLocatedFileInfo(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreUtils:<clinit>()
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:stop()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:undoRename4DstParent(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,org.apache.hadoop.hdfs.server.namenode.INode,int)
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:initializeSecretProvider(javax.servlet.FilterConfig)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBRecordReader:<clinit>()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:bind(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord,int)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:directoryMustExist(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:close()
org.apache.hadoop.mapreduce.lib.db.OracleDataDrivenDBRecordReader:createValue()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:setOutputBufferCapacity(int)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.lib.chain.ChainReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:pauseForTestingAfterNthCheckpoint(java.lang.String,int)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getAggregatedLogsMeta(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean,boolean)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:opt(java.lang.String,double)
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.InvariantsChecker:<clinit>()
org.apache.hadoop.contrib.utils.join.DataJoinJob:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:checkVersion()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.registry.server.services.DeleteCompletionCallback:processResult(org.apache.curator.framework.CuratorFramework,org.apache.curator.framework.api.CuratorEvent)
org.apache.hadoop.fs.local.LocalFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.lib.db.TextSplitter:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:<clinit>()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:read(long,java.nio.ByteBuffer)
org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl:getServer(java.lang.Class,java.lang.Object,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,int)
org.apache.hadoop.mapred.SequenceFileAsTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:initDriver()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:set(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ClusterMonitor:updateNodeResource(org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode,org.apache.hadoop.yarn.api.records.ResourceOption)
org.apache.hadoop.fs.s3a.impl.NetworkBinding:<clinit>()
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$12:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.fs.shell.Truncate:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:stop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$InvalidOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.applications.mawo.server.common.Task:readFields(java.io.DataInput)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:setReadahead(java.lang.Long)
org.apache.hadoop.registry.conf.RegistryConfiguration:iterator()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.MemoryResourceHandler:bootstrap(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream:hasCapability(java.lang.String)
org.apache.hadoop.tools.rumen.MapAttempt20LineHistoryEventEmitter:emitterCore(org.apache.hadoop.tools.rumen.ParsedLine,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:decUsedResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext)
org.apache.hadoop.hdfs.protocol.datatransfer.BlackListBasedTrustedChannelResolver:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:rename(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager:checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode)
org.apache.hadoop.mapreduce.lib.input.CombineSequenceFileInputFormat$SequenceFileRecordReaderWrapper:<init>(org.apache.hadoop.mapreduce.lib.input.CombineFileSplit,org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.Integer)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:getProgress()
org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTrimmedStrings(java.lang.String)
org.apache.hadoop.mapred.lib.NLineInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:size()
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl:hashCode()
org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getLong(java.lang.String,long)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.security.ShellBasedUnixGroupsMapping:<clinit>()
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:verifyAndCreateRemoteLogDir()
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:containerIncreasedOnNode(org.apache.hadoop.yarn.api.records.ContainerId,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:loadEdits(java.lang.Iterable,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getTaskReport(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskReportRequest)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseStorage4Block(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,java.util.List,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processArguments(java.util.LinkedList)
org.apache.hadoop.tools.HadoopArchiveLogsRunner:main(java.lang.String[])
org.apache.hadoop.hdfs.NameNodeProxies:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getAllocation(org.apache.hadoop.yarn.util.resource.ResourceCalculator,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.adl.AdlFileSystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.server.webapp.LogWebService:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap,boolean)
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:start()
org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager:<clinit>()
org.apache.hadoop.examples.dancing.OneSidedPentomino:solve(int[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.DelegationTokenRenewer:reset()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:get(java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.examples.WordMedian:main(java.lang.String[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:flushBuffer(boolean,boolean)
org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics:getCallerUGI()
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:start()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:getEffectiveMaxCapacity(java.lang.String)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.fs.aliyun.oss.OSS:getHomeDirectory()
org.apache.hadoop.yarn.server.webapp.AppBlock:<clinit>()
org.apache.hadoop.hdfs.protocol.EncryptionZoneIterator:hasNext()
org.apache.hadoop.yarn.service.provider.docker.DockerProviderService:buildContainerRetry(org.apache.hadoop.yarn.service.containerlaunch.AbstractLauncher,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService$ComponentLaunchContext,org.apache.hadoop.yarn.service.component.instance.ComponentInstance)
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.s3a.S3AFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Expunge:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:noteFailure(java.lang.Exception)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayThread:<clinit>()
org.apache.hadoop.tools.dynamometer.workloadgenerator.VirtualInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner:connect(java.net.URL)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getSnapshotDiffReport(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.Stat:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.server.blockmanagement.BlockManager:flushBlockOps()
org.apache.hadoop.crypto.key.kms.server.KMS:getKeyNames()
org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.FileChecksumHelper$ReplicatedFileChecksumComputer:makeCompositeCrcResult()
org.apache.hadoop.fs.ftp.FTPFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:recoverContainer(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:recoverContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:renameMeta(java.net.URI)
org.apache.hadoop.yarn.server.nodemanager.DeletionService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode:doCheckpoint()
org.apache.hadoop.mapreduce.v2.security.MRDelegationTokenRenewer:<clinit>()
org.apache.hadoop.fs.local.RawLocalFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapred.LocalContainerLauncher:stop()
org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket)
org.apache.hadoop.yarn.server.resourcemanager.RMInfoMXBean:getHostAndPort()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.fs.azurebfs.utils.DateTimeUtils:isRecentlyModified(java.lang.String,java.time.Instant)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerImagesCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.fs.azure.Wasb:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getErasureCodingPolicies()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:isUpgradeFinalized()
org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff:main(java.lang.String[])
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:logExpireToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.mapred.MapTaskStatus:setDiagnosticInfo(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.ha.IPFailoverProxyProvider:close()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:close()
org.apache.hadoop.conf.ConfigurationWithLogging:logDeprecation(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker:getVolumesLowOnSpace()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$19:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.mapreduce.checkpoint.CheckpointID:readFields(java.io.DataInput)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:writeXml(java.io.Writer)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$22:connect(java.net.URL)
org.apache.hadoop.mapred.lib.db.DBInputFormat:setInput(org.apache.hadoop.mapred.JobConf,java.lang.Class,java.lang.String,java.lang.String,java.lang.String,java.lang.String[])
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.http.HttpFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.streaming.StreamUtil:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$11:connect(java.net.URL)
org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable:run()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntityTypes(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.RuncContainerRuntime:getGroupIdInfo(java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.sls.appmaster.DAGAMSimulator:<clinit>()
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:stop()
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:opt(java.lang.String,float)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:deleteBlockData()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:getPendingAppDiagnosticMessage(java.lang.StringBuilder)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Delete$Rmr:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator:<clinit>()
org.apache.hadoop.fs.shell.Ls$Lsr:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:rollbackContainerUpdate(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseRandom(java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshLogRetentionSettings()
org.apache.hadoop.yarn.server.router.webapp.FederationBlock:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:updateApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:releaseResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource,java.lang.String)
org.apache.hadoop.mapred.lib.TotalOrderPartitioner:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close()
org.apache.hadoop.registry.server.dns.RegistryDNSServer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:setAvailableResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.NameNode:initializeSharedEdits(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.LocalFileSystem:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AInstrumentation:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ImageWriter:writeCacheManagerSection()
org.apache.hadoop.mapred.JobConf:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:<clinit>()
org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logConcat(java.lang.String,java.lang.String[],long,boolean)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:constructFilterListBasedOnFields(java.util.Set)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.fs.ftp.FTPInputStream:toString()
org.apache.hadoop.examples.WordMedian$WordMedianReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.conf.ConfigurationWithLogging:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:setClusterMaxPriority(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:stop()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:processDeleteOnExit()
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:recordModification(int)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:<clinit>()
org.apache.hadoop.fs.shell.XAttrCommands:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.service.ServiceManager:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor$UnixLocalWrapperScriptBuilder:writeLocalWrapperScript(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:write(java.io.DataOutput)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:start()
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:read(long,java.nio.ByteBuffer)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitor:close()
org.apache.hadoop.yarn.conf.YarnConfiguration:setDeprecatedProperties()
org.apache.hadoop.fs.s3a.S3AFileSystem:getObjectMetadata(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:signalContainer(org.apache.hadoop.yarn.api.records.SignalContainerCommand)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:accept(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.fs.cosn.CosNFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetStoragePolicy(java.lang.String,byte)
org.apache.hadoop.yarn.service.containerlaunch.ContainerLaunchService:stop()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:unsetStoragePolicy(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.security.authentication.server.CompositeAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.azurebfs.Abfs:getCanonicalServiceName()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:reencryptEncryptionZone(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction)
org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.Hdfs:getAclStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.api.DNSOperations:register(java.lang.String,org.apache.hadoop.registry.client.types.ServiceRecord)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:moveReservation(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode)
org.apache.hadoop.yarn.sls.SLSRunner$1:createSystemMetricsPublisher()
org.apache.hadoop.fs.http.HttpsFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:seek(long)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:call(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String,org.apache.hadoop.io.Writable,long)
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher$1:run()
org.apache.hadoop.tools.CopyFilter:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.security.SpillCallBackPathsFinder:<clinit>()
org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodeLabelsPage:render(java.lang.Class)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getServerDefaults()
org.apache.hadoop.fs.shell.Delete$Rmdir:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.conf.RegistryConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:getNMResourceInfo(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:stop()
org.apache.hadoop.hdfs.server.namenode.BackupImage:endCheckpoint(org.apache.hadoop.hdfs.server.namenode.CheckpointSignature)
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:scanNextOp()
org.apache.hadoop.fs.FsShell$Help:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setAcl(java.lang.String,java.util.List)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.Adl:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:addApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String,boolean,org.apache.hadoop.yarn.server.resourcemanager.placement.ApplicationPlacementContext)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl:<init>(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler,org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService,long,java.lang.String,java.util.Set,java.util.List)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:readFully(long,java.nio.ByteBuffer)
org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.alias.JavaKeyStoreProvider:keystoreExists()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:isSystemAppsLimitReached()
org.apache.hadoop.yarn.server.router.webapp.AppsBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:moveAppTo(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.fs.s3a.commit.LocalTempDir:tempFile(org.apache.hadoop.conf.Configuration,java.lang.String,long)
org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo:getSchedulerResourceTypes()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.io.ByteWritable$Comparator:newKey()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:<clinit>()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getTaskAttemptReport(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetTaskAttemptReportRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:resumeContainer()
org.apache.hadoop.fs.azurebfs.Abfs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.conf.ConfigurationWithLogging:setFloat(java.lang.String,float)
org.apache.hadoop.yarn.api.impl.pb.client.ClientSCMProtocolPBClientImpl:close()
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:stop()
org.apache.hadoop.mapred.join.OverrideRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getFinalPath(java.lang.String,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3a.audit.impl.LoggingAuditor:<init>()
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:<init>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$QueueBlock:getCallerUGI()
org.apache.hadoop.hdfs.client.HdfsAdmin:getAllStoragePolicies()
org.apache.hadoop.fs.LocalFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:postRoot(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam)
org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:stop()
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getMetadataInputStream(long)
org.apache.hadoop.streaming.AutoInputFormat:getSplits(org.apache.hadoop.mapred.JobConf,int)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:start()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:maybeIgnore(boolean,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer:main(java.lang.String[])
org.apache.hadoop.mapred.TaskAttemptContext:getWorkingDirectory()
org.apache.hadoop.mapred.Mapper:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:getJobAttemptPath(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor:run()
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.JobContext:getMapperClass()
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$ContainerStateIterator:next()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.ServiceRegistryUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.mapreduce.filecache.DistributedCache:addFileToClassPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$StagingDirCleaningService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getKeys()
org.apache.hadoop.fs.ftp.FTPFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:get(java.lang.Class,org.apache.hadoop.hdfs.server.federation.store.records.Query)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsJobsBlock:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.v2.app.AppContext,org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.FsUrlConnection:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.hdfs.SWebHdfsDtFetcher:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics:addAMRuntime(org.apache.hadoop.yarn.api.records.ApplicationId,long,long,long,long)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.io.retry.LossyRetryInvocationHandler:getConnectionId()
org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl:<clinit>()
org.apache.hadoop.lib.server.Server:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.fs.LocalFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UsersManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitterFactory:createFileOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.datanode.DataNode$5:run()
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:injectStorage(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo)
org.apache.hadoop.conf.ConfigurationWithLogging:setDouble(java.lang.String,double)
org.apache.hadoop.fs.shell.SetReplication:runAll()
org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM:retrievePassword(org.apache.hadoop.yarn.security.NMTokenIdentifier)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.fs.shell.MoveCommands$Rename:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$WebHdfsInputStream:readFully(long,byte[])
org.apache.hadoop.conf.ConfigurationWithLogging:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.federation.router.RouterNetworkTopologyServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$FinishedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.examples.terasort.TeraSort:main(java.lang.String[])
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:flush()
org.apache.hadoop.fs.shell.Display$Checksum:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.conf.YarnConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setDouble(java.lang.String,double)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:run(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getValByRegex(java.lang.String)
org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:updateHeartbeat(org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)
org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree:<clinit>()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler:start()
org.apache.hadoop.mapred.lib.MultipleInputs:addInputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:render(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.mapreduce.lib.jobcontrol.JobControl:<clinit>()
org.apache.hadoop.yarn.server.utils.BuilderUtils:newContainerToken(org.apache.hadoop.yarn.api.records.ContainerId,int,java.lang.String,int,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,long,int,byte[],long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:serviceStop()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.common.HostRestrictingAuthorizationFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorJob:main(java.lang.String[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.ftp.FTPFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.fs.LocalFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:updateConfigurableResourceRequirement(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.local.LocalFs:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:commitJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:blockReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,org.apache.hadoop.hdfs.server.protocol.StorageBlockReport[],org.apache.hadoop.hdfs.server.protocol.BlockReportContext)
org.apache.hadoop.lib.service.instrumentation.InstrumentationService:init(org.apache.hadoop.lib.server.Server)
org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl:<init>(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,java.lang.String,org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory,org.apache.hadoop.hdfs.server.datanode.FileIoProvider,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.partition.BinaryPartitioner:setRightOffset(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:reinit(boolean)
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:<clinit>()
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getSendPacketDownstreamAvgInfo()
org.apache.hadoop.mapreduce.v2.api.HSAdminProtocol:refreshJobRetentionSettings()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:moveApplicationAcrossQueues(org.apache.hadoop.yarn.api.protocolrecords.MoveApplicationAcrossQueuesRequest)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:getCallerUGI()
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage:<clinit>()
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:serviceStart()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics)
org.apache.hadoop.fs.azurebfs.Abfs:getHomeDirectory()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getPropertySources(java.lang.String)
org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])
org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:close()
org.apache.hadoop.fs.local.RawLocalFs:getHomeDirectory()
org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.fs.sftp.SFTPFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setOwner(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.JobConf:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getServerDefaults()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:getLength(java.lang.Object)
org.apache.hadoop.fs.shell.SetReplication:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getSocketAddr(java.lang.String,java.lang.String,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:<clinit>()
org.apache.hadoop.tools.rumen.ParsedTask:<clinit>()
org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager:resumeReencryptUpdaterForTesting()
org.apache.hadoop.yarn.server.timelineservice.storage.OfflineAggregationWriter:close()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:getAllBlocks()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetOwner(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:purgeLogsOlderThan(long)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService:serviceStart()
org.apache.hadoop.fs.viewfs.NflyFSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeErasureCodingPolicy(java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.adl.AdlFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.mount.MountInterface:dump(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacitySchedulerPlanFollower:getReservedResources(long,java.util.Set,java.util.Set,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:serviceStop()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:doRollback()
org.apache.hadoop.yarn.server.router.Router:serviceStart()
org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:start()
org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider()
org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:getOwnerInfo(boolean)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getStrings(java.lang.String)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:<clinit>()
org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:<clinit>()
org.apache.hadoop.fs.shell.Ls$Lsr:runAll()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listCacheDirectives(long,org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:key(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getQueueUserAclInfo()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getFile(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:reinitialize(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.api.impl.AHSv2ClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.join.OverrideRecordReader:skip(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:noteFailure(java.lang.Exception)
org.apache.hadoop.util.JsonSerialization:<clinit>()
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$8:getUrl()
org.apache.hadoop.mapred.SkipBadRecords:setSkipOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:internalReserveResources(java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:listCachePools(java.lang.String)
org.apache.hadoop.mapred.FileOutputFormat:setOutputCompressorClass(org.apache.hadoop.mapred.JobConf,java.lang.Class)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:displayError(java.lang.Exception)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:reencryptEncryptionZone(java.lang.String,org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.http.HttpsFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppStateUpdateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.registry.client.api.RegistryOperations:addWriteAccessor(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:getDataNodeProxy(java.lang.String)
org.apache.hadoop.fs.Hdfs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.Paths:getAppAttemptId(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:copyMetadata(java.net.URI)
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponentInstances(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getDelegationToken(org.apache.hadoop.io.Text)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)
org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceProfilesManagerImpl:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpFSFileSystem$2:equals(java.lang.Object)
org.apache.hadoop.fs.s3a.impl.BulkDeleteRetryHandler:<clinit>()
org.apache.hadoop.mapreduce.security.SecureShuffleUtils:<clinit>()
org.apache.hadoop.yarn.service.containerlaunch.JavaCommandLineBuilder:addConfOptions(org.apache.hadoop.conf.Configuration,java.lang.String[])
org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl:compareTo(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$KillOnNewTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.conf.YarnConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.s3a.select.SelectTool:initS3AFileSystem(java.lang.String)
org.apache.hadoop.fs.adl.Adl:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList)
org.apache.hadoop.yarn.service.monitor.probe.HttpProbe:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int)
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$1:run()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:internalAllocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getCpuFrequency()
org.apache.hadoop.hdfs.client.HdfsAdmin:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.ArrayFile$Reader:reset()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getServerDefaults()
org.apache.hadoop.fs.s3a.statistics.impl.S3AMultipartUploaderStatisticsImpl:incCounter(java.lang.String)
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getNetworkBytesRead()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader:readOp(boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:getNewReservationId()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:computeContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.OutboundBandwidthResourceHandler:reacquireContainer(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.JobConf:getKeyFieldComparatorOption()
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage$NodesBlock:<init>(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.yarn.webapp.View$ViewContext)
org.apache.hadoop.fs.LeaseRecoverable:isFileClosed(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.ssl.SSLHostnameVerifier:check(java.lang.String,java.security.cert.X509Certificate)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseRandom(int,java.lang.String,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:getResourceUsageReport()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:reloadFromImageFile(java.io.File,org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:storeNewApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:getReservedResources()
org.apache.hadoop.net.DNS:getHosts(java.lang.String)
org.apache.hadoop.resourceestimator.translator.api.JobMetaData:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeAllocationInfo:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refreshNodes()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.ArrayFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.mapreduce.lib.db.MySQLDataDrivenDBRecordReader:createValue()
org.apache.hadoop.fs.ftp.FTPFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.FsUsage$Df:run(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment:<init>(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:getPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:addXAttrFeature(org.apache.hadoop.hdfs.server.namenode.XAttrFeature,int)
org.apache.hadoop.fs.shell.AclCommands:processRawArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.JobFactory$1:getTaskAttemptInfo(org.apache.hadoop.mapreduce.TaskType,int,int)
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:serviceStop()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.CustomOutputCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:deleteMetadata()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:getFileStatusOrNull(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.ExecutionSummarizer:<clinit>()
org.apache.hadoop.mapreduce.v2.app.webapp.JobsBlock:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:start()
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor:<clinit>()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getServerDefaults()
org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getOutputStreamForKeystore()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:toString()
org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.shell.Test:run(java.lang.String[])
org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.conf.ConfigurationWithLogging:getTrimmed(java.lang.String)
org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:serviceStop()
org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry$RegisteredShm:free()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:checkPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.Abfss:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm:<clinit>()
org.apache.hadoop.mapreduce.Job:setCombinerKeyGroupingComparatorClass(java.lang.Class)
org.apache.hadoop.hdfs.DFSUtil:addPBProtocol(org.apache.hadoop.conf.Configuration,java.lang.Class,com.google.protobuf.BlockingService,org.apache.hadoop.ipc.RPC$Server)
org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:get(java.lang.String,java.lang.String)
org.apache.hadoop.mapred.JobConf:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.registry.conf.RegistryConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:readdirplus(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.OutboundBandwidthResourceHandler:bootstrap(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:health(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.azure.BlockBlobAppendStream:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:setLong(java.lang.String,long)
org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:saveNamespace(long,long)
org.apache.hadoop.ipc.WritableRpcEngine$Server:call(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String,org.apache.hadoop.io.Writable,long)
org.apache.hadoop.fs.ftp.FtpFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logRemoveErasureCodingPolicy(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:updateApplicationStateSynchronously(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.io.compress.zlib.ZlibFactory:<clinit>()
org.apache.hadoop.fs.s3a.S3AFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:expandArgument(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:setupQueueConfigs(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.fs.shell.Count:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.gridmix.ReadRecordFactory:close()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:getTaskAttemptReport(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$GetTaskAttemptReportRequestProto)
org.apache.hadoop.fs.shell.Mkdir:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.nativetask.serde.DoubleWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.LoadManifestsStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:attachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.shell.FsUsage$Dus:runAll()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:listCorruptFileBlocks(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.hdfs.server.federation.router.MountTableRefresherService:stop()
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:<clinit>()
org.apache.hadoop.hdfs.server.datanode.DataNode:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:put(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord,boolean,boolean)
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getLong(java.lang.String,long)
org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:encode(byte[][],byte[][])
org.apache.hadoop.yarn.webapp.view.HeaderBlock:getCallerUGI()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.LocalFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSInotifyEventInputStream:take()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:startLocalizer(org.apache.hadoop.yarn.server.nodemanager.executor.LocalizerStartContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.crypto.CryptoOutputStream:close()
org.apache.hadoop.examples.terasort.TeraOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.namenode.ha.RequestHedgingProxyProvider:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.Class,org.apache.hadoop.hdfs.server.namenode.ha.HAProxyFactory)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:killContainer(org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:removeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier)
org.apache.hadoop.io.UTF8$Comparator:newKey()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:<clinit>()
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getRemainingCapacity()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.api.HSClientProtocol:getDelegationToken(org.apache.hadoop.mapreduce.v2.api.protocolrecords.GetDelegationTokenRequest)
org.apache.hadoop.mapred.JobConf:onlyKeyExists(java.lang.String)
org.apache.hadoop.fs.Hdfs$2:hasNext()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:satisfyStoragePolicy(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.SWebHdfs:getServerDefaults()
org.apache.hadoop.fs.local.RawLocalFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.azure.Wasbs:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsAdmin:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.mapred.FixedLengthInputFormat:setRecordLength(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.http.HttpFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:<clinit>()
org.apache.hadoop.hdfs.DFSStripedInputStream:skip(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:detachContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$SetStoragePolicyOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.conf.ConfigurationWithLogging:getPropsWithPrefix(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy:<clinit>()
org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable:run()
org.apache.hadoop.fs.LocalFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getHomeDirectory()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition:transition(org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl,org.apache.hadoop.mapreduce.v2.app.job.event.JobEvent)
org.apache.hadoop.fs.LocalFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getUsed()
org.apache.hadoop.fs.shell.Delete$Rmdir:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:getSingleTaskCounters(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getLowRedundancyBlocks()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)
org.apache.hadoop.mapred.TaskReport:<init>(org.apache.hadoop.mapred.TaskID,float,java.lang.String,java.lang.String[],long,long,org.apache.hadoop.mapred.Counters)
org.apache.hadoop.fs.s3a.impl.DeleteOperation:<clinit>()
org.apache.hadoop.yarn.client.api.impl.NMClientImpl:start()
org.apache.hadoop.fs.s3a.S3AFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.service.ServiceScheduler:stop()
org.apache.hadoop.yarn.server.resourcemanager.preprocessor.SubmissionContextPreProcessor:<clinit>()
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapred.JobConf:setKeepTaskFilesPattern(java.lang.String)
org.apache.hadoop.mapreduce.lib.db.DataDrivenDBInputFormat:createDBRecordReader(org.apache.hadoop.mapreduce.lib.db.DBInputFormat$DBInputSplit,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getAppResourceUsageReport(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateApplicationStateSynchronously(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.hdfs.HAUtil:setAllowStandbyReads(org.apache.hadoop.conf.Configuration,boolean)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver$NoSplitTextInputFormat:createRecordReader(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.http.HttpFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:populatePathNames(org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerDataNode)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$LeafQueueBlock:render()
org.apache.hadoop.fs.http.HttpsFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.store.driver.StateStoreRecordOperations:put(org.apache.hadoop.hdfs.server.federation.store.records.BaseRecord,boolean,boolean)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.mapred.join.InnerJoinRecordReader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.mapred.join.TupleWritable)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:canRollBackSharedLog(org.apache.hadoop.hdfs.server.common.StorageInfo,int)
org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider:<clinit>()
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapred.TaskUmbilicalProtocol:shuffleError(org.apache.hadoop.mapred.TaskAttemptID,java.lang.String)
org.apache.hadoop.registry.server.services.RegistryAdminService:addWriteAccessor(java.lang.String,java.lang.String)
org.apache.hadoop.ipc.ProtocolMetaInterface:isMethodSupported(java.lang.String)
org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport:setPreemptedMemorySeconds(long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.lib.Interns:<clinit>()
org.apache.hadoop.fs.local.RawLocalFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.TaskAttemptContext:getCounter(java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.S3A:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:cat(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosN:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:get(java.lang.String)
org.apache.hadoop.fs.azurebfs.Abfss:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime:getLocalResources(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getClasses(java.lang.String,java.lang.Class[])
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:close()
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setResourceComparator(java.lang.Class)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:copyMetadata(java.net.URI)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:addResource(java.net.URL)
org.apache.hadoop.yarn.client.api.async.NMClientAsync:rollbackLastReInitializationAsync(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.mapred.TaskAttemptContext:getMapOutputKeyClass()
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:finalizeUpgrade()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,boolean)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:msync()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String,javax.net.ssl.SSLSocket)
org.apache.hadoop.mapred.nativetask.serde.VIntWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$StatusUpdateTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$ReadRunner:getUrl()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logSetXAttrs(java.lang.String,java.util.List,boolean)
org.apache.hadoop.fs.azurebfs.Abfss:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:commitBlockSynchronization(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,long,boolean,boolean,org.apache.hadoop.hdfs.protocol.DatanodeID[],java.lang.String[])
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:must(java.lang.String,float)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumDecomLiveNodes()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:setupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:maybeIgnore(boolean,java.lang.String,java.io.IOException)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler:<clinit>()
org.apache.hadoop.fs.shell.Mkdir:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.protocolPB.RouterAdminProtocol:getDestination(org.apache.hadoop.hdfs.server.federation.store.protocol.GetDestinationRequest)
org.apache.hadoop.hdfs.server.datanode.web.URLDispatcher:channelRead0(io.netty.channel.ChannelHandlerContext,java.lang.Object)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:removeDefaultAcl(java.lang.String)
org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.tools.rumen.RandomSeedGenerator:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.net.URL)
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:unset(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:unsetErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.adl.AdlFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeysMetadata(java.lang.String[])
org.apache.hadoop.hdfs.DFSStripedInputStream:readFully(long,byte[])
org.apache.hadoop.mapreduce.v2.hs.HSAuditLogger:<clinit>()
org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode:updateTotalResource(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:getGlobalConstraint(java.util.Set)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeCacheDirective(long)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getHttpServerBindAddress(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.InconsistentAmazonS3Client:setFailureInjectionPolicy(org.apache.hadoop.fs.s3a.S3AFileSystem,org.apache.hadoop.fs.s3a.FailureInjectionPolicy)
org.apache.hadoop.mapreduce.lib.output.LazyOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:registerApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest)
org.apache.hadoop.yarn.webapp.example.MyApp$MyController:renderJSON(java.lang.Object)
org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceManagerAdministrationProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:opt(java.lang.String,double)
org.apache.hadoop.mapred.lib.db.DBInputFormat:setInput(org.apache.hadoop.mapred.JobConf,java.lang.Class,java.lang.String,java.lang.String)
org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$FinalizeFailedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection:<init>(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.router.webapp.FederationInterceptorREST:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getFloat(java.lang.String,float)
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:innerClose()
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:<clinit>()
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageCorruptionDetector:<clinit>()
org.apache.hadoop.yarn.service.client.SystemServiceManagerImpl:serviceStart()
org.apache.hadoop.mapreduce.TypeConverter:<clinit>()
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getEnteringMaintenanceNodes()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:stop()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:refreshNodes()
org.apache.hadoop.yarn.nodelabels.NonAppendableFSNodeLabelStore:removeClusterNodeLabels(java.util.Collection)
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$Util:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorWebService:putDomain(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,org.apache.hadoop.yarn.api.records.timelineservice.TimelineDomain)
org.apache.hadoop.registry.client.api.DNSOperations:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.registry.server.services.MicroZookeeperService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor:getExposedPorts(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.util.InstrumentedReadLock:lockInterruptibly()
org.apache.hadoop.fs.LocalFileSystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.fs.shell.FsUsage$Dus:run(java.lang.String[])
org.apache.hadoop.fs.cosn.CosNFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp:delegationTokenFromXml(org.apache.hadoop.hdfs.util.XMLUtils$Stanza)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,boolean)
org.apache.hadoop.fs.http.HttpsFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:echo()
org.apache.hadoop.fs.FsShell$Usage:processRawArguments(java.util.LinkedList)
org.apache.hadoop.mapred.JobConf:setSessionId(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.hdfs.server.namenode.BackupImage:renameCheckpoint(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile,org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.hdfs.server.datanode.ProvidedReplica:<clinit>()
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:setModificationTime(long,int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:checkAccess(java.lang.String,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:maybeIgnore(boolean,java.lang.String,java.io.IOException)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:delete(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getGroupsForUser(java.lang.String)
org.apache.hadoop.mapred.FileInputFormat:setInputPathFilter(org.apache.hadoop.mapred.JobConf,java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getHomeDirectory()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:deleteDir(org.apache.hadoop.fs.Path,java.lang.Boolean)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:getEffectiveCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getUserAMResourceLimitPerPartition(java.lang.String,java.lang.String)
org.apache.hadoop.fs.cosn.CosN:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:storeToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.mapred.Queue:<clinit>()
org.apache.hadoop.registry.server.services.RegistryAdminService:zkStat(java.lang.String)
org.apache.hadoop.fs.azurebfs.Abfs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.conf.ConfigurationWithLogging:setBoolean(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElector:start()
org.apache.hadoop.fs.adl.AdlFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils:<clinit>()
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$InMemoryMetadataDB$CorruptedDir:getPath()
org.apache.hadoop.streaming.StreamJob:go()
org.apache.hadoop.hdfs.server.namenode.LeaseManager:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.TaskAttemptFinishingMonitor:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:createReleaseCache()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.fs.shell.CopyCommands$Cp:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:commitJobInternal(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.azure.Wasb:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:close()
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:stopPeriodic()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC:getServer(java.lang.Class,java.lang.Object,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,int)
org.apache.hadoop.mapred.nativetask.util.NativeTaskOutput:getOutputIndexFile()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:addOrUpdateToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation,boolean)
org.apache.hadoop.mapreduce.v2.app.webapp.AMWebServices:<init>(org.apache.hadoop.mapreduce.v2.app.webapp.App,org.apache.hadoop.mapreduce.v2.app.AppContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getUserAMResourceLimit()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:updateApplicationPriority(org.apache.hadoop.yarn.api.records.Priority,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.thirdparty.com.google.common.util.concurrent.SettableFuture,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.mapreduce.filecache.DistributedCache:addCacheArchive(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:close()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:readEntity(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:breakHardLinksIfNeeded()
org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:getBytes()
org.apache.hadoop.fs.viewfs.NflyFSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosN:getCanonicalServiceName()
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)
org.apache.hadoop.fs.azure.BlockBlobAppendStream:write(byte[],int,int)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.hdfs.util.ECPolicyLoader:<clinit>()
org.apache.hadoop.fs.SWebHdfs:listStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.gridmix.JobFactory:<init>(org.apache.hadoop.mapred.gridmix.JobSubmitter,java.io.InputStream,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,java.util.concurrent.CountDownLatch,org.apache.hadoop.mapred.gridmix.UserResolver)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext)
org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setInt(java.lang.String,int)
org.apache.hadoop.fs.HarFileSystem:processDeleteOnExit()
org.apache.hadoop.fs.s3a.commit.magic.MagicS3GuardCommitter:cleanup(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:initOutput(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$14:getUrl()
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)
org.apache.hadoop.hdfs.server.federation.store.StateStoreService:<clinit>()
org.apache.hadoop.yarn.server.sharedcachemanager.ClientProtocolService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceStart()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService:start()
org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ResourceUtilizationTracker:hasResourcesAvailable(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapred.RunningJob:getTaskDiagnostics(org.apache.hadoop.mapred.TaskAttemptID)
org.apache.hadoop.mapreduce.v2.hs.protocolPB.HSAdminRefreshProtocolClientSideTranslatorPB:close()
org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.EntityTypeReader:lookupFlowContext(org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowRowKey,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.fs.shell.Delete$Rmr:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:reorderScheduleEntities()
org.apache.hadoop.mapreduce.v2.app.webapp.TasksBlock:render()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getNodeIds(java.lang.String)
org.apache.hadoop.fs.shell.Display$Checksum:expandArguments(java.util.LinkedList)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:<clinit>()
org.apache.hadoop.fs.s3a.S3ADataBlocks$ByteBufferBlockFactory$ByteBufferBlock:enterClosedState()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:addToExcludedNodes(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:logs()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:removeDefaultAcl(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:clear()
org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector:selectToken(java.net.URI,java.util.Collection,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:calculateAndGetAMResourceLimitPerPartition(java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.permission.FsPermission:<clinit>()
org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl:<clinit>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorFactory:<clinit>()
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:initServices(java.util.List)
org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.client.HdfsAdmin:clearQuota(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.AMRMClient:<clinit>()
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getQueueUserAclInfo(org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:<clinit>()
org.apache.hadoop.filecache.DistributedCache:setLocalFiles(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:getUsed()
org.apache.hadoop.ipc.WritableRpcEngine$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)
org.apache.hadoop.mapreduce.lib.db.MySQLDataDrivenDBRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.mapreduce.lib.db.DBWritable)
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:expandArguments(java.util.LinkedList)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:must(java.lang.String,float)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSStarvedApps$StarvationComparator:compare(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.tools.offlineImageViewer.WebImageViewer:<init>(java.net.InetSocketAddress)
org.apache.hadoop.fs.adl.AdlFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices:get(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:cacheReport(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,java.lang.String,java.util.List)
org.apache.hadoop.yarn.server.sharedcachemanager.SCMAdminProtocolService:serviceStop()
org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:expandArgument(java.lang.String)
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:relaunchContainer(org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerRuntimeContext)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromBytes(byte[])
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getDataInputStream(long)
org.apache.hadoop.mapreduce.v2.api.impl.pb.client.HSClientProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.streaming.StreamInputFormat:listStatus(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])
org.apache.hadoop.mapred.SequenceFileInputFilter:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:startThreads()
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getDecompressor()
org.apache.hadoop.mapreduce.v2.app.webapp.TasksBlock:renderPartial()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl:equals(java.lang.Object)
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:read(byte[],int,int)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$OffsetUrlInputStream:readFully(long,byte[],int,int)
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$ModifyCacheDirectiveInfoOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:start()
org.apache.hadoop.mapreduce.filecache.DistributedCache:getCacheFiles(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.metrics2.lib.MutableRate:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.hdfs.util.PersistentLongFile:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:init(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.HttpOpParam,org.apache.hadoop.hdfs.web.resources.Param[])
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:beforeUnmarshalling(com.amazonaws.Request,com.amazonaws.http.HttpResponse)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:createSnapshot(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:getContainerPid()
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:main(java.lang.String[])
org.apache.hadoop.hdfs.DFSStripedOutputStream:write(byte[],int,int)
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getEnforcementWindow(java.lang.String)
org.apache.hadoop.io.erasurecode.coder.ErasureCodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose()
org.apache.hadoop.yarn.server.resourcemanager.AdminService:start()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher:close()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.azure.CachingAuthorizer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.ErrorBlock:render(java.lang.Class)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.webapp.ApiServer:updateComponent(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.String,org.apache.hadoop.yarn.service.api.records.Component)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.fs.local.LocalFs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:serviceStart()
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.monitor.invariants.MetricsInvariantChecker:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:close()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.JavaSandboxLinuxContainerRuntime:getIpAndHost(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.examples.RandomWriter$RandomMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:delete(java.lang.String,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:setAMResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.lib.MultipleOutputs:getCollector(java.lang.String,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.azure.security.JsonUtils:<clinit>()
org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider:<clinit>()
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:setUsed(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.lib.wsrs.ExceptionProvider:<clinit>()
org.apache.hadoop.mapred.nativetask.StatusReportChecker:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueuePlacementRules(java.util.Collection)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setReservationAcls(java.lang.String,java.util.Map)
org.apache.hadoop.fs.http.HttpsFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run()
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore:checkVersion()
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.HarFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:start()
org.apache.hadoop.fs.adl.AdlFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getLongBytes(java.lang.String,long)
org.apache.hadoop.registry.conf.RegistryConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.mapred.JobConf:getJobEndNotificationCustomNotifierClass()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.fs.s3a.commit.staging.StagingCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.io.compress.bzip2.Bzip2Factory:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:analyzeStorage(org.apache.hadoop.hdfs.server.common.HdfsServerConstants$StartupOption,org.apache.hadoop.hdfs.server.common.Storage,boolean)
org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close()
org.apache.hadoop.fs.s3a.impl.OpenFileSupport:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:isInCurrentState()
org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])
org.apache.hadoop.yarn.server.webapp.AppsBlock:<clinit>()
org.apache.hadoop.fs.FsShell$Help:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.YarnUncaughtExceptionHandler:<clinit>()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure:write(org.apache.hadoop.hdfs.protocol.datatransfer.ReplaceDatanodeOnFailure$Policy,boolean,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory:getDestinationFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.ftp.FtpFs:getHomeDirectory()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getPendingDeletionBlocksCount()
org.apache.hadoop.tools.DistCp:main(java.lang.String[])
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CleanupJobStage:isFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:read(byte[],int,int)
org.apache.hadoop.fs.http.HttpsFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.util.GenericOptionsParser:<clinit>()
org.apache.hadoop.fs.FsShell$Help:run(java.lang.String[])
org.apache.hadoop.fs.s3a.S3A:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:internalIncrPendingResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.resourcemanager.NodesListManager:<clinit>()
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier$SWebHdfsDelegationTokenIdentifier:getBytes()
org.apache.hadoop.fs.HarFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.viewfs.NflyFSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:initDispatcher(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl:toString()
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:lastStep()
org.apache.hadoop.mapred.InputSplitWithLocationInfo:readFields(java.io.DataInput)
org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)
org.apache.hadoop.hdfs.net.TcpPeerServer:<clinit>()
org.apache.hadoop.yarn.security.AMRMTokenIdentifier:getTrackingId()
org.apache.hadoop.registry.client.api.RegistryOperationsFactory:createClient(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:run(java.lang.String[])
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager:<clinit>()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getCpuUsagePercentage()
org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper:setNumberOfThreads(org.apache.hadoop.mapreduce.Job,int)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.lib.TotalOrderPartitioner:setPartitionFile(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)
org.apache.hadoop.CustomOutputCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.LogHandler:handle(org.apache.hadoop.yarn.event.Event)
org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider:<clinit>()
org.apache.hadoop.security.ShellBasedIdMapping:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getUsed()
org.apache.hadoop.fs.adl.AdlFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)
org.apache.hadoop.fs.http.HttpsFileSystem:processDeleteOnExit()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:processDeleteOnExit()
org.apache.hadoop.mapred.JobConf:readFields(java.io.DataInput)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:addAclFeature(org.apache.hadoop.hdfs.server.namenode.AclFeature,int)
org.apache.hadoop.hdfs.server.diskbalancer.command.ExecuteCommand:getNodes(java.lang.String)
org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList)
org.apache.hadoop.tools.mapred.CopyOutputFormat:getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.HAUtil:getNameNodeIdFromAddress(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,java.lang.String[])
org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppRunningOnNodeTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:getProxyAddresses(java.net.URI,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSDirectory:getINode(java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.net.URL,boolean)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:removeRMDTMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,org.apache.hadoop.hdfs.qjournal.client.AsyncLogger$Factory)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:precommitCheckPendingFiles(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:registerSubordinateNamenode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.mapred.TaskLog:getRealTaskLogFileLocation(org.apache.hadoop.mapred.TaskAttemptID,boolean,org.apache.hadoop.mapred.TaskLog$LogName)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.cosn.CosNFileSystem:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithName:setGroup(java.lang.String,int)
org.apache.hadoop.mapred.JobClient:isJobDirValid(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.federation.store.StateStoreUtils:<clinit>()
org.apache.hadoop.mapreduce.Job:close()
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl:<clinit>()
org.apache.hadoop.mapred.MapTask$SkippingRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.client.ClientRMProxy:getProxy(org.apache.hadoop.conf.Configuration,java.lang.Class,java.net.InetSocketAddress)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[])
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:computeDiffBetweenSnapshots(org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot,org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$ChildrenDiff,org.apache.hadoop.hdfs.server.namenode.INodeDirectory)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:serviceStop()
org.apache.hadoop.fs.azure.NativeFileSystemStore:retrieve(java.lang.String,long)
org.apache.hadoop.registry.conf.RegistryConfiguration:get(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat$TextRecordReaderWrapper:close()
org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.fs.cosn.CosNFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.FileOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.registry.server.services.RegistryAdminService:clearWriteAccessors()
org.apache.hadoop.conf.ConfigurationWithLogging:getTrimmedStrings(java.lang.String)
org.apache.hadoop.hdfs.server.federation.metrics.RouterMBean:getClusterId()
org.apache.hadoop.fs.cosn.CosNFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.registry.server.services.AddingCompositeService:serviceStop()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getAppAttempt(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.registry.client.impl.zk.RegistryBindingSource:supplyBindingInformation()
org.apache.hadoop.fs.WebHdfs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FsShellPermissions$Chmod:runAll()
org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.datanode.fsdataset.FsDatasetSpi:getDfsUsed()
org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:getLongBytes(java.lang.String,long)
org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getCurrentIOStatisticsContext()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromInstance(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:<clinit>()
org.apache.hadoop.mapred.Counters$FrameworkGroupImpl:findCounter(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.tools.rumen.ParsedTaskAttempt:<clinit>()
org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl:waitFor(java.util.function.Supplier,int,int)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:startThreads()
org.apache.hadoop.hdfs.server.datanode.FinalizedProvidedReplica:blockDataExists()
org.apache.hadoop.fs.azurebfs.Abfss:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.io.erasurecode.coder.ErasureCoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.service.launcher.AbstractLaunchableService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.reservation.AbstractSchedulerPlanFollower:<clinit>()
org.apache.hadoop.CustomOutputCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.policy.FifoOrderingPolicyForPendingApps:getPreemptionIterator()
org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:afterResponse(com.amazonaws.Request,com.amazonaws.Response)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.mapred.MultiFileSplit:getLocations()
org.apache.hadoop.mapred.nativetask.serde.DoubleWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader:serviceStart()
org.apache.hadoop.mapreduce.lib.db.OracleDBRecordReader:getSelectQuery()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AppPriorityACLConfigurationParser:<clinit>()
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:computeContentSummary(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.mapreduce.lib.map.InverseMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.resourceestimator.solver.impl.LpSolver:<clinit>()
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:opt(java.lang.String,double)
org.apache.hadoop.examples.terasort.TeraOutputFormat:setFinalSync(org.apache.hadoop.mapreduce.JobContext,boolean)
org.apache.hadoop.fs.shell.Mkdir:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackJavaFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Function)
org.apache.hadoop.streaming.StreamInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:downloadConf()
org.apache.hadoop.fs.http.server.HttpFSServerWebApp:<init>()
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.mapred.nativetask.serde.NullWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.fs.s3a.commit.impl.CommitOperations:abortPendingUploadsUnderPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:commitJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.fs.shell.MoveCommands$Rename:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:storeRMDelegationToken(org.apache.hadoop.yarn.security.client.RMDelegationTokenIdentifier,java.lang.Long)
org.apache.hadoop.yarn.service.utils.ApplicationReportSerDeser:fromResource(java.lang.String)
org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:hasNext()
org.apache.hadoop.fs.HarFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:toString()
org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsPage$ContainersLogsBlock:renderPartial()
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processRawArguments(java.util.LinkedList)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:pullNewlyDemotedContainers()
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:init(org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyApplicationContext)
org.apache.hadoop.mapred.JobConf:getStringCollection(java.lang.String)
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.s3a.select.SelectInputStream:skip(long)
org.apache.hadoop.yarn.server.api.AuxiliaryLocalPathHandler:getLocalPathForWrite(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:start()
org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsMappingProvider:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:updatePreemptedSecondsForCustomResources(org.apache.hadoop.yarn.api.records.Resource,long)
org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage:render(java.lang.Class)
org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:next()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue:toString()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSLogsPage:render(java.lang.Class)
org.apache.hadoop.mapred.RunningJob:setupProgress()
org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:listManifests()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.ResourcePlugin:initialize(org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeNewApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.util.RunJar:main(java.lang.String[])
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:removeQueue(java.lang.String)
org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:preCommitJob(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver:isTrusted(java.net.InetAddress)
org.apache.hadoop.registry.cli.RegistryCli:<clinit>()
org.apache.hadoop.yarn.service.webapp.ApiServerWebApp:main(java.lang.String[])
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.util.concurrent.ExecutorHelper:<clinit>()
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getCorruptFiles()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:activateApp(java.lang.String)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:serviceStop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:invalidateCache(java.lang.String)
org.apache.hadoop.fs.adl.AdlFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:setDeprecatedProperties()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:renewDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.RenewDelegationTokenRequest)
org.apache.hadoop.conf.ConfigurationWithLogging:iterator()
org.apache.hadoop.fs.s3a.S3A:getServerDefaults()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Tail:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.shell.Ls$Lsr:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.service.component.Component$ContainerRecoveredTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.hdfs.server.federation.router.RouterFsckServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.router.Router:close()
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:cleanResourceReferences(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.db.MySQLDBRecordReader:nextKeyValue()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:createApplicationMetricsTable(org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:close()
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:buildConnectionString()
org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:isInLatestSnapshot(int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listCacheDirectives(long,org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:submitAppAttempt(java.lang.String)
org.apache.hadoop.hdfs.web.oauth2.ConfRefreshTokenBasedAccessTokenProvider:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.web.SimpleHttpProxyHandler$Forwarder$1:operationComplete(io.netty.util.concurrent.Future)
org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:opt(java.lang.String,int)
org.apache.hadoop.fs.LocalFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.conf.ConfigurationWithLogging:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl:toString()
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getQueueUserAclInfo()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:serviceStop()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:launchContainer(org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext)
org.apache.hadoop.mapreduce.lib.input.FixedLengthRecordReader:<clinit>()
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:close()
org.apache.hadoop.registry.conf.RegistryConfiguration:clear()
org.apache.hadoop.ha.NodeFencer:<clinit>()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getAllStoragePolicies()
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoverPausedContainerLaunch:pauseContainer()
org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getResourceLimitForAllUsers(java.lang.String,org.apache.hadoop.yarn.api.records.Resource,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf:<clinit>()
org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2:call()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:addQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:renewLease(java.lang.String)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPasswordFromConfig(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:computeAndConvertContentSummary(int,org.apache.hadoop.hdfs.server.namenode.ContentSummaryComputationContext)
org.apache.hadoop.yarn.conf.YarnConfiguration:addTags(java.util.Properties)
org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.streaming.PipeCombiner:createOutputReader()
org.apache.hadoop.yarn.server.timeline.recovery.MemoryTimelineStateStore:serviceStart()
org.apache.hadoop.tools.rumen.TraceBuilder:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:storeOrUpdateAMRMTokenSecretManager(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState,boolean)
org.apache.hadoop.fs.s3a.auth.delegation.FullCredentialsTokenBinding:close()
org.apache.hadoop.fs.HarFileSystem:<clinit>()
org.apache.hadoop.mapreduce.jobhistory.JobHistoryCopyService:addIfService(java.lang.Object)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.component.instance.ComponentInstance$ContainerBecomeNotReadyTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.mapred.LocalJobRunner:setLocalMaxRunningMaps(org.apache.hadoop.mapreduce.JobContext,int)
org.apache.hadoop.fs.cosn.CosNFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DistributedFileSystem:getAdditionalTokenIssuers()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseReplicasToDelete(java.util.Collection,java.util.Collection,int,java.util.List,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab()
org.apache.hadoop.fs.s3a.audit.impl.ActiveAuditManagerS3A$SdkRequestHandler:afterAttempt(com.amazonaws.handlers.HandlerAfterAttemptContext)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.hdfs.server.datanode.BlockScanner$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close()
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:removeApplicationAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.webapp.hamlet.HamletGen:<clinit>()
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getPercentRemaining()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getQueueMaxResource(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeSymlink:computeQuotaUsage(org.apache.hadoop.hdfs.server.blockmanagement.BlockStoragePolicySuite,boolean)
org.apache.hadoop.mapred.lib.ChainMapper:addMapper(org.apache.hadoop.mapred.JobConf,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.protocol.datatransfer.Receiver:<clinit>()
org.apache.hadoop.yarn.client.AutoRefreshRMFailoverProxyProvider:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.pipes.BinaryProtocol$TeeOutputStream:close()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerExecCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.fs.ftp.FTPFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.yarn.server.federation.store.utils.FederationMembershipStateStoreInputValidator:<clinit>()
org.apache.hadoop.mapred.JobConf:setJobSubmitHostName(java.lang.String)
org.apache.hadoop.fs.shell.MoveCommands$Rename:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:moveAppFrom(org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit,boolean,boolean)
org.apache.hadoop.hdfs.server.namenode.INodeMap$1:setModificationTime(long,int)
org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:unbuffer()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:getTrashRoots(boolean)
org.apache.hadoop.fs.azure.RemoteSASKeyGeneratorImpl:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterMetricsService:serviceStop()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshClusterMaxPriority(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshClusterMaxPriorityRequest)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:saveNamespace(long,long)
org.apache.hadoop.mapreduce.v2.app.webapp.NavBlock:renderPartial()
org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup()
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$HealthBlock:render()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DiskResourceHandler:bootstrap(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.CentralizedOpportunisticContainerAllocator:<init>(org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager)
org.apache.hadoop.io.ShortWritable$Comparator:newKey()
org.apache.hadoop.mapred.JobConf:getMaxTaskFailuresPerTracker()
org.apache.hadoop.mapreduce.v2.app.webapp.AttemptsPage$FewAttemptsBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearQuotaCommand:run(java.lang.String[])
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:doRollback(org.apache.hadoop.hdfs.server.namenode.FSNamesystem)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:stop()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator:<clinit>()
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectoryWithSnapshotFeature$DirectoryDiff$2:iterator()
org.apache.hadoop.fs.ftp.FTPFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render(java.lang.Class)
org.apache.hadoop.fs.shell.Delete$Rmr:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.NavBlock:render()
org.apache.hadoop.mapred.SkipBadRecords:setMapperMaxSkipRecords(org.apache.hadoop.conf.Configuration,long)
org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl:toString()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:parseTopNodes(org.apache.commons.cli.CommandLine,org.apache.commons.text.TextStringBuilder)
org.apache.hadoop.yarn.server.timeline.TimelineDataManager:stop()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeProxyCACert(java.security.cert.X509Certificate,java.security.PrivateKey)
org.apache.hadoop.hdfs.server.federation.router.RouterSafemodeService:startPeriodic()
org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:commitFile(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.FileEntry,boolean)
org.apache.hadoop.fs.s3a.S3AFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:allocate(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.util.List,java.util.List,java.util.List,java.util.List,java.util.List,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.yarn.server.timeline.EntityCacheItem:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.SampleContainerLogAggregationPolicy:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:incPendingResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:app()
org.apache.hadoop.yarn.conf.YarnConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:refreshServiceAcl()
org.apache.hadoop.yarn.service.client.ApiServiceClient:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.SnapshotCommands:processArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.client.HdfsAdmin:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:runAll()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat:getSplitHosts(org.apache.hadoop.fs.BlockLocation[],long,long,org.apache.hadoop.net.NetworkTopology)
org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.StageAllocatorLowCostAligned:calcCostOfInterval(long,long,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.server.resourcemanager.reservation.RLESparseResourceAllocation,org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.util.resource.ResourceCalculator,long)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.server.federation.router.ConnectionPool:<clinit>()
org.apache.hadoop.hdfs.client.HdfsAdmin:listOpenFiles(java.util.EnumSet,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor:<clinit>()
org.apache.hadoop.fs.FsShellPermissions$Chgrp:displayError(java.lang.Exception)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitTaskStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.server.HSAdminServer:noteFailure(java.lang.Exception)
org.apache.hadoop.mapreduce.security.TokenCache:loadTokens(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathBooleanRunner:connect(java.net.URL)
org.apache.hadoop.mapred.TaskLog:syncLogs(java.lang.String,org.apache.hadoop.mapred.TaskAttemptID,boolean)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:doPostPut(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollector)
org.apache.hadoop.mapred.Counters$GenericGroup:readFields(java.io.DataInput)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setQueueMappings(java.util.List)
org.apache.hadoop.mapred.nativetask.serde.NullWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:get(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.GetOpParam,org.apache.hadoop.hdfs.web.resources.OffsetParam,org.apache.hadoop.hdfs.web.resources.LengthParam,org.apache.hadoop.hdfs.web.resources.RenewerParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,java.util.List,org.apache.hadoop.hdfs.web.resources.XAttrEncodingParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.FsActionParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam,org.apache.hadoop.hdfs.web.resources.OldSnapshotNameParam,org.apache.hadoop.hdfs.web.resources.TokenKindParam,org.apache.hadoop.hdfs.web.resources.TokenServiceParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam,org.apache.hadoop.hdfs.web.resources.StartAfterParam)
org.apache.hadoop.yarn.server.resourcemanager.security.ProxyCAManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.io.retry.RetryUtils:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:stopThreads()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:verifyChecksum(long,java.io.FileInputStream,java.nio.channels.FileChannel,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeAttributesProvider:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:refreshMaximumAllocation(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$RandomTextDataMapper:setup(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.JobEndNotifier:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData)
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:next(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.http.HttpsFileSystem:getCanonicalServiceName()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:serviceStart()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:cleanup(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:write(byte[],int,int)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:maybeCreateSuccessMarkerFromCommits(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,org.apache.hadoop.fs.s3a.commit.AbstractS3ACommitter$ActiveCommit)
org.apache.hadoop.mapred.pipes.BinaryProtocol:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveSuccessFileStage:<clinit>()
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)
org.apache.hadoop.yarn.conf.YarnConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getRaw(java.lang.String)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.tools.GetConf$BackupNodesCommandHandler:doWork(org.apache.hadoop.hdfs.tools.GetConf,java.lang.String[])
org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)
org.apache.hadoop.mapred.IFileOutputStream:close()
org.apache.hadoop.resourceestimator.skylinestore.api.SkylineStore:updateHistory(org.apache.hadoop.resourceestimator.common.api.RecurrenceId,java.util.List)
org.apache.hadoop.hdfs.server.federation.store.StateStoreConnectionMonitorService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$RawKVIteratorReader:close()
org.apache.hadoop.yarn.webapp.view.LipsumBlock:renderPartial()
org.apache.hadoop.yarn.security.ContainerTokenIdentifier:<init>(org.apache.hadoop.yarn.api.records.ContainerId,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.Resource,long,int,long,org.apache.hadoop.yarn.api.records.Priority,long,org.apache.hadoop.yarn.api.records.LogAggregationContext)
org.apache.hadoop.yarn.server.resourcemanager.placement.RejectPlacementRule:<clinit>()
org.apache.hadoop.yarn.appcatalog.controller.AppDetailsController:stopApp(java.lang.String)
org.apache.hadoop.crypto.key.kms.server.KeyAuthorizationKeyProvider:deleteKey(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$3:getUrl()
org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getOutputStreamForKeystore()
org.apache.hadoop.mapreduce.lib.db.MySQLDBRecordReader:next(org.apache.hadoop.io.LongWritable,org.apache.hadoop.mapreduce.lib.db.DBWritable)
org.apache.hadoop.yarn.server.api.ServerRMProxy$ServerRMProtocols:allocate(org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest)
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.IterativePlanner:allocateUser(org.apache.hadoop.yarn.api.records.ReservationId,java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.reservation.Plan,org.apache.hadoop.yarn.api.records.ReservationDefinition,org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationAllocation)
org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.federation.policies.RouterPolicyFacade:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.JHSDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:logExpireToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingInputStream:getS3File()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setAverageCapacity(java.lang.String,float)
org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:expandArguments(java.util.LinkedList)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:renewDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.aliyun.oss.OSS:getCanonicalServiceName()
org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController:cleanOldLogs(org.apache.hadoop.fs.Path,org.apache.hadoop.yarn.api.records.NodeId,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorProtocolService:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:read(org.apache.hadoop.io.ByteBufferPool,int)
org.apache.hadoop.io.compress.DirectDecompressionCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.mapreduce.filecache.DistributedCache:getArchiveTimestamps(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.MapTaskStatus:setFinishTime(long)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setErasureCodingPolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:reserve(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode,org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.Container)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.mapred.lib.MultithreadedMapRunner:<clinit>()
org.apache.hadoop.tools.rumen.HistoryEventEmitter:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setState(java.lang.String,org.apache.hadoop.yarn.api.records.QueueState)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:serviceStart()
org.apache.hadoop.hdfs.DataStreamer:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.local.LocalFs:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.LocalFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.sftp.SFTPFileSystem$1:unbuffer()
org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:stashOriginalFilePermissions()
org.apache.hadoop.yarn.webapp.example.MyApp$MyController:echo()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:exists(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.SWebHdfs:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.AbstractPlacementProcessor:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webapp.ErrorsAndWarningsBlock$WarningMetrics:getCallerUGI()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.io.UTF8:<clinit>()
org.apache.hadoop.fs.azurebfs.Abfs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.DBManager:<clinit>()
org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)
org.apache.hadoop.yarn.server.federation.store.metrics.FederationStateStoreClientMetrics:<clinit>()
org.apache.hadoop.fs.s3a.S3AInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.BlockChecksumHelper$BlockGroupNonStripedChecksumComputer:getBlockInputStream(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.cosn.NativeFileSystemStore:uploadPart(java.io.File,java.lang.String,java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:notifyStoreOperationFailed(java.lang.Exception)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.store.DataBlocks$DiskBlock:enterClosedState()
org.apache.hadoop.registry.server.services.RegistryAdminService:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:processTokenAddOrUpdate(byte[])
org.apache.hadoop.hdfs.tools.DFSAdmin:startReconfiguration(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock:renderPartial()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices:<clinit>()
org.apache.hadoop.mapred.ReduceTask:isCommitRequired()
org.apache.hadoop.fs.azure.Wasbs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getFileLinkInfo(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:setPermission(org.apache.hadoop.fs.permission.FsPermission,int)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getNormalizedResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.client.HdfsDataOutputStream:hasCapability(java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.RouterQuotaUpdateService:stop()
org.apache.hadoop.registry.conf.RegistryConfiguration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)
org.apache.hadoop.crypto.key.CachingKeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier:getTrackingId()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.mapred.TaskAttemptContext:getFileTimestamps()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:addResource(java.io.InputStream,boolean)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseLocalStorage(org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.mapred.nativetask.serde.DoubleWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$BaseFinalTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.filecache.DistributedCache:setArchiveTimestamps(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getBlocks(org.apache.hadoop.hdfs.protocol.DatanodeInfo,long,long)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:bindJVMtoJAASFile(java.io.File)
org.apache.hadoop.yarn.client.cli.RMAdminCLI:main(java.lang.String[])
org.apache.hadoop.fs.ftp.FtpFs:getDelegationTokens(java.lang.String)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadSortComparator:compare(byte[],int,int,byte[],int,int)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getEditLogManifest(long)
org.apache.hadoop.fs.local.LocalFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:getNumEntriesInDatabase()
org.apache.hadoop.fs.SWebHdfs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(java.net.URL,boolean)
org.apache.hadoop.streaming.StreamBaseRecordReader:<clinit>()
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.FadvisedFileRegion:transferTo(java.nio.channels.WritableByteChannel,long)
org.apache.hadoop.mapred.JobClient:getSetupTaskReports(org.apache.hadoop.mapred.JobID)
org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)
org.apache.hadoop.fs.ftp.FTPFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:serviceStop()
org.apache.hadoop.hdfs.server.namenode.BackupImage:purgeOldStorage(org.apache.hadoop.hdfs.server.namenode.NNStorage$NameNodeFile)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceRackFaultTolerantBlockPlacementPolicy:chooseTargetInOrder(int,org.apache.hadoop.net.Node,java.util.Set,long,int,java.util.List,boolean,boolean,java.util.EnumMap)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getHomeDirectory()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics:allocateResources(java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Resource,boolean)
org.apache.hadoop.crypto.key.kms.KMSClientProvider$2:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:<clinit>()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.shell.Display$Text:expandArguments(java.util.LinkedList)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:close()
org.apache.hadoop.security.RuleBasedLdapGroupsMapping:failover(int,int)
org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter:stop()
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage$LeafQueueInfoBlock:render()
org.apache.hadoop.fs.ftp.FtpFs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.local.LocalFs:listLocatedStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)
org.apache.hadoop.fs.shell.Delete$Expunge:expandArgument(java.lang.String)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.LocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList)
org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String[],javax.net.ssl.SSLSocket)
org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:startPeriodic()
org.apache.hadoop.yarn.client.cli.QueueCLI:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.BPServiceActor$LifelineSender$1:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:blockDataExists()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdateContext:<clinit>()
org.apache.hadoop.tools.dynamometer.workloadgenerator.WorkloadMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.tools.DFSAdmin$ClearSpaceQuotaCommand:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.fs.viewfs.NflyFSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.cosn.CosNFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:getQueueUserAclInfo()
org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin:getAvailableVirtualMemorySize()
org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache$4:load(java.lang.Object)
org.apache.hadoop.fs.cosn.CosNFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.metrics2.lib.MethodMetric$3:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.net.unix.DomainSocketWatcher:<clinit>()
org.apache.hadoop.fs.sftp.SFTPFileSystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.mapred.Counters:getCounter(java.lang.Enum)
org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB:close()
org.apache.hadoop.fs.http.HttpFileSystem:getDefaultBlockSize()
org.apache.hadoop.fs.adl.AdlFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.ReplicaWaitingToBeRecovered:getPinning(org.apache.hadoop.fs.LocalFileSystem)
org.apache.hadoop.fs.cosn.CosN:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:opt(java.lang.String,double)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:refreshNodesResources(org.apache.hadoop.yarn.server.api.protocolrecords.RefreshNodesResourcesRequest)
org.apache.hadoop.io.compress.SplittableCompressionCodec:getCompressorType()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeCacheDirective(long)
org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler:<clinit>()
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock$ByteBufferInputStream:reset()
org.apache.hadoop.yarn.server.resourcemanager.webapp.DeSelectFields:<clinit>()
org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.webproxy.ProxyCA:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:createEncryptionZone(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:openForWrite(int)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:registerSubordinateNamenode(org.apache.hadoop.hdfs.server.protocol.NamenodeRegistration)
org.apache.hadoop.security.SecurityUtil:<clinit>()
org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:getHomeDirectory()
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitorImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.examples.pi.DistBbp:main(java.lang.String[])
org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCompressor()
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:addBlockToBeRecovered(org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo)
org.apache.hadoop.fs.s3a.AWSCredentialProviderList:close()
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:<init>(org.apache.hadoop.yarn.server.resourcemanager.ResourceManager,org.apache.hadoop.conf.Configuration,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.fs.shell.Delete$Rmdir:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:must(java.lang.String,int)
org.apache.hadoop.fs.Hdfs:msync()
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:opt(java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:openFile(org.apache.hadoop.fs.PathHandle)
org.apache.hadoop.fs.http.HttpFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:setattr(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:abortJob(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.mapreduce.JobStatus$State)
org.apache.hadoop.yarn.client.api.impl.TimelineV2ClientImpl:close()
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:serviceStop()
org.apache.hadoop.yarn.service.monitor.ServiceMonitor:stop()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getClusterMetrics(org.apache.hadoop.yarn.api.protocolrecords.GetClusterMetricsRequest)
org.apache.hadoop.fs.azurebfs.oauth2.CustomTokenProviderAdapter:isTokenAboutToExpire()
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)
org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:mustLong(java.lang.String,long)
org.apache.hadoop.yarn.server.timelineservice.storage.entity.EntityTableRW:getResult(org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection,org.apache.hadoop.hbase.client.Get)
org.apache.hadoop.streaming.PipeCombiner:configure(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:logs()
org.apache.hadoop.fs.s3a.auth.delegation.RoleTokenBinding:start()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.crypto.key.kms.server.EagerKeyGeneratorKeyProviderCryptoExtension:warmUpEncryptedKeys(java.lang.String[])
org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList)
org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.gridmix.LoadJob$LoadMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.JobConf:setJobSubmitHostAddress(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.NoOpTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timelineservice.security.TimelineV2DelegationTokenSecretManagerService$TimelineV2DelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:moveReplicaFrom(org.apache.hadoop.hdfs.server.datanode.ReplicaInfo,java.io.File)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getCapacityUsed()
org.apache.hadoop.yarn.webapp.hamlet2.HamletGen:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:completedContainer(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer,org.apache.hadoop.yarn.api.records.ContainerStatus,org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEventType,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue,boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathConnectionRunner:getUrl()
org.apache.hadoop.mapreduce.lib.chain.ChainReducer:run(org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.streaming.PipeCombiner:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:getMaximumResourceCapability(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.LocalJobRunner:getLocalMaxRunningMaps(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.local.RawLocalFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.client.HdfsAdmin:setErasureCodingPolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AMFinishedAfterFinalSavingTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage$QueuesBlock:getCallerUGI()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.NodeResourceMonitor:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.SequenceFileAsBinaryOutputFormat:setSequenceFileOutputValueClass(org.apache.hadoop.mapred.JobConf,java.lang.Class)
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor$WindowsLocalWrapperScriptBuilder:writeLocalWrapperScript(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:must(java.lang.String,long)
org.apache.hadoop.registry.server.services.RegistryAdminService:mknode(java.lang.String,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:stop()
org.apache.hadoop.registry.conf.RegistryConfiguration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)
org.apache.hadoop.mapred.jobcontrol.Job:failJob(java.lang.String)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil$RandomTextDataMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:removeChildQueue(java.lang.String)
org.apache.hadoop.fs.azurebfs.Abfs:getServerDefaults()
org.apache.hadoop.yarn.service.ServiceScheduler:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:setDeprecatedProperties()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsOutputStream:write(int)
org.apache.hadoop.yarn.server.timelineservice.storage.reader.SubApplicationEntityReader:constructFilterListBasedOnFilters()
org.apache.hadoop.mapreduce.protocol.ClientProtocol:getFilesystemName()
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:serviceStop()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupState:prepareToExitState(org.apache.hadoop.hdfs.server.namenode.ha.HAContext)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext,org.apache.hadoop.hbase.client.Scan,org.apache.hadoop.hbase.regionserver.RegionScanner)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:setAvailableResources(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.http.server.HttpFSParametersProvider$AclPermissionParam:<init>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getErasureCodingCodecs()
org.apache.hadoop.yarn.server.router.webapp.FederationPage:render(java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:setBalancerBandwidth(long)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.tools.DFSAdmin:main(java.lang.String[])
org.apache.hadoop.fs.shell.CopyCommands$Cp:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.azure.Wasbs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getRange(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])
org.apache.hadoop.fs.cosn.CosNFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,int)
org.apache.hadoop.fs.shell.Delete$Expunge:run(java.lang.String[])
org.apache.hadoop.yarn.server.timelineservice.collector.NodeTimelineCollectorManager:addIfService(java.lang.Object)
org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.service.timelineservice.ServiceTimelinePublisher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:stop()
org.apache.hadoop.yarn.service.ServiceScheduler:start()
org.apache.hadoop.hdfs.server.blockmanagement.ProvidedStorageMap$ProvidedDescriptor:chooseStorage4Block(org.apache.hadoop.fs.StorageType,long)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.NoSplitTextInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getSafemode()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.client.ServerProxy:createRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,long,java.lang.String,long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore:removeApplication(org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp)
org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.api.impl.TimelineConnector:start()
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CreateOutputDirectoriesStage:delete(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.LocalFileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager:serviceStart()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineSchemaCreator:<clinit>()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:init(java.util.Properties)
org.apache.hadoop.hdfs.server.namenode.INodeDirectory:setUser(java.lang.String,int)
org.apache.hadoop.fs.viewfs.ChRootedFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.PositionStripeReader:readParityChunks(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoAppAttempt:setAMResource(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:readFields(java.io.DataInput)
org.apache.hadoop.yarn.api.records.QueueConfigurations:getConfiguredMinCapacity()
org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object)
org.apache.hadoop.fs.shell.Concat:expandArgument(java.lang.String)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.fs.s3a.auth.delegation.S3ADelegationTokens:close()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.client.MRClientService:noteFailure(java.lang.Exception)
org.apache.hadoop.util.InstrumentedWriteLock:tryLock(long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockChecksumCompositeCrcReconstructor:initDecoderIfNecessary()
org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RedirectionErrorPage:render(java.lang.Class)
org.apache.hadoop.mapreduce.security.token.delegation.DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.timelineservice.documentstore.reader.TimelineCollectionReader:<clinit>()
org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator:middleStep()
org.apache.hadoop.mapred.nativetask.serde.FloatWritableSerializer:deserialize(java.io.DataInput,int,org.apache.hadoop.io.Writable)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.router.webapp.NavBlock:renderPartial()
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:<init>(java.lang.Class,java.lang.Class,org.apache.hadoop.mapred.RecordWriter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:calculateAndGetAMResourceLimit()
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorCombiner:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.metrics2.MetricsSystemMXBean:stopMetricsMBeans()
org.apache.hadoop.fs.s3a.select.SelectInputStream:abort()
org.apache.hadoop.fs.sftp.SFTPFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.AMHeartbeatRequestHandler$HeartBeatThreadUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.yarn.service.webapp.ApiServer:getVersion()
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$24:getResponse(java.net.HttpURLConnection)
org.apache.hadoop.tools.mapred.CopyCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:needsTaskCommit(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.registry.client.impl.RegistryOperationsClient:createEnsembleProvider()
org.apache.hadoop.mapreduce.lib.output.NamedCommitterFactory:<clinit>()
org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getErasureCodingPolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getPassword(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupImage:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.SchedulerPlacementProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse)
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseFavouredNodes(java.lang.String,int,java.util.List,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElector:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol:replaceBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.fs.StorageType,org.apache.hadoop.security.token.Token,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo,java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:setLong(java.lang.String,long)
org.apache.hadoop.registry.conf.RegistryConfiguration:setStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:cancelDelegationToken(org.apache.hadoop.security.token.Token)
org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat:checkOutputSpecs(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:getEffectiveCapacity(java.lang.String)
org.apache.hadoop.fs.azurebfs.utils.CachedSASToken:<clinit>()
org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB:killTaskAttempt(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.mapreduce.v2.proto.MRServiceProtos$KillTaskAttemptRequestProto)
org.apache.hadoop.oncrpc.RpcProgram:<init>(java.lang.String,java.lang.String,int,int,int,int,java.net.DatagramSocket,boolean)
org.apache.hadoop.yarn.csi.adaptor.CsiAdaptorServices:close()
org.apache.hadoop.security.LdapGroupsMapping:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService:serviceStart()
org.apache.hadoop.mapred.SortedRanges:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DeletionService:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl:serviceStart()
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:initiateTaskOperation(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.store.DataBlocks:<clinit>()
org.apache.hadoop.fs.azure.SecureWasbRemoteCallHelper:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor:getRunCommand(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.util.MRJobConfUtil:initEncryptedIntermediateConfigsForTesting(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.HarFileSystem:getServerDefaults()
org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher$MultiThreadedDispatcher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.util.MD5FileUtils:verifySavedMD5(java.io.File,org.apache.hadoop.io.MD5Hash)
org.apache.hadoop.hdfs.server.datanode.FileIoProvider$WrappedRandomAccessFile:read()
org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider:<clinit>()
org.apache.hadoop.mapreduce.lib.input.FixedLengthInputFormat:listStatus(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getCanonicalServiceName()
org.apache.hadoop.yarn.sls.SLSRunner$1:recover(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$RMState)
org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:stop()
org.apache.hadoop.fs.local.RawLocalFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.conf.ConfigurationWithLogging:write(java.io.DataOutput)
org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl:setCapability(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:excludeNodeByLoad(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor)
org.apache.hadoop.security.authentication.server.JWTRedirectAuthenticationHandler:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.QueueManagementDynamicEditPolicy:<init>(org.apache.hadoop.yarn.server.resourcemanager.RMContext,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getStoragePolicies()
org.apache.hadoop.fs.shell.CopyCommands$CopyToLocal:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)
org.apache.hadoop.hdfs.server.namenode.EditLogBackupInputStream:resync()
org.apache.hadoop.lib.service.security.GroupsService:init(org.apache.hadoop.lib.server.Server)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppLogAggregationStatusBlock:render()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:createReleaseCache()
org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.PlacementConstraintManager:getConstraints(org.apache.hadoop.yarn.api.records.ApplicationId)
org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator:start()
org.apache.hadoop.yarn.event.AsyncDispatcher:<clinit>()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.lib.aggregate.ValueAggregatorMapper:map(java.lang.Object,java.lang.Object,org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.yarn.client.api.impl.YarnClientImpl:getApplications(org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.security.LocalizerSecurityInfo:<clinit>()
org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.webapp.LogWebServiceUtils:<clinit>()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:removeFromClusterNodeLabels(org.apache.hadoop.yarn.server.api.protocolrecords.RemoveFromClusterNodeLabelsRequest)
org.apache.hadoop.fs.ftp.FTPFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.registry.conf.RegistryConfiguration:getClass(java.lang.String,java.lang.Class)
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:getDataInputStream(long)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:<clinit>()
org.apache.hadoop.mapred.lib.CombineSequenceFileInputFormat:isSplitable(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)
org.apache.hadoop.security.CompositeGroupsMapping:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerPullCommand:preparePrivilegedOperation(org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand,java.lang.String,java.util.Map,org.apache.hadoop.yarn.server.nodemanager.Context)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindow:<clinit>()
org.apache.hadoop.mapreduce.lib.input.NLineInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.server.resourcemanager.federation.FederationStateStoreService:stop()
org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.io.LongWritable:<clinit>()
org.apache.hadoop.hdfs.tools.DFSZKFailoverController:startRPC()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor:stop()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:application()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:submitApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:handleStoreEvent(org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEvent)
org.apache.hadoop.yarn.server.resourcemanager.reservation.CapacityReservationSystem:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:unset(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger:logSuccess(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.ipc.CallerContext,java.lang.String)
org.apache.hadoop.http.HttpRequestLog:<clinit>()
org.apache.hadoop.mapred.JobClient:displayTasks(org.apache.hadoop.mapred.JobID,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.service.webapp.ApiServer:deleteService(javax.servlet.http.HttpServletRequest,java.lang.String)
org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl:hashCode()
org.apache.hadoop.hdfs.server.namenode.BackupImage:hasRollbackFSImage()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:getDataOutputStream(boolean)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:finishApplicationMaster(org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest)
org.apache.hadoop.fs.azurebfs.services.AbfsInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.HAUtilClient:<clinit>()
org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService:serviceStop()
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:moveAllApps(java.lang.String,java.lang.String)
org.apache.hadoop.fs.cosn.CosNInputStream:read(long,byte[],int,int)
org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileReInitTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.health.NodeHealthCheckerService:serviceStop()
org.apache.hadoop.fs.FileSystem:<clinit>()
org.apache.hadoop.security.authentication.util.RandomSignerSecretProvider:rollSecret()
org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:displayError(java.lang.Exception)
org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.security.token.DtUtilShell$Get:validate()
org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:abort()
org.apache.hadoop.resourceestimator.solver.preprocess.SolverPreprocessor:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.VolumeManagerImpl:close()
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:startThreads()
org.apache.hadoop.yarn.server.sharedcachemanager.metrics.SharedCacheUploaderMetrics:<clinit>()
org.apache.hadoop.mapred.gridmix.GenerateDistCacheData$GenDCDataMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.mapred.nativetask.serde.BoolWritableSerializer:deserialize(java.io.DataInput,int,java.lang.Object)
org.apache.hadoop.yarn.server.resourcemanager.AdminService:stop()
org.apache.hadoop.tools.RegexCopyFilter:initialize()
org.apache.hadoop.conf.ConfigurationWithLogging:getStrings(java.lang.String)
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:must(java.lang.String,long)
org.apache.hadoop.hdfs.protocol.HdfsFileStatus:write(java.io.DataOutput)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl:hashCode()
org.apache.hadoop.fs.s3native.NativeS3FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.reservation.FairReservationSystem:close()
org.apache.hadoop.fs.s3a.prefetch.S3APrefetchingInputStream:<clinit>()
org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:<clinit>()
org.apache.hadoop.yarn.util.resource.DominantResourceCalculator:<clinit>()
org.apache.hadoop.streaming.mapreduce.StreamInputFormat:addInputPathRecursively(java.util.List,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.util.SignalLogger$Handler:handle(sun.misc.Signal)
org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager$StoragePurger:purgeLog(org.apache.hadoop.hdfs.server.namenode.FileJournalManager$EditLogFile)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.io.InputStream,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getRaw(java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.webapp.SCMController:renderJSON(java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)
org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.LocalFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:getUsed(org.apache.hadoop.fs.Path)
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_asyncRead_jmhTest:asyncRead_SampleTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.yarn.server.resourcemanager.volume.csi.lifecycle.VolumeImpl:<clinit>()
org.apache.hadoop.conf.Configuration:<clinit>()
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getEstimatedCapacityLostTotal()
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readFully(long,byte[],int,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:createQueueMetricsForCustomResources()
org.apache.hadoop.util.Preconditions:checkState(boolean,java.util.function.Supplier)
org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:delete(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:getHomeDirectory()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue:submitApplicationAttempt(org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp,java.lang.String,boolean)
org.apache.hadoop.fs.LocalFileSystem:getCanonicalServiceName()
org.apache.hadoop.mapred.ShuffleHandler:<init>()
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:serviceStart()
org.apache.hadoop.mapred.gridmix.GenerateData$RawBytesOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditManagerS3A:noteFailure(java.lang.Exception)
org.apache.hadoop.mapred.JobContext:getFileClassPaths()
org.apache.hadoop.mapreduce.lib.output.PartialFileOutputCommitter:commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV2Publisher:start()
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController:appattempt()
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.client.api.async.AMRMClientAsync:getMatchingRequests(org.apache.hadoop.yarn.api.records.Priority,java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.yarn.sls.scheduler.FairSchedulerMetrics:init(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.lib.MultipleTextOutputFormat:getInputFileBasedOutputFileName(org.apache.hadoop.mapred.JobConf,java.lang.String)
org.apache.hadoop.examples.WordStandardDeviation:main(java.lang.String[])
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:metadataExists()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread:<clinit>()
org.apache.hadoop.registry.client.types.Endpoint$Marshal:fromBytes(byte[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.webapp.ApiServer:getComponentInstances(javax.servlet.http.HttpServletRequest,java.lang.String,java.util.List,java.lang.String,java.util.List)
org.apache.hadoop.hdfs.server.balancer.Balancer:<clinit>()
org.apache.hadoop.examples.terasort.TeraGen:main(java.lang.String[])
org.apache.hadoop.mapred.LineRecordReader:<init>(java.io.InputStream,long,long,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.S3ABlockOutputStream:close()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:sendLifeline(org.apache.hadoop.hdfs.server.protocol.DatanodeRegistration,org.apache.hadoop.hdfs.server.protocol.StorageReport[],long,long,int,int,int,org.apache.hadoop.hdfs.server.protocol.VolumeFailureSummary)
org.apache.hadoop.nfs.nfs3.Nfs3Interface:commit(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.fs.shell.find.And:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getPasswordFromCredentialProviders(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock:render()
org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:close()
org.apache.hadoop.ipc.RPC:<clinit>()
org.apache.hadoop.mapreduce.v2.app.local.LocalContainerAllocator:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:serviceStart()
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:logUpdateBlocks(java.lang.String,org.apache.hadoop.hdfs.server.namenode.INodeFile,boolean)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer$HistoryServerSecretManagerService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ConfigurationNodeAttributesProvider:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.registry.client.api.RegistryOperationsFactory:createKerberosInstance(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:serviceStop()
org.apache.hadoop.hdfs.qjournal.server.Journal:close()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:append(org.apache.hadoop.fs.Path)
org.apache.hadoop.util.ShutdownThreadsHelper:<clinit>()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:appendFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:<clinit>()
org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:must(java.lang.String,int)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:resolveLink(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:stop()
org.apache.hadoop.mapred.gridmix.GenerateData:toString()
org.apache.hadoop.mapred.QueueManager:dumpConfiguration(java.io.Writer,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.NflyFSystem:getServerDefaults()
org.apache.hadoop.hdfs.server.datanode.ReplicaBeingWritten:bumpReplicaGS(long)
org.apache.hadoop.ha.HAAdmin:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.collectormanager.NMCollectorService:<clinit>()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getAttributesToNodes(org.apache.hadoop.yarn.api.protocolrecords.GetAttributesToNodesRequest)
org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl:toString()
org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer:serviceStop()
org.apache.hadoop.fs.local.LocalFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.service.ClientAMService:close()
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:setScriptExecutable(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.DU:main(java.lang.String[])
org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryUtils:<clinit>()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo)
org.apache.hadoop.registry.server.dns.RegistryDNS:<clinit>()
org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy:chooseStorage4Block(org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,long,java.util.List,org.apache.hadoop.fs.StorageType)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:create(java.lang.String,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,org.apache.hadoop.io.EnumSetWritable,boolean,short,long,org.apache.hadoop.crypto.CryptoProtocolVersion[],java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.fs.azurebfs.AbfsConfiguration:getString(java.lang.String,java.lang.String)
org.apache.hadoop.examples.RandomTextWriter$RandomTextMapper:run(org.apache.hadoop.mapreduce.Mapper$Context)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BNHAContext:writeUnlock()
org.apache.hadoop.hdfs.protocol.datatransfer.WhitelistBasedTrustedChannelResolver:isTrusted()
org.apache.hadoop.yarn.client.api.SharedCacheClient:release(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices:getAppAttempt(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String)
org.apache.hadoop.mapred.nativetask.serde.VLongWritableSerializer:serialize(org.apache.hadoop.io.Writable,java.io.DataOutput)
org.apache.hadoop.yarn.server.resourcemanager.webapp.MetricsOverviewTable:render()
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getAppAttempts(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:makeQualified(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:submitReservation(org.apache.hadoop.yarn.api.protocolrecords.ReservationSubmissionRequest)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.PlacementConstraintProcessor:finishApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterResponse)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices:getApp(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String)
org.apache.hadoop.fs.shell.SnapshotCommands:expandArgument(java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.HistoryClientService:serviceStop()
org.apache.hadoop.mapreduce.v2.app.webapp.ConfBlock:renderPartial()
org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:getSchedulerAppInfo(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.fs.s3a.audit.impl.NoopAuditor:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.Hdfs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:getEditLogManifest(long)
org.apache.hadoop.yarn.applications.distributedshell.Client:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:serviceStart()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:serviceStop()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.AbortTaskStage:createNewDirectory(java.lang.String,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.viewfs.ChRootedFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.hdfs.server.federation.router.Router:addIfService(java.lang.Object)
org.apache.hadoop.fs.s3a.select.SelectBinding:<clinit>()
org.apache.hadoop.fs.azure.security.TokenUtils:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:scheduler()
org.apache.hadoop.yarn.conf.YarnConfiguration:getClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.v2.api.MRDelegationTokenIdentifier:getUser()
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:stop()
org.apache.hadoop.fs.impl.prefetch.BufferPool:<clinit>()
org.apache.hadoop.fs.FsShellPermissions$Chgrp:run(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.client.RMHAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:save(org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.AbstractManifestData,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.INodeFile:removeXAttrFeature(int)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:expandArguments(java.util.LinkedList)
org.apache.hadoop.mapred.gridmix.JobFactory$1:getNumberMaps()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.fs.LocalFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:finishApplication(org.apache.hadoop.yarn.api.records.ApplicationId,java.lang.String)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$3:connect(java.net.URL)
org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProviderWithIPFailover:<clinit>()
org.apache.hadoop.yarn.server.router.webapp.AboutBlock:renderPartial()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getFileLinkInfo(java.lang.String)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:<clinit>()
org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore:close()
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.conf.ConfigurationWithLogging:getConfResourceAsReader(java.lang.String)
org.apache.hadoop.tools.FileBasedCopyListing:buildListing(org.apache.hadoop.fs.Path,org.apache.hadoop.tools.DistCpContext)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController:failure()
org.apache.hadoop.metrics2.lib.MutableMetricsFactory:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo:populateQueueResourceUsage(org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage)
org.apache.hadoop.yarn.sls.SLSRunner$1:removeService(org.apache.hadoop.service.Service)
org.apache.hadoop.hdfs.DFSStripedInputStream:removeFromLocalDeadNodes(org.apache.hadoop.hdfs.protocol.DatanodeInfo)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:concat(java.lang.String,java.lang.String[])
org.apache.hadoop.ipc.Client$IpcStreams:close()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:setErasureCodingPolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.LocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:getChild(org.apache.hadoop.hdfs.server.namenode.INodeDirectory,byte[],int)
org.apache.hadoop.mapred.JobConf:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getReplanner(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getReservationWindow(java.lang.String)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.cosn.CosNFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.server.services.RegistryAdminService:delete(java.lang.String,boolean)
org.apache.hadoop.mapred.ReduceTask:keepTaskFiles(org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$TimesOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.mapred.TaskAttemptListenerImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.applications.mawo.server.common.SimpleTask:<init>(org.apache.hadoop.applications.mawo.server.common.Task)
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.ReservationListResponsePBImpl:hashCode()
org.apache.hadoop.hdfs.server.datanode.fsdataset.ReplicaOutputStreams:flushChecksumOut()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils:checkSchedContainerChangeRequest(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest,boolean)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:cleanupBeforeRelaunch(org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container)
org.apache.hadoop.mapreduce.Counters:addGroup(org.apache.hadoop.mapreduce.counters.CounterGroupBase)
org.apache.hadoop.mapred.SequenceFileAsBinaryInputFormat:getRecordReader(org.apache.hadoop.mapred.InputSplit,org.apache.hadoop.mapred.JobConf,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:isSecurityEnabled()
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation:<clinit>()
org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:verify(java.lang.String,javax.net.ssl.SSLSession)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.lib.CombineTextInputFormat:isSplitable(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.sls.SLSRunner$1:addService(org.apache.hadoop.service.Service)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getTotalPendingResourcesConsideringUserLimit(org.apache.hadoop.yarn.api.records.Resource,java.lang.String,boolean)
org.apache.hadoop.fs.s3a.commit.files.PendingSet:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getXAttrs(java.lang.String,java.util.List)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.util.DistCpUtils:<clinit>()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStore:applicationAttemptStarted(org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptStartData)
org.apache.hadoop.security.JniBasedUnixGroupsMapping:<clinit>()
org.apache.hadoop.mapred.gridmix.PseudoLocalFs:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:mustDouble(java.lang.String,double)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch:resumeContainer()
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.benchmark.generated.VectoredReadBenchmark_syncRead_jmhTest:syncRead_SampleTime(org.openjdk.jmh.runner.InfraControl,org.openjdk.jmh.infra.ThreadParams)
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getEntity(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor:serviceStop()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:create(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.sftp.SFTPFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String)
org.apache.hadoop.fs.sftp.SFTPFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.registry.client.impl.zk.RegistrySecurity:createACLForUser(org.apache.hadoop.security.UserGroupInformation,int)
org.apache.hadoop.mapreduce.lib.join.OuterJoinRecordReader:createKey()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.tools.dynamometer.workloadgenerator.audit.AuditReplayCommand$PoisonPillCommand:getSimpleUgi()
org.apache.hadoop.fs.sftp.SFTPFileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdater:start()
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.datanode.VolumeScanner:<clinit>()
org.apache.hadoop.fs.http.HttpFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)
org.apache.hadoop.yarn.webapp.view.LipsumBlock:render(java.lang.Class)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:singleTaskCounter()
org.apache.hadoop.fs.azure.Wasbs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CpuResourceHandler:bootstrap(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.federation.metrics.FederationMBean:getNumDecommissioningNodes()
org.apache.hadoop.hdfs.server.federation.store.StateStoreCacheUpdateService:start()
org.apache.hadoop.io.nativeio.NativeIO$Windows:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SetupTaskStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:setEntitlement(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.QueueEntitlement)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setReplication(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:getEditsFromTxid(long)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.yarn.server.timelineservice.collector.AppLevelTimelineCollectorWithAgg:putEntities(org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntities,org.apache.hadoop.security.UserGroupInformation)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService$NMTokensStateIterator:next()
org.apache.hadoop.fs.shell.Display$Checksum:run(java.lang.String[])
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.service.provider.defaultImpl.DefaultClientProvider:validateConfigFiles(java.util.List,java.lang.String,org.apache.hadoop.fs.FileSystem)
org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter:init(javax.servlet.FilterConfig)
org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore:storeNewReservation(org.apache.hadoop.yarn.proto.YarnProtos$ReservationAllocationStateProto,java.lang.String,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker:<clinit>()
org.apache.hadoop.hdfs.tools.DFSHAAdmin:main(java.lang.String[])
org.apache.hadoop.fs.ftp.FTPFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.conf.ConfigurationWithLogging:addResource(java.io.InputStream)
org.apache.hadoop.mapred.lib.MultipleSequenceFileOutputFormat:checkOutputSpecs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.mapred.JobConf)
org.apache.hadoop.mount.MountInterface:umntall(org.apache.hadoop.oncrpc.XDR,int,java.net.InetAddress)
org.apache.hadoop.yarn.server.nodemanager.amrmproxy.AMRMProxyService:addIfService(java.lang.Object)
org.apache.hadoop.hdfs.server.datanode.DataNode:createDataNode(java.lang.String[],org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)
org.apache.hadoop.net.NetworkTopology:<clinit>()
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:seekToNewSource(long)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.examples.DBCountPageView$PageviewReducer:reduce(java.lang.Object,java.lang.Iterable,org.apache.hadoop.mapreduce.Reducer$Context)
org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.shell.Display$Text:displayError(java.lang.Exception)
org.apache.hadoop.fs.aliyun.oss.OSS:open(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore:updateApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)
org.apache.hadoop.fs.adl.Adl:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.util.WeakReferenceMap:<clinit>()
org.apache.hadoop.fs.s3a.impl.ChangeDetectionPolicy$VersionIdChangeDetectionPolicy:onChangeDetected(java.lang.String,java.lang.String,java.lang.String,long,java.lang.String,long)
org.apache.hadoop.fs.adl.AdlFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.mapreduce.CounterGroup:write(java.io.DataOutput)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getPendingReconstructionBlocks()
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:getLogDirPermissions()
org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer:stop()
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:serviceStop()
org.apache.hadoop.yarn.conf.YarnConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.hdfs.server.namenode.ha.StandbyState:setState(org.apache.hadoop.hdfs.server.namenode.ha.HAContext,org.apache.hadoop.hdfs.server.namenode.ha.HAState)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:close()
org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl:<clinit>()
org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.ha.InMemoryAliasMapFailoverProxyProvider:getResolvedHostsIfNecessary(java.util.Collection,java.net.URI)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:opt(java.lang.String,float)
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowRunCoprocessor:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.shell.FsUsage$Dus:expandArgument(java.lang.String)
org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getMinimumAllocation()
org.apache.hadoop.yarn.server.resourcemanager.reservation.planning.SimpleCapacityReplanner:<clinit>()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$NativeAzureFsInputStream:read(long,byte[],int,int)
org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.CommitJobStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.fs.Hdfs:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.metrics.FSNamesystemMBean:getNumEnteringMaintenanceDataNodes()
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:openFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.join.TupleWritable:write(java.io.DataOutput)
org.apache.hadoop.yarn.service.ClientAMService:serviceStop()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:getEZForPath(java.lang.String)
org.apache.hadoop.hdfs.protocolPB.InMemoryAliasMapProtocolClientSideTranslatorPB:close()
org.apache.hadoop.hdfs.server.namenode.sps.StoragePolicySatisfyManager:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.ValidateRenamedFilesStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider:isTokenAboutToExpire()
org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure:create(org.apache.hadoop.fs.Path,boolean,int)
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.util.RunJar:<clinit>()
org.apache.hadoop.fs.shell.Display$Text:run(java.lang.String[])
org.apache.hadoop.fs.azurebfs.services.AbfsHttpOperation$AbfsHttpOperationWithFixedResult:sendRequest(byte[],int,int)
org.apache.hadoop.mapred.join.OverrideRecordReader:add(org.apache.hadoop.mapred.join.ComposableRecordReader)
org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorServer:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher:stop()
org.apache.hadoop.fs.shell.MoveCommands$Rename:getLocalDestination(java.util.LinkedList)
org.apache.hadoop.registry.conf.RegistryConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:rollMasterKey()
org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager:checkAccess(org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier,java.lang.String,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier$AccessMode,org.apache.hadoop.fs.StorageType[])
org.apache.hadoop.hdfs.ClientContext:getFromConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapred.lib.InputSampler:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:deleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.app.MRAppMaster$ContainerAllocatorRouter:close()
org.apache.hadoop.fs.s3a.S3AFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)
org.apache.hadoop.io.compress.DeflateCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:updateModificationTime(long,int)
org.apache.hadoop.crypto.CryptoInputStream:close()
org.apache.hadoop.mapred.lib.CombineTextInputFormat$TextRecordReaderWrapper:<init>(org.apache.hadoop.mapred.lib.CombineFileSplit,org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapred.Reporter,java.lang.Integer)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:read(byte[])
org.apache.hadoop.security.token.Token$PrivateToken:isManaged()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:getLocalPath(java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:renameDir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:job()
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:<init>()
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.webapp.view.FooterBlock:renderPartial()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService$TimelineV1DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics:render(java.lang.Class)
org.apache.hadoop.fs.s3native.NativeS3FileSystem:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.http.HttpsFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.fs.azure.Wasb:getFileStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readFully(long,byte[])
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.hdfs.server.diskbalancer.command.ReportCommand:create(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.Checkpointer:getImageListenAddress()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:getEffectiveCapacityDown(java.lang.String,org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.aliyun.oss.AliyunOSSBlockOutputStream:write(int)
org.apache.hadoop.util.ApplicationClassLoader:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService:serviceStart()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:updateClusterResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.fs.azurebfs.services.AbfsClientThrottlingIntercept:<clinit>()
org.apache.hadoop.fs.cosn.CosNFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:abortJobInternal(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String,java.lang.String[],java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.job.impl.ReduceTaskImpl:canCommit(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId)
org.apache.hadoop.yarn.service.client.ApiServiceClient:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor:registerApplicationMaster(org.apache.hadoop.yarn.api.records.ApplicationAttemptId,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterRequest,org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse)
org.apache.hadoop.hdfs.tools.DelegationTokenFetcher:main(java.lang.String[])
org.apache.hadoop.mapred.gridmix.Gridmix:main(java.lang.String[])
org.apache.hadoop.fs.shell.TouchCommands$Touchz:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.service.utils.PublishedConfigurationOutputter$JsonOutputter:save(java.io.OutputStream)
org.apache.hadoop.conf.ConfigurationWithLogging:getTrimmedStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.tools.DistCpOptions:<clinit>()
org.apache.hadoop.io.file.tfile.Compression:<clinit>()
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,long,java.lang.String,java.lang.String)
org.apache.hadoop.mapreduce.Job$5:run()
org.apache.hadoop.yarn.client.api.impl.TimelineReaderClientImpl:close()
org.apache.hadoop.registry.server.dns.RegistryDNS:signZones()
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider:start()
org.apache.hadoop.fs.s3a.s3guard.S3GuardTool:<clinit>()
org.apache.hadoop.mapred.gridmix.EchoUserResolver:<init>()
org.apache.hadoop.tools.dynamometer.BlockPlacementPolicyAlwaysSatisfied:chooseRemoteRack(int,org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor,java.util.Set,long,int,java.util.List,boolean,java.util.EnumMap)
org.apache.hadoop.registry.conf.RegistryConfiguration:setClass(java.lang.String,java.lang.Class,java.lang.Class)
org.apache.hadoop.hdfs.server.diskbalancer.command.CancelCommand:getNodeList(java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:set(java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.examples.terasort.TeraOutputFormat:getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync:startLogSegment(long,boolean,int)
org.apache.hadoop.fs.shell.FsUsage$Dus:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.YarnChild$2:run()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueMetrics:deactivateApp(java.lang.String)
org.apache.hadoop.fs.s3a.S3AFileSystem:getXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.Head:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock:getCallerUGI()
org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)
org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.resourceestimator.service.ResourceEstimatorService:deleteHistoryResourceSkyline(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:setBooleanIfUnset(java.lang.String,boolean)
org.apache.hadoop.hdfs.server.namenode.BackupImage:openEditLogForWrite(int)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:writeLaunchEnv(java.io.OutputStream,java.util.Map,java.util.Map,java.util.List,org.apache.hadoop.fs.Path,java.lang.String,java.util.LinkedHashSet)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getAdditionalDatanode(java.lang.String,long,org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],java.lang.String[],org.apache.hadoop.hdfs.protocol.DatanodeInfo[],int,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil$2:blockDataExists()
org.apache.hadoop.fs.FsShell$Usage:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:getContentSummary(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapred.nativetask.handlers.BufferPushee:collect(org.apache.hadoop.mapred.nativetask.buffer.InputBuffer)
org.apache.hadoop.tools.mapred.CopyCommitter:commitJobInternal(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.crypto.key.kms.server.KMSWebServer:<clinit>()
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getMostRecentCheckpointTxId()
org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)
org.apache.hadoop.yarn.client.api.impl.DirectTimelineWriter:putEntities(org.apache.hadoop.yarn.api.records.timeline.TimelineEntity[])
org.apache.hadoop.mapred.pipes.PipesMapRunner:run(org.apache.hadoop.mapred.RecordReader,org.apache.hadoop.mapred.OutputCollector,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.OCIContainerRuntime:<clinit>()
org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage:openEditLogForWrite(int)
org.apache.hadoop.hdfs.client.HdfsAdmin:enableErasureCodingPolicy(java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.NetworkPacketTaggingHandlerImpl:<clinit>()
org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[])
org.apache.hadoop.fs.viewfs.InodeTree:<clinit>()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getAvailablePhysicalMemorySize()
org.apache.hadoop.hdfs.tools.DFSAdmin$SetSpaceQuotaCommand:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.hdfs.server.namenode.INodeReference$WithCount:cleanSubtree(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,int,int)
org.apache.hadoop.fs.azure.Wasb:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.s3a.S3AFileSystem:getAdditionalTokenIssuers()
org.apache.hadoop.fs.s3a.S3AFileSystem:cancelDeleteOnExit(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger:logFailure(java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.ipc.WritableRpcEngine$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PartitionQueueMetrics:setAvailableResourcesToQueue(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSInputStream:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.TaskAttemptScanDirectoryStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.hdfs.tools.DFSAdmin$SetQuotaCommand:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil:setCompressionEmulationEnabled(org.apache.hadoop.conf.Configuration,boolean)
org.apache.hadoop.fs.SWebHdfs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsController:info()
org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.sftp.SFTPFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:serviceStop()
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$AllowSnapshotOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService:serviceStop()
org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage$QueueInfoBlock:render(java.lang.Class)
org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:<clinit>()
org.apache.hadoop.fs.shell.Stat:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$1:run()
org.apache.hadoop.yarn.api.records.ApplicationReport:newInstance(org.apache.hadoop.yarn.api.records.ApplicationId,org.apache.hadoop.yarn.api.records.ApplicationAttemptId,java.lang.String,java.lang.String,java.lang.String,java.lang.String,int,org.apache.hadoop.yarn.api.records.Token,org.apache.hadoop.yarn.api.records.YarnApplicationState,java.lang.String,java.lang.String,long,long,org.apache.hadoop.yarn.api.records.FinalApplicationStatus,org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport,java.lang.String,float,java.lang.String,org.apache.hadoop.yarn.api.records.Token,java.util.Set,boolean,org.apache.hadoop.yarn.api.records.Priority,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu.GpuResourcePlugin:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:isDirectory(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:post(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,org.apache.hadoop.hdfs.web.resources.UriFsPathParam,org.apache.hadoop.hdfs.web.resources.PostOpParam,org.apache.hadoop.hdfs.web.resources.ConcatSourcesParam,org.apache.hadoop.hdfs.web.resources.BufferSizeParam,org.apache.hadoop.hdfs.web.resources.ExcludeDatanodesParam,org.apache.hadoop.hdfs.web.resources.NewLengthParam,org.apache.hadoop.hdfs.web.resources.NoRedirectParam)
org.apache.hadoop.yarn.service.ServiceScheduler$1:load(java.lang.Object)
org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:<clinit>()
org.apache.hadoop.fs.shell.Count:expandArgument(java.lang.String)
org.apache.hadoop.hdfs.DataStreamer$StreamerStreams:close()
org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitterFactory:createOutputCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.LocalConfigurationProvider:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.hdfs.DFSStripedOutputStream:computePacketChunkSize(int,int)
org.apache.hadoop.hdfs.server.namenode.INodeFile:isInCurrentState()
org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor:resetPerfCounters()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:listXAttrs(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.XAttrCommands:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithUpgradeDomain:chooseDataNode(java.lang.String,java.util.Collection)
org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)
org.apache.hadoop.fs.viewfs.NflyFSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream:scanNextOp()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt:hasPendingResourceRequest(java.lang.String,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:isInLatestSnapshot(int)
org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationAttemptState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData)
org.apache.hadoop.hdfs.DFSUtilClient$2:rejectedExecution(java.lang.Runnable,java.util.concurrent.ThreadPoolExecutor)
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:stop()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ReservationQueue:apply(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest)
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock:renderPartial()
org.apache.hadoop.metrics2.util.MetricsCache:<clinit>()
org.apache.hadoop.hdfs.ViewDistributedFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)
org.apache.hadoop.hdfs.server.federation.router.RouterHeartbeatService:serviceStart()
org.apache.hadoop.mapred.MapTaskAttemptImpl:recover(org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser$TaskAttemptInfo,org.apache.hadoop.mapreduce.OutputCommitter,boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:writeXml(java.lang.String,java.io.Writer)
org.apache.hadoop.fs.s3a.commit.staging.PartitionedStagingCommitter:listPendingUploads(org.apache.hadoop.fs.s3a.commit.impl.CommitContext,boolean)
org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat$TextRecordReaderWrapper:initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.api.impl.pb.client.CollectorNodemanagerProtocolPBClientImpl:<init>(long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore:updateApplicationState(org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData,boolean)
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl:<clinit>()
org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processRawArguments(java.util.LinkedList)
org.apache.hadoop.fs.azurebfs.services.AbfsPerfTracker:<clinit>()
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:getAllNodes()
org.apache.hadoop.fs.s3a.impl.StoreContext:incrementGauge(org.apache.hadoop.fs.s3a.Statistic,long)
org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:getLocalDestination(java.util.LinkedList)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:processDeleteOnExit()
org.apache.hadoop.yarn.server.nodemanager.webapp.NodePage$NodeBlock:render(java.lang.Class)
org.apache.hadoop.hdfs.tools.offlineImageViewer.PBImageTextWriter$LevelDBMetadataMap:close()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setIfUnset(java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.service.utils.SliderFileSystem:listFSDir(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.converter.FSConfigToCSConfigRuleHandler:handleDynamicMaxAssign()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:renameFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.api.ContainerShellWebSocket:onConnect(org.eclipse.jetty.websocket.api.Session)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:append(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$AliasMapStorageDirectory:clearDirectory()
org.apache.hadoop.mapreduce.v2.app.webapp.JobsBlock:render()
org.apache.hadoop.yarn.server.sharedcache.SharedCacheUtil:<clinit>()
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.RenameFilesStage:mkdirs(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaTracker:<clinit>()
org.apache.hadoop.yarn.client.api.async.NMClientAsync:restartContainerAsync(org.apache.hadoop.yarn.api.records.ContainerId)
org.apache.hadoop.fs.http.client.HttpsFSFileSystem:resolvePath(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.MutableConfScheduler:checkAccess(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.yarn.api.records.QueueACL,java.lang.String)
org.apache.hadoop.io.file.tfile.TFileDumper:<clinit>()
org.apache.hadoop.yarn.service.containerlaunch.JavaCommandLineBuilder:addPrefixedConfOptions(org.apache.hadoop.conf.Configuration,java.lang.String)
org.apache.hadoop.fs.viewfs.NflyFSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.shell.find.Find:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])
org.apache.hadoop.fs.s3a.prefetch.S3ACachingBlockManager:numAvailable()
org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl:toString()
org.apache.hadoop.fs.shell.find.Find:processPathArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.router.Router:<clinit>()
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:getNewApplication(org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationRequest)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:get(java.lang.String)
org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)
org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSErrorsAndWarningsPage:render(java.lang.Class)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:enableECPolicy(java.lang.String)
org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<clinit>()
org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader:<clinit>()
org.apache.hadoop.nfs.nfs3.Nfs3Interface:lookup(org.apache.hadoop.oncrpc.XDR,org.apache.hadoop.oncrpc.RpcInfo)
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver$NoSplitTextInputFormat:getSplits(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:truncate(java.lang.String,long,java.lang.String)
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.server.federation.router.RouterPermissionChecker:checkPermission(java.lang.String,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.server.namenode.INodeAttributes[],org.apache.hadoop.hdfs.server.namenode.INode[],byte[][],int,java.lang.String,int,boolean,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$23:getUrl()
org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock:<init>(org.apache.hadoop.yarn.webapp.View$ViewContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.yarn.server.resourcemanager.ResourceManager)
org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])
org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreZooKeeperImpl:init(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Collection,org.apache.hadoop.hdfs.server.federation.metrics.StateStoreMetrics)
org.apache.hadoop.io.ByteWritable:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore:putWithNoDomainId(org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)
org.apache.hadoop.fs.shell.Count:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler:<clinit>()
org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:listOpenFiles(long)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksBlock:getCallerUGI()
org.apache.hadoop.mapred.pipes.DownwardProtocol:start()
org.apache.hadoop.mapred.Counters$FrameworkGroupImpl:findCounter(java.lang.String)
org.apache.hadoop.yarn.server.timelineservice.storage.TimelineSchemaCreator:main(java.lang.String[])
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:registerAppAttempt(org.apache.hadoop.yarn.api.records.ApplicationAttemptId)
org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer:serviceStart()
org.apache.hadoop.yarn.webapp.view.FooterBlock:getCallerUGI()
org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorJob:createValueAggregatorJobs(java.lang.String[])
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:versionRequest()
org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService:noteFailure(java.lang.Exception)
org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices:postEntities(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,org.apache.hadoop.yarn.api.records.timeline.TimelineEntities)
org.apache.hadoop.registry.server.services.RegistryAdminService:verifyRealmValidity()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])
org.apache.hadoop.hdfs.server.namenode.snapshot.Snapshot$Root:destroyAndCollectBlocks(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext)
org.apache.hadoop.fs.http.HttpsFileSystem:getLength(org.apache.hadoop.fs.Path)
org.apache.hadoop.service.launcher.InterruptEscalator:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getTrimmed(java.lang.String)
org.apache.hadoop.mapreduce.Job:setCacheFiles(java.net.URI[])
org.apache.hadoop.mapred.LocalContainerLauncher:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.api.records.impl.pb.UpdateContainerErrorPBImpl:hashCode()
org.apache.hadoop.hdfs.server.namenode.INodeReference$DstReference:getStoragePolicyID()
org.apache.hadoop.hdfs.server.federation.store.impl.RouterStoreImpl:overrideExpiredRecords(org.apache.hadoop.hdfs.server.federation.store.records.QueryResult)
org.apache.hadoop.yarn.api.records.impl.pb.SchedulingRequestPBImpl:equals(java.lang.Object)
org.apache.hadoop.tools.mapred.CopyCommitter:cleanupJob(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.fs.s3a.select.SelectInputStream:read(long,byte[],int,int)
org.apache.hadoop.hdfs.DistributedFileSystem$HdfsDataOutputStreamBuilder:must(java.lang.String,java.lang.String[])
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.fs.shell.Mkdir:runAll()
org.apache.hadoop.hdfs.server.federation.router.NamenodeHeartbeatService:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.FsShellPermissions$Chgrp:processArgument(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration:addResource(org.apache.hadoop.fs.Path,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:init(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)
org.apache.hadoop.mapred.SkipBadRecords:setReducerMaxSkipGroups(org.apache.hadoop.conf.Configuration,long)
org.apache.hadoop.fs.s3a.S3AFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor:<clinit>()
org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)
org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)
org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)
org.apache.hadoop.mapred.join.OuterJoinRecordReader:createKey()
org.apache.hadoop.yarn.server.timelineservice.storage.flow.FlowScanner:<clinit>()
org.apache.hadoop.hdfs.server.namenode.CheckpointConf:<clinit>()
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:renderText(java.lang.String)
org.apache.hadoop.hdfs.server.namenode.BackupNode:isInSafeMode()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ManagedParentQueue:updateClusterResource(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits)
org.apache.hadoop.hdfs.web.SWebHdfsFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$AppFinishedFinalStateSavedTransition:transition(java.lang.Object,java.lang.Object)
org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)
org.apache.hadoop.fs.cosn.CosNFileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.hdfs.WebHdfsDtFetcher:addDelegationTokens(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials,java.lang.String,java.lang.String)
org.apache.hadoop.fs.impl.prefetch.BufferData:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:setEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.hdfs.StripedDataStreamer:endBlock()
org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<clinit>()
org.apache.hadoop.yarn.server.timelineservice.storage.reader.ApplicationEntityReader:lookupFlowContext(org.apache.hadoop.yarn.server.timelineservice.storage.apptoflow.AppToFlowRowKey,java.lang.String,org.apache.hadoop.conf.Configuration,org.apache.hadoop.hbase.client.Connection)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:close()
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)
org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch:handleContainerExitWithFailure(org.apache.hadoop.yarn.api.records.ContainerId,int,org.apache.hadoop.fs.Path,java.lang.StringBuilder)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getLowRedundancyBlocksCount()
org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin:getNetworkBytesRead()
org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode:setOvercommitTimeOut(long)
org.apache.hadoop.fs.azurebfs.commit.AbfsManifestStoreOperations:mkdirs(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$DisallowSnapshotOp:writeFields(java.io.DataOutputStream,int)
org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:hasCapability(java.lang.String)
org.apache.hadoop.examples.Grep:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock:getCallerUGI()
org.apache.hadoop.tools.dynamometer.workloadgenerator.CreateFileMapper:configureJob(org.apache.hadoop.mapreduce.Job)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler:getMaximumResourceCapability()
org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,short)
org.apache.hadoop.yarn.server.timeline.security.TimelineV1DelegationTokenSecretManagerService:start()
org.apache.hadoop.hdfs.server.datanode.DataNodeMXBean:getNamenodeAddresses()
org.apache.hadoop.io.Text$Comparator:newKey()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.timelineservice.documentstore.DocumentStoreTimelineReaderImpl:<clinit>()
org.apache.hadoop.yarn.service.client.ServiceClient:serviceStart()
org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerOnTimelineStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.nodemanager.nodelabels.ScriptBasedNodeLabelsProvider$NodeLabelScriptRunner:run()
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:removeErasureCodingPolicy(java.lang.String)
org.apache.hadoop.fs.s3a.prefetch.S3AInMemoryInputStream:getS3File()
org.apache.hadoop.fs.shell.Concat:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:processDeleteOnExit()
org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:opt(java.lang.String,java.lang.String)
org.apache.hadoop.fs.s3a.commit.staging.DirectoryStagingCommitter:getCommittedTaskPath(org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceUsage:decAMUsed(org.apache.hadoop.yarn.api.records.Resource)
org.apache.hadoop.hdfs.server.namenode.FSImage:<clinit>()
org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker:<clinit>()
org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.MultiNodeSortingManager:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput:shuffle(org.apache.hadoop.mapreduce.task.reduce.MapHost,java.io.InputStream,long,long,org.apache.hadoop.mapreduce.task.reduce.ShuffleClientMetrics,org.apache.hadoop.mapred.Reporter)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.activities.AllocationActivity:<clinit>()
org.apache.hadoop.yarn.server.timeline.LevelDBCacheTimelineStore:getEntities(java.lang.String,java.lang.Long,java.lang.Long,java.lang.Long,java.lang.String,java.lang.Long,org.apache.hadoop.yarn.server.timeline.NameValuePair,java.util.Collection,java.util.EnumSet,org.apache.hadoop.yarn.server.timeline.TimelineDataManager$CheckAcl)
org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.SWebHdfs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)
org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory:createCommitter(org.apache.hadoop.fs.Path,org.apache.hadoop.mapreduce.TaskAttemptContext)
org.apache.hadoop.hdfs.server.blockmanagement.SlowPeerTracker:<clinit>()
org.apache.hadoop.conf.ConfigurationWithLogging:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:globStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:serviceStop()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithNodeGroup:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet)
org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler:normalizeResourceRequests(java.util.List,java.lang.String)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:delete(org.apache.hadoop.fs.Path,boolean,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.recovery.NullRMStateStore:notifyStoreOperationFailed(java.lang.Exception)
org.apache.hadoop.fs.cosn.CosN:listStatusIterator(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.security.AdminACLsManager:<clinit>()
org.apache.hadoop.fs.shell.CopyCommands$Cp:isMultiThreadNecessary(java.util.LinkedList)
org.apache.hadoop.fs.azure.Wasbs:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:allApplications()
org.apache.hadoop.registry.conf.RegistryConfiguration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)
org.apache.hadoop.util.MachineList:<clinit>()
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:cleanDirectory(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,org.apache.hadoop.hdfs.server.namenode.INodeDirectory,int,int)
org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService:noteFailure(java.lang.Exception)
org.apache.hadoop.hdfs.tools.GetGroups:main(java.lang.String[])
org.apache.hadoop.http.lib.StaticUserWebFilter:<clinit>()
org.apache.hadoop.yarn.service.monitor.ComponentHealthThresholdMonitor:<clinit>()
org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getKeysMetadata(java.lang.String[])
org.apache.hadoop.hdfs.ViewDistributedFileSystem:getEZForPath(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.BackupNode$BackupNodeRpcServer:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)
org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.mapreduce.lib.output.committer.manifest.stages.SaveTaskManifestStage:msync(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.conf.YarnConfiguration:getFloat(java.lang.String,float)
org.apache.hadoop.registry.server.services.RegistryAdminService:zkRead(java.lang.String)
org.apache.hadoop.mapreduce.v2.hs.webapp.HsTasksBlock:render()
org.apache.hadoop.fs.adl.Adl:open(org.apache.hadoop.fs.Path,int)
org.apache.hadoop.fs.http.server.FSOperations$FSListStatusBatch$WrappedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)
org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator:getKeyFieldComparatorOption(org.apache.hadoop.mapreduce.JobContext)
org.apache.hadoop.yarn.sls.SLSRunner$1:startWepApp()
org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:recursePath(org.apache.hadoop.fs.shell.PathData)
org.apache.hadoop.yarn.server.utils.YarnServerBuilderUtils:<clinit>()
org.apache.hadoop.util.NativeLibraryChecker:<clinit>()
org.apache.hadoop.registry.conf.RegistryConfiguration:addResource(java.lang.String)
org.apache.hadoop.mapred.JobConf:addResource(java.io.InputStream,boolean)
org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.security.token.SQLDelegationTokenSecretManagerImpl:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)
org.apache.hadoop.hdfs.server.datanode.ReplicaUnderRecovery:getMetadataInputStream(long)
org.apache.hadoop.hdfs.server.federation.router.RouterWebHdfsMethods:delete(org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.hdfs.web.resources.DelegationParam,org.apache.hadoop.hdfs.web.resources.UserParam,org.apache.hadoop.hdfs.web.resources.DoAsParam,java.lang.String,org.apache.hadoop.hdfs.web.resources.DeleteOpParam,org.apache.hadoop.hdfs.web.resources.RecursiveParam,org.apache.hadoop.hdfs.web.resources.SnapshotNameParam)
org.apache.hadoop.fs.s3a.S3A:getHomeDirectory()
org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.ftp.FTPFileSystem:getTrashRoots(boolean)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:getEnum(java.lang.String,java.lang.Enum)
org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler:start()
org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy:chooseTarget(java.lang.String,int,org.apache.hadoop.net.Node,java.util.List,boolean,java.util.Set,long,org.apache.hadoop.hdfs.protocol.BlockStoragePolicy,java.util.EnumSet,java.util.EnumMap)
org.apache.hadoop.fs.s3a.S3AUtils:flatmapLocatedFiles(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.fs.s3a.S3AUtils$LocatedFileStatusMap)
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:accept(org.apache.hadoop.mapreduce.lib.join.CompositeRecordReader$JoinCollector,org.apache.hadoop.io.WritableComparable)
org.apache.hadoop.fs.s3a.impl.S3AMultipartUploaderBuilder:optDouble(java.lang.String,double)
org.apache.hadoop.tools.DistCpOptionSwitch:addToConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.tools.DistCpOptionSwitch)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:cancelDelegationToken(org.apache.hadoop.yarn.api.protocolrecords.CancelDelegationTokenRequest)
org.apache.hadoop.fs.s3a.S3AUtils:getS3EncryptionKey(java.lang.String,org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)
org.apache.hadoop.mapred.gridmix.Gridmix:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getStrings(java.lang.String,java.lang.String[])
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)
org.apache.hadoop.fs.HarFs:getFileLinkStatus(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.aliyun.oss.OSS:getDelegationTokens(java.lang.String)
org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols:getECBlockGroupStats()
org.apache.hadoop.registry.client.impl.FSRegistryOperationsService:addIfService(java.lang.Object)
org.apache.hadoop.yarn.server.sharedcachemanager.CleanerService:stop()
org.apache.hadoop.fs.azure.Wasbs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])
org.apache.hadoop.hdfs.DFSClient$DFSDataInputStream:unbuffer()
org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path)
org.apache.hadoop.fs.s3a.InconsistentS3ClientFactory:buildAmazonS3Client(com.amazonaws.ClientConfiguration,org.apache.hadoop.fs.s3a.S3ClientFactory$S3ClientCreationParameters)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:logExpireTokens(java.util.Collection)
org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService:close()
org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker:close()
org.apache.hadoop.mapred.SequenceFileInputFilter$FilterRecordReader:createValue()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)
org.apache.hadoop.hdfs.server.federation.router.security.token.ZKDelegationTokenSecretManagerImpl:getDelegationKey(int)
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:getHomeDirectory()
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getPattern(java.lang.String,java.util.regex.Pattern)
org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processArguments(java.util.LinkedList)
org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<clinit>()
org.apache.hadoop.metrics2.lib.MethodMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)
org.apache.hadoop.mapred.nativetask.serde.NullWritableSerializer:serialize(java.lang.Object,java.io.DataOutput)
org.apache.hadoop.yarn.server.nodemanager.webapp.NMController:node()
org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.timelineservice.RMTimelineCollectorManager:<clinit>()
org.apache.hadoop.hdfs.server.federation.router.RouterHttpServer:close()
org.apache.hadoop.yarn.sls.appmaster.StreamAMSimulator:firstStep()
org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.NativePmemMappableBlockLoader:load(long,java.io.FileInputStream,java.io.FileInputStream,java.lang.String,org.apache.hadoop.hdfs.ExtendedBlockId)
org.apache.hadoop.hdfs.server.protocol.BalancerProtocols:setAcl(java.lang.String,java.util.List)
org.apache.hadoop.yarn.sls.SLSRunner$1:createRMDelegatedNodeLabelsUpdater()
org.apache.hadoop.fs.adl.AdlFileSystem:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:hasCapability(java.lang.String)
org.apache.hadoop.hdfs.web.oauth2.ConfCredentialBasedAccessTokenProvider:setConf(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.event.AsyncDispatcher:noteFailure(java.lang.Exception)
org.apache.hadoop.fs.cosn.CosNOutputStream:<clinit>()
org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:setDropBehind(java.lang.Boolean)
org.apache.hadoop.mapred.TaskAttemptContext:getArchiveClassPaths()
org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.InMemoryLevelDBAliasMapClient$LevelDbReader:iterator()
org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer:stop()
org.apache.hadoop.tools.dynamometer.blockgenerator.GenerateBlockImagesDriver:main(java.lang.String[])
org.apache.hadoop.yarn.server.resourcemanager.resource.DynamicResourceConfiguration:getStringCollection(java.lang.String)
org.apache.hadoop.fs.shell.Ls$Lsr:displayError(java.lang.Exception)
org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService:<clinit>()
org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)
org.apache.hadoop.fs.s3a.impl.CreateFileBuilder:must(java.lang.String,int)
org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore:serviceStart()
org.apache.hadoop.mapreduce.lib.join.InnerJoinRecordReader:<init>(int,org.apache.hadoop.conf.Configuration,int,java.lang.Class)
org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<clinit>()
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.PlanQueue:addChildQueue(org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AutoCreatedLeafQueue:assignContainers(org.apache.hadoop.yarn.api.records.Resource,org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.SchedulingMode)
org.apache.hadoop.fs.cosn.CosNFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.nodemanager.WindowsSecureContainerExecutor$ElevatedFileSystem$ElevatedRawLocalFilesystem:fixRelativePart(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.server.resourcemanager.reservation.ReservationSystemUtil:toResources(java.util.Map)
org.apache.hadoop.hdfs.server.namenode.BackupNode:getHttpServerAddress(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.fs.viewfs.ViewFs$2:hasNext()
org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore:serviceInit(org.apache.hadoop.conf.Configuration)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt:<clinit>()
org.apache.hadoop.fs.adl.AdlFileSystem:createNewFile(org.apache.hadoop.fs.Path)
org.apache.hadoop.yarn.client.ClientRMProxy$ClientRMProtocols:updateNodeResource(org.apache.hadoop.yarn.server.api.protocolrecords.UpdateNodeResourceRequest)
org.apache.hadoop.hdfs.web.WebHdfsFileSystem$FsPathOutputStreamRunner$1:hasCapability(java.lang.String)
org.apache.hadoop.hdfs.nfs.conf.NfsConfiguration:getConfResourceAsInputStream(java.lang.String)
org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager:<init>(org.apache.hadoop.conf.Configuration,java.net.URI,org.apache.hadoop.hdfs.server.protocol.NamespaceInfo,java.lang.String)
org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration:addResource(java.io.InputStream,java.lang.String,boolean)
org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem:getReplication(org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.snapshot.DirectorySnapshottableFeature:clear(org.apache.hadoop.hdfs.server.namenode.INode$ReclaimContext,org.apache.hadoop.hdfs.server.namenode.INodeDirectory)
org.apache.hadoop.fs.LocalFileSystem:listStatus(org.apache.hadoop.fs.Path[])
org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices:getFlowRuns(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.yarn.sls.scheduler.SLSFairScheduler:handleContainerUpdates(org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt,org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerUpdates)
org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector:<clinit>()
org.apache.hadoop.hdfs.shortcircuit.DfsClientShm:registerSlot(int,org.apache.hadoop.hdfs.ExtendedBlockId)
org.apache.hadoop.hdfs.ViewDistributedFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org.apache.hadoop.hdfs.server.namenode.NameNodeMXBean:getProvidedCapacity()
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.TokenIdentifier)
org.apache.hadoop.fs.s3a.S3A:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])
org.apache.hadoop.fs.shell.CopyCommands$Merge:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)
org.apache.hadoop.fs.s3a.S3AFileSystem:<clinit>()
